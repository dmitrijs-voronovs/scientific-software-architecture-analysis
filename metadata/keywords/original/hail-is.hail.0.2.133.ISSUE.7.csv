id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/pull/7639:103,Integrability,interface,interface,103,"Remaining:; ### Non-staged code; PR here: https://github.com/hail-is/hail/pull/7880 . Effectively same interface, but with a fix for the override-def-with-optional-arguments issue. ### Remaining question:. edit: From our conversion this afternoon (1/14 @4pm), I think these questions are potentially answered: be consistent, may change. #### Fundamental Types; I've so far settled on copyFromType requiring that the sourceType is of the same type as the type the method is being called on (for instance if this isOfType PBaseStruct, then sourceType isOfType PBaseStruct), but only checks the elements, fields' fundamentalTypes. This may be too loose: I do not fully understand the architecture of fundamentalTypes and so want to clarify this point. #### Top-level non-missing semantics; For non-collection types (those that don't have nested fields), I expect that the function has an address pointing to data. So, for instance, in `val ptyp =PArray(PInt32())`, I expect `ptyp.copyFromType(...)` to get an address pointing to data. The inspiration from this came from RVB, which requires the same (first item in stack can't be null, but the nth-1 item can be null). So it is the responsibility of collection types to check `is*Missing`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7639
https://github.com/hail-is/hail/pull/7641:336,Availability,rollback,rollback,336,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7641
https://github.com/hail-is/hail/pull/7641:503,Availability,down,down,503,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7641
https://github.com/hail-is/hail/pull/7641:872,Availability,error,error,872,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7641
https://github.com/hail-is/hail/pull/7641:336,Deployability,rollback,rollback,336,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7641
https://github.com/hail-is/hail/pull/7641:974,Performance,perform,performance,974,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7641
https://github.com/hail-is/hail/pull/7641:1061,Performance,optimiz,optimizing-innodb-transaction-management,1061,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7641
https://github.com/hail-is/hail/pull/7641:1158,Performance,perform,performance-ro-txn,1158,"You can now say:. ```; async with db.start() as tx:; await tx.just_execute(sql); row = await tx.execute_fetchone(sql, args); ...; ```. Transactions support all the database utility functions. If the transaction context manager exists with an exception, the transaction is rolled back, otherwise it is committed. You also can explicitly rollback or commit the transaction, although it can't be used again after that. I also added an execute_many function. I use this in the front end instead of dropping down to aiomysql to create explicit transactions. Note on internal changes: I no longer use autocommit now that transaction boundaries are explicit. You can start a read only transaction, and I do that by default for execute_and_fetch{one, all}, although maybe those should be renamed select_and_fetch{one, all} to make their read-only nature apparent (MySQL throws an error if you try to modify something in a read-only transaction). This follows mysql best transaction performance recommendations as described here: https://dev.mysql.com/doc/refman/5.6/en/optimizing-innodb-transaction-management.html and https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7641
https://github.com/hail-is/hail/pull/7643:98,Integrability,interface,interface,98,"Sorry, code movement made the diff a bit messy. Basically: support the same search and pagination interface in the batch/jobs API endpoint as we do now in the batch UI page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7643
https://github.com/hail-is/hail/issues/7644:372,Deployability,configurat,configuration,372,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644
https://github.com/hail-is/hail/issues/7644:1182,Deployability,deploy,deployMode,1182,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644
https://github.com/hail-is/hail/issues/7644:372,Modifiability,config,configuration,372,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644
https://github.com/hail-is/hail/issues/7644:1327,Security,hash,hash,1327,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644
https://github.com/hail-is/hail/issues/7644:30,Testability,log,log,30,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644
https://github.com/hail-is/hail/issues/7644:967,Testability,log,logConf,967,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644
https://github.com/hail-is/hail/pull/7645:82,Availability,failure,failure,82,"For attributes, same as /jobs, and state queries: open, closed, running, success, failure, cancelled, and complete. Also added an `open` batch state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7645
https://github.com/hail-is/hail/pull/7646:202,Testability,test,test,202,"This vectorized form yields a 3x+ benefit for MakeNDArray. Up to 120x for the copy function alone (when top bit is missing), 6x for 200k full array (copy function only, no missing values), 3x in python test of MakeNDArray for a 200k full array. I tried to find a reasonable set of tests. A future improvement: check the complementary case, where missingness bytes are initiated to -1 (all 1's in 2's complement), but this requires not using ScalaToRegionValue, since that does not allow non-0 initialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646
https://github.com/hail-is/hail/pull/7646:281,Testability,test,tests,281,"This vectorized form yields a 3x+ benefit for MakeNDArray. Up to 120x for the copy function alone (when top bit is missing), 6x for 200k full array (copy function only, no missing values), 3x in python test of MakeNDArray for a 200k full array. I tried to find a reasonable set of tests. A future improvement: check the complementary case, where missingness bytes are initiated to -1 (all 1's in 2's complement), but this requires not using ScalaToRegionValue, since that does not allow non-0 initialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646
https://github.com/hail-is/hail/issues/7647:235,Availability,error,error,235,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7647
https://github.com/hail-is/hail/issues/7647:656,Availability,Error,ErrorHandling,656,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7647
https://github.com/hail-is/hail/issues/7647:682,Availability,Error,ErrorHandling,682,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7647
https://github.com/hail-is/hail/issues/7647:1791,Availability,Error,Error,1791,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7647
https://github.com/hail-is/hail/issues/7647:411,Deployability,release,release,411,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7647
https://github.com/hail-is/hail/pull/7648:201,Testability,test,tests,201,"`.toSeq` is insufficient because a lazy stream is a `Seq`. `.toArray` forces the data to be realized in memory before the file is closed. We convert back to Seq for nice matching syntax. Dunno why the tests didn't catch this, I guess somehow the stream is valid long enough after the file being closed to work for the tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7648
https://github.com/hail-is/hail/pull/7648:318,Testability,test,tests,318,"`.toSeq` is insufficient because a lazy stream is a `Seq`. `.toArray` forces the data to be realized in memory before the file is closed. We convert back to Seq for nice matching syntax. Dunno why the tests didn't catch this, I guess somehow the stream is valid long enough after the file being closed to work for the tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7648
https://github.com/hail-is/hail/pull/7649:23,Usability,simpl,simplest,23,I thought this was the simplest way of solving this. I could also have checked the container name in Container.run and selected the right semaphore there.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7649
https://github.com/hail-is/hail/pull/7655:23,Testability,test,test,23,"We don't even run this test currently, but we were generating a warning everytime we ran pytest about the fact that it would always be true if we did run it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7655
https://github.com/hail-is/hail/pull/7656:91,Availability,error,error,91,"Fixes #7584. We already require this in Scala, but since we don't require it in python the error message is bad.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7656
https://github.com/hail-is/hail/pull/7656:97,Integrability,message,message,97,"Fixes #7584. We already require this in Scala, but since we don't require it in python the error message is bad.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7656
https://github.com/hail-is/hail/pull/7661:11,Availability,error,error,11,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7660-default-ne0pow1za1v4' '--format=value(name)'; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7661
https://github.com/hail-is/hail/pull/7661:255,Availability,ERROR,ERROR,255,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7660-default-ne0pow1za1v4' '--format=value(name)'; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7661
https://github.com/hail-is/hail/pull/7664:168,Deployability,deploy,deploy,168,"(probably partial) list of things we'll have to do manually once this goes in:. - rename account batch2 => batch (and gsa-key, tokens, etc.); - rebuild database; - had deploy (which includes cleaning up old logs and bucket); - batch service account, roles and role bindings",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7664
https://github.com/hail-is/hail/pull/7664:207,Testability,log,logs,207,"(probably partial) list of things we'll have to do manually once this goes in:. - rename account batch2 => batch (and gsa-key, tokens, etc.); - rebuild database; - had deploy (which includes cleaning up old logs and bucket); - batch service account, roles and role bindings",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7664
https://github.com/hail-is/hail/pull/7665:58,Deployability,deploy,deployment,58,https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7665
https://github.com/hail-is/hail/pull/7666:11,Availability,error,error,11,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7663-default-ccnvti4s750b' '--format=value(name)' --project hail-vdc; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; Deleted [https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-o2tmf].; ERROR: (gcloud.compute.instances.delete) Could not fetch resource:; - The resource 'projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-8z4dd' was not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7666
https://github.com/hail-is/hail/pull/7666:421,Availability,ERROR,ERROR,421,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7663-default-ccnvti4s750b' '--format=value(name)' --project hail-vdc; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; Deleted [https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-o2tmf].; ERROR: (gcloud.compute.instances.delete) Could not fetch resource:; - The resource 'projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-8z4dd' was not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7666
https://github.com/hail-is/hail/pull/7672:171,Availability,failure,failure,171,"Submitting a pipeline now looks like:. ```; $ hail pipeline.py; Submitted batch 120, see https://batch2.hail.is/batches/120; Waiting for batch 120...; Batch 120 complete: failure; ```. FYI @konradjk Pipeline.run now passes through kwargs to the backend. BatchBackend supports two new args: wait (default True) to wait for the pipeline to finish, and open (default False) to open the batch URL in the browser. It no longer attempts to print the failed jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7672
https://github.com/hail-is/hail/pull/7672:13,Deployability,pipeline,pipeline,13,"Submitting a pipeline now looks like:. ```; $ hail pipeline.py; Submitted batch 120, see https://batch2.hail.is/batches/120; Waiting for batch 120...; Batch 120 complete: failure; ```. FYI @konradjk Pipeline.run now passes through kwargs to the backend. BatchBackend supports two new args: wait (default True) to wait for the pipeline to finish, and open (default False) to open the batch URL in the browser. It no longer attempts to print the failed jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7672
https://github.com/hail-is/hail/pull/7672:51,Deployability,pipeline,pipeline,51,"Submitting a pipeline now looks like:. ```; $ hail pipeline.py; Submitted batch 120, see https://batch2.hail.is/batches/120; Waiting for batch 120...; Batch 120 complete: failure; ```. FYI @konradjk Pipeline.run now passes through kwargs to the backend. BatchBackend supports two new args: wait (default True) to wait for the pipeline to finish, and open (default False) to open the batch URL in the browser. It no longer attempts to print the failed jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7672
https://github.com/hail-is/hail/pull/7672:199,Deployability,Pipeline,Pipeline,199,"Submitting a pipeline now looks like:. ```; $ hail pipeline.py; Submitted batch 120, see https://batch2.hail.is/batches/120; Waiting for batch 120...; Batch 120 complete: failure; ```. FYI @konradjk Pipeline.run now passes through kwargs to the backend. BatchBackend supports two new args: wait (default True) to wait for the pipeline to finish, and open (default False) to open the batch URL in the browser. It no longer attempts to print the failed jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7672
https://github.com/hail-is/hail/pull/7672:326,Deployability,pipeline,pipeline,326,"Submitting a pipeline now looks like:. ```; $ hail pipeline.py; Submitted batch 120, see https://batch2.hail.is/batches/120; Waiting for batch 120...; Batch 120 complete: failure; ```. FYI @konradjk Pipeline.run now passes through kwargs to the backend. BatchBackend supports two new args: wait (default True) to wait for the pipeline to finish, and open (default False) to open the batch URL in the browser. It no longer attempts to print the failed jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7672
https://github.com/hail-is/hail/pull/7673:18,Energy Efficiency,allocate,allocate,18,"1) Removes region.allocate called on PContainer instances, in favor of Container's allocate instance method.; 2) Introduces a copyeFrom. Goal was to remove contentsByteSize, because as used represents an allocation/copy implementation detail that does not need to live on a public interface. Part of upcoming introduction of PContainer trait and complimentary PCanonicalArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7673
https://github.com/hail-is/hail/pull/7673:83,Energy Efficiency,allocate,allocate,83,"1) Removes region.allocate called on PContainer instances, in favor of Container's allocate instance method.; 2) Introduces a copyeFrom. Goal was to remove contentsByteSize, because as used represents an allocation/copy implementation detail that does not need to live on a public interface. Part of upcoming introduction of PContainer trait and complimentary PCanonicalArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7673
https://github.com/hail-is/hail/pull/7673:281,Integrability,interface,interface,281,"1) Removes region.allocate called on PContainer instances, in favor of Container's allocate instance method.; 2) Introduces a copyeFrom. Goal was to remove contentsByteSize, because as used represents an allocation/copy implementation detail that does not need to live on a public interface. Part of upcoming introduction of PContainer trait and complimentary PCanonicalArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7673
https://github.com/hail-is/hail/pull/7677:140,Modifiability,variab,variable,140,"part of the work for getting TableMapPartitions working. you can use `Ref` or `In` nodes that have type `TStream` as emittable streams. the variable/argument must be bound to a `Iterator[RegionValue]`, which will be iterated over and emitted as part of the stream. this stream can then be composed like other streams. there's no fancy region management going on in this PR since i'm going to handle that by deep copying in TableMapPartitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7677
https://github.com/hail-is/hail/pull/7682:74,Testability,test,tests,74,"This PR enables calling QR decomposition via LAPACK. It's passing all the tests, and I've made some initial cleanup passes, but there's probably more that could be done to simplify this. . I referenced the numpy QR implementation when doing this, it may be helpful to understand the goal outside of the dealing with all the overhead of reading staged code: https://github.com/numpy/numpy/blob/0aeab48b9e914d1dc7041b7e3f3b7e575ffcbbed/numpy/linalg/linalg.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7682
https://github.com/hail-is/hail/pull/7682:172,Usability,simpl,simplify,172,"This PR enables calling QR decomposition via LAPACK. It's passing all the tests, and I've made some initial cleanup passes, but there's probably more that could be done to simplify this. . I referenced the numpy QR implementation when doing this, it may be helpful to understand the goal outside of the dealing with all the overhead of reading staged code: https://github.com/numpy/numpy/blob/0aeab48b9e914d1dc7041b7e3f3b7e575ffcbbed/numpy/linalg/linalg.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7682
https://github.com/hail-is/hail/pull/7683:70,Modifiability,config,config,70,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683
https://github.com/hail-is/hail/pull/7683:272,Modifiability,config,config,272,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683
https://github.com/hail-is/hail/pull/7683:553,Modifiability,config,config,553,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683
https://github.com/hail-is/hail/pull/7683:336,Security,access,access,336,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683
https://github.com/hail-is/hail/pull/7683:394,Testability,test,test,394,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683
https://github.com/hail-is/hail/pull/7683:427,Testability,test,tests,427,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683
https://github.com/hail-is/hail/pull/7683:530,Testability,test,tests,530,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683
https://github.com/hail-is/hail/pull/7688:200,Usability,undo,undocumented,200,"This PR generates documentation for all current ndarray functionality. Wondering if to coincide with this we should also remove the `_` from `hl._nd`. I mostly had it there so people wouldn't poke at undocumented functionality, but this seems to work ok now. . Resolves #7531",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7688
https://github.com/hail-is/hail/pull/7694:2040,Deployability,deploy,deploy,2040,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1243,Energy Efficiency,schedul,scheduler,1243,"ete user.; - Bit of database reorg, user_id is gone, users table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1737,Security,access,access,1737,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:41,Testability,log,logic,41,"Building off your previous user creation logic PR. Summary of changes:; - Add auth/users admin page with list of users and automated add, delete user.; - Bit of database reorg, user_id is gone, users table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1141,Testability,log,logic,1141,"ete user.; - Bit of database reorg, user_id is gone, users table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1224,Testability,log,logic,1224,"ete user.; - Bit of database reorg, user_id is gone, users table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1646,Testability,test,testing,1646,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1677,Testability,test,tests,1677,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1711,Testability,test,test,1711,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1853,Testability,test,test,1853,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1934,Testability,test,tested,1934,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:1956,Testability,log,logic,1956,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:2092,Testability,test,test,2092,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/pull/7694:2123,Testability,log,logic,2123,"rs table is now auth.users.; - Added state to users table, `active` for active users. Adding (deleting) users just sets state to `adding` (`deleting`); - Added a new service (doesn't serve requests) auth-driver that watches the database and processes adding, deleting users. Only one replica, runs every ~60s; - Don't actually delete users, just mark them deleted. This is so we don't lose billing information for users we're deleting. This will need more thought once we understand what the financial record keeping constraints are. Maybe we purge users after 90 days? Maybe keep them forever?; - Added Auth > Users header link; - gsa-key now has a single file, `key.json` instead of `privateKeyData`.; - Added cleanup_auth_tables dev-only build step; - batch account not actually needed, removed (but we keep around batch-gsa-key). I have two more code sharing PRs after this goes in:; - Move GoogleClient to gear and unify with the existing logic in batch (which is a bit disorganized); - EventHandler is basically the same logic in the batch scheduler. Again, move to gear and use there. One last change that might be nice but I didn't bother with: auth could send auth-driver a notification when the database changes so it can process requests immediately. (We do this in batch, for example.) Maybe we if we expect to be adding more users. Obviously, we'll have to purge the old user resources and re-add the users once this goes in. A note on testing: I'm reluctant to give tests the privileges necessary to test this (basically full access to the google project and the cluster). I think I'm inclined to chalk this up a ""infrastructure"" and plan to test it on a separate staging k8s cluster for infrastructure changes. For now, I tested almost all the logic with a slightly tweaked version to get around the permissions issues with dev deploy and it looks good. The only thing I couldn't test was the database creation logic since that's virtualized inside a single database in dev namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694
https://github.com/hail-is/hail/issues/7701:478,Energy Efficiency,allocate,allocate,478,"Found in EArray _buildSkip. Will require PTypes to be passed through _buildSkip, and for there to be a PType chosen on fields that are not found on the PType passed to _buildInplaceDecoder. I could see this either happening via an analog to _decodePType, (_encodePType), aka a canonical representation, or by explicitly passing through PTypes to Etypes. There are also more assumptions made about PType representations in EArray, EBaseStruct than I would like (ex: `mbytes := r.allocate(const(1), nMissing.toL),` in EArray _buildSkip)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7701
https://github.com/hail-is/hail/pull/7702:637,Energy Efficiency,allocate,allocated,637,"Remaining is that found in Etypes, in the _buildSkip function. This is slightly tricky, because there is a place in the code where there is no corresponding PType, and the solution to fix that is a bit involved, or if straightforward, beyond my current understanding of ETypes. I made an issue here: https://github.com/hail-is/hail/issues/7701. Stacked on https://github.com/hail-is/hail/pull/7687. edit: I removed the ETypes issue, by creating a packBitsToBytes function on UnsafeUtils. We may not want this change however, because I think array packing may needs to be the same as the array implementation (I think readBytes fills the allocated memory with the InputBuffer's encoded missingness data, which needs same number of bytes as what is encoded), in which case that coupling becomes less clear if the utility function is on UnsafeUtils. I could move it back to PContainer, or may _buildSkip take a ptype. . There are other places where (n + 7) >>> 3 are used, so this seems pretty general, hence UnsafeUtils (where we have some other bitwise ops, happy to move elsewhere). PTuple is one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7702
https://github.com/hail-is/hail/pull/7702:776,Modifiability,coupling,coupling,776,"Remaining is that found in Etypes, in the _buildSkip function. This is slightly tricky, because there is a place in the code where there is no corresponding PType, and the solution to fix that is a bit involved, or if straightforward, beyond my current understanding of ETypes. I made an issue here: https://github.com/hail-is/hail/issues/7701. Stacked on https://github.com/hail-is/hail/pull/7687. edit: I removed the ETypes issue, by creating a packBitsToBytes function on UnsafeUtils. We may not want this change however, because I think array packing may needs to be the same as the array implementation (I think readBytes fills the allocated memory with the InputBuffer's encoded missingness data, which needs same number of bytes as what is encoded), in which case that coupling becomes less clear if the utility function is on UnsafeUtils. I could move it back to PContainer, or may _buildSkip take a ptype. . There are other places where (n + 7) >>> 3 are used, so this seems pretty general, hence UnsafeUtils (where we have some other bitwise ops, happy to move elsewhere). PTuple is one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7702
https://github.com/hail-is/hail/pull/7702:475,Safety,Unsafe,UnsafeUtils,475,"Remaining is that found in Etypes, in the _buildSkip function. This is slightly tricky, because there is a place in the code where there is no corresponding PType, and the solution to fix that is a bit involved, or if straightforward, beyond my current understanding of ETypes. I made an issue here: https://github.com/hail-is/hail/issues/7701. Stacked on https://github.com/hail-is/hail/pull/7687. edit: I removed the ETypes issue, by creating a packBitsToBytes function on UnsafeUtils. We may not want this change however, because I think array packing may needs to be the same as the array implementation (I think readBytes fills the allocated memory with the InputBuffer's encoded missingness data, which needs same number of bytes as what is encoded), in which case that coupling becomes less clear if the utility function is on UnsafeUtils. I could move it back to PContainer, or may _buildSkip take a ptype. . There are other places where (n + 7) >>> 3 are used, so this seems pretty general, hence UnsafeUtils (where we have some other bitwise ops, happy to move elsewhere). PTuple is one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7702
https://github.com/hail-is/hail/pull/7702:834,Safety,Unsafe,UnsafeUtils,834,"Remaining is that found in Etypes, in the _buildSkip function. This is slightly tricky, because there is a place in the code where there is no corresponding PType, and the solution to fix that is a bit involved, or if straightforward, beyond my current understanding of ETypes. I made an issue here: https://github.com/hail-is/hail/issues/7701. Stacked on https://github.com/hail-is/hail/pull/7687. edit: I removed the ETypes issue, by creating a packBitsToBytes function on UnsafeUtils. We may not want this change however, because I think array packing may needs to be the same as the array implementation (I think readBytes fills the allocated memory with the InputBuffer's encoded missingness data, which needs same number of bytes as what is encoded), in which case that coupling becomes less clear if the utility function is on UnsafeUtils. I could move it back to PContainer, or may _buildSkip take a ptype. . There are other places where (n + 7) >>> 3 are used, so this seems pretty general, hence UnsafeUtils (where we have some other bitwise ops, happy to move elsewhere). PTuple is one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7702
https://github.com/hail-is/hail/pull/7702:1006,Safety,Unsafe,UnsafeUtils,1006,"Remaining is that found in Etypes, in the _buildSkip function. This is slightly tricky, because there is a place in the code where there is no corresponding PType, and the solution to fix that is a bit involved, or if straightforward, beyond my current understanding of ETypes. I made an issue here: https://github.com/hail-is/hail/issues/7701. Stacked on https://github.com/hail-is/hail/pull/7687. edit: I removed the ETypes issue, by creating a packBitsToBytes function on UnsafeUtils. We may not want this change however, because I think array packing may needs to be the same as the array implementation (I think readBytes fills the allocated memory with the InputBuffer's encoded missingness data, which needs same number of bytes as what is encoded), in which case that coupling becomes less clear if the utility function is on UnsafeUtils. I could move it back to PContainer, or may _buildSkip take a ptype. . There are other places where (n + 7) >>> 3 are used, so this seems pretty general, hence UnsafeUtils (where we have some other bitwise ops, happy to move elsewhere). PTuple is one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7702
https://github.com/hail-is/hail/pull/7702:798,Usability,clear,clear,798,"Remaining is that found in Etypes, in the _buildSkip function. This is slightly tricky, because there is a place in the code where there is no corresponding PType, and the solution to fix that is a bit involved, or if straightforward, beyond my current understanding of ETypes. I made an issue here: https://github.com/hail-is/hail/issues/7701. Stacked on https://github.com/hail-is/hail/pull/7687. edit: I removed the ETypes issue, by creating a packBitsToBytes function on UnsafeUtils. We may not want this change however, because I think array packing may needs to be the same as the array implementation (I think readBytes fills the allocated memory with the InputBuffer's encoded missingness data, which needs same number of bytes as what is encoded), in which case that coupling becomes less clear if the utility function is on UnsafeUtils. I could move it back to PContainer, or may _buildSkip take a ptype. . There are other places where (n + 7) >>> 3 are used, so this seems pretty general, hence UnsafeUtils (where we have some other bitwise ops, happy to move elsewhere). PTuple is one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7702
https://github.com/hail-is/hail/pull/7706:93,Modifiability,config,config,93,Unfortunately this is hard to test in the current setup because the tests never get the root config file (which has no database set).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7706
https://github.com/hail-is/hail/pull/7706:30,Testability,test,test,30,Unfortunately this is hard to test in the current setup because the tests never get the root config file (which has no database set).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7706
https://github.com/hail-is/hail/pull/7706:68,Testability,test,tests,68,Unfortunately this is hard to test in the current setup because the tests never get the root config file (which has no database set).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7706
https://github.com/hail-is/hail/pull/7713:89,Deployability,deploy,deploy,89,"Added simple UI for billing projects, create, add and remove users. Hand-tested with dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7713
https://github.com/hail-is/hail/pull/7713:73,Testability,test,tested,73,"Added simple UI for billing projects, create, add and remove users. Hand-tested with dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7713
https://github.com/hail-is/hail/pull/7713:6,Usability,simpl,simple,6,"Added simple UI for billing projects, create, add and remove users. Hand-tested with dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7713
https://github.com/hail-is/hail/pull/7715:28,Availability,error,errors,28,"Adds retry for specific 500 errors:. ```; aiodocker.exceptions.DockerError: DockerError(500, 'error creating overlay mount to /var/lib/docker/overlay2/545a1337742e0292d9ed197b06fe900146c85ab06e468843cd0461c3f34df50d/merged: device or resource busy'; ```. ```; aiodocker.exceptions.DockerError: DockerError(500, 'Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7715
https://github.com/hail-is/hail/pull/7715:94,Availability,error,error,94,"Adds retry for specific 500 errors:. ```; aiodocker.exceptions.DockerError: DockerError(500, 'error creating overlay mount to /var/lib/docker/overlay2/545a1337742e0292d9ed197b06fe900146c85ab06e468843cd0461c3f34df50d/merged: device or resource busy'; ```. ```; aiodocker.exceptions.DockerError: DockerError(500, 'Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7715
https://github.com/hail-is/hail/pull/7715:414,Safety,Timeout,Timeout,414,"Adds retry for specific 500 errors:. ```; aiodocker.exceptions.DockerError: DockerError(500, 'error creating overlay mount to /var/lib/docker/overlay2/545a1337742e0292d9ed197b06fe900146c85ab06e468843cd0461c3f34df50d/merged: device or resource busy'; ```. ```; aiodocker.exceptions.DockerError: DockerError(500, 'Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7715
https://github.com/hail-is/hail/pull/7716:59,Testability,test,test,59,"This works, I'm still cleaning it up though. PRing for the test suite to see if I broke anything.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7716
https://github.com/hail-is/hail/pull/7717:101,Availability,error,error,101,"Ran hit this with a typo in --num-workers. We just exited, instead of calling google or returning an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7717
https://github.com/hail-is/hail/pull/7719:1729,Availability,redundant,redundant,1729,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:1511,Deployability,pipeline,pipeline,1511,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:978,Performance,optimiz,optimization,978,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:1302,Performance,Optimiz,Optimize,1302,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:1818,Performance,optimiz,optimizations,1818,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:1964,Performance,optimiz,optimization,1964,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:1729,Safety,redund,redundant,1729,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:1324,Usability,Simpl,Simplify,1324,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7719:1785,Usability,Simpl,Simplify,1785,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7719
https://github.com/hail-is/hail/pull/7720:13,Availability,redundant,redundant,13,This creates redundant bindings that interfere with our ability to apply certain simplification rules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7720
https://github.com/hail-is/hail/pull/7720:13,Safety,redund,redundant,13,This creates redundant bindings that interfere with our ability to apply certain simplification rules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7720
https://github.com/hail-is/hail/pull/7720:81,Usability,simpl,simplification,81,This creates redundant bindings that interfere with our ability to apply certain simplification rules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7720
https://github.com/hail-is/hail/pull/7722:124,Deployability,deploy,deploying,124,"If the yaml sets the step to `runIfRequired: true`, this change should only run the step if specifically asked for when dev deploying.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7722
https://github.com/hail-is/hail/pull/7723:31,Testability,test,test,31,"This is for the batch callback test, so I can make test_batch a direct pod.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7723
https://github.com/hail-is/hail/pull/7724:6,Testability,benchmark,benchmarks,6,Added benchmarks for NDArray addition and multiplication. There are 2 for multiplication because when I add something to use BLAS instead of just naive multiply it will improve the one that is using float64s.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7724
https://github.com/hail-is/hail/pull/7726:138,Availability,down,downstream,138,"When the number of fields in `x` is really huge, this optimization creates huge chains of `Let` bindings which cause stack size issues in downstream IR analysis passes. (we can remove this cap once our passes will no longer run into stack size issues on very deep IRs.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7726
https://github.com/hail-is/hail/pull/7726:54,Performance,optimiz,optimization,54,"When the number of fields in `x` is really huge, this optimization creates huge chains of `Let` bindings which cause stack size issues in downstream IR analysis passes. (we can remove this cap once our passes will no longer run into stack size issues on very deep IRs.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7726
https://github.com/hail-is/hail/pull/7728:2,Performance,concurren,concurrent,2,4 concurrent builds for 10 engineers with a couple branches each just isn't tenable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7728
https://github.com/hail-is/hail/pull/7729:811,Performance,perform,performance,811,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729
https://github.com/hail-is/hail/pull/7729:1052,Safety,Safe,SafeRow,1052,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729
https://github.com/hail-is/hail/pull/7729:526,Testability,test,test,526,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729
https://github.com/hail-is/hail/pull/7729:519,Usability,simpl,simple,519,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729
https://github.com/hail-is/hail/pull/7729:599,Usability,simpl,simple,599,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729
https://github.com/hail-is/hail/pull/7729:887,Usability,clear,clearing,887,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729
https://github.com/hail-is/hail/pull/7729:992,Usability,simpl,simple,992,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729
https://github.com/hail-is/hail/pull/7733:218,Performance,perform,performant,218,"@tpoterba per your comments have largely left the PStruct varargs constructor in use in the PCanonicalStruct implementation. I think it would be simpler to just use the normal IndexedSeq constructor, and slightly more performant, and if you're interested in that could issue a separate PR. The only change in the implementation of PCanonicalStruct from the master version of PStruct is that I pass through requiredeness in all construction operations. Previously a few, like rename would not do this. Notably this only happened when they used the more complex varargs constructor, and seemed like a bug. The empty constructor was removed because it wasn't necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7733
https://github.com/hail-is/hail/pull/7733:145,Usability,simpl,simpler,145,"@tpoterba per your comments have largely left the PStruct varargs constructor in use in the PCanonicalStruct implementation. I think it would be simpler to just use the normal IndexedSeq constructor, and slightly more performant, and if you're interested in that could issue a separate PR. The only change in the implementation of PCanonicalStruct from the master version of PStruct is that I pass through requiredeness in all construction operations. Previously a few, like rename would not do this. Notably this only happened when they used the more complex varargs constructor, and seemed like a bug. The empty constructor was removed because it wasn't necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7733
https://github.com/hail-is/hail/pull/7736:145,Integrability,interface,interface,145,- emit streams for `If` and `ReadPartition` IR. ; - delete `Emit.emitOldArrayIterator()` because all streamable IR's can be emitted with the new interface. before this is merged we probably want to make sure there are no significant performance regressions or anything of that sort,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7736
https://github.com/hail-is/hail/pull/7736:233,Performance,perform,performance,233,- emit streams for `If` and `ReadPartition` IR. ; - delete `Emit.emitOldArrayIterator()` because all streamable IR's can be emitted with the new interface. before this is merged we probably want to make sure there are no significant performance regressions or anything of that sort,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7736
https://github.com/hail-is/hail/pull/7738:15,Deployability,release,release,15,Change log for release 0.2.29. Currently I have the date set for tomorrow since i didn't know if it would make it in today. Could always change it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7738
https://github.com/hail-is/hail/pull/7738:7,Testability,log,log,7,Change log for release 0.2.29. Currently I have the date set for tomorrow since i didn't know if it would make it in today. Could always change it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7738
https://github.com/hail-is/hail/pull/7739:50,Deployability,configurat,configurations,50,This means people who don't use hailctl get these configurations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7739
https://github.com/hail-is/hail/pull/7739:50,Modifiability,config,configurations,50,This means people who don't use hailctl get these configurations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7739
https://github.com/hail-is/hail/pull/7740:245,Deployability,pipeline,pipelines,245,"This helps with method size if we're doing a lot of ; ```; Let(""foo"", <value>,; Let(""bar"", <value2>,; … )); ```; where the size of the code generated by the values are potentially quite large. (I think doing this will unblock one of @konradjk's pipelines for now.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7740
https://github.com/hail-is/hail/issues/7746:3,Safety,avoid,avoid,3,"To avoid casting to PStruct (`representation.asInstanceOf[PStruct]`), which requires assumptions about the implementation of the complex type, and which thereby is inconsistent with the use of the abstract class (say PLocus) in the pattern match/argument. . Comes up in:. https://github.com/hail-is/hail/blob/e363f372c5418e0a0cd81ca0360677eaed5f8156/hail/src/main/scala/is/hail/expr/ir/BinarySearch.scala; https://github.com/hail-is/hail/blob/7c2f5fe6c0d066ee49eefc3729749012b66556f5/hail/src/main/scala/is/hail/annotations/UnsafeRow.scala; https://github.com/hail-is/hail/blob/8b2e4d202beaeb6b65d07dabc95355bb71634ba3/hail/src/main/scala/is/hail/expr/types/encoded/EBaseStruct.scala; https://github.com/hail-is/hail/blob/d25d49b34312bcf2180b316349dc723b3e094b4b/hail/src/main/scala/is/hail/expr/ir/functions/LocusFunctions.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7746
https://github.com/hail-is/hail/issues/7746:524,Safety,Unsafe,UnsafeRow,524,"To avoid casting to PStruct (`representation.asInstanceOf[PStruct]`), which requires assumptions about the implementation of the complex type, and which thereby is inconsistent with the use of the abstract class (say PLocus) in the pattern match/argument. . Comes up in:. https://github.com/hail-is/hail/blob/e363f372c5418e0a0cd81ca0360677eaed5f8156/hail/src/main/scala/is/hail/expr/ir/BinarySearch.scala; https://github.com/hail-is/hail/blob/7c2f5fe6c0d066ee49eefc3729749012b66556f5/hail/src/main/scala/is/hail/annotations/UnsafeRow.scala; https://github.com/hail-is/hail/blob/8b2e4d202beaeb6b65d07dabc95355bb71634ba3/hail/src/main/scala/is/hail/expr/types/encoded/EBaseStruct.scala; https://github.com/hail-is/hail/blob/d25d49b34312bcf2180b316349dc723b3e094b4b/hail/src/main/scala/is/hail/expr/ir/functions/LocusFunctions.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7746
https://github.com/hail-is/hail/pull/7750:483,Modifiability,extend,extend,483,"@tpoterba there is one minor implementation difference, wanted to check if you considered it too far afield: removed `PStringOptional` and `PStringRequired`. These are used in only 3 classes, and *Optional/Required classes are inconsistently used in the codebase anyways. By removing them we have fewer legacy constructors hanging off PArray.; * Furthermore, by adding the final class modifier to PCanonicalString, I'm not sure we can implement a case object in the same way (cannot extend PCanonicalString, which means places that expect a PType, like `StagedBlockLinkedListSuite.scala:159`, won't type check, if we simply make a PStringOptional with a constructor that calls `PCanonicalStruct()`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7750
https://github.com/hail-is/hail/pull/7750:617,Usability,simpl,simply,617,"@tpoterba there is one minor implementation difference, wanted to check if you considered it too far afield: removed `PStringOptional` and `PStringRequired`. These are used in only 3 classes, and *Optional/Required classes are inconsistently used in the codebase anyways. By removing them we have fewer legacy constructors hanging off PArray.; * Furthermore, by adding the final class modifier to PCanonicalString, I'm not sure we can implement a case object in the same way (cannot extend PCanonicalString, which means places that expect a PType, like `StagedBlockLinkedListSuite.scala:159`, won't type check, if we simply make a PStringOptional with a constructor that calls `PCanonicalStruct()`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7750
https://github.com/hail-is/hail/pull/7751:41,Modifiability,rewrite,rewrite,41,"Uses method in line with PStruct, PTuple rewrite to avoid unimplemented def, at the cost of more lines of code.; * lazy in line with PStruct and PTuple fundamentalTypes, and also it doesn't seem right to have more than one fundamental type for a single instance of the class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7751
https://github.com/hail-is/hail/pull/7751:52,Safety,avoid,avoid,52,"Uses method in line with PStruct, PTuple rewrite to avoid unimplemented def, at the cost of more lines of code.; * lazy in line with PStruct and PTuple fundamentalTypes, and also it doesn't seem right to have more than one fundamental type for a single instance of the class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7751
https://github.com/hail-is/hail/pull/7752:227,Integrability,depend,depends,227,"@tpoterba in line with your recommendation (https://github.com/hail-is/hail/pull/7712#discussion_r358433830 I put unsafeOrdering) I put unsafeOrdering on PSet, PDict. However I couldn't move the other constructor, because that depends on arrayRep. I can move this constructor from PArrayBackedContainer to PCanonicalSet and PCanonicalDict, or make a protected arrayRep on PSet, PDict and move the constructor there (which I don't think we want, since that implies that all implementations will have an array representation, which I don't think we know).; * That being said, I don't like having 2 constructors for the same method in 2 different places, makes it much harder to understand, to me. I think I would prefer putting unsafeOrdering both on the canonical implementation, or leave them on PArrayBackedContainer (since that is the canonical implementation for the subset of functions implemented there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7752
https://github.com/hail-is/hail/pull/7752:114,Safety,unsafe,unsafeOrdering,114,"@tpoterba in line with your recommendation (https://github.com/hail-is/hail/pull/7712#discussion_r358433830 I put unsafeOrdering) I put unsafeOrdering on PSet, PDict. However I couldn't move the other constructor, because that depends on arrayRep. I can move this constructor from PArrayBackedContainer to PCanonicalSet and PCanonicalDict, or make a protected arrayRep on PSet, PDict and move the constructor there (which I don't think we want, since that implies that all implementations will have an array representation, which I don't think we know).; * That being said, I don't like having 2 constructors for the same method in 2 different places, makes it much harder to understand, to me. I think I would prefer putting unsafeOrdering both on the canonical implementation, or leave them on PArrayBackedContainer (since that is the canonical implementation for the subset of functions implemented there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7752
https://github.com/hail-is/hail/pull/7752:136,Safety,unsafe,unsafeOrdering,136,"@tpoterba in line with your recommendation (https://github.com/hail-is/hail/pull/7712#discussion_r358433830 I put unsafeOrdering) I put unsafeOrdering on PSet, PDict. However I couldn't move the other constructor, because that depends on arrayRep. I can move this constructor from PArrayBackedContainer to PCanonicalSet and PCanonicalDict, or make a protected arrayRep on PSet, PDict and move the constructor there (which I don't think we want, since that implies that all implementations will have an array representation, which I don't think we know).; * That being said, I don't like having 2 constructors for the same method in 2 different places, makes it much harder to understand, to me. I think I would prefer putting unsafeOrdering both on the canonical implementation, or leave them on PArrayBackedContainer (since that is the canonical implementation for the subset of functions implemented there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7752
https://github.com/hail-is/hail/pull/7752:726,Safety,unsafe,unsafeOrdering,726,"@tpoterba in line with your recommendation (https://github.com/hail-is/hail/pull/7712#discussion_r358433830 I put unsafeOrdering) I put unsafeOrdering on PSet, PDict. However I couldn't move the other constructor, because that depends on arrayRep. I can move this constructor from PArrayBackedContainer to PCanonicalSet and PCanonicalDict, or make a protected arrayRep on PSet, PDict and move the constructor there (which I don't think we want, since that implies that all implementations will have an array representation, which I don't think we know).; * That being said, I don't like having 2 constructors for the same method in 2 different places, makes it much harder to understand, to me. I think I would prefer putting unsafeOrdering both on the canonical implementation, or leave them on PArrayBackedContainer (since that is the canonical implementation for the subset of functions implemented there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7752
https://github.com/hail-is/hail/pull/7755:151,Usability,clear,clear,151,"Closes out the first reorg work. There are a bunch of utility methods on PBinary, but these are non-trivial to remove. There are places where it isn't clear to me that we have a PBinary instance for instance. For example,. ```scala; srvb.addString(""hello""),; ```. Would need a PString (and the fundamentalType of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7755
https://github.com/hail-is/hail/issues/7756:44,Testability,log,login,44,"I can't use `hailctl batch list`; ```; Last login: Wed Dec 18 15:05:20 on ttys001; # hailctl batch list; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 103, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/batch/cli.py"", line 97, in main; jmp[args.module].main(args, pass_through_args, client); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/batch/list_batches.py"", line 39, in main; batch_list = client.list_batches(success=success, complete=complete, attributes=attributes); TypeError: list_batches() got an unexpected keyword argument 'success'; # ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7756
https://github.com/hail-is/hail/issues/7758:70,Testability,log,log,70,For example:; https://ci.hail.is/batches/756/jobs/7. used to have the log:; ```; +python3 scale_test.py dbuf-0.dbuf 10 40000 1000; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7758
https://github.com/hail-is/hail/pull/7759:330,Deployability,deploy,deploy,330,"For some reason, all output was suppressed so I wasn't even seeing my debugging output. 502 is labelled ""transient"" but it's not always. I still don't know what's wrong, but something like this would have prevented me from running in circles trying to figure out what the hell changed (this works in a PR, it's only broken in dev deploy, don't know why yet).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7759
https://github.com/hail-is/hail/pull/7760:373,Performance,cache,cached,373,"I had to recreate from #7593 because I force-pushed after are rebase. cc: @cseed . Unfortunately, you're the only one around to review John. There's so many issues with this at current, but I think it would be better to get something in so I can actually start making forward progress towards something better. At a very practical level, I want various docker images to be cached instead of constantly rebuilding them as I try to develop this further.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7760
https://github.com/hail-is/hail/pull/7766:4,Testability,test,test,4,see test for example.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7766
https://github.com/hail-is/hail/pull/7767:4,Integrability,message,messages,4,Two messages fixed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7767
https://github.com/hail-is/hail/issues/7769:257,Availability,Error,Error,257,```; /usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py:192: Warning: Converting column '' from VARCHAR to TEXT; await self._do_get_result(); ```. These mysterious two lines are printed to my batch log when I run a big test (1000 jobs). The [Server Error Reference](https://dev.mysql.com/doc/refman/8.0/en/server-error-reference.html) indicates this is error 1246. A [MySQL bug](https://bugs.mysql.com/bug.php?id=26090) suggests that ER_AUTO_CONVERT might be raised when one attempts to insert unicode into a varchar column. I don't understand why our column has the empty string as a name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7769
https://github.com/hail-is/hail/issues/7769:321,Availability,error,error-reference,321,```; /usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py:192: Warning: Converting column '' from VARCHAR to TEXT; await self._do_get_result(); ```. These mysterious two lines are printed to my batch log when I run a big test (1000 jobs). The [Server Error Reference](https://dev.mysql.com/doc/refman/8.0/en/server-error-reference.html) indicates this is error 1246. A [MySQL bug](https://bugs.mysql.com/bug.php?id=26090) suggests that ER_AUTO_CONVERT might be raised when one attempts to insert unicode into a varchar column. I don't understand why our column has the empty string as a name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7769
https://github.com/hail-is/hail/issues/7769:361,Availability,error,error,361,```; /usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py:192: Warning: Converting column '' from VARCHAR to TEXT; await self._do_get_result(); ```. These mysterious two lines are printed to my batch log when I run a big test (1000 jobs). The [Server Error Reference](https://dev.mysql.com/doc/refman/8.0/en/server-error-reference.html) indicates this is error 1246. A [MySQL bug](https://bugs.mysql.com/bug.php?id=26090) suggests that ER_AUTO_CONVERT might be raised when one attempts to insert unicode into a varchar column. I don't understand why our column has the empty string as a name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7769
https://github.com/hail-is/hail/issues/7769:206,Testability,log,log,206,```; /usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py:192: Warning: Converting column '' from VARCHAR to TEXT; await self._do_get_result(); ```. These mysterious two lines are printed to my batch log when I run a big test (1000 jobs). The [Server Error Reference](https://dev.mysql.com/doc/refman/8.0/en/server-error-reference.html) indicates this is error 1246. A [MySQL bug](https://bugs.mysql.com/bug.php?id=26090) suggests that ER_AUTO_CONVERT might be raised when one attempts to insert unicode into a varchar column. I don't understand why our column has the empty string as a name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7769
https://github.com/hail-is/hail/issues/7769:227,Testability,test,test,227,```; /usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py:192: Warning: Converting column '' from VARCHAR to TEXT; await self._do_get_result(); ```. These mysterious two lines are printed to my batch log when I run a big test (1000 jobs). The [Server Error Reference](https://dev.mysql.com/doc/refman/8.0/en/server-error-reference.html) indicates this is error 1246. A [MySQL bug](https://bugs.mysql.com/bug.php?id=26090) suggests that ER_AUTO_CONVERT might be raised when one attempts to insert unicode into a varchar column. I don't understand why our column has the empty string as a name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7769
https://github.com/hail-is/hail/issues/7770:61,Availability,Failure,Failure,61,"The batch UI column is named ""Failed"" but it means ""bad"" or ""Failure and Error"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7770
https://github.com/hail-is/hail/issues/7770:73,Availability,Error,Error,73,"The batch UI column is named ""Failed"" but it means ""bad"" or ""Failure and Error"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7770
https://github.com/hail-is/hail/pull/7771:42,Testability,Benchmark,Benchmark,42,"They were quadratic. Bad bad bad bad bad. Benchmark now takes ~7s, compared to roughly the amount of time it; would take for my laptop to naturally sublimate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7771
https://github.com/hail-is/hail/pull/7773:126,Modifiability,extend,extend,126,"This PR uses BLAS `dgemm` function to multiply matrices when we are multiplying two 2 dimensional tensors of floats. I should extend this to work in the arbitrary tensor case with floats, but I have not done so yet. I'll do the extending after I've reworked the NDArray code to use column major, which I think will make it less complicated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7773
https://github.com/hail-is/hail/pull/7773:228,Modifiability,extend,extending,228,"This PR uses BLAS `dgemm` function to multiply matrices when we are multiplying two 2 dimensional tensors of floats. I should extend this to work in the arbitrary tensor case with floats, but I have not done so yet. I'll do the extending after I've reworked the NDArray code to use column major, which I think will make it less complicated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7773
https://github.com/hail-is/hail/pull/7776:25,Testability,test,tests,25,Moved the relevant Scala tests into python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7776
https://github.com/hail-is/hail/issues/7778:162,Deployability,pipeline,pipeline,162,"Given a list of things that look like; ```; {'batch_id': 767,; 'job_id': 1,; 'state': 'Success',; 'spec': {'command': ['/bin/bash',; '-c',; 'set -e; mkdir -p /io/pipeline/pipeline-f0c3c92aa1c4/__TASK__0/; true'],; 'image': 'gcr.io/hail-vdc/benchmark_tpoterba:latest',; 'job_id': 1,; 'mount_docker_socket': False,; 'resources': {'cpu': '1', 'memory': '7G'},; 'pvc_size': '100G',; 'secrets': [{'namespace': 'batch-pods',; 'name': 'dking-gsa-key',; 'mount_path': '/gsa-key',; 'mount_in_copy': True}],; 'env': []},; 'attributes': {'task_uid': '__TASK__0', 'name': 'replicate_0'},; 'status': {'worker': 'batch-worker-default-5t5e9',; 'batch_id': 767,; 'job_id': 1,; 'attempt_id': 'be692b',; 'user': 'dking',; 'state': 'succeeded',; 'container_statuses': {'main': {'name': 'main',; 'state': 'succeeded',; 'timing': {'pulling': {'start_time': 1576710190946,; 'finish_time': 1576710248882,; 'duration': 57936},; 'creating': {'start_time': 1576710248882,; 'finish_time': 1576710248963,; 'duration': 81},; 'runtime': {'start_time': 1576710248963,; 'finish_time': 1576710250461,; 'duration': 1498},; 'starting': {'start_time': 1576710248963,; 'finish_time': 1576710249898,; 'duration': 935},; 'running': {'start_time': 1576710249898,; 'finish_time': 1576710250461,; 'duration': 563},; 'uploading_log': {'start_time': 1576710250464,; 'finish_time': 1576710250742,; 'duration': 278},; 'deleting': {'start_time': 1576710250743,; 'finish_time': 1576710250776,; 'duration': 33}},; 'container_status': {'state': 'exited',; 'started_at': '2019-12-18T23:04:09.890460985Z',; 'finished_at': '2019-12-18T23:04:10.111873413Z',; 'out_of_memory': False,; 'exit_code': 0}}},; 'start_time': 1576710248963,; 'end_time': 1576710250461},; 'msec_mcpu': 2796766,; 'cost': '$0.0000'}; ```. I'd like to be able to load this list as a struct with one command. One would think this arcane magic would do it:; ```; In [16]: t = hl.Table.parallelize([hl.struct(**x) for x in jobs]) ; ```; but of course, nested fields. Probably some partia",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7778
https://github.com/hail-is/hail/issues/7778:171,Deployability,pipeline,pipeline-,171,"Given a list of things that look like; ```; {'batch_id': 767,; 'job_id': 1,; 'state': 'Success',; 'spec': {'command': ['/bin/bash',; '-c',; 'set -e; mkdir -p /io/pipeline/pipeline-f0c3c92aa1c4/__TASK__0/; true'],; 'image': 'gcr.io/hail-vdc/benchmark_tpoterba:latest',; 'job_id': 1,; 'mount_docker_socket': False,; 'resources': {'cpu': '1', 'memory': '7G'},; 'pvc_size': '100G',; 'secrets': [{'namespace': 'batch-pods',; 'name': 'dking-gsa-key',; 'mount_path': '/gsa-key',; 'mount_in_copy': True}],; 'env': []},; 'attributes': {'task_uid': '__TASK__0', 'name': 'replicate_0'},; 'status': {'worker': 'batch-worker-default-5t5e9',; 'batch_id': 767,; 'job_id': 1,; 'attempt_id': 'be692b',; 'user': 'dking',; 'state': 'succeeded',; 'container_statuses': {'main': {'name': 'main',; 'state': 'succeeded',; 'timing': {'pulling': {'start_time': 1576710190946,; 'finish_time': 1576710248882,; 'duration': 57936},; 'creating': {'start_time': 1576710248882,; 'finish_time': 1576710248963,; 'duration': 81},; 'runtime': {'start_time': 1576710248963,; 'finish_time': 1576710250461,; 'duration': 1498},; 'starting': {'start_time': 1576710248963,; 'finish_time': 1576710249898,; 'duration': 935},; 'running': {'start_time': 1576710249898,; 'finish_time': 1576710250461,; 'duration': 563},; 'uploading_log': {'start_time': 1576710250464,; 'finish_time': 1576710250742,; 'duration': 278},; 'deleting': {'start_time': 1576710250743,; 'finish_time': 1576710250776,; 'duration': 33}},; 'container_status': {'state': 'exited',; 'started_at': '2019-12-18T23:04:09.890460985Z',; 'finished_at': '2019-12-18T23:04:10.111873413Z',; 'out_of_memory': False,; 'exit_code': 0}}},; 'start_time': 1576710248963,; 'end_time': 1576710250461},; 'msec_mcpu': 2796766,; 'cost': '$0.0000'}; ```. I'd like to be able to load this list as a struct with one command. One would think this arcane magic would do it:; ```; In [16]: t = hl.Table.parallelize([hl.struct(**x) for x in jobs]) ; ```; but of course, nested fields. Probably some partia",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7778
https://github.com/hail-is/hail/issues/7778:1780,Performance,load,load,1780,"nd': ['/bin/bash',; '-c',; 'set -e; mkdir -p /io/pipeline/pipeline-f0c3c92aa1c4/__TASK__0/; true'],; 'image': 'gcr.io/hail-vdc/benchmark_tpoterba:latest',; 'job_id': 1,; 'mount_docker_socket': False,; 'resources': {'cpu': '1', 'memory': '7G'},; 'pvc_size': '100G',; 'secrets': [{'namespace': 'batch-pods',; 'name': 'dking-gsa-key',; 'mount_path': '/gsa-key',; 'mount_in_copy': True}],; 'env': []},; 'attributes': {'task_uid': '__TASK__0', 'name': 'replicate_0'},; 'status': {'worker': 'batch-worker-default-5t5e9',; 'batch_id': 767,; 'job_id': 1,; 'attempt_id': 'be692b',; 'user': 'dking',; 'state': 'succeeded',; 'container_statuses': {'main': {'name': 'main',; 'state': 'succeeded',; 'timing': {'pulling': {'start_time': 1576710190946,; 'finish_time': 1576710248882,; 'duration': 57936},; 'creating': {'start_time': 1576710248882,; 'finish_time': 1576710248963,; 'duration': 81},; 'runtime': {'start_time': 1576710248963,; 'finish_time': 1576710250461,; 'duration': 1498},; 'starting': {'start_time': 1576710248963,; 'finish_time': 1576710249898,; 'duration': 935},; 'running': {'start_time': 1576710249898,; 'finish_time': 1576710250461,; 'duration': 563},; 'uploading_log': {'start_time': 1576710250464,; 'finish_time': 1576710250742,; 'duration': 278},; 'deleting': {'start_time': 1576710250743,; 'finish_time': 1576710250776,; 'duration': 33}},; 'container_status': {'state': 'exited',; 'started_at': '2019-12-18T23:04:09.890460985Z',; 'finished_at': '2019-12-18T23:04:10.111873413Z',; 'out_of_memory': False,; 'exit_code': 0}}},; 'start_time': 1576710248963,; 'end_time': 1576710250461},; 'msec_mcpu': 2796766,; 'cost': '$0.0000'}; ```. I'd like to be able to load this list as a struct with one command. One would think this arcane magic would do it:; ```; In [16]: t = hl.Table.parallelize([hl.struct(**x) for x in jobs]) ; ```; but of course, nested fields. Probably some partial specification of the type will be necessary, but I would like to avoid specifying the whole thing if possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7778
https://github.com/hail-is/hail/issues/7778:2068,Safety,avoid,avoid,2068,"nd': ['/bin/bash',; '-c',; 'set -e; mkdir -p /io/pipeline/pipeline-f0c3c92aa1c4/__TASK__0/; true'],; 'image': 'gcr.io/hail-vdc/benchmark_tpoterba:latest',; 'job_id': 1,; 'mount_docker_socket': False,; 'resources': {'cpu': '1', 'memory': '7G'},; 'pvc_size': '100G',; 'secrets': [{'namespace': 'batch-pods',; 'name': 'dking-gsa-key',; 'mount_path': '/gsa-key',; 'mount_in_copy': True}],; 'env': []},; 'attributes': {'task_uid': '__TASK__0', 'name': 'replicate_0'},; 'status': {'worker': 'batch-worker-default-5t5e9',; 'batch_id': 767,; 'job_id': 1,; 'attempt_id': 'be692b',; 'user': 'dking',; 'state': 'succeeded',; 'container_statuses': {'main': {'name': 'main',; 'state': 'succeeded',; 'timing': {'pulling': {'start_time': 1576710190946,; 'finish_time': 1576710248882,; 'duration': 57936},; 'creating': {'start_time': 1576710248882,; 'finish_time': 1576710248963,; 'duration': 81},; 'runtime': {'start_time': 1576710248963,; 'finish_time': 1576710250461,; 'duration': 1498},; 'starting': {'start_time': 1576710248963,; 'finish_time': 1576710249898,; 'duration': 935},; 'running': {'start_time': 1576710249898,; 'finish_time': 1576710250461,; 'duration': 563},; 'uploading_log': {'start_time': 1576710250464,; 'finish_time': 1576710250742,; 'duration': 278},; 'deleting': {'start_time': 1576710250743,; 'finish_time': 1576710250776,; 'duration': 33}},; 'container_status': {'state': 'exited',; 'started_at': '2019-12-18T23:04:09.890460985Z',; 'finished_at': '2019-12-18T23:04:10.111873413Z',; 'out_of_memory': False,; 'exit_code': 0}}},; 'start_time': 1576710248963,; 'end_time': 1576710250461},; 'msec_mcpu': 2796766,; 'cost': '$0.0000'}; ```. I'd like to be able to load this list as a struct with one command. One would think this arcane magic would do it:; ```; In [16]: t = hl.Table.parallelize([hl.struct(**x) for x in jobs]) ; ```; but of course, nested fields. Probably some partial specification of the type will be necessary, but I would like to avoid specifying the whole thing if possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7778
https://github.com/hail-is/hail/pull/7783:181,Availability,error,error,181,"- retry every deadlock in two deadlock prone SQL operations; - add prometheus metrics for cores; - fix prometheus when you're not in the default namespace; - retry every docker 500 error, it's 500, not our fault, just retry, right?; - create a billing account for the dev deploying user; - rewrite a couple queries to harmonize table locking a bit; - add globals to delete tables script; - fix list_batches, which was broken by the query language changes; - include primary services developers' namespaces in prometheus monitoring. Fixes #7756",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783
https://github.com/hail-is/hail/pull/7783:206,Availability,fault,fault,206,"- retry every deadlock in two deadlock prone SQL operations; - add prometheus metrics for cores; - fix prometheus when you're not in the default namespace; - retry every docker 500 error, it's 500, not our fault, just retry, right?; - create a billing account for the dev deploying user; - rewrite a couple queries to harmonize table locking a bit; - add globals to delete tables script; - fix list_batches, which was broken by the query language changes; - include primary services developers' namespaces in prometheus monitoring. Fixes #7756",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783
https://github.com/hail-is/hail/pull/7783:272,Deployability,deploy,deploying,272,"- retry every deadlock in two deadlock prone SQL operations; - add prometheus metrics for cores; - fix prometheus when you're not in the default namespace; - retry every docker 500 error, it's 500, not our fault, just retry, right?; - create a billing account for the dev deploying user; - rewrite a couple queries to harmonize table locking a bit; - add globals to delete tables script; - fix list_batches, which was broken by the query language changes; - include primary services developers' namespaces in prometheus monitoring. Fixes #7756",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783
https://github.com/hail-is/hail/pull/7783:520,Energy Efficiency,monitor,monitoring,520,"- retry every deadlock in two deadlock prone SQL operations; - add prometheus metrics for cores; - fix prometheus when you're not in the default namespace; - retry every docker 500 error, it's 500, not our fault, just retry, right?; - create a billing account for the dev deploying user; - rewrite a couple queries to harmonize table locking a bit; - add globals to delete tables script; - fix list_batches, which was broken by the query language changes; - include primary services developers' namespaces in prometheus monitoring. Fixes #7756",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783
https://github.com/hail-is/hail/pull/7783:290,Modifiability,rewrite,rewrite,290,"- retry every deadlock in two deadlock prone SQL operations; - add prometheus metrics for cores; - fix prometheus when you're not in the default namespace; - retry every docker 500 error, it's 500, not our fault, just retry, right?; - create a billing account for the dev deploying user; - rewrite a couple queries to harmonize table locking a bit; - add globals to delete tables script; - fix list_batches, which was broken by the query language changes; - include primary services developers' namespaces in prometheus monitoring. Fixes #7756",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783
https://github.com/hail-is/hail/pull/7784:243,Availability,toler,tolerations,243,"We need at least one pool without taints to schedule the kube-system services (e.g. dns). Therefore, I propose:; - make the non-preemptible pool untainted,; - keep taint on preemptibles so kube-system pods are not scheduled there,; - and keep tolerations for preemptible pods,; - use nodeSelector to force preemptible pods to be scheduled on the preemptible pool. In fact, I put nodeSelectos on all pods, although it isn't strictly necessary for non-preemptible pods. Sound good?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784
https://github.com/hail-is/hail/pull/7784:44,Energy Efficiency,schedul,schedule,44,"We need at least one pool without taints to schedule the kube-system services (e.g. dns). Therefore, I propose:; - make the non-preemptible pool untainted,; - keep taint on preemptibles so kube-system pods are not scheduled there,; - and keep tolerations for preemptible pods,; - use nodeSelector to force preemptible pods to be scheduled on the preemptible pool. In fact, I put nodeSelectos on all pods, although it isn't strictly necessary for non-preemptible pods. Sound good?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784
https://github.com/hail-is/hail/pull/7784:214,Energy Efficiency,schedul,scheduled,214,"We need at least one pool without taints to schedule the kube-system services (e.g. dns). Therefore, I propose:; - make the non-preemptible pool untainted,; - keep taint on preemptibles so kube-system pods are not scheduled there,; - and keep tolerations for preemptible pods,; - use nodeSelector to force preemptible pods to be scheduled on the preemptible pool. In fact, I put nodeSelectos on all pods, although it isn't strictly necessary for non-preemptible pods. Sound good?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784
https://github.com/hail-is/hail/pull/7784:329,Energy Efficiency,schedul,scheduled,329,"We need at least one pool without taints to schedule the kube-system services (e.g. dns). Therefore, I propose:; - make the non-preemptible pool untainted,; - keep taint on preemptibles so kube-system pods are not scheduled there,; - and keep tolerations for preemptible pods,; - use nodeSelector to force preemptible pods to be scheduled on the preemptible pool. In fact, I put nodeSelectos on all pods, although it isn't strictly necessary for non-preemptible pods. Sound good?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784
https://github.com/hail-is/hail/pull/7786:1255,Testability,Test,Tested,1255,"Creates a responsive table whose dimensions are defined on the parent (allowing child elements to be set as a percentage of that table), by setting width of the parent based on viewport. If the table exceeds that width, it will scroll, such that the elements above the table are still fixed to the flex-end position. See https://github.com/hail-is/hail/pull/7777. Narrow view (very slightly wider, because 75% of 653 is > 75% of 600, and table is in fact 653px at minimum, even when you set 600px min width):; <img width=""774"" alt=""Screenshot 2019-12-27 15 38 52"" src=""https://user-images.githubusercontent.com/5543229/71533028-73d10280-28c4-11ea-99be-bea06bc67a10.png"">. Wide view:; <img width=""1920"" alt=""Screenshot 2019-12-27 15 38 58"" src=""https://user-images.githubusercontent.com/5543229/71533029-73d10280-28c4-11ea-98ef-7b8e3afe3ca3.png"">. Table that is too wide is scrollable (wider than 1024px):; <img width=""804"" alt=""Screenshot 2019-12-27 16 09 18"" src=""https://user-images.githubusercontent.com/5543229/71532988-4be19f00-28c4-11ea-915a-1e038f179d1f.png"">; after scrolling right:; <img width=""1453"" alt=""Screenshot 2019-12-27 16 09 12"" src=""https://user-images.githubusercontent.com/5543229/71532989-4be19f00-28c4-11ea-9fbd-0270c881d085.png"">. Tested manually in browser in Firefox and Chrome.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7786
https://github.com/hail-is/hail/pull/7786:10,Usability,responsiv,responsive,10,"Creates a responsive table whose dimensions are defined on the parent (allowing child elements to be set as a percentage of that table), by setting width of the parent based on viewport. If the table exceeds that width, it will scroll, such that the elements above the table are still fixed to the flex-end position. See https://github.com/hail-is/hail/pull/7777. Narrow view (very slightly wider, because 75% of 653 is > 75% of 600, and table is in fact 653px at minimum, even when you set 600px min width):; <img width=""774"" alt=""Screenshot 2019-12-27 15 38 52"" src=""https://user-images.githubusercontent.com/5543229/71533028-73d10280-28c4-11ea-99be-bea06bc67a10.png"">. Wide view:; <img width=""1920"" alt=""Screenshot 2019-12-27 15 38 58"" src=""https://user-images.githubusercontent.com/5543229/71533029-73d10280-28c4-11ea-98ef-7b8e3afe3ca3.png"">. Table that is too wide is scrollable (wider than 1024px):; <img width=""804"" alt=""Screenshot 2019-12-27 16 09 18"" src=""https://user-images.githubusercontent.com/5543229/71532988-4be19f00-28c4-11ea-915a-1e038f179d1f.png"">; after scrolling right:; <img width=""1453"" alt=""Screenshot 2019-12-27 16 09 12"" src=""https://user-images.githubusercontent.com/5543229/71532989-4be19f00-28c4-11ea-9fbd-0270c881d085.png"">. Tested manually in browser in Firefox and Chrome.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7786
https://github.com/hail-is/hail/pull/7792:197,Availability,Error,Error,197,"Fixes #2921. Example:. ```; In [3]: mt = mt.annotate_entries(PL=[0]). In [4]: hl.split_multi_hts(mt)._force_count_rows(). ... hundreds of lines of java trace ... Hail version: 0.2.30-bae67f384161; Error summary: HailException: array index out of bounds: index=1, length=1; ----------; Python traceback:; File ""<ipython-input-4-ba11ec1cd68e>"", line 1, in <module>; hl.split_multi_hts(mt)._force_count_rows(). File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2246, in split_multi_hts; (hl.range(0, 3).map(lambda i:. File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2250, in <lambda>; ).map(lambda j: split.PL[j])))))). File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2250, in <lambda>; ).map(lambda j: split.PL[j])))))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7792
https://github.com/hail-is/hail/pull/7793:4,Availability,error,error,4,"The error this fixes is in the line: ; `if not step.run_if_requested or step.name in requested_step_names`. when `requested_step_names is None`, you cannot check if a value is `in` it. I chose not to make the default value of `requested_step_names = []`, since it's dangerous to do that in python (that single mutable list will be shared across invocations of the constructor)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7793
https://github.com/hail-is/hail/pull/7794:12,Testability,test,test,12,"Added a new test to make sure type promotion worked correctly (multiplying int matrix by float matrix), found a requiredness issue instead.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7794
https://github.com/hail-is/hail/pull/7799:88,Testability,test,test,88,Instead print summary. I am tired of scrolling past the 5k-field table output in the CI test logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7799
https://github.com/hail-is/hail/pull/7799:93,Testability,log,logs,93,Instead print summary. I am tired of scrolling past the 5k-field table output in the CI test logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7799
https://github.com/hail-is/hail/pull/7801:163,Performance,cache,cache,163,This is sufficiently large to permit the transmission of the Hail JAR; which is about 38 MB. I will use this to test and eventually normally; use the Gradle build cache server.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7801
https://github.com/hail-is/hail/pull/7801:112,Testability,test,test,112,This is sufficiently large to permit the transmission of the Hail JAR; which is about 38 MB. I will use this to test and eventually normally; use the Gradle build cache server.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7801
https://github.com/hail-is/hail/issues/7806:23,Testability,test,test,23,"I just finished up the test suite for time functions that Milo had written in Scala, but we should probably move the whole thing to python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7806
https://github.com/hail-is/hail/pull/7807:16,Performance,optimiz,optimization,16,"This PR adds an optimization where a two dimensional matrix multiply of matrices containing float32 or float64s will be performed by SGEMM or DGEMM respectively. In the future, we should use this during tensor multiplies as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7807
https://github.com/hail-is/hail/pull/7807:120,Performance,perform,performed,120,"This PR adds an optimization where a two dimensional matrix multiply of matrices containing float32 or float64s will be performed by SGEMM or DGEMM respectively. In the future, we should use this during tensor multiplies as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7807
https://github.com/hail-is/hail/pull/7814:59,Deployability,deploy,deploy,59,"Does this look right? We aren't including the test repo on deploy, so we shouldn't create it/can't test against it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814
https://github.com/hail-is/hail/pull/7814:46,Testability,test,test,46,"Does this look right? We aren't including the test repo on deploy, so we shouldn't create it/can't test against it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814
https://github.com/hail-is/hail/pull/7814:99,Testability,test,test,99,"Does this look right? We aren't including the test repo on deploy, so we shouldn't create it/can't test against it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814
https://github.com/hail-is/hail/pull/7816:31,Testability,test,test,31,don't rely on defaults in cost test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7816
https://github.com/hail-is/hail/pull/7819:45,Deployability,deploy,deploy-config,45,"This is a pod running in default that mounts deploy-config and database-server-config. `mysql` will connect to the database as root. Useful for troubleshooting. I often have it running, but figure it should be official. Doesn't run in test since it doesn't have a database-server-config.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7819
https://github.com/hail-is/hail/pull/7819:52,Modifiability,config,config,52,"This is a pod running in default that mounts deploy-config and database-server-config. `mysql` will connect to the database as root. Useful for troubleshooting. I often have it running, but figure it should be official. Doesn't run in test since it doesn't have a database-server-config.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7819
https://github.com/hail-is/hail/pull/7819:79,Modifiability,config,config,79,"This is a pod running in default that mounts deploy-config and database-server-config. `mysql` will connect to the database as root. Useful for troubleshooting. I often have it running, but figure it should be official. Doesn't run in test since it doesn't have a database-server-config.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7819
https://github.com/hail-is/hail/pull/7819:280,Modifiability,config,config,280,"This is a pod running in default that mounts deploy-config and database-server-config. `mysql` will connect to the database as root. Useful for troubleshooting. I often have it running, but figure it should be official. Doesn't run in test since it doesn't have a database-server-config.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7819
https://github.com/hail-is/hail/pull/7819:235,Testability,test,test,235,"This is a pod running in default that mounts deploy-config and database-server-config. `mysql` will connect to the database as root. Useful for troubleshooting. I often have it running, but figure it should be official. Doesn't run in test since it doesn't have a database-server-config.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7819
https://github.com/hail-is/hail/pull/7822:85,Usability,clear,clearing,85,"Uses the existent Region.setMemory function, and checks requiredeness before setting/clearing missing bits.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7822
https://github.com/hail-is/hail/pull/7823:23,Testability,test,test,23,Made a new PR so I can test on CI without reauthorizing repeatedly. I'll assign Patrick again once I've finished addressing all his previous comments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7823
https://github.com/hail-is/hail/issues/7824:419,Availability,error,error,419,"Using this TSV,; ```; id	foo_1	foo_2	foo_3	bar_1	bar_2	bar_3; a	1	2	3	4	5	6; b	7	8	9	10	11	12; c	13	14	15	16	17	18; ```. This code; ```; import hail as hl. ds = hl.import_matrix_table(""test.tsv"", row_fields={""id"": hl.tstr}, entry_type=hl.tfloat); ds = ds.annotate_cols(prefix=ds.col_id.split(""_"")[0]). t = ds.group_cols_by(ds.prefix).aggregate(**{"""": hl.agg.approx_median(ds.x)}).make_table(); t.show(); ```. Throws an error `HailException: approx_cdf already initialized` on Hail 0.2.28 and 0.2.30. On Hail 0.2.26, that code worked and output; ```; +--------+-----+----------+----------+; | row_id | id | bar | foo |; +--------+-----+----------+----------+; | int64 | str | float64 | float64 |; +--------+-----+----------+----------+; | 0 | ""a"" | 5.00e+00 | 2.00e+00 |; | 1 | ""b"" | 1.10e+01 | 8.00e+00 |; | 2 | ""c"" | 1.70e+01 | 1.40e+01 |; +--------+-----+----------+----------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7824
https://github.com/hail-is/hail/issues/7824:185,Testability,test,test,185,"Using this TSV,; ```; id	foo_1	foo_2	foo_3	bar_1	bar_2	bar_3; a	1	2	3	4	5	6; b	7	8	9	10	11	12; c	13	14	15	16	17	18; ```. This code; ```; import hail as hl. ds = hl.import_matrix_table(""test.tsv"", row_fields={""id"": hl.tstr}, entry_type=hl.tfloat); ds = ds.annotate_cols(prefix=ds.col_id.split(""_"")[0]). t = ds.group_cols_by(ds.prefix).aggregate(**{"""": hl.agg.approx_median(ds.x)}).make_table(); t.show(); ```. Throws an error `HailException: approx_cdf already initialized` on Hail 0.2.28 and 0.2.30. On Hail 0.2.26, that code worked and output; ```; +--------+-----+----------+----------+; | row_id | id | bar | foo |; +--------+-----+----------+----------+; | int64 | str | float64 | float64 |; +--------+-----+----------+----------+; | 0 | ""a"" | 5.00e+00 | 2.00e+00 |; | 1 | ""b"" | 1.10e+01 | 8.00e+00 |; | 2 | ""c"" | 1.70e+01 | 1.40e+01 |; +--------+-----+----------+----------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7824
https://github.com/hail-is/hail/pull/7825:25,Energy Efficiency,schedul,scheduler,25,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7825
https://github.com/hail-is/hail/pull/7825:173,Integrability,wrap,wrapper,173,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7825
https://github.com/hail-is/hail/pull/7825:100,Performance,cache,cache,100,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7825
https://github.com/hail-is/hail/pull/7825:44,Testability,test,tested,44,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7825
https://github.com/hail-is/hail/pull/7825:64,Testability,log,log,64,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7825
https://github.com/hail-is/hail/pull/7825:114,Testability,test,tested,114,"This should speed up the scheduler a bit. I tested it with some log statements to ensure there were cache hits. I tested the cleanup loop works before I added a try, except wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7825
https://github.com/hail-is/hail/issues/7826:119,Energy Efficiency,allocate,allocate,119,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:865,Energy Efficiency,allocate,allocated,865,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:629,Integrability,wrap,wrappers,629,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:54,Modifiability,parameteriz,parameterizations,54,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:239,Modifiability,parameteriz,parameterizations,239,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:507,Modifiability,inherit,inheritors,507,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:550,Modifiability,inherit,inheritors,550,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:581,Modifiability,parameteriz,parameterizations,581,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:653,Modifiability,parameteriz,parameterization,653,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1418,Modifiability,parameteriz,parameterization,1418,"(or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:100,Performance,load,load,100,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:196,Performance,load,load,196,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:479,Performance,load,loadElement,479,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:523,Performance,load,loadField,523,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:675,Performance,load,loadElement,675,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:710,Performance,load,loadElement,710,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:978,Performance,load,load,978,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1125,Performance,load,load,1125,"m Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1266,Performance,load,load,1266,"for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the origina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1298,Performance,load,loading,1298,"for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the origina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1507,Performance,load,load,1507,"nt (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hol",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1914,Performance,perform,performance,1914,"edge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. Tim Poterba: ok, I think I'm convinced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:2222,Performance,load,load,2222,"edge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. Tim Poterba: ok, I think I'm convinced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:2671,Performance,load,loadElement,2671,"edge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. Tim Poterba: ok, I think I'm convinced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:2765,Performance,load,loadElement,2765,"edge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. Tim Poterba: ok, I think I'm convinced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1854,Security,access,accessed,1854,"our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. Alex Kotlar:; In which case I don't think the region argument ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7826:1496,Usability,clear,clear,1496,"nt (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. Patrick Schultz: It might be that the lazy datastructure should really own the region(s) it uses for on-demand computation, rather than getting them from its callers. Tim Poterba: hmmm, you're right. Passing the region on load is not sufficient -- that region needs to be the owning region for the original data. Alex Kotlar: Would we want to associate an instance of a PType with a single region?. Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hol",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826
https://github.com/hail-is/hail/issues/7829:0,Performance,load,loadField,0,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:11,Performance,load,loadElement,11,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:106,Performance,load,load,106,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:210,Performance,load,loadInt,210,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:409,Performance,load,loadField,409,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:559,Performance,load,loadField,559,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:616,Performance,load,loadElement,616,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:640,Performance,load,loadField,640,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:866,Performance,load,loadLength,866,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:881,Performance,load,loadField,881,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:905,Performance,load,loadLength,905,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:922,Performance,load,loadInt,922,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/issues/7829:185,Safety,unsafe,unsafe,185,"loadField, loadElement returns the field/element address and not the corresponding value (requiring a 2nd load operations specific to the underlying type to retrieve the value, such as unsafe.getInt via Memory.loadInt). Makes for slightly strange semantics, as seen in ArrayElementLengthCheckAggregator.scala:. ```scala; def copyFromAddress(src: Code[Long]): Code[Unit] = {; val srcOff = fb.newField[Long]; //loadField is actually an offset/memory address; // and is actually a no-op unless the field is a pointer type (array currently); val initOffset = typ.loadField(srcOff, 0); //same ; val eltOffset = arrayType.loadElement(region, typ.loadField(srcOff, 1), idx) . Code(; srcOff := src,; init(initContainer.copyFrom(initOffset), initLen = false),; typ.isFieldMissing(srcOff, 1).mux(; Code(typ.setFieldMissing(off, 1),; lenRef := -1),; Code(; lenRef := arrayType.loadLength(typ.loadField(srcOff, 1)), #loadLength calls loadInt on the address returned; seq(container.copyFrom(eltOffset))))); }; ```. cc @tpoterba in case you disagree",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7829
https://github.com/hail-is/hail/pull/7831:50,Usability,clear,clearMissingBits,50,"Replaces setAllMissing (1, staged-only function), clearMissingBits (3 functions with both staged and unstated implementations) with staged/non-staged initialization functions, mirroring the PContainer API. Semantically this is more correct (for instance clearMissingBits was used mostly behind the condition like `if (init)`), and represents 2 fewer functions to maintain (3 if you count that we may have eventually wanted a setAllMissing in the non-staged world)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7831
https://github.com/hail-is/hail/pull/7831:254,Usability,clear,clearMissingBits,254,"Replaces setAllMissing (1, staged-only function), clearMissingBits (3 functions with both staged and unstated implementations) with staged/non-staged initialization functions, mirroring the PContainer API. Semantically this is more correct (for instance clearMissingBits was used mostly behind the condition like `if (init)`), and represents 2 fewer functions to maintain (3 if you count that we may have eventually wanted a setAllMissing in the non-staged world)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7831
https://github.com/hail-is/hail/pull/7833:54,Deployability,update,update,54,- change ready cores to a trigger; - added select for update to critical calls in sql; - added activation and deactivation times to instances table; - added removed parameter to instances table; don't delete instances from table that are removed; - schedule in parallel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7833
https://github.com/hail-is/hail/pull/7833:249,Energy Efficiency,schedul,schedule,249,- change ready cores to a trigger; - added select for update to critical calls in sql; - added activation and deactivation times to instances table; - added removed parameter to instances table; don't delete instances from table that are removed; - schedule in parallel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7833
https://github.com/hail-is/hail/pull/7836:79,Testability,test,test,79,Closes #4922 . I don't really see the problem with having them in two separate test methods though.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7836
https://github.com/hail-is/hail/pull/7838:54,Deployability,update,update,54,- change ready cores to a trigger; - added select for update to critical calls in sql; - added activation and deactivation times to instances table; - added removed parameter to instances table; don't delete instances from table that are removed; - schedule in parallel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7838
https://github.com/hail-is/hail/pull/7838:249,Energy Efficiency,schedul,schedule,249,- change ready cores to a trigger; - added select for update to critical calls in sql; - added activation and deactivation times to instances table; - added removed parameter to instances table; don't delete instances from table that are removed; - schedule in parallel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7838
https://github.com/hail-is/hail/issues/7839:603,Availability,echo,echo,603,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/issues/7839:284,Deployability,pipeline,pipeline,284,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/issues/7839:360,Deployability,Pipeline,Pipeline,360,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/issues/7839:848,Deployability,pipeline,pipeline,848,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/issues/7839:857,Deployability,pipeline,pipeline,857,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/issues/7839:1028,Deployability,pipeline,pipeline,1028,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/issues/7839:2173,Integrability,message,message,2173,"ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, in submit; b = await b_resp.json(); File ""/usr/local/lib/python3.7/site-packages/aiohttp/client_reqrep.py"", line 1032, in json; headers=self.headers); ContentTypeError: 0, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='https://internal.hail.is/dking/batch/api/v1alpha/batches/create. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/issues/7839:152,Modifiability,enhance,enhanced,152,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7839
https://github.com/hail-is/hail/pull/7840:118,Integrability,rout,routers,118,"`client_max_size` was being ignored, I fixed that. I also increased it to 8MB. Increasing further requires changes to routers and gateways, so for now I ignore that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7840
https://github.com/hail-is/hail/pull/7841:141,Energy Efficiency,allocate,allocated,141,"Overschedule batch workers by 2 cores. Changes:; - `free_cores_mcpu` in `compute_fair_share` includes the overschedule cores, which will get allocated to users, and; - when scheduling, schedule if the job fits on the node with the overschedule cores, or the node is not yet overscheduled. This allows us to overschedule jobs that need >2 cores. This shouldn't go in until after @jigold's parallel scheduler PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7841
https://github.com/hail-is/hail/pull/7841:173,Energy Efficiency,schedul,scheduling,173,"Overschedule batch workers by 2 cores. Changes:; - `free_cores_mcpu` in `compute_fair_share` includes the overschedule cores, which will get allocated to users, and; - when scheduling, schedule if the job fits on the node with the overschedule cores, or the node is not yet overscheduled. This allows us to overschedule jobs that need >2 cores. This shouldn't go in until after @jigold's parallel scheduler PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7841
https://github.com/hail-is/hail/pull/7841:185,Energy Efficiency,schedul,schedule,185,"Overschedule batch workers by 2 cores. Changes:; - `free_cores_mcpu` in `compute_fair_share` includes the overschedule cores, which will get allocated to users, and; - when scheduling, schedule if the job fits on the node with the overschedule cores, or the node is not yet overscheduled. This allows us to overschedule jobs that need >2 cores. This shouldn't go in until after @jigold's parallel scheduler PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7841
https://github.com/hail-is/hail/pull/7841:397,Energy Efficiency,schedul,scheduler,397,"Overschedule batch workers by 2 cores. Changes:; - `free_cores_mcpu` in `compute_fair_share` includes the overschedule cores, which will get allocated to users, and; - when scheduling, schedule if the job fits on the node with the overschedule cores, or the node is not yet overscheduled. This allows us to overschedule jobs that need >2 cores. This shouldn't go in until after @jigold's parallel scheduler PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7841
https://github.com/hail-is/hail/pull/7844:40,Deployability,release,release,40,"This fixes a bug in the fifo sem. If we release a large job, we would only start at most 1, not as many as we could. This could actually lead to worker deadlock.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7844
https://github.com/hail-is/hail/pull/7849:5,Safety,sanity check,sanity check,5,"As a sanity check, we've halved the amount of disk per worker. CPU is $0.01 core hour. So 0.003 / 2 = 0.0015",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7849
https://github.com/hail-is/hail/pull/7852:52,Availability,error,errors,52,- run cancels in parallel with scheduling; - handle errors asap rather than serially after all jobs have finished,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7852
https://github.com/hail-is/hail/pull/7852:31,Energy Efficiency,schedul,scheduling,31,- run cancels in parallel with scheduling; - handle errors asap rather than serially after all jobs have finished,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7852
https://github.com/hail-is/hail/pull/7855:62,Availability,down,down,62,shutdowns in createDatabase2 is a list of deployments to shut down before applying any migrations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855
https://github.com/hail-is/hail/pull/7855:42,Deployability,deploy,deployments,42,shutdowns in createDatabase2 is a list of deployments to shut down before applying any migrations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855
https://github.com/hail-is/hail/pull/7857:19,Testability,test,test-instance,19,The database name `test-instance` is causing trouble in: https://github.com/hail-is/hail/pull/7856,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7857
https://github.com/hail-is/hail/pull/7858:6,Availability,error,errors,6,a few errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7858
https://github.com/hail-is/hail/pull/7859:27,Availability,error,errors,27,"which will retry transient errors. router-resolver is part of the infrastructure, so I will hand-deploy once you're happy with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7859
https://github.com/hail-is/hail/pull/7859:97,Deployability,deploy,deploy,97,"which will retry transient errors. router-resolver is part of the infrastructure, so I will hand-deploy once you're happy with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7859
https://github.com/hail-is/hail/pull/7859:35,Integrability,rout,router-resolver,35,"which will retry transient errors. router-resolver is part of the infrastructure, so I will hand-deploy once you're happy with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7859
https://github.com/hail-is/hail/pull/7861:84,Testability,test,test,84,Add a sentence to strptime docs to explain useful information about rounding. Add a test with negative timestamp.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7861
https://github.com/hail-is/hail/pull/7864:0,Testability,Test,Tests,0,"Tests scale of commands, to some degree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864
https://github.com/hail-is/hail/pull/7867:11,Performance,perform,performance,11,Fixes O(N) performance of mt.entries().show(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7867
https://github.com/hail-is/hail/pull/7868:8,Modifiability,inherit,inherit,8,"* don't inherit from BaseType, which doesn't really fit.; * remove pyString methods; * remove _toPretty. Stacked on #7744, don't review until that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7868
https://github.com/hail-is/hail/pull/7875:103,Availability,failure,failures,103,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:824,Availability,error,error,824,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:917,Availability,failure,failures,917,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:1086,Availability,error,errors,1086,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:307,Integrability,message,messages,307,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:520,Modifiability,config,configurable,520,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:620,Modifiability,config,configurable,620,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:1023,Modifiability,config,configurable,1023,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:1063,Modifiability,config,configurable,1063,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:85,Testability,test,test,85,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:248,Testability,log,log,248,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:297,Testability,log,log,297,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:548,Testability,test,testing,548,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7875:1053,Testability,log,logging,1053,"- front_end returns 200 OK if a bunch is already inserted for an open batch; - add a test that inserts failures on every third http request made by a batch builder; - add `MultipleExceptions` which can be raised and have many causes; - set minimum log level of aioclient to WARNING, so users see `log.warn` messages; - increase bunch byte size to 8MiB (was 8MB), increase bunch size to 8 * 1024 (was 1000, which, for typical Konrad jobs (1kB) prevents fully filling the HTTP request); - make the previous two parameters configurable (primarily for testing purposes); - souped up AsyncThrottledGather to bail out after a configurable number of exceptions. For the restartable client we:; 1. create the batch, if that succeeds we never try to create again; 2. create the json-encoded job_spec bunches, this only fails on user error; 3. submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; 4. if any request fails, raise an exception, which is caught by outer `submit`, which retries a configurable number of times, logging a configurable number of errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875
https://github.com/hail-is/hail/pull/7876:0,Usability,Progress bar,Progress bars,0,"Progress bars for batch submit (which work even if individual bunches fail). Stacked on #7875. <img width=""885"" alt=""Screen Shot 2020-01-14 at 3 53 40 PM"" src=""https://user-images.githubusercontent.com/106194/72382076-829e1e80-36e6-11ea-9626-1e67e5aa54ce.png"">. Now also works in Jupyter Notebook (you get a GUI bar). I added `hailtop.utils.tqdm` and `hailtop.utils.TQDM_DEFAULT_DISABLE` because the ""correct"" default argument for disable is different between Jupyter Notebook and the console tqdm. In particular, console tqdm treats `None` as ""if I'm connected to a TTY, display, otherwise hide"". Jupyter tqdm treats `None` as ""do not display"" (which is clearly the wrong default, but 🤷‍♀ ).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7876
https://github.com/hail-is/hail/pull/7876:655,Usability,clear,clearly,655,"Progress bars for batch submit (which work even if individual bunches fail). Stacked on #7875. <img width=""885"" alt=""Screen Shot 2020-01-14 at 3 53 40 PM"" src=""https://user-images.githubusercontent.com/106194/72382076-829e1e80-36e6-11ea-9626-1e67e5aa54ce.png"">. Now also works in Jupyter Notebook (you get a GUI bar). I added `hailtop.utils.tqdm` and `hailtop.utils.TQDM_DEFAULT_DISABLE` because the ""correct"" default argument for disable is different between Jupyter Notebook and the console tqdm. In particular, console tqdm treats `None` as ""if I'm connected to a TTY, display, otherwise hide"". Jupyter tqdm treats `None` as ""do not display"" (which is clearly the wrong default, but 🤷‍♀ ).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7876
https://github.com/hail-is/hail/pull/7879:0,Testability,Benchmark,Benchmarks,0,Benchmarks vs a hardcoded take-the-old-path branch:. ```; Geometric mean: 99.5%; Simple mean: 99.8%; Median: 99.5%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7879
https://github.com/hail-is/hail/pull/7879:81,Usability,Simpl,Simple,81,Benchmarks vs a hardcoded take-the-old-path branch:. ```; Geometric mean: 99.5%; Simple mean: 99.8%; Median: 99.5%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7879
https://github.com/hail-is/hail/pull/7885:11,Deployability,UPDATE,UPDATE,11,I used FOR UPDATE when I should have used LOCK IN SHARE MODE. I added the diff between initial.sql and the new migration at the top. Not sure if this is helpful or what the standard for figuring out how to track changes should be.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7885
https://github.com/hail-is/hail/pull/7886:10,Testability,test,test,10,and add a test!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886
https://github.com/hail-is/hail/pull/7887:341,Deployability,deploy,deployed,341,"Now that createDatabase is gone, rename createDatabase2Step => createDatabase2, and accept createDatabase in build.yaml for creating database. In follow up PRs, I will:; - rename createDatabase2 => createDatabase in build.yaml,; - don't support createDatabase2, completing the change. I can't do this in one change because this PR is tested/deployed by the _previous_ CI, not the one in this PR, so it has to be done in stages. Such is the microservices life.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7887
https://github.com/hail-is/hail/pull/7887:334,Testability,test,tested,334,"Now that createDatabase is gone, rename createDatabase2Step => createDatabase2, and accept createDatabase in build.yaml for creating database. In follow up PRs, I will:; - rename createDatabase2 => createDatabase in build.yaml,; - don't support createDatabase2, completing the change. I can't do this in one change because this PR is tested/deployed by the _previous_ CI, not the one in this PR, so it has to be done in stages. Such is the microservices life.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7887
https://github.com/hail-is/hail/pull/7891:91,Testability,benchmark,benchmark,91,"After #7879 merges, I'll rebase this, adding regions to `mux` and `decode`. Then we should benchmark before merging this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7891
https://github.com/hail-is/hail/issues/7893:42,Testability,test,test,42,"```; 'test_t_x'.split('_', 1); Out[20]: ['test', 't_x']; hl.eval(hl.str('test_t_x').split('_', 1)); Out[18]: ['test_t_x']; hl.eval(hl.str('test_t_x').split('_', 2)); Out[19]: ['test', 't_x']; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7893
https://github.com/hail-is/hail/issues/7893:177,Testability,test,test,177,"```; 'test_t_x'.split('_', 1); Out[20]: ['test', 't_x']; hl.eval(hl.str('test_t_x').split('_', 1)); Out[18]: ['test_t_x']; hl.eval(hl.str('test_t_x').split('_', 2)); Out[19]: ['test', 't_x']; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7893
https://github.com/hail-is/hail/pull/7897:379,Availability,failure,failure,379,"This is an improvement, but I think we should reconsider the batch state. Thinking out loud: The batch and CI UIs have slightly different displays. If you're running, have had a failed job, but also been cancelled, what should your state be? I think we either need columns in the batch display for open/closed, cancelled, complete and the simple state (open, running, cancelled, failure, success ... where the latter 3 mean complete), or display compound states like ""running failure cancelled"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7897
https://github.com/hail-is/hail/pull/7897:476,Availability,failure,failure,476,"This is an improvement, but I think we should reconsider the batch state. Thinking out loud: The batch and CI UIs have slightly different displays. If you're running, have had a failed job, but also been cancelled, what should your state be? I think we either need columns in the batch display for open/closed, cancelled, complete and the simple state (open, running, cancelled, failure, success ... where the latter 3 mean complete), or display compound states like ""running failure cancelled"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7897
https://github.com/hail-is/hail/pull/7897:339,Usability,simpl,simple,339,"This is an improvement, but I think we should reconsider the batch state. Thinking out loud: The batch and CI UIs have slightly different displays. If you're running, have had a failed job, but also been cancelled, what should your state be? I think we either need columns in the batch display for open/closed, cancelled, complete and the simple state (open, running, cancelled, failure, success ... where the latter 3 mean complete), or display compound states like ""running failure cancelled"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7897
https://github.com/hail-is/hail/pull/7902:60,Deployability,update,updated,60,"cc: @cseed . It's not the nicest thing, but it's OK. I only updated `front_end.py`. If y'all like this solution, I can go carefully fix up the rest, but you gotta be careful with this about global state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7902
https://github.com/hail-is/hail/pull/7903:195,Safety,avoid,avoid,195,"Ready to review once copy non-staged pr goes in. Left a few FIXME notes where signatures exist that rely entirely on region because of now-removed ptype regions. I elected to keep these as is to avoid complicating the review, but I think if we plan to keep such unused args around, we should document why. Related to: https://github.com/hail-is/hail/issues/7826",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7903
https://github.com/hail-is/hail/pull/7904:775,Availability,ping,ping,775,"This is mostly straightforward, except in the case of PBinary and PString, where I elected to move static methods to instance methods. This was done because these methods completely depend on the PType, and having them as static methods prevents use of non-canonical versions of these methods (regardless of where they are). This includes functions like allocate, which deal with memory layout, and therefore must be configurable by ptype. Places where these static methods are used often include places where a PString or PBinary are passed around. Will finish this up after I get back most likely, or we can punt on the PStirng/PBinary issue for later (but I think it's worth doing now for the reasons outlined above). Stacked on https://github.com/hail-is/hail/pull/7903; ping @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904
https://github.com/hail-is/hail/pull/7904:354,Energy Efficiency,allocate,allocate,354,"This is mostly straightforward, except in the case of PBinary and PString, where I elected to move static methods to instance methods. This was done because these methods completely depend on the PType, and having them as static methods prevents use of non-canonical versions of these methods (regardless of where they are). This includes functions like allocate, which deal with memory layout, and therefore must be configurable by ptype. Places where these static methods are used often include places where a PString or PBinary are passed around. Will finish this up after I get back most likely, or we can punt on the PStirng/PBinary issue for later (but I think it's worth doing now for the reasons outlined above). Stacked on https://github.com/hail-is/hail/pull/7903; ping @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904
https://github.com/hail-is/hail/pull/7904:182,Integrability,depend,depend,182,"This is mostly straightforward, except in the case of PBinary and PString, where I elected to move static methods to instance methods. This was done because these methods completely depend on the PType, and having them as static methods prevents use of non-canonical versions of these methods (regardless of where they are). This includes functions like allocate, which deal with memory layout, and therefore must be configurable by ptype. Places where these static methods are used often include places where a PString or PBinary are passed around. Will finish this up after I get back most likely, or we can punt on the PStirng/PBinary issue for later (but I think it's worth doing now for the reasons outlined above). Stacked on https://github.com/hail-is/hail/pull/7903; ping @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904
https://github.com/hail-is/hail/pull/7904:417,Modifiability,config,configurable,417,"This is mostly straightforward, except in the case of PBinary and PString, where I elected to move static methods to instance methods. This was done because these methods completely depend on the PType, and having them as static methods prevents use of non-canonical versions of these methods (regardless of where they are). This includes functions like allocate, which deal with memory layout, and therefore must be configurable by ptype. Places where these static methods are used often include places where a PString or PBinary are passed around. Will finish this up after I get back most likely, or we can punt on the PStirng/PBinary issue for later (but I think it's worth doing now for the reasons outlined above). Stacked on https://github.com/hail-is/hail/pull/7903; ping @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904
https://github.com/hail-is/hail/pull/7905:583,Availability,ERROR,ERROR,583,fixes. ```; + gcloud -q auth activate-service-account --key-file=/test-gsa-key/key.json; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + set +e; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7890-default-b9qytu01zuim' '--format=value(name)' --project hail-vdc; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; Deleted [https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch-worker-pr-7890-default-b9qytu01zuim-15e3g].; ERROR: (gcloud.compute.instances.delete) Could not fetch resource:; - The resource 'projects/hail-vdc/zones/us-central1-a/instances/batch-worker-pr-7890-default-b9qytu01zuim-hr6b9' was not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7905
https://github.com/hail-is/hail/pull/7905:66,Testability,test,test-gsa-key,66,fixes. ```; + gcloud -q auth activate-service-account --key-file=/test-gsa-key/key.json; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + set +e; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7890-default-b9qytu01zuim' '--format=value(name)' --project hail-vdc; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; Deleted [https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch-worker-pr-7890-default-b9qytu01zuim-15e3g].; ERROR: (gcloud.compute.instances.delete) Could not fetch resource:; - The resource 'projects/hail-vdc/zones/us-central1-a/instances/batch-worker-pr-7890-default-b9qytu01zuim-hr6b9' was not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7905
https://github.com/hail-is/hail/pull/7905:133,Testability,test,test-,133,fixes. ```; + gcloud -q auth activate-service-account --key-file=/test-gsa-key/key.json; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + set +e; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7890-default-b9qytu01zuim' '--format=value(name)' --project hail-vdc; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; Deleted [https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch-worker-pr-7890-default-b9qytu01zuim-15e3g].; ERROR: (gcloud.compute.instances.delete) Could not fetch resource:; - The resource 'projects/hail-vdc/zones/us-central1-a/instances/batch-worker-pr-7890-default-b9qytu01zuim-hr6b9' was not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7905
https://github.com/hail-is/hail/pull/7907:23,Deployability,deploy,deploy,23,I tested this with dev deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7907
https://github.com/hail-is/hail/pull/7907:2,Testability,test,tested,2,I tested this with dev deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7907
https://github.com/hail-is/hail/pull/7910:30,Deployability,deploy,deploy,30,I want to do testing with dev deploy. Putting this up so I can get feedback.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7910
https://github.com/hail-is/hail/pull/7910:13,Testability,test,testing,13,I want to do testing with dev deploy. Putting this up so I can get feedback.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7910
https://github.com/hail-is/hail/pull/7910:67,Usability,feedback,feedback,67,I want to do testing with dev deploy. Putting this up so I can get feedback.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7910
https://github.com/hail-is/hail/pull/7913:33,Usability,responsiv,responsive,33,This should put the issue of non-responsive cancels to rest.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7913
https://github.com/hail-is/hail/pull/7915:78,Testability,test,test,78,We'd mixed up nullable and required. The default for nullable is False. Added test. This fixes the 500 Konrad ran into.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915
https://github.com/hail-is/hail/pull/7916:263,Deployability,update,update,263,"I liked your diff idea, so I added a new file: batch/sql/estimated-current.txt. This is meant to be the SQL we'd use for initial.sql if we recreated the batch database. It should have collective migrations applied to it. So when we add a new migration, we should update the estimated current which will give informative documentation for the current change. I use ""estimated"" and ""txt"" because it isn't tested or validated in any way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7916
https://github.com/hail-is/hail/pull/7916:413,Security,validat,validated,413,"I liked your diff idea, so I added a new file: batch/sql/estimated-current.txt. This is meant to be the SQL we'd use for initial.sql if we recreated the batch database. It should have collective migrations applied to it. So when we add a new migration, we should update the estimated current which will give informative documentation for the current change. I use ""estimated"" and ""txt"" because it isn't tested or validated in any way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7916
https://github.com/hail-is/hail/pull/7916:403,Testability,test,tested,403,"I liked your diff idea, so I added a new file: batch/sql/estimated-current.txt. This is meant to be the SQL we'd use for initial.sql if we recreated the batch database. It should have collective migrations applied to it. So when we add a new migration, we should update the estimated current which will give informative documentation for the current change. I use ""estimated"" and ""txt"" because it isn't tested or validated in any way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7916
https://github.com/hail-is/hail/pull/7917:57,Deployability,deploy,deploy,57,"I think this will work, but I haven't tested it with dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7917
https://github.com/hail-is/hail/pull/7917:38,Testability,test,tested,38,"I think this will work, but I haven't tested it with dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7917
https://github.com/hail-is/hail/issues/7921:899,Testability,assert,assert,899,"Current `getNestedElementPTypesOfSameType` method calls the canonical constructor of each PType it matches on. 2 options: 1) keep the implementation mostly as is, but call `ptype.copy` which calls the appropriate concrete constructor without requiring reflection; 2) use the CastRename (pType.deepRename) pattern. GIven that we've chosen in the case of CastRename not to follow the `getNestedElementPTypesOfSameType` pattern, I think #2 is the more constant option w.r.t our codebase: . default implementation:. ```scala; def unify(ptypes: Seq[PType]) =; ptypes.head.setRequired(ptypes.forall(_.required)); ```. On PCanonicalArray; ```scala; override def unify(ptypes: Seq[PType]) = {; val et = unify(ptypes.map(_.asInstanceOf[PArray].elementType); PCanonicalArray(et, ptypes.forall(_.required)); }; ```. Called from InferPType:. ```scala; def getNestedElementPTypes(ptypes: Seq[PType]): PType = {; assert(ptypes.forall(_.virtualType.isOfType(ptypes.head.virtualType))); ptypes.head.unify(ptypes: Seq[PType]); }; ```. This is necessary for non-canonical physical types to work within the InferPTypes pass. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7921
https://github.com/hail-is/hail/issues/7926:52,Testability,stub,stub,52,"As it stands, assumes canonical-only ptypes. Should stub this.virtualType == that.virtualType, or remove it altogether in favor of having callers use the comparison directly. Once requiredeness removed from virtual types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7926
https://github.com/hail-is/hail/issues/7928:23,Integrability,interface,interface,23,"PBaseStruct becomes an interface with only implementations that are re-parameterizations of its abstract methods. PStruct and PTuple inherit. PCanonicalStruct gets the PBaseStruct implementations, and PCanonicalTuple implements its concrete methods by calling PCanonicalStruct's.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7928
https://github.com/hail-is/hail/issues/7928:71,Modifiability,parameteriz,parameterizations,71,"PBaseStruct becomes an interface with only implementations that are re-parameterizations of its abstract methods. PStruct and PTuple inherit. PCanonicalStruct gets the PBaseStruct implementations, and PCanonicalTuple implements its concrete methods by calling PCanonicalStruct's.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7928
https://github.com/hail-is/hail/issues/7928:133,Modifiability,inherit,inherit,133,"PBaseStruct becomes an interface with only implementations that are re-parameterizations of its abstract methods. PStruct and PTuple inherit. PCanonicalStruct gets the PBaseStruct implementations, and PCanonicalTuple implements its concrete methods by calling PCanonicalStruct's.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7928
https://github.com/hail-is/hail/issues/7930:411,Deployability,Update,Updates,411,"Functions like `sample_qc` and `variant_qc` do not produce the expected results on sex chromosomes for samples represented as diploid homozygotes or missing. To me, it seems like the right solution is to represent XY individuals as haploid on X and Y, and XX individuals as 0-ploid (but called) on Y. To my estimation, variant_qc will mostly compute *the right thing*™ if the data is represented in such a way. Updates to the call stats aggregator to count by ploidy may produce QoL improvements in this world as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7930
https://github.com/hail-is/hail/pull/7932:189,Availability,error,errors,189,I followed the rabbit hole form https://github.com/hail-is/hail/pull/7922 and was a bit concerned that we weren't verifying the stream met our expectations. I don't think we need to handle errors more gracefully (these assertions should only fail on malformed files).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7932
https://github.com/hail-is/hail/pull/7932:219,Testability,assert,assertions,219,I followed the rabbit hole form https://github.com/hail-is/hail/pull/7922 and was a bit concerned that we weren't verifying the stream met our expectations. I don't think we need to handle errors more gracefully (these assertions should only fail on malformed files).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7932
https://github.com/hail-is/hail/pull/7933:920,Deployability,update,update,920,"ured out the ""right"" way to do cancellation. I introduce the following notions:; - a job is cancellable if it is ready or running, it hasn't been cancelled, but if the batch is cancelled, it will be cancelled (not always_run),; - a job is runnable if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:1182,Deployability,update,update,1182,"atch is cancelled, it will be cancelled (not always_run),; - a job is runnable if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:1717,Deployability,update,updated,1717,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:639,Energy Efficiency,schedul,scheduler,639,"I finally figured out the ""right"" way to do cancellation. I introduce the following notions:; - a job is cancellable if it is ready or running, it hasn't been cancelled, but if the batch is cancelled, it will be cancelled (not always_run),; - a job is runnable if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:690,Energy Efficiency,schedul,scheduler,690,"I finally figured out the ""right"" way to do cancellation. I introduce the following notions:; - a job is cancellable if it is ready or running, it hasn't been cancelled, but if the batch is cancelled, it will be cancelled (not always_run),; - a job is runnable if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:940,Energy Efficiency,efficient,efficiently,940,"ured out the ""right"" way to do cancellation. I introduce the following notions:; - a job is cancellable if it is ready or running, it hasn't been cancelled, but if the batch is cancelled, it will be cancelled (not always_run),; - a job is runnable if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:1285,Energy Efficiency,schedul,scheduler,1285,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:1769,Energy Efficiency,schedul,scheduler,1769,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:2130,Energy Efficiency,schedul,scheduler,2130,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:1865,Performance,load,load,1865,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:1927,Testability,test,testing,1927,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7933:2046,Testability,test,tests,2046,"le if it is ready but has not been cancelled. Now we aim to incremental maintain the following information:. Globally:; - runnable jobs and cores. Per user:; - runnable and running jobs and cores,; - running cancelled jobs, and; - ready cancelled jobs. The global runnable cores are needed by the instance pool controller. The per-user stats are needed by the three threads of the scheduler:; - for the fair share allocator and the scheduler,; - to cancel running jobs on workers that have been cancelled (because the batch was cancelled),; - to cancel ready jobs that have been cancelled (either because the batch was cancelled or a parent failed). In order to update these values efficiently when a batch is cancelled, we also track in `batch_cancellable_resources` table, per batch:; - cancellable ready jobs and cores,; - cancellable running jobs and cores.; I added a `cancel_batch` procedure that uses these values to update ready_cores and user_resources when a batch is cancelled. I also reorganized the threads of the scheduler. Each one uses the above structures to compute a fair share for each user of work to do in a give iteration (dividing up 1000 tasks, with a per-user min of 20). Those tasks are then executed with 100-way parallelism. Other changes:; - I added a recompute_incremental procedure for recomputing all the incremental structures,; - I added a batches.state field (open, running or complete) and removed the closed column,; - I updated the batch and jobs indexes to make sure all scheduler queries are probably indexed. This isn't the case right now and we're seeing a lot of load on the database because of it. I'm going to do some more testing and possibly rename some stuff, but it is passing and the incremental structures all line up at the end of the tests. The only issue I see is I loop through all running batches for a user in the scheduler. If a user submits many many small batches, this could be an issue. I plan to address this in a later PR. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933
https://github.com/hail-is/hail/pull/7934:0,Testability,Benchmark,Benchmark,0,Benchmark:; ```; $ hail-bench compare /tmp/before.json /tmp/after.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; write_profile_mt 100.4% 31.130 31.249; ----------------------; Geometric mean: 100.4%; Simple mean: 100.4%; Median: 100.4%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7934
https://github.com/hail-is/hail/pull/7934:211,Usability,Simpl,Simple,211,Benchmark:; ```; $ hail-bench compare /tmp/before.json /tmp/after.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; write_profile_mt 100.4% 31.130 31.249; ----------------------; Geometric mean: 100.4%; Simple mean: 100.4%; Median: 100.4%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7934
https://github.com/hail-is/hail/pull/7939:39,Testability,log,logs,39,"I use text search for ""fail"" in the CI logs, and this always pops up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7939
https://github.com/hail-is/hail/pull/7940:56,Usability,feedback,feedback,56,"Assigned Tim since he has looked at this a bit, open to feedback from whoever though. . Admittedly, there is still a good amount of white space on second page right hand side, and the bottom of the first page could be condensed to buy us more space, but I want to publish what's currently here and take a break from working on it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7940
https://github.com/hail-is/hail/pull/7942:91,Availability,ERROR,ERROR,91,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7942
https://github.com/hail-is/hail/pull/7942:169,Availability,error,error,169,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7942
https://github.com/hail-is/hail/pull/7942:42,Deployability,install,install,42,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7942
https://github.com/hail-is/hail/pull/7942:398,Deployability,install,installed,398,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7942
https://github.com/hail-is/hail/pull/7942:436,Deployability,install,install,436,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7942
https://github.com/hail-is/hail/pull/7942:346,Modifiability,plugin,plugins,346,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7942
https://github.com/hail-is/hail/pull/7942:464,Testability,test,tests,464,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7942
https://github.com/hail-is/hail/pull/7943:5,Modifiability,refactor,refactor,5,Also refactor BGEN ptype logic. stacked on #7941,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7943
https://github.com/hail-is/hail/pull/7943:25,Testability,log,logic,25,Also refactor BGEN ptype logic. stacked on #7941,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7943
https://github.com/hail-is/hail/pull/7944:0,Testability,Test,Tested,0,"Tested on my laptop, works now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7944
https://github.com/hail-is/hail/pull/7948:1043,Testability,test,test,1043,"Since #5913, `__getattr__` for ArrayStructExpressions and SetStructExpressions calls `__getitem__` so that collection expressions behave more like tables. However, this means that when the requested attribute is not a field of the collection's elements, `__getattr__` throws a KeyError when it [should throw an AttributeError](https://docs.python.org/3/reference/datamodel.html#object.__getattr__). This breaks `hasattr`, since it [checks whether `getattr` raises an AttributeError](https://docs.python.org/3/library/functions.html#hasattr). ; ```python; hasattr(hl.struct(foo=""bar""), ""someattribute""); # False. hasattr(hl.literal([hl.struct(foo=""bar"")]), ""someattribute""); # KeyError: StructExpression instance has no field 'someattribute'; # Hint: use 'describe()' to show the names of all data fields.; ```. This changes `__getattr__` to catch the KeyError thrown by `__getitem__` and throw an AttributeError instead. ---. I found this because the broken `hasattr` prevents Array/SetStructExpressions from being used in pytest parametrized test cases (parametrize calls `hasattr(val, ""__name__"")`).; ```python; @pytest.mark.parametrize(; ""i,o"", [; (hl.literal([hl.utils.Struct(foo=1)]),; [hl.utils.Struct(foo=1)]),; ],; ); def test_parametrize(i, o):; assert hl.eval(i) == o. # KeyError: StructExpression instance has no field '__name__'; # Hint: use 'describe()' to show the names of all data fields.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7948
https://github.com/hail-is/hail/pull/7948:1255,Testability,assert,assert,1255,"Since #5913, `__getattr__` for ArrayStructExpressions and SetStructExpressions calls `__getitem__` so that collection expressions behave more like tables. However, this means that when the requested attribute is not a field of the collection's elements, `__getattr__` throws a KeyError when it [should throw an AttributeError](https://docs.python.org/3/reference/datamodel.html#object.__getattr__). This breaks `hasattr`, since it [checks whether `getattr` raises an AttributeError](https://docs.python.org/3/library/functions.html#hasattr). ; ```python; hasattr(hl.struct(foo=""bar""), ""someattribute""); # False. hasattr(hl.literal([hl.struct(foo=""bar"")]), ""someattribute""); # KeyError: StructExpression instance has no field 'someattribute'; # Hint: use 'describe()' to show the names of all data fields.; ```. This changes `__getattr__` to catch the KeyError thrown by `__getitem__` and throw an AttributeError instead. ---. I found this because the broken `hasattr` prevents Array/SetStructExpressions from being used in pytest parametrized test cases (parametrize calls `hasattr(val, ""__name__"")`).; ```python; @pytest.mark.parametrize(; ""i,o"", [; (hl.literal([hl.utils.Struct(foo=1)]),; [hl.utils.Struct(foo=1)]),; ],; ); def test_parametrize(i, o):; assert hl.eval(i) == o. # KeyError: StructExpression instance has no field '__name__'; # Hint: use 'describe()' to show the names of all data fields.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7948
https://github.com/hail-is/hail/pull/7951:8,Testability,benchmark,benchmarks,8,We need benchmarks for densification before we merge this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7951
https://github.com/hail-is/hail/pull/7952:340,Deployability,release,release,340,"This PR introduces a new test case that was failing. It fixes two problems:. 1. needed to bind `n` to `blockSize` so that it didn't serialize the whole IR. ; 2. needed to add references to the regions from producer to `targetRegion` to ensure that the filter test passes. . Question: Does this keep too much garbage in memory? Ideally, I'd release the references to the regions of my producer once I finished constructing the new `RegionValue`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952
https://github.com/hail-is/hail/pull/7952:25,Testability,test,test,25,"This PR introduces a new test case that was failing. It fixes two problems:. 1. needed to bind `n` to `blockSize` so that it didn't serialize the whole IR. ; 2. needed to add references to the regions from producer to `targetRegion` to ensure that the filter test passes. . Question: Does this keep too much garbage in memory? Ideally, I'd release the references to the regions of my producer once I finished constructing the new `RegionValue`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952
https://github.com/hail-is/hail/pull/7952:259,Testability,test,test,259,"This PR introduces a new test case that was failing. It fixes two problems:. 1. needed to bind `n` to `blockSize` so that it didn't serialize the whole IR. ; 2. needed to add references to the regions from producer to `targetRegion` to ensure that the filter test passes. . Question: Does this keep too much garbage in memory? Ideally, I'd release the references to the regions of my producer once I finished constructing the new `RegionValue`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952
https://github.com/hail-is/hail/pull/7953:146,Energy Efficiency,schedul,scheduler,146,Changes:; - move LoggingTimer to hailtop.utils; - add timer_description option to fetchall functions in gear.database; - add descriptions for all scheduler queries. We have to call the timer inside fetchall because it is an async generator. The other database functions can be timed by the client code with LoggingTimer directly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7953
https://github.com/hail-is/hail/pull/7953:17,Testability,Log,LoggingTimer,17,Changes:; - move LoggingTimer to hailtop.utils; - add timer_description option to fetchall functions in gear.database; - add descriptions for all scheduler queries. We have to call the timer inside fetchall because it is an async generator. The other database functions can be timed by the client code with LoggingTimer directly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7953
https://github.com/hail-is/hail/pull/7953:307,Testability,Log,LoggingTimer,307,Changes:; - move LoggingTimer to hailtop.utils; - add timer_description option to fetchall functions in gear.database; - add descriptions for all scheduler queries. We have to call the timer inside fetchall because it is an async generator. The other database functions can be timed by the client code with LoggingTimer directly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7953
https://github.com/hail-is/hail/pull/7955:31,Energy Efficiency,schedul,scheduler,31,"If there is no work to do, the scheduler threads should wait. This is likely causing the database load. run_if_changed isn't waiting if there is no work to do, so all three threads are spinning as fast as possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7955
https://github.com/hail-is/hail/pull/7955:98,Performance,load,load,98,"If there is no work to do, the scheduler threads should wait. This is likely causing the database load. run_if_changed isn't waiting if there is no work to do, so all three threads are spinning as fast as possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7955
https://github.com/hail-is/hail/pull/7957:108,Performance,perform,performance,108,128 MiB partitions are a much more reasonable default than 1MB. This will result; in better file read/write performance.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7957
https://github.com/hail-is/hail/pull/7958:25,Availability,fault,fault,25,"Tests fail, segmentation fault, issue in copyFromType added test in PBaseStruct, haven't solved yet, no remaining time today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958
https://github.com/hail-is/hail/pull/7958:0,Testability,Test,Tests,0,"Tests fail, segmentation fault, issue in copyFromType added test in PBaseStruct, haven't solved yet, no remaining time today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958
https://github.com/hail-is/hail/pull/7958:60,Testability,test,test,60,"Tests fail, segmentation fault, issue in copyFromType added test in PBaseStruct, haven't solved yet, no remaining time today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958
https://github.com/hail-is/hail/pull/7961:242,Energy Efficiency,schedul,scheduler,242,"Changes:; - Add WaitableSharedPool that allows you to submit a collection of jobs (which start running as soon as they are submitted) against a shared AsyncWorkerPool and then `wait()` at the end for all submitted jobs to complete.; - Use in scheduler threads. I also unscientifically tweaked a few of the settings:; - Use queue_size=100 in shared async worker pool in scheduler. This is 1x the parallelism, which seems like plenty. (Queue size is how much pending work to queue up before blocking.); - Drop work per iteration from 1000 to 300.; - Increase the number of database connections to 50. This is 1/2 the parallelism in the scheduler, the argument being half the work goes into servicing web requests (e.g. talking to workers) and the other half talking to the database. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7961
https://github.com/hail-is/hail/pull/7961:369,Energy Efficiency,schedul,scheduler,369,"Changes:; - Add WaitableSharedPool that allows you to submit a collection of jobs (which start running as soon as they are submitted) against a shared AsyncWorkerPool and then `wait()` at the end for all submitted jobs to complete.; - Use in scheduler threads. I also unscientifically tweaked a few of the settings:; - Use queue_size=100 in shared async worker pool in scheduler. This is 1x the parallelism, which seems like plenty. (Queue size is how much pending work to queue up before blocking.); - Drop work per iteration from 1000 to 300.; - Increase the number of database connections to 50. This is 1/2 the parallelism in the scheduler, the argument being half the work goes into servicing web requests (e.g. talking to workers) and the other half talking to the database. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7961
https://github.com/hail-is/hail/pull/7961:634,Energy Efficiency,schedul,scheduler,634,"Changes:; - Add WaitableSharedPool that allows you to submit a collection of jobs (which start running as soon as they are submitted) against a shared AsyncWorkerPool and then `wait()` at the end for all submitted jobs to complete.; - Use in scheduler threads. I also unscientifically tweaked a few of the settings:; - Use queue_size=100 in shared async worker pool in scheduler. This is 1x the parallelism, which seems like plenty. (Queue size is how much pending work to queue up before blocking.); - Drop work per iteration from 1000 to 300.; - Increase the number of database connections to 50. This is 1/2 the parallelism in the scheduler, the argument being half the work goes into servicing web requests (e.g. talking to workers) and the other half talking to the database. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7961
https://github.com/hail-is/hail/pull/7961:434,Performance,Queue,Queue,434,"Changes:; - Add WaitableSharedPool that allows you to submit a collection of jobs (which start running as soon as they are submitted) against a shared AsyncWorkerPool and then `wait()` at the end for all submitted jobs to complete.; - Use in scheduler threads. I also unscientifically tweaked a few of the settings:; - Use queue_size=100 in shared async worker pool in scheduler. This is 1x the parallelism, which seems like plenty. (Queue size is how much pending work to queue up before blocking.); - Drop work per iteration from 1000 to 300.; - Increase the number of database connections to 50. This is 1/2 the parallelism in the scheduler, the argument being half the work goes into servicing web requests (e.g. talking to workers) and the other half talking to the database. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7961
https://github.com/hail-is/hail/pull/7961:473,Performance,queue,queue,473,"Changes:; - Add WaitableSharedPool that allows you to submit a collection of jobs (which start running as soon as they are submitted) against a shared AsyncWorkerPool and then `wait()` at the end for all submitted jobs to complete.; - Use in scheduler threads. I also unscientifically tweaked a few of the settings:; - Use queue_size=100 in shared async worker pool in scheduler. This is 1x the parallelism, which seems like plenty. (Queue size is how much pending work to queue up before blocking.); - Drop work per iteration from 1000 to 300.; - Increase the number of database connections to 50. This is 1/2 the parallelism in the scheduler, the argument being half the work goes into servicing web requests (e.g. talking to workers) and the other half talking to the database. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7961
https://github.com/hail-is/hail/pull/7962:591,Availability,checkpoint,checkpoint,591,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962
https://github.com/hail-is/hail/pull/7962:708,Energy Efficiency,reduce,reduced,708,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962
https://github.com/hail-is/hail/pull/7962:370,Performance,perform,performing,370,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962
https://github.com/hail-is/hail/pull/7962:522,Performance,cache,caches,522,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962
https://github.com/hail-is/hail/pull/7962:666,Performance,cache,cache,666,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962
https://github.com/hail-is/hail/pull/7962:335,Safety,avoid,avoided,335,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962
https://github.com/hail-is/hail/pull/7963:123,Modifiability,variab,variables,123,"A few improvements:. 1. Show `n_partitions` in the ""Exploring Tables"" section. Fixes #7827 ; 2. Show `drop` in the ""Subset variables"" section (based on user question).; 3. Advertise `hl.plot.show` instead of `bokeh.io.show`. Fixes #7911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7963
https://github.com/hail-is/hail/pull/7966:81,Integrability,interface,interface,81,"Not sure if you wanted to change the name or anything, but I presume this is the interface you were thinking?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7966
https://github.com/hail-is/hail/pull/7971:185,Availability,reliab,reliable,185,"It is impossible to submit large batches without this. What happens? The timeout per request is 60s. We have 50 x 8MB = 400MB worth of requests in flight. That means the client needs a reliable sustained MINIMUM bandwidth of ~7MB/s to not time out. This doesn't seem reasonable. Without this change, Konrad wasn't able to submit a large batch (although it probably would have gone through eventually with enough retry/backoff). With this, 136K jobs took 2-3m to submit. FYI @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7971
https://github.com/hail-is/hail/pull/7971:73,Safety,timeout,timeout,73,"It is impossible to submit large batches without this. What happens? The timeout per request is 60s. We have 50 x 8MB = 400MB worth of requests in flight. That means the client needs a reliable sustained MINIMUM bandwidth of ~7MB/s to not time out. This doesn't seem reasonable. Without this change, Konrad wasn't able to submit a large batch (although it probably would have gone through eventually with enough retry/backoff). With this, 136K jobs took 2-3m to submit. FYI @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7971
https://github.com/hail-is/hail/pull/7972:34,Usability,clear,clear,34,"After watching batch a bit, it is clear 5m is way too long.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7972
https://github.com/hail-is/hail/issues/7973:52,Testability,test,tests,52,"When virtual type requiredeness is removed, fix all tests that pass a value for required. IRSuite in particular have tests of different virtual type requiredeness that should be fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7973
https://github.com/hail-is/hail/issues/7973:117,Testability,test,tests,117,"When virtual type requiredeness is removed, fix all tests that pass a value for required. IRSuite in particular have tests of different virtual type requiredeness that should be fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7973
https://github.com/hail-is/hail/pull/7975:416,Modifiability,config,config,416,"This larger benchmark shows clearer separation between the old pc-relate approach and the current one. this branch (which include's master's improvements); ```; 2020-01-27 13:16:20,975: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:18:12,886: INFO: burn in: 111.90s; 2020-01-27 13:19:56,255: INFO: run 1: 103.35s; 2020-01-27 13:21:46,801: INFO: run 2: 110.54s; 2020-01-27 13:23:39,147: INFO: run 3: 112.37s; {""config"": {""cores"": 1, ""version"": ""0.2.31-68d448411ab5"", ""timestamp"": ""2020-01-27 13:23:39.157122"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""pc_relate_big"", ""failed"": false, ""timed_out"": false, ""times"": [103.35172498200001, 110.53654034999997, 112.369625832]}]}; ```; before improvements `becbbc6d2` (run against this branch's new benchmark); ```; 2020-01-27 13:25:15,789: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:27:25,725: INFO: burn in: 129.92s; 2020-01-27 13:29:44,260: INFO: run 1: 138.48s; 2020-01-27 13:31:49,675: INFO: run 2: 125.40s; 2020-01-27 13:33:59,580: INFO: run 3: 129.86s; ```. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7975
https://github.com/hail-is/hail/pull/7975:12,Testability,benchmark,benchmark,12,"This larger benchmark shows clearer separation between the old pc-relate approach and the current one. this branch (which include's master's improvements); ```; 2020-01-27 13:16:20,975: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:18:12,886: INFO: burn in: 111.90s; 2020-01-27 13:19:56,255: INFO: run 1: 103.35s; 2020-01-27 13:21:46,801: INFO: run 2: 110.54s; 2020-01-27 13:23:39,147: INFO: run 3: 112.37s; {""config"": {""cores"": 1, ""version"": ""0.2.31-68d448411ab5"", ""timestamp"": ""2020-01-27 13:23:39.157122"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""pc_relate_big"", ""failed"": false, ""timed_out"": false, ""times"": [103.35172498200001, 110.53654034999997, 112.369625832]}]}; ```; before improvements `becbbc6d2` (run against this branch's new benchmark); ```; 2020-01-27 13:25:15,789: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:27:25,725: INFO: burn in: 129.92s; 2020-01-27 13:29:44,260: INFO: run 1: 138.48s; 2020-01-27 13:31:49,675: INFO: run 2: 125.40s; 2020-01-27 13:33:59,580: INFO: run 3: 129.86s; ```. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7975
https://github.com/hail-is/hail/pull/7975:537,Testability,benchmark,benchmarks,537,"This larger benchmark shows clearer separation between the old pc-relate approach and the current one. this branch (which include's master's improvements); ```; 2020-01-27 13:16:20,975: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:18:12,886: INFO: burn in: 111.90s; 2020-01-27 13:19:56,255: INFO: run 1: 103.35s; 2020-01-27 13:21:46,801: INFO: run 2: 110.54s; 2020-01-27 13:23:39,147: INFO: run 3: 112.37s; {""config"": {""cores"": 1, ""version"": ""0.2.31-68d448411ab5"", ""timestamp"": ""2020-01-27 13:23:39.157122"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""pc_relate_big"", ""failed"": false, ""timed_out"": false, ""times"": [103.35172498200001, 110.53654034999997, 112.369625832]}]}; ```; before improvements `becbbc6d2` (run against this branch's new benchmark); ```; 2020-01-27 13:25:15,789: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:27:25,725: INFO: burn in: 129.92s; 2020-01-27 13:29:44,260: INFO: run 1: 138.48s; 2020-01-27 13:31:49,675: INFO: run 2: 125.40s; 2020-01-27 13:33:59,580: INFO: run 3: 129.86s; ```. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7975
https://github.com/hail-is/hail/pull/7975:751,Testability,benchmark,benchmark,751,"This larger benchmark shows clearer separation between the old pc-relate approach and the current one. this branch (which include's master's improvements); ```; 2020-01-27 13:16:20,975: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:18:12,886: INFO: burn in: 111.90s; 2020-01-27 13:19:56,255: INFO: run 1: 103.35s; 2020-01-27 13:21:46,801: INFO: run 2: 110.54s; 2020-01-27 13:23:39,147: INFO: run 3: 112.37s; {""config"": {""cores"": 1, ""version"": ""0.2.31-68d448411ab5"", ""timestamp"": ""2020-01-27 13:23:39.157122"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""pc_relate_big"", ""failed"": false, ""timed_out"": false, ""times"": [103.35172498200001, 110.53654034999997, 112.369625832]}]}; ```; before improvements `becbbc6d2` (run against this branch's new benchmark); ```; 2020-01-27 13:25:15,789: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:27:25,725: INFO: burn in: 129.92s; 2020-01-27 13:29:44,260: INFO: run 1: 138.48s; 2020-01-27 13:31:49,675: INFO: run 2: 125.40s; 2020-01-27 13:33:59,580: INFO: run 3: 129.86s; ```. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7975
https://github.com/hail-is/hail/pull/7975:28,Usability,clear,clearer,28,"This larger benchmark shows clearer separation between the old pc-relate approach and the current one. this branch (which include's master's improvements); ```; 2020-01-27 13:16:20,975: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:18:12,886: INFO: burn in: 111.90s; 2020-01-27 13:19:56,255: INFO: run 1: 103.35s; 2020-01-27 13:21:46,801: INFO: run 2: 110.54s; 2020-01-27 13:23:39,147: INFO: run 3: 112.37s; {""config"": {""cores"": 1, ""version"": ""0.2.31-68d448411ab5"", ""timestamp"": ""2020-01-27 13:23:39.157122"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""pc_relate_big"", ""failed"": false, ""timed_out"": false, ""times"": [103.35172498200001, 110.53654034999997, 112.369625832]}]}; ```; before improvements `becbbc6d2` (run against this branch's new benchmark); ```; 2020-01-27 13:25:15,789: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:27:25,725: INFO: burn in: 129.92s; 2020-01-27 13:29:44,260: INFO: run 1: 138.48s; 2020-01-27 13:31:49,675: INFO: run 2: 125.40s; 2020-01-27 13:33:59,580: INFO: run 3: 129.86s; ```. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7975
https://github.com/hail-is/hail/pull/7978:96,Modifiability,config,configured,96,"These changes enable hailctl clusters to work correctly in an Broad GCP Security Best Practices configured project, with minor hail-specific set-up. See further details in team chat.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7978
https://github.com/hail-is/hail/pull/7978:72,Security,Secur,Security,72,"These changes enable hailctl clusters to work correctly in an Broad GCP Security Best Practices configured project, with minor hail-specific set-up. See further details in team chat.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7978
https://github.com/hail-is/hail/issues/7979:12,Modifiability,rewrite,rewrite,12,"In order to rewrite `linear_regression_rows`, I'm going to need a way to get data out of ndarray type. Need something like https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.html . I'm going to break with the numpy matching though and call it `to_array`, since hail doesn't use lists.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7979
https://github.com/hail-is/hail/pull/7981:41,Performance,latency,latency,41,- long running jobs are causing too much latency for interactive stuff (e.g. CI); - overscheduling core calculation wasn't behaving how we wanted,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7981
https://github.com/hail-is/hail/pull/7986:126,Testability,test,testing,126,"I accidentally got the dimensions wrong in one of the `matmulShape` cases, causing this issue: #7982. This didn't come out in testing because in my 2 dimensional by 1 dimensional ndarray test, I used a square 2 dimensional array, so the shape was the same on either side. This PR adds another test and makes the fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7986
https://github.com/hail-is/hail/pull/7986:187,Testability,test,test,187,"I accidentally got the dimensions wrong in one of the `matmulShape` cases, causing this issue: #7982. This didn't come out in testing because in my 2 dimensional by 1 dimensional ndarray test, I used a square 2 dimensional array, so the shape was the same on either side. This PR adds another test and makes the fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7986
https://github.com/hail-is/hail/pull/7986:293,Testability,test,test,293,"I accidentally got the dimensions wrong in one of the `matmulShape` cases, causing this issue: #7982. This didn't come out in testing because in my 2 dimensional by 1 dimensional ndarray test, I used a square 2 dimensional array, so the shape was the same on either side. This PR adds another test and makes the fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7986
https://github.com/hail-is/hail/issues/7988:761,Energy Efficiency,allocate,allocate,761,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:805,Energy Efficiency,allocate,allocate,805,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:871,Energy Efficiency,allocate,allocate,871,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:903,Energy Efficiency,Allocate,Allocated,903,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:1106,Energy Efficiency,allocate,allocate,1106,"ypes are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete implementations (canonical/non). PString. - Concrete implementations (canonical/non). PBinary. - Concrete implementations (canonical/non). ## <a name=""complex-ptypes""></a> Complex PTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:1152,Energy Efficiency,allocate,allocate,1152,"ypes are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete implementations (canonical/non). PString. - Concrete implementations (canonical/non). PBinary. - Concrete implementations (canonical/non). ## <a name=""complex-ptypes""></a> Complex PTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:1320,Energy Efficiency,allocate,allocated,1320,"ypes are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete implementations (canonical/non). PString. - Concrete implementations (canonical/non). PBinary. - Concrete implementations (canonical/non). ## <a name=""complex-ptypes""></a> Complex PTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:3727,Energy Efficiency,Allocate,Allocates,3727,"troduced they will follow the strucutre outlined for non-primitive types. PFloat32. - Represents a 4 byte float. PFloat64. - Represents an 8 byte float. PInt32. - Represents a 4 byte integer. PInt64. - Represents an 8 byte integer. PVoid. <br/>. # Common methods. ```scala; def constructAtAddress(mb: MethodBuilder, addr: Code[Long], region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Unit]; def constructAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4031,Energy Efficiency,allocate,allocate,4031,">. # Common methods. ```scala; def constructAtAddress(mb: MethodBuilder, addr: Code[Long], region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Unit]; def constructAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4073,Energy Efficiency,allocate,allocate,4073,"mb: MethodBuilder, addr: Code[Long], region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Unit]; def constructAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementInd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4128,Energy Efficiency,allocate,allocate,4128," PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Unit]; def constructAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the elem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4204,Energy Efficiency,Allocate,Allocate,4204,"uctAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4505,Energy Efficiency,allocate,allocated,4505,"de[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:8455,Energy Efficiency,allocate,allocate,8455,"s: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]; ```. - Interconvert between column and row major. ```scala; def construct(flags: Code[Int], offset: Code[Int], shapeBuilder: (StagedRegionValueBuilder => Code[Unit]),; stridesBuilder: (StagedRegionValueBuilder => Code[Unit]), data: Code[Long], mb: MethodBuilder): Code[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:8491,Energy Efficiency,allocate,allocate,8491,"s: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]; ```. - Interconvert between column and row major. ```scala; def construct(flags: Code[Int], offset: Code[Int], shapeBuilder: (StagedRegionValueBuilder => Code[Unit]),; stridesBuilder: (StagedRegionValueBuilder => Code[Unit]), data: Code[Long], mb: MethodBuilder): Code[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:8542,Energy Efficiency,Allocate,Allocate,8542,"orAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]; ```. - Interconvert between column and row major. ```scala; def construct(flags: Code[Int], offset: Code[Int], shapeBuilder: (StagedRegionValueBuilder => Code[Unit]),; stridesBuilder: (StagedRegionValueBuilder => Code[Unit]), data: Code[Long], mb: MethodBuilder): Code[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField(address: Long, fieldIdx: Int): Long; def storeField(address: Code[Long], fieldIdx:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:799,Integrability,wrap,wrap,799,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:1120,Integrability,wrap,wrapping,1120,"ypes are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete implementations (canonical/non). PString. - Concrete implementations (canonical/non). PBinary. - Concrete implementations (canonical/non). ## <a name=""complex-ptypes""></a> Complex PTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:7328,Integrability,depend,dependent,7328,"parray""></a> PNDArray. An abstract class for multidimensional arrays (tensors) that have a row-major or column-major layout. ## Core Methods. ```scala; val shape: StaticallyKnownField[PTuple, Long]; val strides: StaticallyKnownField[PTuple, Long]; ```. - Defines the tensor shape. ```scala; def loadElementToIRIntermediate(indices: Array[Code[Long]], ndAddress: Code[Long], mb: MethodBuilder): Code[_]; ```. - Load the element's primitive representation, as indexed by `indices`, which specifies the element index at every dimension in the PNDArray's shape. ```scala; def linearizeIndicesRowMajor(indices: Array[Code[Long]], shapeArray: Array[Code[Long]], mb: MethodBuilder): Code[Long]; ```. - Get the off-heap index of the element (since NDArray elements are stored as a 1D series of bytes off-heap). ```scala; def unlinearizeIndexRowMajor(index: Code[Long], shapeArray: Array[Code[Long]], mb: MethodBuilder): (Code[Unit], Array[Code[Long]]); ```. - Generate the index path that represents the virtual, shape-dependent index into an arbitrary tensor. ```scala; def copyRowMajorToColumnMajor(rowMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]; ```. - Interconvert between column and row major. ```scala; def construct(flags: Code[Int], offset: Code[Int], shapeBuilder: (StagedRegionValueBuilder => Code[Unit]),; stridesBuilder: (StagedRegionValueBuilder => Code[Unit]), data: Code[Long], mb: MethodBuilder): Code[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:356,Performance,perform,performance,356,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:823,Performance,perform,perform,823,"cc @tpoterba . Here because as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:976,Performance,load,loadAddress,976,"e as we discovered, dev forum has a relatively short editing window. Will post to dev forum when ""complete"" and ready for broader discussion. # What are Physical Types?. Physical types are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete imp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:1180,Performance,load,loading,1180,"ypes are the classes that manage in-memory representations of Hail Types (Virtual Types), for both staged and unstaged code. # Motivation:. - Improve performance by building specialized memory representations for data; - Make it easier for developers to work with in memory representations of Hail types. # Project technical goals:. - Remove requiredness from virtual types; - Implement at least one non-canonical physical type. # Relation to regions. The methods that take regions are those that construct a new in-memory representation (are either `def allocate` or convenience methods that wrap `allocate` and may perform some complex operations before calling `allocate`, e.g `copyFromType`). Allocated addresses may be read using static Region methods (e.g `Region.loadAddress`), because they are absolute memory addresses rather than relative to some region offset. Long-term, methods besides `allocate` and wrapping methods, which need to allocate (for instance lazy-loading BGEN data) will be given the ability to do so without taking region as an argument (values will be associated with the regions that allocated them). Namely, regions may be placed on the values that own them. # Physical Type organization. ## Constructible types. Every PType has a ""fundamentalType"", which is the is the constructible representation for that type. It is, by default equal to the PType itself, but this may not always be the case (e.g [ComplexPType](#complex-ptypes)). ## Collection PTypes. [PArray](#parray). - Concrete implementations (canonical/non). [PSet](#pset). - Concrete implementations (canonical/non). [PDict](#pdict). - Concrete implementations (canonical/non). [PNDArray](#pndict). - Concrete implementations (canonical/non). [PTuple](#ptuple). - Concrete implementations (canonical/non). PStruct. - Concrete implementations (canonical/non). PString. - Concrete implementations (canonical/non). PBinary. - Concrete implementations (canonical/non). ## <a name=""complex-ptypes""></a> Complex PTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:3396,Performance,Perform,Performs,3396,"different than any physical type arguments passed to it. PLocus. - Concrete implementations (canonical/non). PCall. - Concrete implementations (canonical/non). PInterval. - Concrete implementations (canonical/non). ## Primitive Types. While long-term these may have canonical and non-canonical types, that is outside the scope of this design document. When non-canonical primitives are introduced they will follow the strucutre outlined for non-primitive types. PFloat32. - Represents a 4 byte float. PFloat64. - Represents an 8 byte float. PInt32. - Represents a 4 byte integer. PInt64. - Represents an 8 byte integer. PVoid. <br/>. # Common methods. ```scala; def constructAtAddress(mb: MethodBuilder, addr: Code[Long], region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Unit]; def constructAtAddress(addr: Long, region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Unit; ```. - Constructs a new value at `addr`, from `srcAddrss`; - Performs a deep copy when `srcPType != this`, or when `forceDeep == true`. ```scala; def copyFromType(mb: MethodBuilder, region: Code[Region], srcPType: PType, srcAddress: Code[Long], forceDeep: Boolean): Code[Long] = ...; def copyFromType(region: Region, srcPType: PType, srcAddress: Long, forceDeep: Boolean): Long = ...; ```. - Allocates a new address and calls constructAtAddress; - For operations that can be shallow, returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4805,Performance,load,loadLength,4805,", returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4852,Performance,load,loadLength,4852,"array""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:4973,Performance,load,loadElement,4973," contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly differe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:5041,Performance,load,loadElement,5041,"cate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:5203,Performance,load,loads,5203,"r an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:6612,Performance,load,loadElementToIRIntermediate,6612,"damentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalDict. A PCanonicalArray-backed implementation of PDict. # <a name=""parray""></a> PNDArray. An abstract class for multidimensional arrays (tensors) that have a row-major or column-major layout. ## Core Methods. ```scala; val shape: StaticallyKnownField[PTuple, Long]; val strides: StaticallyKnownField[PTuple, Long]; ```. - Defines the tensor shape. ```scala; def loadElementToIRIntermediate(indices: Array[Code[Long]], ndAddress: Code[Long], mb: MethodBuilder): Code[_]; ```. - Load the element's primitive representation, as indexed by `indices`, which specifies the element index at every dimension in the PNDArray's shape. ```scala; def linearizeIndicesRowMajor(indices: Array[Code[Long]], shapeArray: Array[Code[Long]], mb: MethodBuilder): Code[Long]; ```. - Get the off-heap index of the element (since NDArray elements are stored as a 1D series of bytes off-heap). ```scala; def unlinearizeIndexRowMajor(index: Code[Long], shapeArray: Array[Code[Long]], mb: MethodBuilder): (Code[Unit], Array[Code[Long]]); ```. - Generate the index path that represents the virtual, shape-dependent index into an arbitrary tensor. ```scala; def copyRowMajorToColumnMajor(rowMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:6727,Performance,Load,Load,6727,"tation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalDict. A PCanonicalArray-backed implementation of PDict. # <a name=""parray""></a> PNDArray. An abstract class for multidimensional arrays (tensors) that have a row-major or column-major layout. ## Core Methods. ```scala; val shape: StaticallyKnownField[PTuple, Long]; val strides: StaticallyKnownField[PTuple, Long]; ```. - Defines the tensor shape. ```scala; def loadElementToIRIntermediate(indices: Array[Code[Long]], ndAddress: Code[Long], mb: MethodBuilder): Code[_]; ```. - Load the element's primitive representation, as indexed by `indices`, which specifies the element index at every dimension in the PNDArray's shape. ```scala; def linearizeIndicesRowMajor(indices: Array[Code[Long]], shapeArray: Array[Code[Long]], mb: MethodBuilder): Code[Long]; ```. - Get the off-heap index of the element (since NDArray elements are stored as a 1D series of bytes off-heap). ```scala; def unlinearizeIndexRowMajor(index: Code[Long], shapeArray: Array[Code[Long]], mb: MethodBuilder): (Code[Unit], Array[Code[Long]]); ```. - Generate the index path that represents the virtual, shape-dependent index into an arbitrary tensor. ```scala; def copyRowMajorToColumnMajor(rowMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]. def copyColumnMajorToRowMajor(colMajorAddress: Code[Long], targetAddress: Code[Long], nRows: Code[Long], nCols: Code[Long], mb: MethodBuilder): Code[Unit]; ```. - Interconvert between column and row major. ```scala; def construct(flags: Code[Int], offset: Code[In",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:9311,Performance,load,loadField,9311,"e[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField(address: Long, fieldIdx: Int): Long; def storeField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Store field at a given memory address; - (This does not exist yet, but should I believe). ## <a name=""ptuple"">PCanonicalTuple</a>. An immutible, fixed-length collection of ordered values (of possibly different types). Number of elements known statically, and just like PCanonicalStruct, elements are stored inline, rather than behind a pointer to a collection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:9362,Performance,load,loadField,9362,"e[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField(address: Long, fieldIdx: Int): Long; def storeField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Store field at a given memory address; - (This does not exist yet, but should I believe). ## <a name=""ptuple"">PCanonicalTuple</a>. An immutible, fixed-length collection of ordered values (of possibly different types). Number of elements known statically, and just like PCanonicalStruct, elements are stored inline, rather than behind a pointer to a collection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:9428,Performance,Load,Load,9428,"e[Long]; }; ```. - Construct the NDArray off-heap. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalNDArray. A PCanonicalArray-backed NDArray. # <a name=""ptuple"">PTuple</a>. An immutible, collection of ordered values, whose elements may be of different types. ## Core methods. ```scala; val _types: IndexedSeq[PTupleField]; ```. - The ordered representation of physical types that represent this collection. ```scala; def allocate(region: Region): Long; def allocate(region: Code[Region]): Code[Long]; ```. - Allocate enough memory off-heap to store the requested elements. ```scala; def initialize(address: Long, setMissing: Boolean = false): Unit; def stagedInitialize(address: Code[Long], setMissing: Boolean = false): Code[Unit]; ```; - Set element missingness and store element length. ```scala; def isFieldDefined(address: Long, fieldIdx: Int): Boolean; def isFieldDefined(address: Code[Long], fieldIdx: Code[Int]): Boolean; ```. ```scala; def setFieldMissing(address: Long, fieldIdx: Int): Unit; def setFieldMissing(address: Code[Long], fieldIdx: Int): Code[Unit]. def setFieldPresent(address: Long, fieldIdx: Int): Unit; def setFieldPresent(address: Code[Long], fieldIdx: Int): Code[Unit]; }; ```; - Set field present of missing at a given memory address. ```scala; def loadField(address: Long, fieldIdx: Int): Long; def loadField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Load field at a given memory address. ```scala; def storeField(address: Long, fieldIdx: Int): Long; def storeField(address: Code[Long], fieldIdx: Int): Code[Long]; ```; - Store field at a given memory address; - (This does not exist yet, but should I believe). ## <a name=""ptuple"">PCanonicalTuple</a>. An immutible, fixed-length collection of ordered values (of possibly different types). Number of elements known statically, and just like PCanonicalStruct, elements are stored inline, rather than behind a pointer to a collection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/issues/7988:5353,Security,access,accessed,5353,"g: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly different from keys) PType. ## Core Methods. ```scala; def elementType: PStruct; ```. - The PStruct representation of the key/value pair. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalDict. A PCanonicalArray-backed implementation of PDict. # <a name=""parray""></a> PNDArray. An abstract",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7988
https://github.com/hail-is/hail/pull/7989:168,Deployability,pipeline,pipelines,168,"We were reading the column values per row. This was bad. Introduce the LiftMeOut node that is evaluated in the; `InterpretNonCompilable` lowering pass. The benchmarked pipelines are still exponential in the number of; iterations of QC, which I will fix in a forthcoming PR. Before:. ```; 2020-01-29 12:36:38,135: INFO: [1/1] Running variant_and_sample_qc_nested_with_filters...; 2020-01-29 12:38:31,070: INFO: burn in: 112.93s; 2020-01-29 12:40:17,863: INFO: run 1: 106.79s; 2020-01-29 12:42:01,471: INFO: run 2: 103.61s; 2020-01-29 12:43:47,330: INFO: run 3: 105.86s; ```. After:; ```; 2020-01-29 12:30:18,619: INFO: [1/1] Running variant_and_sample_qc_nested_with_filters...; 2020-01-29 12:31:24,969: INFO: burn in: 66.34s; 2020-01-29 12:32:24,797: INFO: run 1: 59.82s; 2020-01-29 12:33:22,849: INFO: run 2: 58.05s; 2020-01-29 12:34:20,667: INFO: run 3: 57.81s; ```. SSDs are fast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7989
https://github.com/hail-is/hail/pull/7989:156,Testability,benchmark,benchmarked,156,"We were reading the column values per row. This was bad. Introduce the LiftMeOut node that is evaluated in the; `InterpretNonCompilable` lowering pass. The benchmarked pipelines are still exponential in the number of; iterations of QC, which I will fix in a forthcoming PR. Before:. ```; 2020-01-29 12:36:38,135: INFO: [1/1] Running variant_and_sample_qc_nested_with_filters...; 2020-01-29 12:38:31,070: INFO: burn in: 112.93s; 2020-01-29 12:40:17,863: INFO: run 1: 106.79s; 2020-01-29 12:42:01,471: INFO: run 2: 103.61s; 2020-01-29 12:43:47,330: INFO: run 3: 105.86s; ```. After:; ```; 2020-01-29 12:30:18,619: INFO: [1/1] Running variant_and_sample_qc_nested_with_filters...; 2020-01-29 12:31:24,969: INFO: burn in: 66.34s; 2020-01-29 12:32:24,797: INFO: run 1: 59.82s; 2020-01-29 12:33:22,849: INFO: run 2: 58.05s; 2020-01-29 12:34:20,667: INFO: run 3: 57.81s; ```. SSDs are fast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7989
https://github.com/hail-is/hail/pull/7990:18,Testability,log,logging,18,"Also increase the logging in the batch tests: set the log format, log when tests start/end.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990
https://github.com/hail-is/hail/pull/7990:39,Testability,test,tests,39,"Also increase the logging in the batch tests: set the log format, log when tests start/end.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990
https://github.com/hail-is/hail/pull/7990:54,Testability,log,log,54,"Also increase the logging in the batch tests: set the log format, log when tests start/end.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990
https://github.com/hail-is/hail/pull/7990:66,Testability,log,log,66,"Also increase the logging in the batch tests: set the log format, log when tests start/end.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990
https://github.com/hail-is/hail/pull/7990:75,Testability,test,tests,75,"Also increase the logging in the batch tests: set the log format, log when tests start/end.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990
https://github.com/hail-is/hail/pull/7992:60,Testability,test,testing,60,"Mac didn't show these, missed it. I need to create a better testing system for UI. . overflow: scroll can force the browser to show scrollbars even when they are not necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7992
https://github.com/hail-is/hail/pull/7994:26,Deployability,deploy,deploy,26,"FYI, I tested this with a deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7994
https://github.com/hail-is/hail/pull/7994:7,Testability,test,tested,7,"FYI, I tested this with a deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7994
https://github.com/hail-is/hail/pull/7996:906,Deployability,Pipeline,Pipeline,906,"This fixes a few of problems:; - Batch.jobs() was discarding the job json objects and returning an Job objects. This is not OK because we need the metadata from the json objects for efficiency (e.g. in CI PR/batch pages).; - We confused Job.status(), which is the job json object, with the status field of that object. In particular, we got that wrong in BatchFormatVersion.get_status_exit_code_duration in the call to Job.{exit_code, total_duration_msecs}. Add a test to check we got this right.; - Return the duration in msecs, not a string, because as an API, the string is useless. humanize it before sending it to the API. I think the batch client is (1) has a lot of legacy baggage that can be cleaned up, and (2) is massive overkill and can probably be cleaned up. It should probably have no objects, pass around job_id ints or (batch_id, job_id) tuples, and be a thin wrapper around request calls. Pipeline should be the high level API. Finally, I think we need a little document (just a comment) about the format of GET /batches/batch_id and GET /jobs/job_id. It's changed enough that it is getting a little hard to follow. I'm going to dev deploy to test the UI. Don't approve until it checks out, please. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7996
https://github.com/hail-is/hail/pull/7996:1150,Deployability,deploy,deploy,1150,"This fixes a few of problems:; - Batch.jobs() was discarding the job json objects and returning an Job objects. This is not OK because we need the metadata from the json objects for efficiency (e.g. in CI PR/batch pages).; - We confused Job.status(), which is the job json object, with the status field of that object. In particular, we got that wrong in BatchFormatVersion.get_status_exit_code_duration in the call to Job.{exit_code, total_duration_msecs}. Add a test to check we got this right.; - Return the duration in msecs, not a string, because as an API, the string is useless. humanize it before sending it to the API. I think the batch client is (1) has a lot of legacy baggage that can be cleaned up, and (2) is massive overkill and can probably be cleaned up. It should probably have no objects, pass around job_id ints or (batch_id, job_id) tuples, and be a thin wrapper around request calls. Pipeline should be the high level API. Finally, I think we need a little document (just a comment) about the format of GET /batches/batch_id and GET /jobs/job_id. It's changed enough that it is getting a little hard to follow. I'm going to dev deploy to test the UI. Don't approve until it checks out, please. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7996
https://github.com/hail-is/hail/pull/7996:876,Integrability,wrap,wrapper,876,"This fixes a few of problems:; - Batch.jobs() was discarding the job json objects and returning an Job objects. This is not OK because we need the metadata from the json objects for efficiency (e.g. in CI PR/batch pages).; - We confused Job.status(), which is the job json object, with the status field of that object. In particular, we got that wrong in BatchFormatVersion.get_status_exit_code_duration in the call to Job.{exit_code, total_duration_msecs}. Add a test to check we got this right.; - Return the duration in msecs, not a string, because as an API, the string is useless. humanize it before sending it to the API. I think the batch client is (1) has a lot of legacy baggage that can be cleaned up, and (2) is massive overkill and can probably be cleaned up. It should probably have no objects, pass around job_id ints or (batch_id, job_id) tuples, and be a thin wrapper around request calls. Pipeline should be the high level API. Finally, I think we need a little document (just a comment) about the format of GET /batches/batch_id and GET /jobs/job_id. It's changed enough that it is getting a little hard to follow. I'm going to dev deploy to test the UI. Don't approve until it checks out, please. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7996
https://github.com/hail-is/hail/pull/7996:464,Testability,test,test,464,"This fixes a few of problems:; - Batch.jobs() was discarding the job json objects and returning an Job objects. This is not OK because we need the metadata from the json objects for efficiency (e.g. in CI PR/batch pages).; - We confused Job.status(), which is the job json object, with the status field of that object. In particular, we got that wrong in BatchFormatVersion.get_status_exit_code_duration in the call to Job.{exit_code, total_duration_msecs}. Add a test to check we got this right.; - Return the duration in msecs, not a string, because as an API, the string is useless. humanize it before sending it to the API. I think the batch client is (1) has a lot of legacy baggage that can be cleaned up, and (2) is massive overkill and can probably be cleaned up. It should probably have no objects, pass around job_id ints or (batch_id, job_id) tuples, and be a thin wrapper around request calls. Pipeline should be the high level API. Finally, I think we need a little document (just a comment) about the format of GET /batches/batch_id and GET /jobs/job_id. It's changed enough that it is getting a little hard to follow. I'm going to dev deploy to test the UI. Don't approve until it checks out, please. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7996
https://github.com/hail-is/hail/pull/7996:1160,Testability,test,test,1160,"This fixes a few of problems:; - Batch.jobs() was discarding the job json objects and returning an Job objects. This is not OK because we need the metadata from the json objects for efficiency (e.g. in CI PR/batch pages).; - We confused Job.status(), which is the job json object, with the status field of that object. In particular, we got that wrong in BatchFormatVersion.get_status_exit_code_duration in the call to Job.{exit_code, total_duration_msecs}. Add a test to check we got this right.; - Return the duration in msecs, not a string, because as an API, the string is useless. humanize it before sending it to the API. I think the batch client is (1) has a lot of legacy baggage that can be cleaned up, and (2) is massive overkill and can probably be cleaned up. It should probably have no objects, pass around job_id ints or (batch_id, job_id) tuples, and be a thin wrapper around request calls. Pipeline should be the high level API. Finally, I think we need a little document (just a comment) about the format of GET /batches/batch_id and GET /jobs/job_id. It's changed enough that it is getting a little hard to follow. I'm going to dev deploy to test the UI. Don't approve until it checks out, please. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7996
https://github.com/hail-is/hail/pull/7997:19,Testability,test,test,19,I ran my migration test. Worked perfectly. I say this is ready to go.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7997
https://github.com/hail-is/hail/pull/7998:217,Testability,test,tests,217,"Scenario: we mark a ready job cancelled, and it doesn't change the free core counts. It may cause another job to cancellable or runnable (if it was always_run). This is another timing issue that was causing the batch tests to take longer than necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7998
https://github.com/hail-is/hail/pull/8000:334,Deployability,deploy,deployed,334,"Apologies, should have caught this. I think we can improve the UI testing picture. In particular, would be useful to have a staging area (we may already entirely have this with dev namespace, I would like to have a shared space for the reviewer (and myself) to play with the implementation issued in the PR, maybe a link in CI to the deployed web app?), test data (so we can see the table filled), and a local dev strategy with hot reloading (make a change, script recompiles and reloads your browser). I also need to be more familiar with developing behind dev namespaces. Before:; <img width=""1484"" alt=""Screenshot 2020-01-29 22 56 56"" src=""https://user-images.githubusercontent.com/5543229/73418742-a79baf80-42ea-11ea-990d-c7ce43660c90.png"">. After:; <img width=""1480"" alt=""Screenshot 2020-01-29 22 56 31"" src=""https://user-images.githubusercontent.com/5543229/73418726-9a7ec080-42ea-11ea-8453-7a0956dc3c67.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8000
https://github.com/hail-is/hail/pull/8000:66,Testability,test,testing,66,"Apologies, should have caught this. I think we can improve the UI testing picture. In particular, would be useful to have a staging area (we may already entirely have this with dev namespace, I would like to have a shared space for the reviewer (and myself) to play with the implementation issued in the PR, maybe a link in CI to the deployed web app?), test data (so we can see the table filled), and a local dev strategy with hot reloading (make a change, script recompiles and reloads your browser). I also need to be more familiar with developing behind dev namespaces. Before:; <img width=""1484"" alt=""Screenshot 2020-01-29 22 56 56"" src=""https://user-images.githubusercontent.com/5543229/73418742-a79baf80-42ea-11ea-990d-c7ce43660c90.png"">. After:; <img width=""1480"" alt=""Screenshot 2020-01-29 22 56 31"" src=""https://user-images.githubusercontent.com/5543229/73418726-9a7ec080-42ea-11ea-8453-7a0956dc3c67.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8000
https://github.com/hail-is/hail/pull/8000:354,Testability,test,test,354,"Apologies, should have caught this. I think we can improve the UI testing picture. In particular, would be useful to have a staging area (we may already entirely have this with dev namespace, I would like to have a shared space for the reviewer (and myself) to play with the implementation issued in the PR, maybe a link in CI to the deployed web app?), test data (so we can see the table filled), and a local dev strategy with hot reloading (make a change, script recompiles and reloads your browser). I also need to be more familiar with developing behind dev namespaces. Before:; <img width=""1484"" alt=""Screenshot 2020-01-29 22 56 56"" src=""https://user-images.githubusercontent.com/5543229/73418742-a79baf80-42ea-11ea-990d-c7ce43660c90.png"">. After:; <img width=""1480"" alt=""Screenshot 2020-01-29 22 56 31"" src=""https://user-images.githubusercontent.com/5543229/73418726-9a7ec080-42ea-11ea-8453-7a0956dc3c67.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8000
https://github.com/hail-is/hail/pull/8007:246,Integrability,interface,interface,246,"This just adds a convenience function to get the diagonal of a 2d ndarray. The numpy version offers more functionality when you have something greater than 2 dimensions, but I haven't needed that yet so starting with this. It won't be a breaking interface change to add more arguments to this function to support what numpy does.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8007
https://github.com/hail-is/hail/pull/8008:4,Testability,test,tests,4,"jvm tests don't actually trigger the condition, so need at test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008
https://github.com/hail-is/hail/pull/8008:59,Testability,test,test,59,"jvm tests don't actually trigger the condition, so need at test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008
https://github.com/hail-is/hail/issues/8009:143,Availability,error,error,143,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009
https://github.com/hail-is/hail/issues/8009:193,Availability,error,error,193,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009
https://github.com/hail-is/hail/issues/8009:934,Modifiability,config,configureAndCreateSparkContext,934,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009
https://github.com/hail-is/hail/issues/8009:368,Performance,load,loaded,368,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009
https://github.com/hail-is/hail/issues/8009:564,Performance,load,loaded,564,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009
https://github.com/hail-is/hail/issues/8010:0,Testability,Benchmark,Benchmark,0,"Benchmark methods:; ```python; @benchmark(args=profile_25.handle('mt')); def variant_and_sample_qc_nested_with_filters_2(mt_path):; mt = hl.read_matrix_table(mt_path); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .8); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .8); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .98); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .98); mt.count(). @benchmark(args=profile_25.handle('mt')); def variant_and_sample_qc_nested_with_filters_4(mt_path):; mt = hl.read_matrix_table(mt_path); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .8); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .8); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .98); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .98); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .99); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .99); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .999); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .999); mt.count(); ```; Runtime:; ```; 2020-01-29 14:26:59,755: INFO: [1/2] Running variant_and_sample_qc_nested_with_filters_2...; 2020-01-29 14:28:05,672: INFO: burn in: 65.91s; 2020-01-29 14:29:05,205: INFO: run 1: 59.53s; 2020-01-29 14:30:00,993: INFO: run 2: 55.79s; 2020-01-29 14:31:00,966: INFO: run 3: 59.97s; 2020-01-29 14:31:00,966: INFO: [2/2] Running variant_and_sample_qc_nested_with_filters_4...; 2020-01-29 14:51:21,937: INFO: burn in: 1220.90s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8010
https://github.com/hail-is/hail/issues/8010:32,Testability,benchmark,benchmark,32,"Benchmark methods:; ```python; @benchmark(args=profile_25.handle('mt')); def variant_and_sample_qc_nested_with_filters_2(mt_path):; mt = hl.read_matrix_table(mt_path); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .8); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .8); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .98); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .98); mt.count(). @benchmark(args=profile_25.handle('mt')); def variant_and_sample_qc_nested_with_filters_4(mt_path):; mt = hl.read_matrix_table(mt_path); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .8); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .8); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .98); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .98); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .99); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .99); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .999); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .999); mt.count(); ```; Runtime:; ```; 2020-01-29 14:26:59,755: INFO: [1/2] Running variant_and_sample_qc_nested_with_filters_2...; 2020-01-29 14:28:05,672: INFO: burn in: 65.91s; 2020-01-29 14:29:05,205: INFO: run 1: 59.53s; 2020-01-29 14:30:00,993: INFO: run 2: 55.79s; 2020-01-29 14:31:00,966: INFO: run 3: 59.97s; 2020-01-29 14:31:00,966: INFO: [2/2] Running variant_and_sample_qc_nested_with_filters_4...; 2020-01-29 14:51:21,937: INFO: burn in: 1220.90s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8010
https://github.com/hail-is/hail/issues/8010:483,Testability,benchmark,benchmark,483,"Benchmark methods:; ```python; @benchmark(args=profile_25.handle('mt')); def variant_and_sample_qc_nested_with_filters_2(mt_path):; mt = hl.read_matrix_table(mt_path); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .8); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .8); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .98); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .98); mt.count(). @benchmark(args=profile_25.handle('mt')); def variant_and_sample_qc_nested_with_filters_4(mt_path):; mt = hl.read_matrix_table(mt_path); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .8); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .8); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .98); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .98); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .99); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .99); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.call_rate >= .999); mt = hl.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.call_rate >= .999); mt.count(); ```; Runtime:; ```; 2020-01-29 14:26:59,755: INFO: [1/2] Running variant_and_sample_qc_nested_with_filters_2...; 2020-01-29 14:28:05,672: INFO: burn in: 65.91s; 2020-01-29 14:29:05,205: INFO: run 1: 59.53s; 2020-01-29 14:30:00,993: INFO: run 2: 55.79s; 2020-01-29 14:31:00,966: INFO: run 3: 59.97s; 2020-01-29 14:31:00,966: INFO: [2/2] Running variant_and_sample_qc_nested_with_filters_4...; 2020-01-29 14:51:21,937: INFO: burn in: 1220.90s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8010
https://github.com/hail-is/hail/issues/8011:531,Availability,error,error,531,"I know why it happened (was on an earlier version) but this is a general thing I've seen:; ```; Traceback (most recent call last):; File ""saige_pan_ancestry.py"", line 247, in <module>; main(args); File ""saige_pan_ancestry.py"", line 210, in main; n_threads=n_threads); File ""/home/konradk/ukb_common/utils/saige_pipeline.py"", line 238, in load_results_into_hail; load_data_task.always_run().depends_on(*tasks_to_hold); TypeError: 'TaskResourceFile' object is not callable; ```; say you did `Task.always_runn()` - it would give this error bc it creates a file and then can't call it as a function. Not sure what the long term solution is, but just thought I'd raise it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8011
https://github.com/hail-is/hail/pull/8012:33,Testability,test,tests,33,"Stacked on #7959 . Utterly fails tests. At first glance, not sure this is possible without having InferPType called from within def pType on null.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8012
https://github.com/hail-is/hail/pull/8014:12,Availability,error,error,12,"We saw this error in production:; ```; pymysql.err.IntegrityError: (1062, ""Duplicate entry '7433-432443' for key 'PRIMARY'""); ```; hand deploy in progress already.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8014
https://github.com/hail-is/hail/pull/8014:136,Deployability,deploy,deploy,136,"We saw this error in production:; ```; pymysql.err.IntegrityError: (1062, ""Duplicate entry '7433-432443' for key 'PRIMARY'""); ```; hand deploy in progress already.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8014
https://github.com/hail-is/hail/pull/8014:51,Security,Integrity,IntegrityError,51,"We saw this error in production:; ```; pymysql.err.IntegrityError: (1062, ""Duplicate entry '7433-432443' for key 'PRIMARY'""); ```; hand deploy in progress already.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8014
https://github.com/hail-is/hail/pull/8016:113,Deployability,deploy,deploy,113,"I hardcoded us-central1, which is the only thing we're using right now. Otherwise, we'd have to change CI before deploy. Also, clearly a fixed global zone is naive, so I think we have to reconsider the GCP configuration going forward. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8016
https://github.com/hail-is/hail/pull/8016:206,Deployability,configurat,configuration,206,"I hardcoded us-central1, which is the only thing we're using right now. Otherwise, we'd have to change CI before deploy. Also, clearly a fixed global zone is naive, so I think we have to reconsider the GCP configuration going forward. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8016
https://github.com/hail-is/hail/pull/8016:206,Modifiability,config,configuration,206,"I hardcoded us-central1, which is the only thing we're using right now. Otherwise, we'd have to change CI before deploy. Also, clearly a fixed global zone is naive, so I think we have to reconsider the GCP configuration going forward. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8016
https://github.com/hail-is/hail/pull/8016:127,Usability,clear,clearly,127,"I hardcoded us-central1, which is the only thing we're using right now. Otherwise, we'd have to change CI before deploy. Also, clearly a fixed global zone is naive, so I think we have to reconsider the GCP configuration going forward. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8016
https://github.com/hail-is/hail/pull/8021:465,Testability,test,tests,465,"For block matrices, we apparently have a notion of ""tensor shape"" vs ""matrix shape"". I am so far not a huge fan of this, as I don't think `BlockMatrix` was really designed to be anything other than a matrix. Anyway, the bug here is that the types contain the ""tensor shape"", and the block matrix filtering code was acting under the assumption that the types contained the ""matrix shape"". I've tried to be explicit when naming them below. . I also added some python tests to catch this. We have Scala filtering tests as well, so just added enough python tests to convince myself this bug was fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8021
https://github.com/hail-is/hail/pull/8021:510,Testability,test,tests,510,"For block matrices, we apparently have a notion of ""tensor shape"" vs ""matrix shape"". I am so far not a huge fan of this, as I don't think `BlockMatrix` was really designed to be anything other than a matrix. Anyway, the bug here is that the types contain the ""tensor shape"", and the block matrix filtering code was acting under the assumption that the types contained the ""matrix shape"". I've tried to be explicit when naming them below. . I also added some python tests to catch this. We have Scala filtering tests as well, so just added enough python tests to convince myself this bug was fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8021
https://github.com/hail-is/hail/pull/8021:553,Testability,test,tests,553,"For block matrices, we apparently have a notion of ""tensor shape"" vs ""matrix shape"". I am so far not a huge fan of this, as I don't think `BlockMatrix` was really designed to be anything other than a matrix. Anyway, the bug here is that the types contain the ""tensor shape"", and the block matrix filtering code was acting under the assumption that the types contained the ""matrix shape"". I've tried to be explicit when naming them below. . I also added some python tests to catch this. We have Scala filtering tests as well, so just added enough python tests to convince myself this bug was fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8021
https://github.com/hail-is/hail/pull/8023:20,Availability,error,error,20,"1. If we receive an error other than 404 from Google when asking about an instance, we should raise. This is unexpected. (The later lines will fail anyway because spec is `None`); 2. (the main issue) if the instance is not active, do not bother contacting it and, crucially, continue `check_on_instance` eventually learning the instance does not exist.; 3. Drop timeout to 5s to talk to a batch agent. Fixes the zombie instance issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8023
https://github.com/hail-is/hail/pull/8023:362,Safety,timeout,timeout,362,"1. If we receive an error other than 404 from Google when asking about an instance, we should raise. This is unexpected. (The later lines will fail anyway because spec is `None`); 2. (the main issue) if the instance is not active, do not bother contacting it and, crucially, continue `check_on_instance` eventually learning the instance does not exist.; 3. Drop timeout to 5s to talk to a batch agent. Fixes the zombie instance issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8023
https://github.com/hail-is/hail/pull/8023:315,Usability,learn,learning,315,"1. If we receive an error other than 404 from Google when asking about an instance, we should raise. This is unexpected. (The later lines will fail anyway because spec is `None`); 2. (the main issue) if the instance is not active, do not bother contacting it and, crucially, continue `check_on_instance` eventually learning the instance does not exist.; 3. Drop timeout to 5s to talk to a batch agent. Fixes the zombie instance issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8023
https://github.com/hail-is/hail/pull/8024:65,Energy Efficiency,charge,charge,65,add 0.004/instance-hr external IP cost; add 0.01/core-hr service charge; also reorganized the code a little bit. Corresponds to ~$0.021704/core-hr for the current setup. FYI @konradjk,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8024
https://github.com/hail-is/hail/pull/8025:105,Testability,log,log,105,"- Rename pod_status to job.; - Fix job to work with new batch.; - Fix get to work with new batch.; - Fix log to work with new batch.; - Unify formatting options across job, get, and log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8025
https://github.com/hail-is/hail/pull/8025:182,Testability,log,log,182,"- Rename pod_status to job.; - Fix job to work with new batch.; - Fix get to work with new batch.; - Fix log to work with new batch.; - Unify formatting options across job, get, and log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8025
https://github.com/hail-is/hail/issues/8027:5,Deployability,pipeline,pipeline,5,"This pipeline:; ```python3; # cwd = repo_root/hail; vcf2 = hl.import_vcf('src/test/resources/gvcfs/HG00268.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcf1 = hl.import_vcf('src/test/resources/gvcfs/HG00096.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcfs = [vcf1.rows().key_by('locus'), vcf2.rows().key_by('locus')]; ht = hl.Table.multi_way_zip_join(vcfs, 'data', 'new_globals'); ht._force_count(); ```; Fails with:; ```; java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:224); at is.hail.rvd.RVD.<init>(RVD.scala:46); at is.hail.rvd.RVD$.apply(RVD.scala:1411); at is.hail.expr.ir.TableMultiWayZipJoin.execute(TableIR.scala:925); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8027
https://github.com/hail-is/hail/issues/8027:78,Testability,test,test,78,"This pipeline:; ```python3; # cwd = repo_root/hail; vcf2 = hl.import_vcf('src/test/resources/gvcfs/HG00268.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcf1 = hl.import_vcf('src/test/resources/gvcfs/HG00096.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcfs = [vcf1.rows().key_by('locus'), vcf2.rows().key_by('locus')]; ht = hl.Table.multi_way_zip_join(vcfs, 'data', 'new_globals'); ht._force_count(); ```; Fails with:; ```; java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:224); at is.hail.rvd.RVD.<init>(RVD.scala:46); at is.hail.rvd.RVD$.apply(RVD.scala:1411); at is.hail.expr.ir.TableMultiWayZipJoin.execute(TableIR.scala:925); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8027
https://github.com/hail-is/hail/issues/8027:188,Testability,test,test,188,"This pipeline:; ```python3; # cwd = repo_root/hail; vcf2 = hl.import_vcf('src/test/resources/gvcfs/HG00268.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcf1 = hl.import_vcf('src/test/resources/gvcfs/HG00096.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcfs = [vcf1.rows().key_by('locus'), vcf2.rows().key_by('locus')]; ht = hl.Table.multi_way_zip_join(vcfs, 'data', 'new_globals'); ht._force_count(); ```; Fails with:; ```; java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:224); at is.hail.rvd.RVD.<init>(RVD.scala:46); at is.hail.rvd.RVD$.apply(RVD.scala:1411); at is.hail.expr.ir.TableMultiWayZipJoin.execute(TableIR.scala:925); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8027
https://github.com/hail-is/hail/pull/8028:25,Modifiability,refactor,refactor,25,Based on #7681. I had to refactor `ParameterPack.newFields` a bit to get it to work with `ParameterPack.array`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8028
https://github.com/hail-is/hail/issues/8029:168,Availability,error,error,168,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:958,Availability,Error,Error,958,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:1186,Availability,error,error,1186,".py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:1280,Availability,error,error,1280,".py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:1695,Availability,error,error,1695,".py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:2500,Deployability,pipeline,pipeline,2500," already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_time"": 1580760867038; },; ""spec"": {; ""command"": [; ""/bin/bash"",; ""-c"",; ""set -e; mkdir -p /io/pipeline/pipeline-3dea50d54013/__TASK__18/; /bin/true""; ],; ""image"": ""ubuntu:18.04"",; ""job_id"": 19,; ""mount_docker_socket"": false,; ""resources"": {; ""cpu"": ""0.001"",; ""memory"": ""375M""; },; ""secrets"": [; {; ""namespace"": ""dking"",; ""name"": ""dking-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""env"": []; },; ""attributes"": {; ""name"": ""18""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:2509,Deployability,pipeline,pipeline-,2509," already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_time"": 1580760867038; },; ""spec"": {; ""command"": [; ""/bin/bash"",; ""-c"",; ""set -e; mkdir -p /io/pipeline/pipeline-3dea50d54013/__TASK__18/; /bin/true""; ],; ""image"": ""ubuntu:18.04"",; ""job_id"": 19,; ""mount_docker_socket"": false,; ""resources"": {; ""cpu"": ""0.001"",; ""memory"": ""375M""; },; ""secrets"": [; {; ""namespace"": ""dking"",; ""name"": ""dking-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""env"": []; },; ""attributes"": {; ""name"": ""18""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:82,Energy Efficiency,schedul,scheduler,82,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:95,Energy Efficiency,schedul,schedule,95,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:673,Performance,load,loads,673,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:2208,Performance,load,loads,2208," already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_time"": 1580760867038; },; ""spec"": {; ""command"": [; ""/bin/bash"",; ""-c"",; ""set -e; mkdir -p /io/pipeline/pipeline-3dea50d54013/__TASK__18/; /bin/true""; ],; ""image"": ""ubuntu:18.04"",; ""job_id"": 19,; ""mount_docker_socket"": false,; ""resources"": {; ""cpu"": ""0.001"",; ""memory"": ""375M""; },; ""secrets"": [; {; ""namespace"": ""dking"",; ""name"": ""dking-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""env"": []; },; ""attributes"": {; ""name"": ""18""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/issues/8029:108,Safety,timeout,timeout,108,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8029
https://github.com/hail-is/hail/pull/8030:279,Deployability,deploy,deploy,279,"# Changes; - include the GCP profiler utility in a new service-base only docker requirements; - include necessary `build-essentials` (in particular, `gcc`) in the service-base image; - enable profiler utility for batch driver when HAIL_SHOULD_PROFILE is set, which is set on dev deploy or deploy. # Profiler Information; We have [enough quota](https://console.cloud.google.com/iam-admin/quotas?_ga=2.207444499.1054557266.1580858577-1098760465.1578424762&project=hail-vdc&folder&organizationId=548622027621&service=cloudprofiler.googleapis.com) to handle a substantial number of parallel profiled batch-driver instances. I don't think we have enough to handle a deluge of PRs, so I disabled profiling of PRs. Stackdriver Profiling is currently free, but we should keep an eye on it. When it becomes a paid product, we may reconsider these settings. # Example. I ran a pipeline with 1000 `/bin/true` jobs and then went to the [profiler page](; https://console.cloud.google.com/profiler;timespan=10m;end=2020-02-03T20:50:00.000Z/batch-driver/CPU?project=hail-vdc).; <img width=""1274"" alt=""Screen Shot 2020-02-03 at 3 55 55 PM"" src=""https://user-images.githubusercontent.com/106194/73690123-acbb7e80-469d-11ea-83e6-c3499c3e3c87.png"">. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8030
https://github.com/hail-is/hail/pull/8030:289,Deployability,deploy,deploy,289,"# Changes; - include the GCP profiler utility in a new service-base only docker requirements; - include necessary `build-essentials` (in particular, `gcc`) in the service-base image; - enable profiler utility for batch driver when HAIL_SHOULD_PROFILE is set, which is set on dev deploy or deploy. # Profiler Information; We have [enough quota](https://console.cloud.google.com/iam-admin/quotas?_ga=2.207444499.1054557266.1580858577-1098760465.1578424762&project=hail-vdc&folder&organizationId=548622027621&service=cloudprofiler.googleapis.com) to handle a substantial number of parallel profiled batch-driver instances. I don't think we have enough to handle a deluge of PRs, so I disabled profiling of PRs. Stackdriver Profiling is currently free, but we should keep an eye on it. When it becomes a paid product, we may reconsider these settings. # Example. I ran a pipeline with 1000 `/bin/true` jobs and then went to the [profiler page](; https://console.cloud.google.com/profiler;timespan=10m;end=2020-02-03T20:50:00.000Z/batch-driver/CPU?project=hail-vdc).; <img width=""1274"" alt=""Screen Shot 2020-02-03 at 3 55 55 PM"" src=""https://user-images.githubusercontent.com/106194/73690123-acbb7e80-469d-11ea-83e6-c3499c3e3c87.png"">. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8030
https://github.com/hail-is/hail/pull/8030:867,Deployability,pipeline,pipeline,867,"# Changes; - include the GCP profiler utility in a new service-base only docker requirements; - include necessary `build-essentials` (in particular, `gcc`) in the service-base image; - enable profiler utility for batch driver when HAIL_SHOULD_PROFILE is set, which is set on dev deploy or deploy. # Profiler Information; We have [enough quota](https://console.cloud.google.com/iam-admin/quotas?_ga=2.207444499.1054557266.1580858577-1098760465.1578424762&project=hail-vdc&folder&organizationId=548622027621&service=cloudprofiler.googleapis.com) to handle a substantial number of parallel profiled batch-driver instances. I don't think we have enough to handle a deluge of PRs, so I disabled profiling of PRs. Stackdriver Profiling is currently free, but we should keep an eye on it. When it becomes a paid product, we may reconsider these settings. # Example. I ran a pipeline with 1000 `/bin/true` jobs and then went to the [profiler page](; https://console.cloud.google.com/profiler;timespan=10m;end=2020-02-03T20:50:00.000Z/batch-driver/CPU?project=hail-vdc).; <img width=""1274"" alt=""Screen Shot 2020-02-03 at 3 55 55 PM"" src=""https://user-images.githubusercontent.com/106194/73690123-acbb7e80-469d-11ea-83e6-c3499c3e3c87.png"">. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8030
https://github.com/hail-is/hail/pull/8031:87,Deployability,pipeline,pipelines,87,"The `assert(_ptype2 == null)` check in InferPType is breaking here on; certain complex pipelines in a way I don't want to debug. There's no IR sharing within the IR (see utility I added), but there; must be subtrees that are inferred multiple times in different Compile; calls. This is a safe stop-gap.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8031
https://github.com/hail-is/hail/pull/8031:288,Safety,safe,safe,288,"The `assert(_ptype2 == null)` check in InferPType is breaking here on; certain complex pipelines in a way I don't want to debug. There's no IR sharing within the IR (see utility I added), but there; must be subtrees that are inferred multiple times in different Compile; calls. This is a safe stop-gap.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8031
https://github.com/hail-is/hail/pull/8031:5,Testability,assert,assert,5,"The `assert(_ptype2 == null)` check in InferPType is breaking here on; certain complex pipelines in a way I don't want to debug. There's no IR sharing within the IR (see utility I added), but there; must be subtrees that are inferred multiple times in different Compile; calls. This is a safe stop-gap.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8031
https://github.com/hail-is/hail/pull/8037:88,Performance,perform,performance-improvements,88,See here for the problem I'm solving with this PR: https://dev.hail.is/t/ndarray-matmul-performance-improvements/176. cc @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8037
https://github.com/hail-is/hail/pull/8038:58,Availability,error,errors,58,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8038
https://github.com/hail-is/hail/pull/8038:142,Deployability,pipeline,pipeline,142,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8038
https://github.com/hail-is/hail/pull/8038:234,Deployability,pipeline,pipeline,234,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8038
https://github.com/hail-is/hail/pull/8038:30,Testability,test,test,30,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8038
https://github.com/hail-is/hail/pull/8038:280,Testability,test,test,280,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8038
https://github.com/hail-is/hail/pull/8040:361,Performance,perform,performance,361,"This PR separates the ability to broadcast values in generated code on the backend from the collectDArray function. In order to support this, I did three things:. - allowed the function builder to build and store Encoder/Decoder objects, which wasn't strictly necessary but makes generating encoders/decoders for staged code much easier (I'll probably run some performance comparisons on this, shortly, but hopefully this doesn't introduce unacceptable amounts of overhead.); - added broadcast support on BackendUtils to be able to broadcast serialized Hail values with a given decoder; - rewrote the CollectDistributedArray codegen to exercise the broadcasting on BackendUtils instead of manually serializing/broadcasting/deserializing the globals in generated code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8040
https://github.com/hail-is/hail/pull/8042:89,Safety,safe,safer,89,"pytest and jvm-test pass, so it seems the looser assertion can be tightened to make this safer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8042
https://github.com/hail-is/hail/pull/8042:15,Testability,test,test,15,"pytest and jvm-test pass, so it seems the looser assertion can be tightened to make this safer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8042
https://github.com/hail-is/hail/pull/8042:49,Testability,assert,assertion,49,"pytest and jvm-test pass, so it seems the looser assertion can be tightened to make this safer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8042
https://github.com/hail-is/hail/pull/8045:114,Availability,down,downstream,114,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:1444,Availability,avail,available,1444,"dbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 200 OK; < Content-Type: application/json; < Date: Wed, 05 Feb 2020 20:59:27 GMT; < Content-Length: 88; <; {; 	""service"": {; 		""namespace"": ""default"",; 		""name"": ""gateway""; 	},; 	""localEndpoints"": 1; }; ```; ```; }dking@gke-vd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:547,Energy Efficiency,allocate,allocated,547,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:376,Performance,Load,LoadBalancer,376,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:436,Performance,load,loadbalancing,436,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:450,Performance,load,loadBalancers,450,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:494,Performance,load,loadBalancerIP,494,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:731,Performance,Load,LoadBalancer,731,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:762,Performance,Load,LoadBalancer,762,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:1546,Performance,Load,LoadBalancer,1546,"ich we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 200 OK; < Content-Type: application/json; < Date: Wed, 05 Feb 2020 20:59:27 GMT; < Content-Length: 88; <; {; 	""service"": {; 		""namespace"": ""default"",; 		""name"": ""gateway""; 	},; 	""localEndpoints"": 1; }; ```; ```; }dking@gke-vdc-non-preemptible-pool-5-80798769-kp8n ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:1747,Performance,load,loadbalancer,1747,"adBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 200 OK; < Content-Type: application/json; < Date: Wed, 05 Feb 2020 20:59:27 GMT; < Content-Length: 88; <; {; 	""service"": {; 		""namespace"": ""default"",; 		""name"": ""gateway""; 	},; 	""localEndpoints"": 1; }; ```; ```; }dking@gke-vdc-non-preemptible-pool-5-80798769-kp8n ~ $ curl -v localhost:32029; * Trying 127.0.0.1...; * TCP_NODELAY set; * Connected to localhost (127.0.0.1) port 32029 (#0); > GET / HTTP/1.1; > Host: localhost:32029; > User-Agent: curl/7.64.1; > Accept: */*; >; < HTTP/1.1 503 Service Unavailable; < Conte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:217,Security,expose,exposed,217,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:570,Security,expose,expose,570,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/pull/8045:66,Testability,log,log,66,"cc: the ""services team"" @cseed, @johnc1231. This fixes gateway to log the user's IP. Forthcoming PRs will fix all downstream; services. ---. There are two important pieces of which to be aware:. - The gateway pod are exposed via the gateway Service, which is the only; object modified in this PR.; - K8s fulfills our request for the gateway Service by creating a [Google TCP; LoadBalancer](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list). Moreover,; we specify `loadBalancerIP` which is a manually (outside of k8s) allocated IP; which we expose on the public internet. When you `curl https://hail.is` this is what happens:. - Your packet travels across the internet until it reaches the Google TCP; LoadBalancer; - The Google TCP LoadBalancer selects one of the kubernetes nodes to send the; packet to (in principle, it could send the packet to *any* node, even nodes; that do not have a gateway pod).; - Some part of k8s receives the packet and discovers the nodes that host a; gateway pod.; - It selects a gateway pod and forwards the packet to the node (possibly itself); hosting that gateway pod. In doing so, *it must replace the source IP of the; packet with its own, internal, IP*. Note that this is happening at the TCP layer, so no HTTP headers are set. When; the gateway `nginx` receives the packet, there is no trace of the source; IP. Kubernetes has a feature called `externalTrafficPolicy` which is available; in GCP and Azure and preserves the source IP. Kubernetes achieves this by; failing the TCP LoadBalancer healthchecks on nodes without matching pods (in our; case, gateway). The k8s docs on [Source IPs](https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-loadbalancer) further explain this strategy. Here's what the healthchecks look like for two; nodes, one hosting a gateway pod and one not hosting a gateway pod (note the; HTTP status code):. ```; dking@gke-vdc-preemptible-pool-2-9aa4dbeb-wvxk ~ $ curl -v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045
https://github.com/hail-is/hail/issues/8047:119,Safety,timeout,timeout,119,https://github.com/hail-is/hail/pull/8045 seems to have made some requests to services take obscene amounts of time or timeout. Two recent curls of ci.hail.is took 20s and 35s. Growing node pools leading to out of date hosts rendering a service not accessible:; https://github.com/kubernetes/kubernetes/issues/39423#issuecomment-370478433,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8047
https://github.com/hail-is/hail/issues/8047:249,Security,access,accessible,249,https://github.com/hail-is/hail/pull/8045 seems to have made some requests to services take obscene amounts of time or timeout. Two recent curls of ci.hail.is took 20s and 35s. Growing node pools leading to out of date hosts rendering a service not accessible:; https://github.com/kubernetes/kubernetes/issues/39423#issuecomment-370478433,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8047
https://github.com/hail-is/hail/pull/8048:46,Deployability,update,update,46,Someone changed this at one point and did not update the makefile. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8048
https://github.com/hail-is/hail/pull/8049:107,Availability,error,error,107,This PR makes docker calls idempotent and adds a timeout for docker calls in the retry function. I got the error codes to ignore from the older docker documentation at the bottom of each API call: https://docs.docker.com/engine/api/v1.30/#operation/ContainerDelete. FYI: @cseed since you had opinions on the timeout times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8049
https://github.com/hail-is/hail/pull/8049:49,Safety,timeout,timeout,49,This PR makes docker calls idempotent and adds a timeout for docker calls in the retry function. I got the error codes to ignore from the older docker documentation at the bottom of each API call: https://docs.docker.com/engine/api/v1.30/#operation/ContainerDelete. FYI: @cseed since you had opinions on the timeout times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8049
https://github.com/hail-is/hail/pull/8049:308,Safety,timeout,timeout,308,This PR makes docker calls idempotent and adds a timeout for docker calls in the retry function. I got the error codes to ignore from the older docker documentation at the bottom of each API call: https://docs.docker.com/engine/api/v1.30/#operation/ContainerDelete. FYI: @cseed since you had opinions on the timeout times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8049
https://github.com/hail-is/hail/pull/8051:45,Deployability,pipeline,pipeline,45,"Here's a sketch of what I think the lowering pipeline for BlockMatrix would look like. The relevant bits:. - BlockMatrixType gets an additional `definedBlocks` field to track sparseness.; - LowerBlockMatrixIR.lower on BlockMatrixIRs defines the transformations from BlockMatrixIRs to BlockMatrixStage, and LowerBlockMatrixIR.lower on value IRs with BlockMatrixIR children define rules for transforming the lowered BlockMatrixStage children into IRs, similarly to lowering in TableIRs.; - BlockMatrixStage consists of basically 3 things:; - blockContext: a matrix of contexts necessary for each partition computation (e.g. filenames of blocks that each partition needs to read, literal NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Mo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:2229,Deployability,pipeline,pipelines,2229," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:2313,Deployability,pipeline,pipeline,2313," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:1441,Integrability,depend,depending,1441,"h partition computation (e.g. filenames of blocks that each partition needs to read, literal NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or Bloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:1542,Testability,log,logic,1542,"h partition computation (e.g. filenames of blocks that each partition needs to read, literal NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or Bloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:1867,Testability,test,test,1867," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:2016,Testability,test,testing,2016," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:2061,Testability,test,tested,2061," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:2105,Testability,test,test,2105," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:2177,Testability,test,test,2177," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8051:2447,Testability,benchmark,benchmarking,2447," NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Monday. Notes on testing:; I'm envisioning that this would be tested really similarly to how we currently test Table lowering; for now, adding an execution strategy in the Scala test framework and using it on specific BlockMatrix pipelines that we expect to be fully lowerable. Once we have a specific/non-trivial pipeline that is fully lowerable, we can add a feature flag that controls whether or not we attempt to lower the execution, and start benchmarking on the python end. I think the second threshold is probably most quickly achieved by lowering the BlockMatrixFromValue node (or BlockMatrixRead, for larger multiplies) and implementing/lowering a NDArrayFromBlockMatrix node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8051
https://github.com/hail-is/hail/pull/8052:650,Safety,safe,safe,650,"It seems that sessions sometimes become inaccessible to auth. Using some; logging, I realized that `/login` will set some session parameters that do not; reappear in `/oauth2callback`. While trying to debug this, I deleted my cookie; and everything started working again. Luckily, my phone was still borked. The; fix is to use `new_session` which I discovered with a big red warning in; aiohttp-session's docs: [Always use new_session() instead of get_session() in; your login views to guard against Session Fixation; attacks!](https://aiohttp-session.readthedocs.io/en/stable/reference.html#aiohttp_session.new_session). If nothing else, we are now safe from session fixation attacks. I do not; understand why this is necessary for correctness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052
https://github.com/hail-is/hail/pull/8052:518,Security,attack,attacks,518,"It seems that sessions sometimes become inaccessible to auth. Using some; logging, I realized that `/login` will set some session parameters that do not; reappear in `/oauth2callback`. While trying to debug this, I deleted my cookie; and everything started working again. Luckily, my phone was still borked. The; fix is to use `new_session` which I discovered with a big red warning in; aiohttp-session's docs: [Always use new_session() instead of get_session() in; your login views to guard against Session Fixation; attacks!](https://aiohttp-session.readthedocs.io/en/stable/reference.html#aiohttp_session.new_session). If nothing else, we are now safe from session fixation attacks. I do not; understand why this is necessary for correctness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052
https://github.com/hail-is/hail/pull/8052:677,Security,attack,attacks,677,"It seems that sessions sometimes become inaccessible to auth. Using some; logging, I realized that `/login` will set some session parameters that do not; reappear in `/oauth2callback`. While trying to debug this, I deleted my cookie; and everything started working again. Luckily, my phone was still borked. The; fix is to use `new_session` which I discovered with a big red warning in; aiohttp-session's docs: [Always use new_session() instead of get_session() in; your login views to guard against Session Fixation; attacks!](https://aiohttp-session.readthedocs.io/en/stable/reference.html#aiohttp_session.new_session). If nothing else, we are now safe from session fixation attacks. I do not; understand why this is necessary for correctness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052
https://github.com/hail-is/hail/pull/8052:74,Testability,log,logging,74,"It seems that sessions sometimes become inaccessible to auth. Using some; logging, I realized that `/login` will set some session parameters that do not; reappear in `/oauth2callback`. While trying to debug this, I deleted my cookie; and everything started working again. Luckily, my phone was still borked. The; fix is to use `new_session` which I discovered with a big red warning in; aiohttp-session's docs: [Always use new_session() instead of get_session() in; your login views to guard against Session Fixation; attacks!](https://aiohttp-session.readthedocs.io/en/stable/reference.html#aiohttp_session.new_session). If nothing else, we are now safe from session fixation attacks. I do not; understand why this is necessary for correctness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052
https://github.com/hail-is/hail/pull/8052:101,Testability,log,login,101,"It seems that sessions sometimes become inaccessible to auth. Using some; logging, I realized that `/login` will set some session parameters that do not; reappear in `/oauth2callback`. While trying to debug this, I deleted my cookie; and everything started working again. Luckily, my phone was still borked. The; fix is to use `new_session` which I discovered with a big red warning in; aiohttp-session's docs: [Always use new_session() instead of get_session() in; your login views to guard against Session Fixation; attacks!](https://aiohttp-session.readthedocs.io/en/stable/reference.html#aiohttp_session.new_session). If nothing else, we are now safe from session fixation attacks. I do not; understand why this is necessary for correctness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052
https://github.com/hail-is/hail/pull/8052:471,Testability,log,login,471,"It seems that sessions sometimes become inaccessible to auth. Using some; logging, I realized that `/login` will set some session parameters that do not; reappear in `/oauth2callback`. While trying to debug this, I deleted my cookie; and everything started working again. Luckily, my phone was still borked. The; fix is to use `new_session` which I discovered with a big red warning in; aiohttp-session's docs: [Always use new_session() instead of get_session() in; your login views to guard against Session Fixation; attacks!](https://aiohttp-session.readthedocs.io/en/stable/reference.html#aiohttp_session.new_session). If nothing else, we are now safe from session fixation attacks. I do not; understand why this is necessary for correctness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052
https://github.com/hail-is/hail/issues/8053:6,Availability,error,error,6,"```; ""error"": ""Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1586,Availability,error,error,1586,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1594,Availability,error,error,1594,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1293,Energy Efficiency,adapt,adapters,1293,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:5000,Energy Efficiency,adapt,adapter,5000,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:5087,Energy Efficiency,adapt,adapters,5087,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1293,Integrability,adapter,adapters,1293,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:2941,Integrability,wrap,wrapped,2941,"r=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_ty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:5000,Integrability,adapter,adapter,5000,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:5087,Integrability,adapter,adapters,5087,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1293,Modifiability,adapt,adapters,1293,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:5000,Modifiability,adapt,adapter,5000,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:5087,Modifiability,adapt,adapters,5087,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:3137,Performance,concurren,concurrent,3137,"ut. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1095,Safety,timeout,timeout,1095,"pool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1326,Safety,timeout,timeout,1326,"conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:1334,Safety,timeout,timeout,1334,"conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:2159,Safety,timeout,timeout,2159,"nother exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:2307,Safety,timeout,timeout,2307,"ne 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/issues/8053:5272,Safety,timeout,timeout,5272,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053
https://github.com/hail-is/hail/pull/8054:438,Availability,error,errors,438,"This fixes two bugs:; 1. The container logs weren't being cached. This made the logs ""disappear"" for previous tasks while the job was still running. FYI @konradjk . 2. My job got stuck in ""running"" even though the job was deleted from the worker because writing the status to GCS timed out and we didn't actually mark the job complete. I'm not sure if we should always try to retry writing the status rather than failing on non-transient errors. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:2022,Availability,error,error,2022,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-pack",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:2030,Availability,error,error,2030,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-pack",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:1729,Energy Efficiency,adapt,adapters,1729,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:4639,Energy Efficiency,adapt,adapter,4639,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:4726,Energy Efficiency,adapt,adapters,4726,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:1729,Integrability,adapter,adapters,1729,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:4639,Integrability,adapter,adapter,4639,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:4726,Integrability,adapter,adapters,4726,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:1729,Modifiability,adapt,adapters,1729,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:4639,Modifiability,adapt,adapter,4639,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:4726,Modifiability,adapt,adapters,4726,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:58,Performance,cache,cached,58,"This fixes two bugs:; 1. The container logs weren't being cached. This made the logs ""disappear"" for previous tasks while the job was still running. FYI @konradjk . 2. My job got stuck in ""running"" even though the job was deleted from the worker because writing the status to GCS timed out and we didn't actually mark the job complete. I'm not sure if we should always try to retry writing the status rather than failing on non-transient errors. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:1531,Safety,timeout,timeout,1531,"pool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:1762,Safety,timeout,timeout,1762,"conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:1770,Safety,timeout,timeout,1770,"conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:2595,Safety,timeout,timeout,2595,"nother exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:2743,Safety,timeout,timeout,2743,"ne 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:4911,Safety,timeout,timeout,4911,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:39,Testability,log,logs,39,"This fixes two bugs:; 1. The container logs weren't being cached. This made the logs ""disappear"" for previous tasks while the job was still running. FYI @konradjk . 2. My job got stuck in ""running"" even though the job was deleted from the worker because writing the status to GCS timed out and we didn't actually mark the job complete. I'm not sure if we should always try to retry writing the status rather than failing on non-transient errors. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8054:80,Testability,log,logs,80,"This fixes two bugs:; 1. The container logs weren't being cached. This made the logs ""disappear"" for previous tasks while the job was still running. FYI @konradjk . 2. My job got stuck in ""running"" even though the job was deleted from the worker because writing the status to GCS timed out and we didn't actually mark the job complete. I'm not sure if we should always try to retry writing the status rather than failing on non-transient errors. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8054
https://github.com/hail-is/hail/pull/8055:61,Deployability,release,release,61,Source of instability in the short term; will add back after release,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8055
https://github.com/hail-is/hail/pull/8058:210,Availability,down,downstream,210,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8058
https://github.com/hail-is/hail/pull/8058:558,Deployability,update,update,558,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8058
https://github.com/hail-is/hail/pull/8058:323,Integrability,rout,router,323,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8058
https://github.com/hail-is/hail/pull/8058:338,Integrability,rout,router,338,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8058
https://github.com/hail-is/hail/pull/8058:410,Modifiability,config,configures,410,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8058
https://github.com/hail-is/hail/pull/8058:236,Testability,log,log,236,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8058
https://github.com/hail-is/hail/pull/8059:224,Deployability,configurat,configuration,224,More details at #8058. - Include the user's IP in the site logs.; - Fix out of date Makefile. I recognize there's duplication of log format. Abstracting over that doesn't seem *that* valuable and requires putting the shared configuration into a file in the root of hail and then arranging for the shared config file to be in the docker context. It's all kind of annoying and seems low value.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8059
https://github.com/hail-is/hail/pull/8059:224,Modifiability,config,configuration,224,More details at #8058. - Include the user's IP in the site logs.; - Fix out of date Makefile. I recognize there's duplication of log format. Abstracting over that doesn't seem *that* valuable and requires putting the shared configuration into a file in the root of hail and then arranging for the shared config file to be in the docker context. It's all kind of annoying and seems low value.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8059
https://github.com/hail-is/hail/pull/8059:304,Modifiability,config,config,304,More details at #8058. - Include the user's IP in the site logs.; - Fix out of date Makefile. I recognize there's duplication of log format. Abstracting over that doesn't seem *that* valuable and requires putting the shared configuration into a file in the root of hail and then arranging for the shared config file to be in the docker context. It's all kind of annoying and seems low value.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8059
https://github.com/hail-is/hail/pull/8059:59,Testability,log,logs,59,More details at #8058. - Include the user's IP in the site logs.; - Fix out of date Makefile. I recognize there's duplication of log format. Abstracting over that doesn't seem *that* valuable and requires putting the shared configuration into a file in the root of hail and then arranging for the shared config file to be in the docker context. It's all kind of annoying and seems low value.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8059
https://github.com/hail-is/hail/pull/8059:129,Testability,log,log,129,More details at #8058. - Include the user's IP in the site logs.; - Fix out of date Makefile. I recognize there's duplication of log format. Abstracting over that doesn't seem *that* valuable and requires putting the shared configuration into a file in the root of hail and then arranging for the shared config file to be in the docker context. It's all kind of annoying and seems low value.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8059
https://github.com/hail-is/hail/pull/8061:65,Deployability,update,update,65,"Moved the cheatsheets a few weeks ago, had to wait for a website update to safely take the old ones out of the repo. . For reference, new location is here: ; https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/_static/cheatsheets/hail_tables_cheat_sheet.pdf; https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/_static/cheatsheets/hail_matrix_tables_cheat_sheet.pdf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8061
https://github.com/hail-is/hail/pull/8061:75,Safety,safe,safely,75,"Moved the cheatsheets a few weeks ago, had to wait for a website update to safely take the old ones out of the repo. . For reference, new location is here: ; https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/_static/cheatsheets/hail_tables_cheat_sheet.pdf; https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/_static/cheatsheets/hail_matrix_tables_cheat_sheet.pdf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8061
https://github.com/hail-is/hail/pull/8063:121,Performance,optimiz,optimize,121,"wip. A few remaining tests fail. Includes a number of fixes to InferPType, InferType. Plan is to get this working before optimize, since that is the simple case, and then move to pre-simplify",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063
https://github.com/hail-is/hail/pull/8063:21,Testability,test,tests,21,"wip. A few remaining tests fail. Includes a number of fixes to InferPType, InferType. Plan is to get this working before optimize, since that is the simple case, and then move to pre-simplify",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063
https://github.com/hail-is/hail/pull/8063:149,Usability,simpl,simple,149,"wip. A few remaining tests fail. Includes a number of fixes to InferPType, InferType. Plan is to get this working before optimize, since that is the simple case, and then move to pre-simplify",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063
https://github.com/hail-is/hail/pull/8063:183,Usability,simpl,simplify,183,"wip. A few remaining tests fail. Includes a number of fixes to InferPType, InferType. Plan is to get this working before optimize, since that is the simple case, and then move to pre-simplify",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063
https://github.com/hail-is/hail/pull/8065:130,Performance,perform,performed,130,"In the Sphinx theme which the Hail docs use, the search page does not show anything unless a query has been provided and a search performed. https://github.com/readthedocs/sphinx_rtd_theme/blob/master/sphinx_rtd_theme/search.html. Thus, the Search link on the [home page of the Hail 0.2 docs](https://hail.is/docs/0.2/index.html) leads to a [blank page](https://hail.is/docs/0.2/search.html). ![image](https://user-images.githubusercontent.com/1156625/74118640-44333c80-4b8a-11ea-9147-7a0d188d44a0.png). To avoid confusion, this change removes the link to the search page from the home page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8065
https://github.com/hail-is/hail/pull/8065:507,Safety,avoid,avoid,507,"In the Sphinx theme which the Hail docs use, the search page does not show anything unless a query has been provided and a search performed. https://github.com/readthedocs/sphinx_rtd_theme/blob/master/sphinx_rtd_theme/search.html. Thus, the Search link on the [home page of the Hail 0.2 docs](https://hail.is/docs/0.2/index.html) leads to a [blank page](https://hail.is/docs/0.2/search.html). ![image](https://user-images.githubusercontent.com/1156625/74118640-44333c80-4b8a-11ea-9147-7a0d188d44a0.png). To avoid confusion, this change removes the link to the search page from the home page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8065
https://github.com/hail-is/hail/pull/8067:346,Energy Efficiency,allocate,allocated,346,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. cc @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8067
https://github.com/hail-is/hail/pull/8067:52,Performance,optimiz,optimization,52,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. cc @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8067
https://github.com/hail-is/hail/pull/8067:459,Safety,avoid,avoid,459,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. cc @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8067
https://github.com/hail-is/hail/pull/8070:158,Deployability,Update,Update,158,"* Fix a bug in `calculate_new_intervals`, when the default reference does not match; `reference_genome`; * Make positional argument names valid python ids; * Update various argument help; * Add `--log` argument allowing the user to specify the logfile; * If `--overwrite` is not present, check that `out_file` exists",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8070
https://github.com/hail-is/hail/pull/8070:197,Testability,log,log,197,"* Fix a bug in `calculate_new_intervals`, when the default reference does not match; `reference_genome`; * Make positional argument names valid python ids; * Update various argument help; * Add `--log` argument allowing the user to specify the logfile; * If `--overwrite` is not present, check that `out_file` exists",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8070
https://github.com/hail-is/hail/pull/8070:244,Testability,log,logfile,244,"* Fix a bug in `calculate_new_intervals`, when the default reference does not match; `reference_genome`; * Make positional argument names valid python ids; * Update various argument help; * Add `--log` argument allowing the user to specify the logfile; * If `--overwrite` is not present, check that `out_file` exists",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8070
https://github.com/hail-is/hail/pull/8071:79,Testability,log,logic,79,"adds a notion of sparsity to BlockMatrixType; I've lifted all of the necessary logic except for Map/Map2, which needs #8072 to implement in a sensical way. I had originally implemented this as a dense boolean matrix, which makes a lot of transformations simpler, but for really large (and very sparse) block matrices, you end up needing ~GB of space just to store the sparsity matrix, which is pretty untenable. This version basically preserves the structure of the version on GridPartitioner, except that we're storing the unlinearized form of the block index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8071
https://github.com/hail-is/hail/pull/8071:254,Usability,simpl,simpler,254,"adds a notion of sparsity to BlockMatrixType; I've lifted all of the necessary logic except for Map/Map2, which needs #8072 to implement in a sensical way. I had originally implemented this as a dense boolean matrix, which makes a lot of transformations simpler, but for really large (and very sparse) block matrices, you end up needing ~GB of space just to store the sparsity matrix, which is pretty untenable. This version basically preserves the structure of the version on GridPartitioner, except that we're storing the unlinearized form of the block index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8071
https://github.com/hail-is/hail/pull/8073:222,Security,expose,expose,222,The primary use case I have in mind for this right now is to lower a BlockMatrixCollect node; I don't *think* this can be easily implemented in terms of other nodes. Currently I've only implemented this in Scala but I can expose it in python if necessary.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8073
https://github.com/hail-is/hail/issues/8076:412,Availability,error,error,412,"I found that aggregating sorted arrays gives incorrect results and crashes when trying to do so without converting the array to string. This code explains what I found pretty well:. ```python; > mt = hl.balding_nichols_model(1, 10, 10). # Aggregate concatenated alleles (works fine); > mt.aggregate_rows(hl.agg.counter(hl.delimit(mt.alleles, '|'))); {'A|C': 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076
https://github.com/hail-is/hail/issues/8076:881,Availability,ERROR,ERROR,881,"I found that aggregating sorted arrays gives incorrect results and crashes when trying to do so without converting the array to string. This code explains what I found pretty well:. ```python; > mt = hl.balding_nichols_model(1, 10, 10). # Aggregate concatenated alleles (works fine); > mt.aggregate_rows(hl.agg.counter(hl.delimit(mt.alleles, '|'))); {'A|C': 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076
https://github.com/hail-is/hail/issues/8076:1209,Availability,error,error,1209,"s_model(1, 10, 10). # Aggregate concatenated alleles (works fine); > mt.aggregate_rows(hl.agg.counter(hl.delimit(mt.alleles, '|'))); {'A|C': 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it woul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076
https://github.com/hail-is/hail/issues/8076:2142,Availability,down,down,2142,": 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it would be nice if there was a way to bring it back up either automatically or manually. Hail version: 0.2.30-2ae07d872f43; Spark version: 2.4.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076
https://github.com/hail-is/hail/issues/8076:2202,Availability,down,down,2202,": 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it would be nice if there was a way to bring it back up either automatically or manually. Hail version: 0.2.30-2ae07d872f43; Spark version: 2.4.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076
https://github.com/hail-is/hail/issues/8076:1131,Integrability,protocol,protocol,1131," This code explains what I found pretty well:. ```python; > mt = hl.balding_nichols_model(1, 10, 10). # Aggregate concatenated alleles (works fine); > mt.aggregate_rows(hl.agg.counter(hl.delimit(mt.alleles, '|'))); {'A|C': 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076
https://github.com/hail-is/hail/issues/8076:1535,Safety,avoid,avoid,1535,": 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python clients so I have to restart my process and re-initialize Hail. Is there already functionality implemented for bringing that server up if it's down? I'd imagine segfaults aren't the only reason it could down, so it would be nice if there was a way to bring it back up either automatically or manually. Hail version: 0.2.30-2ae07d872f43; Spark version: 2.4.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076
https://github.com/hail-is/hail/issues/8078:70,Availability,error,error,70,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078
https://github.com/hail-is/hail/issues/8078:432,Availability,ERROR,ERROR,432,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078
https://github.com/hail-is/hail/issues/8078:685,Modifiability,config,config,685,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078
https://github.com/hail-is/hail/issues/8078:767,Modifiability,variab,variable,767,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078
https://github.com/hail-is/hail/issues/8078:112,Testability,test,test,112,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078
https://github.com/hail-is/hail/issues/8078:185,Testability,test,test,185,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078
https://github.com/hail-is/hail/issues/8078:280,Testability,test,test,280,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078
https://github.com/hail-is/hail/pull/8080:193,Energy Efficiency,reduce,reduce,193,* Add an empty multi write benchmark; * Add python only combiner benchmarks (4ms / iteration of `transform_gvcf`!); * Make the merge and write nothing benchmark actually finish on a laptop and reduce the time it spends in python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8080
https://github.com/hail-is/hail/pull/8080:27,Testability,benchmark,benchmark,27,* Add an empty multi write benchmark; * Add python only combiner benchmarks (4ms / iteration of `transform_gvcf`!); * Make the merge and write nothing benchmark actually finish on a laptop and reduce the time it spends in python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8080
https://github.com/hail-is/hail/pull/8080:65,Testability,benchmark,benchmarks,65,* Add an empty multi write benchmark; * Add python only combiner benchmarks (4ms / iteration of `transform_gvcf`!); * Make the merge and write nothing benchmark actually finish on a laptop and reduce the time it spends in python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8080
https://github.com/hail-is/hail/pull/8080:151,Testability,benchmark,benchmark,151,* Add an empty multi write benchmark; * Add python only combiner benchmarks (4ms / iteration of `transform_gvcf`!); * Make the merge and write nothing benchmark actually finish on a laptop and reduce the time it spends in python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8080
https://github.com/hail-is/hail/pull/8081:776,Modifiability,refactor,refactoring,776,"* Require AbstractRVDSpec to take a makeEncoder.; * Add a struct to hold all the encoders needed to write matrix columns and globals.; * Make MatrixValue.{writeGlobals,writeCols} methods take encoders and AbstractTypedCodecSpecs. Basically, where possible try to separate 'compile something' from 'use that compiled code'. Finalize write is currently not parallelize-able. This is a problem for; `write_matrix_tables` as it creates a large portion of work that is; single threaded. The reason for this is twofold. First, finalizeWrite; compiles encoders for columns, globals, and the globals' globals.; Second, MatrixValue is not serializable, and I don't think it can be.; This is a little trickier as it will require the exploding/broadcasting; of the globals/columns. This refactoring removes compiling encoders from; this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8081
https://github.com/hail-is/hail/pull/8081:826,Testability,log,logic,826,"* Require AbstractRVDSpec to take a makeEncoder.; * Add a struct to hold all the encoders needed to write matrix columns and globals.; * Make MatrixValue.{writeGlobals,writeCols} methods take encoders and AbstractTypedCodecSpecs. Basically, where possible try to separate 'compile something' from 'use that compiled code'. Finalize write is currently not parallelize-able. This is a problem for; `write_matrix_tables` as it creates a large portion of work that is; single threaded. The reason for this is twofold. First, finalizeWrite; compiles encoders for columns, globals, and the globals' globals.; Second, MatrixValue is not serializable, and I don't think it can be.; This is a little trickier as it will require the exploding/broadcasting; of the globals/columns. This refactoring removes compiling encoders from; this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8081
https://github.com/hail-is/hail/issues/8083:1576,Availability,error,error,1576,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:1584,Availability,error,error,1584,"py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:1283,Energy Efficiency,adapt,adapters,1283,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:4990,Energy Efficiency,adapt,adapter,4990,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:5077,Energy Efficiency,adapt,adapters,5077,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:1283,Integrability,adapter,adapters,1283,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:2931,Integrability,wrap,wrapped,2931,"r=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_ty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:4990,Integrability,adapter,adapter,4990,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:5077,Integrability,adapter,adapters,5077,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:1283,Modifiability,adapt,adapters,1283,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:4990,Modifiability,adapt,adapter,4990,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:5077,Modifiability,adapt,adapters,5077,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:3127,Performance,concurren,concurrent,3127,"ut. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:1085,Safety,timeout,timeout,1085,"pool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:1316,Safety,timeout,timeout,1316,"conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:1324,Safety,timeout,timeout,1324,"conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:2149,Safety,timeout,timeout,2149,"nother exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:2297,Safety,timeout,timeout,2297,"ne 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 293, in run; await self.get_container_log()); File ""/usr/local/lib/python3.6/site-packages/batch/log_store.py"", line 36, in write_log_file; return await self.gcs.write_gs_file(path, data); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 37, in write_gs_file; return await self._wrapped_write_gs_file(self, uri, string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 56, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/site-packages/hailtop/utils/utils.py"", line 35, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/issues/8083:5262,Safety,timeout,timeout,5262,"/utils.py"", line 35, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/site-packages/batch/google_storage.py"", line 65, in _write_gs_file; f.upload_from_string(string, *args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1257, in upload_from_string; predefined_acl=predefined_acl,; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1157, in upload_from_file; client, file_obj, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 1063, in _do_upload; client, stream, content_type, size, num_retries, predefined_acl; File ""/usr/local/lib/python3.6/site-packages/google/cloud/storage/blob.py"", line 857, in _do_multipart_upload; response = upload.transmit(transport, data, object_metadata, content_type); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py"", line 106, in transmit; retry_strategy=self._retry_strategy,; File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request; return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy); File ""/usr/local/lib/python3.6/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry; response = func(); File ""/usr/local/lib/python3.6/site-packages/google/auth/transport/requests.py"", line 317, in request; **kwargs; File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send; r = adapter.send(request, **kwargs); File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8083
https://github.com/hail-is/hail/pull/8084:73,Modifiability,refactor,refactor,73,Have a new test to target that will verify correct code generation. Also refactor parameterpack a bit to be more traceable. High prio because this is blocking ptypes work.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8084
https://github.com/hail-is/hail/pull/8084:11,Testability,test,test,11,Have a new test to target that will verify correct code generation. Also refactor parameterpack a bit to be more traceable. High prio because this is blocking ptypes work.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8084
https://github.com/hail-is/hail/pull/8085:346,Energy Efficiency,allocate,allocated,346,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. Stacked on #8084, don't review until that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8085
https://github.com/hail-is/hail/pull/8085:52,Performance,optimiz,optimization,52,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. Stacked on #8084, don't review until that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8085
https://github.com/hail-is/hail/pull/8085:459,Safety,avoid,avoid,459,"We're currently emitting the explicit node (without optimization!).; This design is incrementally better, and lets us do ptyping more easily. The right solution is to do generate a method as the node suggests, but; there are some issues to sort out here, like how to return a missing; value. We may need to return a (possibly null) pointer to an allocated; value, which could be inefficient. Pushing ptypes/requiredness fully; through the system would let us avoid this in many cases. Stacked on #8084, don't review until that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8085
https://github.com/hail-is/hail/pull/8086:72,Deployability,pipeline,pipeline,72,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:97,Deployability,pipeline,pipeline,97,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:151,Deployability,pipeline,pipeline-docs,151,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:241,Deployability,pipeline,pipeline-docs,241,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:340,Deployability,pipeline,pipeline-docs,340,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:980,Deployability,pipeline,pipeline,980,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:272,Integrability,depend,depend,272,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:312,Integrability,depend,depends,312,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8086:736,Modifiability,inherit,inherited,736,"- Added a new build step `test_pipeline_docs` that runs doctest for the pipeline module; - Added pipeline docs to the hail/Makefile with a new target `pipeline-docs`. I also changed the `make-docs` target to be `base-docs` and `hail-docs`. `pipeline-docs` and `hail-docs` depend on `base-docs` and `upload-docs` depends on `hail-docs` and `pipeline-docs`; - Fixed a bunch of places in the documentation for clarity and to make the doctests work.; - Note, some examples are skipped because we can't run docker within docker in local mode. We can consider at another time point running all of the examples with the BatchBackend; - There's a bunch of Sphinx stuff I had to do to get the docs to render how I wanted them to with regards to inherited members (it was including all string methods by default even though). It's possible I can clean that up a bit, but I think it's fine for now.; - Added a link to the docs to the batch dropdown menu.; - Docs will appear at hail.is/docs/pipeline for now. Eventually everything will be renamed to batch, but I elected not to do that now.; - I checked the dropdown works correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8086
https://github.com/hail-is/hail/pull/8087:489,Testability,test,testing,489,"This PR describes a lowering framework for BlockMatrixIR -> CollectDistributedArray. As part of this, I split out a `LowerIR` function from the `LowerTableIR` value rules so that we can apply lowering more modularly, if desired. It delegates to `LowerTableIR` and `LowerBlockMatrixIR` for the relevant nodes but lets us use both the table lowerer and/or the block matrix lowerer together as desired. (this PR turned into quite a large change, since I ended up implementing enough to start testing the lowering, but I'm happy to break it up, too.). cc @cseed @tpoterba @danking @patrick-schultz @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8087
https://github.com/hail-is/hail/pull/8088:196,Availability,error,error,196,"Fixes #8083 . The format of the status is ; ```; # {; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int; # }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8088
https://github.com/hail-is/hail/pull/8088:237,Availability,error,error,237,"Fixes #8083 . The format of the status is ; ```; # {; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int; # }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8088
https://github.com/hail-is/hail/pull/8089:515,Testability,test,testing,515,"Defines, but does not implement, a BlockMatrixCollect value IR node. This is kind of an awkward node currently. We cannot compile nodes with BlockMatrixIRs. We cannot interpret nodes with NDArrayIRs. So we can't compile this or interpret this, unless we define an interpreted version of NDArray (or a compilable version of BlockMatrix). I think this node might eventually be useful for Hail BlockMatrix -> Hail NDArray -> Python ndarray conversions, but for now the intention is to use this as a quick way to start testing BlockMatrix lowering.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8089
https://github.com/hail-is/hail/pull/8093:1240,Performance,perform,performing,1240,"Timing:. ```; mysql> SELECT job_id, spec, cores_mcpu FROM jobs WHERE batch_id = 10 AND state = 'Ready' AND always_run = 0 AND `cancelled` = 0 LIMIT 1;; +---------+------------------------------------------------------------------+------------+; | job_id | spec | cores_mcpu |; +---------+------------------------------------------------------------------+------------+; | 1606431 | [[[""batch-pods"", ""konradk-gsa-key"", ""/gsa-key"", 1]], null, 1, 1] | 1000 |; +---------+------------------------------------------------------------------+------------+; 1 row in set (1.22 sec). mysql> SELECT job_id, spec, cores_mcpu FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled) WHERE batch_id = 10 AND state = 'Ready' AND always_run = 0 AND `cancelled` = 0 LIMIT 1;; +---------+------------------------------------------------------------------+------------+; | job_id | spec | cores_mcpu |; +---------+------------------------------------------------------------------+------------+; | 1606431 | [[[""batch-pods"", ""konradk-gsa-key"", ""/gsa-key"", 1]], null, 1, 1] | 1000 |; +---------+------------------------------------------------------------------+------------+; 1 row in set (0.00 sec); ```. Explain verifies that the original query was performing a full batch scan.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8093
https://github.com/hail-is/hail/pull/8098:48,Deployability,Pipeline,Pipeline,48,Nik ran into humanize not being imported to use Pipeline. I added it to the requirements.txt that I believe is the correct requirements.txt. I think the other ones in the docker folder are just for creating docker images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8098
https://github.com/hail-is/hail/pull/8099:87,Testability,log,logic,87,"This lets RVB copy values of a different type. To do this, I needed to; delete all the logic in RVB and move it to PType. This also was an; opportunity to slightly simplify the existing copy code. I realize this may be a challenge to review. Happy to walk through any part as necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099
https://github.com/hail-is/hail/pull/8099:164,Usability,simpl,simplify,164,"This lets RVB copy values of a different type. To do this, I needed to; delete all the logic in RVB and move it to PType. This also was an; opportunity to slightly simplify the existing copy code. I realize this may be a challenge to review. Happy to walk through any part as necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099
https://github.com/hail-is/hail/pull/8100:431,Deployability,patch,patch,431,"Root cause: https://github.com/googleapis/google-auth-library-python/issues/443. `google-auth-oauthlib` does not pin any version of `google-auth` (I [created a PR](https://github.com/googleapis/google-auth-library-python-oauthlib/pull/71) to start pinning major & minor version). However, that wouldn't have saved us, the change (it increasingly appears to have been a bug, not an accidental breaking change) was introduced in the patch version 1.11.1. This pins *us* to 1.11.0. I am subscribed to the bug and will remove the patch-version pin once a fix is released.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8100
https://github.com/hail-is/hail/pull/8100:526,Deployability,patch,patch-version,526,"Root cause: https://github.com/googleapis/google-auth-library-python/issues/443. `google-auth-oauthlib` does not pin any version of `google-auth` (I [created a PR](https://github.com/googleapis/google-auth-library-python-oauthlib/pull/71) to start pinning major & minor version). However, that wouldn't have saved us, the change (it increasingly appears to have been a bug, not an accidental breaking change) was introduced in the patch version 1.11.1. This pins *us* to 1.11.0. I am subscribed to the bug and will remove the patch-version pin once a fix is released.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8100
https://github.com/hail-is/hail/pull/8100:558,Deployability,release,released,558,"Root cause: https://github.com/googleapis/google-auth-library-python/issues/443. `google-auth-oauthlib` does not pin any version of `google-auth` (I [created a PR](https://github.com/googleapis/google-auth-library-python-oauthlib/pull/71) to start pinning major & minor version). However, that wouldn't have saved us, the change (it increasingly appears to have been a bug, not an accidental breaking change) was introduced in the patch version 1.11.1. This pins *us* to 1.11.0. I am subscribed to the bug and will remove the patch-version pin once a fix is released.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8100
https://github.com/hail-is/hail/pull/8102:4,Security,Access,AccessLogger,4,Add AccessLogger which standardizes access logging across our infrastructure and crucially prints the X-Real-IP header. Also includes a fix for the google auth issue and two missing tables from delete_auth_tables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8102
https://github.com/hail-is/hail/pull/8102:36,Security,access,access,36,Add AccessLogger which standardizes access logging across our infrastructure and crucially prints the X-Real-IP header. Also includes a fix for the google auth issue and two missing tables from delete_auth_tables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8102
https://github.com/hail-is/hail/pull/8102:43,Testability,log,logging,43,Add AccessLogger which standardizes access logging across our infrastructure and crucially prints the X-Real-IP header. Also includes a fix for the google auth issue and two missing tables from delete_auth_tables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8102
https://github.com/hail-is/hail/pull/8105:22,Testability,test,tests,22,"Seeing if this passes tests. If so, it's a bit simpler",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8105
https://github.com/hail-is/hail/pull/8105:47,Usability,simpl,simpler,47,"Seeing if this passes tests. If so, it's a bit simpler",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8105
https://github.com/hail-is/hail/issues/8106:14,Availability,error,error,14,"The following error occurs when trying to liftover GRCh37 to GRCh38 for the UK Biobank GWAS for chr22. Any suggestions for this?. The code used is:. ```; chr=sys.argv[1]. bgen=""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr""+chr+""_v3.bgen""; sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; # hl.index_bgen(bgen); mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); print(mt.describe()); rg37 = hl.get_reference('GRCh37'); rg38 = hl.get_reference('GRCh38'); rg37.add_liftover('file:///restricted/projectnb/ukbiobank/ad/analysis/liftover/grch37_to_grch38.over.chain.gz', rg38); mt = mt.annotate_rows(new_locus=hl.liftover(mt.locus, 'GRCh38'), old_locus=mt.locus); mt = mt.filter_rows(hl.is_defined(mt.new_locus)); # mt = mt.key_rows_by(locus=mt.new_locus); print(mt.describe()); mt = mt.key_rows_by(locus=mt.new_locus,alleles=mt.alleles); print(mt.describe()); hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); ```. ```; Version 0.2.19-c6ec8b76eb26; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/liftover/hail-20200214-1434-0.2.19-c6ec8b76eb26.log; 2020-02-14 14:35:19 Hail: INFO: Number of BGEN files parsed: 1; 2020-02-14 14:35:19 Hail: INFO: Number of samples in BGEN files: 487409; 2020-02-14 14:35:19 Hail: INFO: Number of variants across all BGEN files: 1255683. Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'new_locus': locus<GRCh38>; 'old_locus': locus<GRCh37>; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'ne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:3395,Availability,failure,failure,3395,"tnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOn) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:7424,Availability,Failure,Failure,7424,"ark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at io.n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:16207,Availability,failure,failure,16207,"; at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum alloreamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_15657888296Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:20244,Availability,Failure,Failure,20244,"computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.ti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:30238,Availability,Error,Error,30238,"tations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.19-c6ec8b76eb26; Error summary: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum allowaamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:30294,Availability,failure,failure,30294,"tations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.19-c6ec8b76eb26; Error summary: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum allowaamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:34333,Availability,Failure,Failure,34333,"computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.ti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:2583,Deployability,install,install,2583,": str; 'new_locus': locus<GRCh38>; 'old_locus': locus<GRCh37>; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'new_locus' (row); 'old_locus' (row); [Stage 0:======================================================>(292 + 1) / 293]2020-02-14 14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:2715,Deployability,install,install,2715,"<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'new_locus' (row); 'old_locus' (row); [Stage 0:======================================================>(292 + 1) / 293]2020-02-14 14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:2848,Deployability,install,install,2848,"-----------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'new_locus' (row); 'old_locus' (row); [Stage 0:======================================================>(292 + 1) / 293]2020-02-14 14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:2982,Deployability,install,install,2982,"cus' (row); 'old_locus' (row); [Stage 0:======================================================>(292 + 1) / 293]2020-02-14 14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:3115,Deployability,install,install,3115,"14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:3232,Deployability,install,install,3232,"Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOn) at org.apache.spark.network.server.TransportRequestHandler.handle(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:7153,Energy Efficiency,schedul,scheduler,7153,".scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.st org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportReques",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:19974,Energy Efficiency,schedul,scheduler,19974,"c.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:23907,Energy Efficiency,schedul,scheduler,23907,xt.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:23947,Energy Efficiency,schedul,scheduler,23947,oMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24045,Energy Efficiency,schedul,scheduler,24045,okeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24142,Energy Efficiency,schedul,scheduler,24142,ractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24393,Energy Efficiency,schedul,scheduler,24393,lRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.Pa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24473,Energy Efficiency,schedul,scheduler,24473,erCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24563,Energy Efficiency,schedul,scheduler,24563,nnelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24660,Energy Efficiency,schedul,scheduler,24660,channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24755,Energy Efficiency,schedul,scheduler,24755,netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24918,Energy Efficiency,schedul,scheduler,24918,eduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:34063,Energy Efficiency,schedul,scheduler,34063,"c.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:2808,Integrability,wrap,wrapper,2808,"lumn key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'new_locus' (row); 'old_locus' (row); [Stage 0:======================================================>(292 + 1) / 293]2020-02-14 14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INFO fields.; [Stage 2:> (0 + 168) / 279][Stage 2:> (14 + 1) / 293]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.ni",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:4063,Integrability,Wrap,Wrappers,4063,"execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOn) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jpark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:4093,Integrability,Wrap,Wrappers,4093,"e/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOn) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jpark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:8051,Integrability,Wrap,Wrappers,8051,"D.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channelxt.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHaetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at nnelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:8081,Integrability,Wrap,Wrappers,8081,"pache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channelxt.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHaetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at nnelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:10102,Integrability,Message,MessageToMessageDecoder,10102,) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstracead(DefaultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at ed(NioEventLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.Nio at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thrpark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at eChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:10138,Integrability,Message,MessageToMessageDecoder,10138,ChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstracead(DefaultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at ed(NioEventLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.Nio at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thrpark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at eChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:14466,Integrability,Wrap,WrappedMatrixWriter,14466,(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:968); at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1517); at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1505); at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1505); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1505); at is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:66); at is.hail.io.vcf.ExportVCF$.apply(ExportVCF.scala:474); at is.hail.expr.ir.MatrixVCFWriter.apply(MatrixWriter.scala:48); at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:743); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply(CompileAndEvaluate.scala:33); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:24); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:33); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:86); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:86); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethod,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:16878,Integrability,Wrap,Wrappers,16878,"invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum alloreamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_15657888296Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)eout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MessageTotractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)work.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadxt.java:348) at io.netty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:16908,Integrability,Wrap,Wrappers,16908,"282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum alloreamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_15657888296Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)eout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MessageTotractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)work.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadxt.java:348) at io.netty.channel.AbstractCha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:17571,Integrability,Message,MessageTotractChannelHandlerContext,17571,ByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)eout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MessageTotractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)work.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadxt.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.channeleChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerC.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLootLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) at io.netty.util.cotRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.java:745) at org.apache.spark.storage.ShuffleBlockFet.next(ShuffleBlockFetcherIte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:20869,Integrability,Wrap,Wrappers,20869,"sRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstraceChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:20899,Integrability,Wrap,Wrappers,20899,"g.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstraceChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:22927,Integrability,Message,MessageToMessageDecoder,22927,o.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apach,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:22963,Integrability,Message,MessageToMessageDecoder,22963,lHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:28520,Integrability,Wrap,WrappedMatrixWriter,28520,(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:968); at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1517); at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1505); at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1505); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1505); at is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:66); at is.hail.io.vcf.ExportVCF$.apply(ExportVCF.scala:474); at is.hail.expr.ir.MatrixVCFWriter.apply(MatrixWriter.scala:48); at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:743); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply(CompileAndEvaluate.scala:33); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:24); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:33); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:86); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:86); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethod,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:30967,Integrability,Wrap,Wrappers,30967,"ommands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.19-c6ec8b76eb26; Error summary: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum allowaamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)eout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MessageTotractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)work.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadxt.java:348) at io.netty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:30997,Integrability,Wrap,Wrappers,30997,"and.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.19-c6ec8b76eb26; Error summary: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum allowaamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)eout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MessageTotractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)work.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadxt.java:348) at io.netty.channel.AbstractCha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:31660,Integrability,Message,MessageTotractChannelHandlerContext,31660,ByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)eout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MessageTotractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)work.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelReadxt.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.channeleChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerC.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLootLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) at io.netty.util.cotRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.java:745) at org.apache.spark.storage.ShuffleBlockFet.next(ShuffleBlockFetcherIte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:34958,Integrability,Wrap,Wrappers,34958,"sRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstraceChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:34988,Integrability,Wrap,Wrappers,34988,"g.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstraceChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:37016,Integrability,Message,MessageToMessageDecoder,37016,hannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:37052,Integrability,Message,MessageToMessageDecoder,37052,hannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:7294,Performance,concurren,concurrent,7294,"he.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerConte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:7377,Performance,concurren,concurrent,7377,"k.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.sparkapache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.sparkitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDDputeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MaD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpointla:90) at org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:4cala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$ailureException: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Ab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:9541,Performance,concurren,concurrent,9541,nvokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHaetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at nnelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(A1359) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstracead(DefaultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at ed(NioEventLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.Nio at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thrpark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at eChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:10942,Performance,concurren,concurrent,10942,Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:11038,Performance,concurren,concurrent,11038,nvokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunct,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:20114,Performance,concurren,concurrent,20114,"apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:20197,Performance,concurren,concurrent,20197,"spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:22366,Performance,concurren,concurrent,22366,annelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:23771,Performance,concurren,concurrent,23771,(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:23867,Performance,concurren,concurrent,23867,eChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:34203,Performance,concurren,concurrent,34203,"apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:34286,Performance,concurren,concurrent,34286,"spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.Mappark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iteratoadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartiti288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scat org.apache.spark.scheduler.Task.run(Task.scala:121) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at ) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.rception: Failure while fetching StreamChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:36455,Performance,concurren,concurrent,36455,annelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:37860,Performance,concurren,concurrent,37860,hannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:37956,Performance,concurren,concurrent,37956,hannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoot io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) at java.lang.Thread.run(Thread.javawork.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:3374,Safety,abort,aborted,3374,"tnb/ukbiobank/ad/analysis/liftover/liftover.py"", line 29, in <module>; hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); File ""</share/pkg.7/hail/0.2.19/install/python/lib/python3.6/site-packages/decorator.py:decorator-gen-1289>"", line 2, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 585, in wrapper; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 513, in export_vcf; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 108, in execute; File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.19/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 221, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the mmChunkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOn) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:4473,Safety,timeout,timeout,4473,"nkId{streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_ion(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrow.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBloa:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.Na.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOn) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.netty.handler.codec.MRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jpark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at io.nettyxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelnnel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.netty.channel.nio.NioEventLoop.processSelectedKey(Nio.NioEventLoop.pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:8459,Safety,timeout,timeout,8459,"reamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/t sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.javannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:40rage.BlockManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.rator$$anon$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.sparkler.processFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channelxt.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHaetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerCactChannelHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at nnelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(A1359) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.Abstracead(DefaultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at ed(NioEventLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:11106,Safety,abort,aborted,11106,c.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032); at org.apache.spark.rdd.PairRDDFunctio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:16186,Safety,abort,aborted,16186,"; at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum alloreamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_15657888296Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.netty.channel.Abst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:21277,Safety,timeout,timeout,21277,"streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstraceChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24077,Safety,abort,abortStage,24077,348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.r,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24174,Safety,abort,abortStage,24174,ext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:24416,Safety,abort,abortStage,24416,ava:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:30273,Safety,abort,aborted,30273,"tations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); at is.hail.backend.Backend.execute(Backend.scala:86); at is.hail.backend.Backend.executeJSON(Backend.scala:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.19-c6ec8b76eb26; Error summary: SparkException: Job aborted due to stage failure: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) has failed the maximum allowaamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616Exception.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOExcee.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at org.apache.spark.shuffle.IndexShuffleBlockResolvt org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61) at org.apache.spark.network.netty.NettyBloction.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamMt org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:101) at org.apache.spark.network.server.TransportractChannelHandlerContext.java:362) at io.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:35366,Safety,timeout,timeout,35366,"streamId=830947795015, chunkIndex=0}: java.nio.file.NoSuchFileException: /data03/hadoop/yarn/local/usnio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) xFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at ckManager.getBlockData(BlockManager.scala:382) at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61non$11.next(Iterator.scala:410) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.apache.spark.networkessFetchRequest(TransportRequestHandler.java:130) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.jnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstraceChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerConnnel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.ielHandlerContext.java:340) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.nettylerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractCt io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelultChannelPipeline.java:935) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138) at io.nettyentLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:1046,Testability,LOG,LOGGING,1046,"nk GWAS for chr22. Any suggestions for this?. The code used is:. ```; chr=sys.argv[1]. bgen=""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr""+chr+""_v3.bgen""; sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; # hl.index_bgen(bgen); mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); print(mt.describe()); rg37 = hl.get_reference('GRCh37'); rg38 = hl.get_reference('GRCh38'); rg37.add_liftover('file:///restricted/projectnb/ukbiobank/ad/analysis/liftover/grch37_to_grch38.over.chain.gz', rg38); mt = mt.annotate_rows(new_locus=hl.liftover(mt.locus, 'GRCh38'), old_locus=mt.locus); mt = mt.filter_rows(hl.is_defined(mt.new_locus)); # mt = mt.key_rows_by(locus=mt.new_locus); print(mt.describe()); mt = mt.key_rows_by(locus=mt.new_locus,alleles=mt.alleles); print(mt.describe()); hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); ```. ```; Version 0.2.19-c6ec8b76eb26; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/liftover/hail-20200214-1434-0.2.19-c6ec8b76eb26.log; 2020-02-14 14:35:19 Hail: INFO: Number of BGEN files parsed: 1; 2020-02-14 14:35:19 Hail: INFO: Number of samples in BGEN files: 487409; 2020-02-14 14:35:19 Hail: INFO: Number of variants across all BGEN files: 1255683. Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'new_locus': locus<GRCh38>; 'old_locus': locus<GRCh37>; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'new_locus' (row); 'old_locus' (row); [Stage 0:========================================",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/issues/8106:1158,Testability,log,log,1158,"/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; # hl.index_bgen(bgen); mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); print(mt.describe()); rg37 = hl.get_reference('GRCh37'); rg38 = hl.get_reference('GRCh38'); rg37.add_liftover('file:///restricted/projectnb/ukbiobank/ad/analysis/liftover/grch37_to_grch38.over.chain.gz', rg38); mt = mt.annotate_rows(new_locus=hl.liftover(mt.locus, 'GRCh38'), old_locus=mt.locus); mt = mt.filter_rows(hl.is_defined(mt.new_locus)); # mt = mt.key_rows_by(locus=mt.new_locus); print(mt.describe()); mt = mt.key_rows_by(locus=mt.new_locus,alleles=mt.alleles); print(mt.describe()); hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); ```. ```; Version 0.2.19-c6ec8b76eb26; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/liftover/hail-20200214-1434-0.2.19-c6ec8b76eb26.log; 2020-02-14 14:35:19 Hail: INFO: Number of BGEN files parsed: 1; 2020-02-14 14:35:19 Hail: INFO: Number of samples in BGEN files: 487409; 2020-02-14 14:35:19 Hail: INFO: Number of variants across all BGEN files: 1255683. Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'new_locus': locus<GRCh38>; 'old_locus': locus<GRCh37>; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'new_locus' (row); 'old_locus' (row); [Stage 0:======================================================>(292 + 1) / 293]2020-02-14 14:38:52 Hail: INFO: Ordering unsorted dataset with netw; 2020-02-14 14:38:52 Hail: WARN: export_vcf found no row field 'info'. Emitting no INF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106
https://github.com/hail-is/hail/pull/8107:35,Deployability,configurat,configuration,35,"@danking the changes in the router configuration for the blog are causing an infinite redirect loop when you try to go to https://blog.hail.is. I'd like to change them back to how they were before (I added the X-Real-IP line that was not in the original configuration, which I think is the change you were introducing in that PR, although I have no idea how that works.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107
https://github.com/hail-is/hail/pull/8107:254,Deployability,configurat,configuration,254,"@danking the changes in the router configuration for the blog are causing an infinite redirect loop when you try to go to https://blog.hail.is. I'd like to change them back to how they were before (I added the X-Real-IP line that was not in the original configuration, which I think is the change you were introducing in that PR, although I have no idea how that works.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107
https://github.com/hail-is/hail/pull/8107:28,Integrability,rout,router,28,"@danking the changes in the router configuration for the blog are causing an infinite redirect loop when you try to go to https://blog.hail.is. I'd like to change them back to how they were before (I added the X-Real-IP line that was not in the original configuration, which I think is the change you were introducing in that PR, although I have no idea how that works.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107
https://github.com/hail-is/hail/pull/8107:35,Modifiability,config,configuration,35,"@danking the changes in the router configuration for the blog are causing an infinite redirect loop when you try to go to https://blog.hail.is. I'd like to change them back to how they were before (I added the X-Real-IP line that was not in the original configuration, which I think is the change you were introducing in that PR, although I have no idea how that works.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107
https://github.com/hail-is/hail/pull/8107:254,Modifiability,config,configuration,254,"@danking the changes in the router configuration for the blog are causing an infinite redirect loop when you try to go to https://blog.hail.is. I'd like to change them back to how they were before (I added the X-Real-IP line that was not in the original configuration, which I think is the change you were introducing in that PR, although I have no idea how that works.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107
https://github.com/hail-is/hail/pull/8112:25,Availability,error,error,25,Addresses this transient error I just encountered.; ```; + docker push gcr.io/hail-vdc/ci-intermediate:tn05m3kr79i2; The push refers to repository [gcr.io/hail-vdc/ci-intermediate]; Get https://gcr.io/v2/: dial tcp: lookup gcr.io: Temporary failure in name resolution; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8112
https://github.com/hail-is/hail/pull/8112:241,Availability,failure,failure,241,Addresses this transient error I just encountered.; ```; + docker push gcr.io/hail-vdc/ci-intermediate:tn05m3kr79i2; The push refers to repository [gcr.io/hail-vdc/ci-intermediate]; Get https://gcr.io/v2/: dial tcp: lookup gcr.io: Temporary failure in name resolution; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8112
https://github.com/hail-is/hail/issues/8114:2505,Availability,Error,Error,2505,$mcJ$sp(CompileAndEvaluate.scala:26); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1$$anonfun$1.apply(CompileAndEvaluate.scala:26); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1$$anonfun$1.apply(CompileAndEvaluate.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Hail version: 0.2.32-e973d7f3c15c; Error summary: ArrayIndexOutOfBoundsException: 3366; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8114
https://github.com/hail-is/hail/pull/8116:49,Integrability,protocol,protocol,49,The proxy should propagate an existing forwarded protocol and; host if either exists. See the definition of `$updated_...` in; `nginx.conf`. Revert blog to standard proxy rules now that proxy rules are correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8116
https://github.com/hail-is/hail/issues/8118:5,Availability,Error,Error,5,"```; Error summary: ZipException: File does not conform to block gzip format.; ```; Good to print the file in a large pipeline, but especially tricky in a glob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8118
https://github.com/hail-is/hail/issues/8118:118,Deployability,pipeline,pipeline,118,"```; Error summary: ZipException: File does not conform to block gzip format.; ```; Good to print the file in a large pipeline, but especially tricky in a glob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8118
https://github.com/hail-is/hail/pull/8121:10,Availability,error,error,10,We got an error because a BatchBuilder has no id attribute,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8121
https://github.com/hail-is/hail/pull/8122:368,Availability,error,error,368,"This node filters an NDArray to a smaller NDArray that has only the specified rows/cols/etc. along a specific axis, in the specified order. It will be used for lowering BlockMatrixFilter. If the array of indices along a given axis is missing, then we preserve the original indexing along that axis. If an element in the array of indices is missing, we throw a runtime error. The filter/don't filter decision for a given axis could also be lifted to a compile-time decision, rather than a runtime decision (say by having the type of the filter along that axis being `ttuple[]` instead of `tarray<tint64>`). Unlike NDArraySlice, it does not give the option of reducing the number of dimensions of the NDArray, although I can change that if we have a use for it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8122
https://github.com/hail-is/hail/pull/8123:562,Availability,error,error,562,"Right now, IR function registration fails in the following case.; - Multiple Python HailContexts are running on the same cluster; - User on HailContext A calls a function (eg. create histogram) that triggers `hl.experimental.define_function`; * An anonymous function is used to create a new `Set` in `IRFunctionRegistry.irRegistry`; - User on HailContext B calls the same function; * The anonymous function is added to create a `Set` of size `2` in `IRFunctionRegistry.irRegistry`, as anonymous functions are never considered equivalent; * This triggers a fatal error `Multiple functions found that satisfy ...`. By changing the definition of `IRFunctionRegistry.irRegistry` from a `MultiMap(functionName -> Set[argumentTypes, returnType, alwaysInline, anonymousFunction])` to a `Map(functionName -> Map((argumentTypes, returnType, alwaysInline) -> anonymousFunction))` , we ensure function registration is idempotent as we do not compare on the `anonymousFunction`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8123
https://github.com/hail-is/hail/pull/8128:30,Energy Efficiency,schedul,scheduler,30,"Here I go, killing again! The scheduler is dead. Long live batch, the one true scheduler!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8128
https://github.com/hail-is/hail/pull/8128:79,Energy Efficiency,schedul,scheduler,79,"Here I go, killing again! The scheduler is dead. Long live batch, the one true scheduler!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8128
https://github.com/hail-is/hail/pull/8129:2191,Deployability,pipeline,pipeline,2191,"lder` and a `JoinPointBuilder`. It then produces six pieces of information for its consumer to use, packaged into a `Source` object. A `Stream` must satisfy the following invariants:; * `close` and `close0` must be okay to emit multiple times, i.e. they should be function calls, or a small number of function calls. (This is because I don't see a way to unify the control paths of the cases where a producer ends the stream, and where the consumer ends it.); * `eos` and `push` must be emitted at most once.; * Any local variables created by the stream must be initialized to dummy values in `setup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any consumption of the stream. (This is primarily used for flatMap, where `firstPull` pulls from the outer stream, and `pull` pulls from the inner stream.); * `push` must ultimately either give control back to `pull`, or run `close`.; * `setup0` must dominate all other code provided by the `Stream`. I.e., all possible control flow paths leading to any code in `Source` must go through `setup0`. (In particular, in flatMa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:2815,Energy Efficiency,consumption,consumption,2815,"st be initialized to dummy values in `setup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any consumption of the stream. (This is primarily used for flatMap, where `firstPull` pulls from the outer stream, and `pull` pulls from the inner stream.); * `push` must ultimately either give control back to `pull`, or run `close`.; * `setup0` must dominate all other code provided by the `Stream`. I.e., all possible control flow paths leading to any code in `Source` must go through `setup0`. (In particular, in flatMap, the `setup0` of the inner stream must be run before the outer stream is started.). The ""tests"" only print out streams along with ""setup"" and ""close"" messages to visually ensure they are being triggered properly; obviously those will have to change before this is merged. My plan is to write an instrumented range that sets flags in its setup and close code, so those flags can be inspected in asserts. But if speed is more important, I can put off those tests. The informal tests give me high confidence there are currently no bugs at the level of the s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:613,Integrability,wrap,wraps,613,"This PR adds a new staged stream abstraction, which is:. ```; case class Source[+A](; setup0: Code[Unit],; close0: Code[Unit],; setup: Code[Unit],; close: Code[Unit],; firstPull: Option[Code[Ctrl]],; pull: Code[Ctrl]). abstract class Stream[+A] {; def apply(; eos: Code[Ctrl],; push: A => Code[Ctrl]; )(implicit ctx: EmitStreamContext; ): Source[A]; }; ```. It implements fold, forEach, unfold (and range using unfold), map, flatMap, filter, zip, and mux (if cond then stream1 else stream2). It also implements emit rules for `ArrayFold` and `ArrayFor`, plus a initial reimplimentation of `EmitStream` which just wraps the result of `EmitStream` in a converter from the old to the new stream representation. Porting the rest of the stream nodes from `EmitStream` should be straightforward. Once that is done, I will use the setup/close hooks to implement region management in the stream emitter. A `Stream` takes two pieces of information from its consumer: a `Code` to run when at the end of the stream, and a `Code` to run when the stream produces a value. (When I say ""run when..."", I really mean ""inline at the point in the control flow at which...""). It also takes an `EmitStreamContext`, which is just a bundle of a `MethodBuilder` and a `JoinPointBuilder`. It then produces six pieces of information for its consumer to use, packaged into a `Source` object. A `Stream` must satisfy the following invariants:; * `close` and `close0` must be okay to emit multiple times, i.e. they should be function calls, or a small number of function calls. (This is because I don't see a way to unify the control paths of the cases where a producer ends the stream, and where the consumer ends it.); * `eos` and `push` must be emitted at most once.; * Any local variables created by the stream must be initialized to dummy values in `setup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a valu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:3385,Integrability,message,messages,3385,"etup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any consumption of the stream. (This is primarily used for flatMap, where `firstPull` pulls from the outer stream, and `pull` pulls from the inner stream.); * `push` must ultimately either give control back to `pull`, or run `close`.; * `setup0` must dominate all other code provided by the `Stream`. I.e., all possible control flow paths leading to any code in `Source` must go through `setup0`. (In particular, in flatMap, the `setup0` of the inner stream must be run before the outer stream is started.). The ""tests"" only print out streams along with ""setup"" and ""close"" messages to visually ensure they are being triggered properly; obviously those will have to change before this is merged. My plan is to write an instrumented range that sets flags in its setup and close code, so those flags can be inspected in asserts. But if speed is more important, I can put off those tests. The informal tests give me high confidence there are currently no bugs at the level of the stream combinators. cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:1754,Modifiability,variab,variables,1754,"l use the setup/close hooks to implement region management in the stream emitter. A `Stream` takes two pieces of information from its consumer: a `Code` to run when at the end of the stream, and a `Code` to run when the stream produces a value. (When I say ""run when..."", I really mean ""inline at the point in the control flow at which...""). It also takes an `EmitStreamContext`, which is just a bundle of a `MethodBuilder` and a `JoinPointBuilder`. It then produces six pieces of information for its consumer to use, packaged into a `Source` object. A `Stream` must satisfy the following invariants:; * `close` and `close0` must be okay to emit multiple times, i.e. they should be function calls, or a small number of function calls. (This is because I don't see a way to unify the control paths of the cases where a producer ends the stream, and where the consumer ends it.); * `eos` and `push` must be emitted at most once.; * Any local variables created by the stream must be initialized to dummy values in `setup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:3324,Testability,test,tests,3324,"etup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any consumption of the stream. (This is primarily used for flatMap, where `firstPull` pulls from the outer stream, and `pull` pulls from the inner stream.); * `push` must ultimately either give control back to `pull`, or run `close`.; * `setup0` must dominate all other code provided by the `Stream`. I.e., all possible control flow paths leading to any code in `Source` must go through `setup0`. (In particular, in flatMap, the `setup0` of the inner stream must be run before the outer stream is started.). The ""tests"" only print out streams along with ""setup"" and ""close"" messages to visually ensure they are being triggered properly; obviously those will have to change before this is merged. My plan is to write an instrumented range that sets flags in its setup and close code, so those flags can be inspected in asserts. But if speed is more important, I can put off those tests. The informal tests give me high confidence there are currently no bugs at the level of the stream combinators. cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:3629,Testability,assert,asserts,3629,"etup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any consumption of the stream. (This is primarily used for flatMap, where `firstPull` pulls from the outer stream, and `pull` pulls from the inner stream.); * `push` must ultimately either give control back to `pull`, or run `close`.; * `setup0` must dominate all other code provided by the `Stream`. I.e., all possible control flow paths leading to any code in `Source` must go through `setup0`. (In particular, in flatMap, the `setup0` of the inner stream must be run before the outer stream is started.). The ""tests"" only print out streams along with ""setup"" and ""close"" messages to visually ensure they are being triggered properly; obviously those will have to change before this is merged. My plan is to write an instrumented range that sets flags in its setup and close code, so those flags can be inspected in asserts. But if speed is more important, I can put off those tests. The informal tests give me high confidence there are currently no bugs at the level of the stream combinators. cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:3690,Testability,test,tests,3690,"etup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any consumption of the stream. (This is primarily used for flatMap, where `firstPull` pulls from the outer stream, and `pull` pulls from the inner stream.); * `push` must ultimately either give control back to `pull`, or run `close`.; * `setup0` must dominate all other code provided by the `Stream`. I.e., all possible control flow paths leading to any code in `Source` must go through `setup0`. (In particular, in flatMap, the `setup0` of the inner stream must be run before the outer stream is started.). The ""tests"" only print out streams along with ""setup"" and ""close"" messages to visually ensure they are being triggered properly; obviously those will have to change before this is merged. My plan is to write an instrumented range that sets flags in its setup and close code, so those flags can be inspected in asserts. But if speed is more important, I can put off those tests. The informal tests give me high confidence there are currently no bugs at the level of the stream combinators. cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8129:3710,Testability,test,tests,3710,"etup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any consumption of the stream. (This is primarily used for flatMap, where `firstPull` pulls from the outer stream, and `pull` pulls from the inner stream.); * `push` must ultimately either give control back to `pull`, or run `close`.; * `setup0` must dominate all other code provided by the `Stream`. I.e., all possible control flow paths leading to any code in `Source` must go through `setup0`. (In particular, in flatMap, the `setup0` of the inner stream must be run before the outer stream is started.). The ""tests"" only print out streams along with ""setup"" and ""close"" messages to visually ensure they are being triggered properly; obviously those will have to change before this is merged. My plan is to write an instrumented range that sets flags in its setup and close code, so those flags can be inspected in asserts. But if speed is more important, I can put off those tests. The informal tests give me high confidence there are currently no bugs at the level of the stream combinators. cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129
https://github.com/hail-is/hail/pull/8145:54,Usability,undo,undocumented,54,"Use ToArray(StreamRange(...)) instead; added tstream (undocumented), StreamRange to Python. This is the first step in: https://dev.hail.is/t/cleaning-up-titerable-hierarchy/181/2. Next I will make the Array* operations Stream-only. Then I can rip out TStreamable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8145
https://github.com/hail-is/hail/issues/8146:74,Availability,error,error-output-from-a-hail-pipeline,74,See discuss post: https://discuss.hail.is/t/redirect-or-find-vep-or-other-error-output-from-a-hail-pipeline/1308/9?u=danking. It looks like Hail isn’t capturing all the VEP output. Can someone look into this? Probably the way we’re executing external commands needs to also capture stderr and print it. Assigning Tim for delegation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146
https://github.com/hail-is/hail/issues/8146:99,Deployability,pipeline,pipeline,99,See discuss post: https://discuss.hail.is/t/redirect-or-find-vep-or-other-error-output-from-a-hail-pipeline/1308/9?u=danking. It looks like Hail isn’t capturing all the VEP output. Can someone look into this? Probably the way we’re executing external commands needs to also capture stderr and print it. Assigning Tim for delegation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146
https://github.com/hail-is/hail/issues/8147:24,Testability,log,log,24,Please check the change log and git history to determine if removing persist from VEP.scala was intentional. Please respond to discuss forum upon completion. . https://discuss.hail.is/t/parallel-hail-tasks/865/16?u=danking. Dice says John.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8147
https://github.com/hail-is/hail/pull/8148:205,Integrability,wrap,wrapped,205,Replace with ToArray(MakeStream(...)); Keep MakeArray ctor for compatibility; FIXME ToArray memoization rule isn't complete; FIXME? hack to make ArrayZip of MakeStreams work; FIXME MakeStream needs to get wrapped,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148
https://github.com/hail-is/hail/pull/8149:967,Availability,down,down,967,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:37,Energy Efficiency,schedul,schedule,37,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:81,Energy Efficiency,schedul,scheduled,81,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:881,Energy Efficiency,schedul,scheduler,881,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:941,Energy Efficiency,schedul,scheduler,941,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:1009,Energy Efficiency,schedul,schedule,1009,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:1728,Energy Efficiency,schedul,scheduler,1728,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:1803,Energy Efficiency,schedul,scheduling,1803,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:1820,Energy Efficiency,schedul,schedules,1820,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:1316,Performance,load,load,1316,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:866,Safety,timeout,timeout,866,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:1055,Safety,timeout,timeout,1055,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:47,Testability,log,log,47,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:100,Testability,log,log,100,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:1888,Testability,log,logging,1888,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8149:653,Usability,clear,clear,653,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149
https://github.com/hail-is/hail/pull/8156:501,Modifiability,extend,extendNA,501,"~~Stacked on #8140~~. This PR adds support for the new streams in the emitter for ArrayFilter, ArrayZip, ArrayFlatMap, If, and Let. It also changes the semantics of `Stream` slightly: Before, a producer had to close itself before calling EOS on its consumer. Now, the consumer is responsible for closing the producer after the producer calls EOS. This makes flatMap slightly more complicated, but it allows a significant simplification to zip. To implement the different modes of `ArrayZip`, I added `extendNA` and `take`. `extendNA` turns a stream into an infinite stream, where values after the end are missing, to allow the consumer to decide what to do when the stream ends. `take` is then needed to end infinite streams. Both use `COption` to encode whether the child stream has ended / whether to end the child stream. I still intend to convert uses of `COption` to use `CodeConditional` after lowering is unblocked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156
https://github.com/hail-is/hail/pull/8156:524,Modifiability,extend,extendNA,524,"~~Stacked on #8140~~. This PR adds support for the new streams in the emitter for ArrayFilter, ArrayZip, ArrayFlatMap, If, and Let. It also changes the semantics of `Stream` slightly: Before, a producer had to close itself before calling EOS on its consumer. Now, the consumer is responsible for closing the producer after the producer calls EOS. This makes flatMap slightly more complicated, but it allows a significant simplification to zip. To implement the different modes of `ArrayZip`, I added `extendNA` and `take`. `extendNA` turns a stream into an infinite stream, where values after the end are missing, to allow the consumer to decide what to do when the stream ends. `take` is then needed to end infinite streams. Both use `COption` to encode whether the child stream has ended / whether to end the child stream. I still intend to convert uses of `COption` to use `CodeConditional` after lowering is unblocked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156
https://github.com/hail-is/hail/pull/8156:421,Usability,simpl,simplification,421,"~~Stacked on #8140~~. This PR adds support for the new streams in the emitter for ArrayFilter, ArrayZip, ArrayFlatMap, If, and Let. It also changes the semantics of `Stream` slightly: Before, a producer had to close itself before calling EOS on its consumer. Now, the consumer is responsible for closing the producer after the producer calls EOS. This makes flatMap slightly more complicated, but it allows a significant simplification to zip. To implement the different modes of `ArrayZip`, I added `extendNA` and `take`. `extendNA` turns a stream into an infinite stream, where values after the end are missing, to allow the consumer to decide what to do when the stream ends. `take` is then needed to end infinite streams. Both use `COption` to encode whether the child stream has ended / whether to end the child stream. I still intend to convert uses of `COption` to use `CodeConditional` after lowering is unblocked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156
https://github.com/hail-is/hail/issues/8158:150,Integrability,interface,interface,150,"analogous to gcloud auth login --no-launch-browser for authenticating on a remote server where we can't use localhost callback. We also need a Python interface that can be used from a Jupyter notebook, something like:. ```; hailtop.auth.login(); ==> returns URL to navigate to; hailtop.auth.complete_login('<verification ID>'); ```. where the verification ID is obtained from Google and pasted in. - [ ] hailctl auth login --no-launch-browser; - [ ] remote Python auth flow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8158
https://github.com/hail-is/hail/issues/8158:55,Security,authenticat,authenticating,55,"analogous to gcloud auth login --no-launch-browser for authenticating on a remote server where we can't use localhost callback. We also need a Python interface that can be used from a Jupyter notebook, something like:. ```; hailtop.auth.login(); ==> returns URL to navigate to; hailtop.auth.complete_login('<verification ID>'); ```. where the verification ID is obtained from Google and pasted in. - [ ] hailctl auth login --no-launch-browser; - [ ] remote Python auth flow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8158
https://github.com/hail-is/hail/issues/8158:25,Testability,log,login,25,"analogous to gcloud auth login --no-launch-browser for authenticating on a remote server where we can't use localhost callback. We also need a Python interface that can be used from a Jupyter notebook, something like:. ```; hailtop.auth.login(); ==> returns URL to navigate to; hailtop.auth.complete_login('<verification ID>'); ```. where the verification ID is obtained from Google and pasted in. - [ ] hailctl auth login --no-launch-browser; - [ ] remote Python auth flow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8158
https://github.com/hail-is/hail/issues/8158:237,Testability,log,login,237,"analogous to gcloud auth login --no-launch-browser for authenticating on a remote server where we can't use localhost callback. We also need a Python interface that can be used from a Jupyter notebook, something like:. ```; hailtop.auth.login(); ==> returns URL to navigate to; hailtop.auth.complete_login('<verification ID>'); ```. where the verification ID is obtained from Google and pasted in. - [ ] hailctl auth login --no-launch-browser; - [ ] remote Python auth flow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8158
https://github.com/hail-is/hail/issues/8158:417,Testability,log,login,417,"analogous to gcloud auth login --no-launch-browser for authenticating on a remote server where we can't use localhost callback. We also need a Python interface that can be used from a Jupyter notebook, something like:. ```; hailtop.auth.login(); ==> returns URL to navigate to; hailtop.auth.complete_login('<verification ID>'); ```. where the verification ID is obtained from Google and pasted in. - [ ] hailctl auth login --no-launch-browser; - [ ] remote Python auth flow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8158
https://github.com/hail-is/hail/issues/8159:72,Testability,log,login,72,There are some additional Google hoops to jump through to enable oauth2 login from non-organization accounts.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8159
https://github.com/hail-is/hail/issues/8161:18,Availability,error,error,18,"Masahiro got this error message: . ```; File ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:1033,Availability,Error,Error,1033,"ile ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.backend.Backend$$anonfun$exe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:1218,Availability,error,error,1218,"); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.utils.package$.using(package.scala:596); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); at is.hail.expr.ir.ExecuteCont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:24,Integrability,message,message,24,"Masahiro got this error message: . ```; File ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:448,Integrability,wrap,wrapper,448,"Masahiro got this error message: . ```; File ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:1579,Integrability,Wrap,WrappedArray,1579,", in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.utils.package$.using(package.scala:596); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); at is.hail.backend.Backend.execute(Backend.scala:56); at is.hail.backend.Backend.executeJSON(Backend.scala:62); at sun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:1600,Integrability,Wrap,WrappedArray,1600," Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.utils.package$.using(package.scala:596); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); at is.hail.backend.Backend.execute(Backend.scala:56); at is.hail.backend.Backend.executeJSON(Backend.scala:62); at sun.reflect.NativeMe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:774,Performance,load,loads,774,"Masahiro got this error message: . ```; File ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:1137,Testability,Assert,AssertionError,1137," f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.utils.package$.using(package.scala:596); at is.hail.expr.ir.Exe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:1153,Testability,assert,assertion,1153," f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); at is.hail.utils.package$.using(package.scala:596); at is.hail.expr.ir.Exe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8161:4234,Testability,assert,assertionError,4234,"Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); at is.hail.backend.Backend.execute(Backend.scala:56); at is.hail.backend.Backend.executeJSON(Backend.scala:62); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748); ```. When running this code:. ```; import hail as hl. # cf. https://github.com/Nealelab/ukb_common/blob/master/saige/extract_vcf_from_mt.py; def gt_to_gp(mt, gt_location: str = 'GT', gp_location: str = 'GP'):; return mt.annotate_entries(; **{; gp_location:; hl.map(lambda i: hl.cond(mt[gt_location] == i, 1.0, 0.0), hl.range(0, hl.triangle(hl.len(mt.alleles)))); }). chrom = 1; mt = hl.read_matrix_table(f'gs://ukbb-hail/ukb31063.dosage.pGT.gwas_samples.chr{chrom}.mt'). # write bgen as well; mt = gt_to_gp(mt); hl.export_bgen(mt, filename); ```. Turns out he did not handle missingness correctly, needed something more like this:. ```; def gt_to_gp(mt, location: str = 'GP'):; return mt.annotate_entries(**{location: hl.or_missing(; hl.is_defined(mt.GT),; hl.map(lambda i: hl.cond(mt.GT.unphased_diploid_gt_index() == i, 1.0, 0.0),; hl.range(0, hl.triangle(hl.len(mt.alleles)))))}); ```. We should at least make a change so that the user doesn't get an assertionError in this scenario, and should maybe also consider providing some sort of `gt_to_gp` function in Hail since Konrad also uses this thing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8161
https://github.com/hail-is/hail/issues/8163:71,Deployability,pipeline,pipeline,71,"The dice came up Patrick for compiler team. From Masa via Zulip:. full pipeline is literally gt_to_gp then export_bgen. ```; import hail as hl; import atexit; import datetime. atexit.register(; lambda: hl.copy_log(f'gs://ukbb-hail/export_hardcall_bgen_{datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")}.log')). # cf. https://github.com/Nealelab/ukb_common/blob/master/saige/extract_vcf_from_mt.py; def gt_to_gp(mt, gt_location: str = 'GT', gp_location: str = 'GP'):; return mt.annotate_entries(; **{; gp_location:; hl.or_missing(; hl.is_defined(mt[gt_location]),; hl.map(lambda i: hl.cond(mt[gt_location] == i, 1.0, 0.0),; hl.range(0, hl.triangle(hl.len(mt.alleles))))); }). chrom = 1; mt = hl.read_matrix_table(f'gs://ukbb-hail/ukb31063.dosage.pGT.gwas_samples.chr{chrom}.mt'). # write bgen as well; mt = gt_to_gp(mt); hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); ```; doesn't work with highmem, working now on ultramem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8163
https://github.com/hail-is/hail/issues/8163:304,Testability,log,log,304,"The dice came up Patrick for compiler team. From Masa via Zulip:. full pipeline is literally gt_to_gp then export_bgen. ```; import hail as hl; import atexit; import datetime. atexit.register(; lambda: hl.copy_log(f'gs://ukbb-hail/export_hardcall_bgen_{datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")}.log')). # cf. https://github.com/Nealelab/ukb_common/blob/master/saige/extract_vcf_from_mt.py; def gt_to_gp(mt, gt_location: str = 'GT', gp_location: str = 'GP'):; return mt.annotate_entries(; **{; gp_location:; hl.or_missing(; hl.is_defined(mt[gt_location]),; hl.map(lambda i: hl.cond(mt[gt_location] == i, 1.0, 0.0),; hl.range(0, hl.triangle(hl.len(mt.alleles))))); }). chrom = 1; mt = hl.read_matrix_table(f'gs://ukbb-hail/ukb31063.dosage.pGT.gwas_samples.chr{chrom}.mt'). # write bgen as well; mt = gt_to_gp(mt); hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); ```; doesn't work with highmem, working now on ultramem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8163
https://github.com/hail-is/hail/issues/8165:364,Availability,error,error,364,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:424,Availability,error,error,424,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:520,Availability,Error,Error,520,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:947,Availability,error,error,947,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1043,Availability,Error,Error,1043,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1447,Availability,down,down,1447,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1460,Availability,error,error,1460,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1479,Availability,error,error,1479,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:26,Deployability,release,release,26,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:74,Deployability,upgrade,upgraded,74,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:370,Integrability,message,message,370,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:411,Modifiability,sandbox,sandbox,411,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:469,Modifiability,sandbox,sandbox,469,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:934,Modifiability,sandbox,sandbox,934,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:992,Modifiability,sandbox,sandbox,992,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1286,Modifiability,Sandbox,SandboxChanged,1286,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1378,Modifiability,sandbox,sandbox,1378,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:411,Testability,sandbox,sandbox,411,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:469,Testability,sandbox,sandbox,469,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:934,Testability,sandbox,sandbox,934,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:992,Testability,sandbox,sandbox,992,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1286,Testability,Sandbox,SandboxChanged,1286,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/issues/8165:1378,Testability,sandbox,sandbox,1378,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8165
https://github.com/hail-is/hail/pull/8166:39,Integrability,message,message-using-markdown,39,https://zulipchat.com/help/format-your-message-using-markdown#mentions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8166
https://github.com/hail-is/hail/pull/8168:126,Testability,test,tests,126,New IR node that makes an array of zeros. Good for initializing large things. Mostly I want this so I can do some memory leak tests that don't take multiple minutes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8168
https://github.com/hail-is/hail/pull/8169:63,Security,access,access,63,I think this is the correct fix. The batch in default needs to access service accounts in other namespaces.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8169
https://github.com/hail-is/hail/issues/8170:251,Availability,error,error,251,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8170
https://github.com/hail-is/hail/issues/8170:290,Availability,error,error,290,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8170
https://github.com/hail-is/hail/issues/8170:303,Availability,Error,Error,303,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8170
https://github.com/hail-is/hail/issues/8170:392,Availability,error,error,392,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8170
https://github.com/hail-is/hail/issues/8170:383,Integrability,protocol,protocol,383,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8170
https://github.com/hail-is/hail/issues/8170:496,Modifiability,config,config,496,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8170
https://github.com/hail-is/hail/pull/8171:556,Security,audit,audited,556,"Summary of changes:; - removed TStreamable; - All IR inputs must be TStream or TArray, statically; - renamed Array* nodes to be Stream* (e.g. ArrayMap to StreamMap, etc.); - ToArray, ToDict and ToSet take TStream only; - LowerArrayToStream is gone. This effectively happens in the front end. Most of the code is peppering ToArray and ToStream in the right places. I think this is a nice improvement in two ways:. I makes the IR more transparent by making ToArray, a potentially expensive operation, completely explicit. (Uses of ToArray should probably be audited.). Second, I think it cleans up the flow in Emit/Stream. Now, streams are always compiled by EmitStream, and non-stream values that correspond to EmitTriplets in the code are compiled by Emit only. Finally, I fixed a bug in array sorter that would throw an assert on sets with NaNs (compare false to themselves). I'm a little surprised this isn't failing any tests in master.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171
https://github.com/hail-is/hail/pull/8171:821,Testability,assert,assert,821,"Summary of changes:; - removed TStreamable; - All IR inputs must be TStream or TArray, statically; - renamed Array* nodes to be Stream* (e.g. ArrayMap to StreamMap, etc.); - ToArray, ToDict and ToSet take TStream only; - LowerArrayToStream is gone. This effectively happens in the front end. Most of the code is peppering ToArray and ToStream in the right places. I think this is a nice improvement in two ways:. I makes the IR more transparent by making ToArray, a potentially expensive operation, completely explicit. (Uses of ToArray should probably be audited.). Second, I think it cleans up the flow in Emit/Stream. Now, streams are always compiled by EmitStream, and non-stream values that correspond to EmitTriplets in the code are compiled by Emit only. Finally, I fixed a bug in array sorter that would throw an assert on sets with NaNs (compare false to themselves). I'm a little surprised this isn't failing any tests in master.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171
https://github.com/hail-is/hail/pull/8171:923,Testability,test,tests,923,"Summary of changes:; - removed TStreamable; - All IR inputs must be TStream or TArray, statically; - renamed Array* nodes to be Stream* (e.g. ArrayMap to StreamMap, etc.); - ToArray, ToDict and ToSet take TStream only; - LowerArrayToStream is gone. This effectively happens in the front end. Most of the code is peppering ToArray and ToStream in the right places. I think this is a nice improvement in two ways:. I makes the IR more transparent by making ToArray, a potentially expensive operation, completely explicit. (Uses of ToArray should probably be audited.). Second, I think it cleans up the flow in Emit/Stream. Now, streams are always compiled by EmitStream, and non-stream values that correspond to EmitTriplets in the code are compiled by Emit only. Finally, I fixed a bug in array sorter that would throw an assert on sets with NaNs (compare false to themselves). I'm a little surprised this isn't failing any tests in master.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171
https://github.com/hail-is/hail/pull/8177:199,Availability,error,errors,199,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8177:393,Availability,down,down,393,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8177:112,Energy Efficiency,reduce,reduces,112,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8177:80,Integrability,interface,interface,80,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8177:240,Integrability,depend,depend,240,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8177:157,Performance,load,loading,157,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8177:225,Performance,load,loading,225,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8177:574,Performance,load,loadElementToIRIntermediate,574,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8177
https://github.com/hail-is/hail/pull/8178:30,Deployability,deploy,deploy,30,"I verified this manually. The deploy account didn't exist when I started this PR, but it still had a role grant in the project's IAM policy. Now the account still does not exist *and* the role grant is gone. The `login` forces you to switch to your account since we're deleting the service account (as which you're probably authenticated).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8178
https://github.com/hail-is/hail/pull/8178:324,Security,authenticat,authenticated,324,"I verified this manually. The deploy account didn't exist when I started this PR, but it still had a role grant in the project's IAM policy. Now the account still does not exist *and* the role grant is gone. The `login` forces you to switch to your account since we're deleting the service account (as which you're probably authenticated).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8178
https://github.com/hail-is/hail/pull/8178:213,Testability,log,login,213,"I verified this manually. The deploy account didn't exist when I started this PR, but it still had a role grant in the project's IAM policy. Now the account still does not exist *and* the role grant is gone. The `login` forces you to switch to your account since we're deleting the service account (as which you're probably authenticated).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8178
https://github.com/hail-is/hail/pull/8179:157,Integrability,depend,dependent,157,"This the beginning of some Code infrastructure changes and cleanup. The plan is to clean up the tangle of builder classes. I'm still working on the plan for dependent functions, but the skeleton of the rest looks like:. ```; class ClassBuilder[C]; def newMethod(suffix: String, argsInfo: Array[TypeInfo[_]], returnInfo: TypeInfo[_]): MethodBuilder; def newField[T]: ...; def result(): () => C. class MethodBuilder:; def newLocal[T]: LocalRef[T]; def emit(c: Code): Unit. class FunctionBuilder[C]:; val classBuilder: ClassBuilder[C]; def applyMethod: MethodBuilder; def result(): () => C = classBuilder.result(). class EmitMethodBuilder; val mb: MethodBuilder // contain don't extend; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8179
https://github.com/hail-is/hail/pull/8179:676,Modifiability,extend,extend,676,"This the beginning of some Code infrastructure changes and cleanup. The plan is to clean up the tangle of builder classes. I'm still working on the plan for dependent functions, but the skeleton of the rest looks like:. ```; class ClassBuilder[C]; def newMethod(suffix: String, argsInfo: Array[TypeInfo[_]], returnInfo: TypeInfo[_]): MethodBuilder; def newField[T]: ...; def result(): () => C. class MethodBuilder:; def newLocal[T]: LocalRef[T]; def emit(c: Code): Unit. class FunctionBuilder[C]:; val classBuilder: ClassBuilder[C]; def applyMethod: MethodBuilder; def result(): () => C = classBuilder.result(). class EmitMethodBuilder; val mb: MethodBuilder // contain don't extend; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8179
https://github.com/hail-is/hail/pull/8180:365,Modifiability,parameteriz,parameterize,365,"I wanted `WriteValue` to be able to essentially generate filenames exactly like we currently do in crdd.writePartitions, so I added a `HailTaskContext` concept to be able to mimic that behavior. It'll currently generate files named `prefix` if run on the master, and files named `prefix-stage-partition-…` if run in a distributed setting. We may eventually want to parameterize the behavior of `WriteValue` to have it do different things with the filename, but I think this will be enough to start moving over some of the writers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8180
https://github.com/hail-is/hail/pull/8183:14,Deployability,release,release,14,Don't want to release with this instability,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8183
https://github.com/hail-is/hail/pull/8185:14,Deployability,release,release,14,"bug, blocking release",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8185
https://github.com/hail-is/hail/pull/8186:194,Integrability,depend,dependent,194,Stacked on: https://github.com/hail-is/hail/pull/8179. This adds ModuleBuilder. A module is a collection of classes whose bytecode should be loaded together (like a function and some associated dependent functions). Dependent functions now add themselves to the module.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8186
https://github.com/hail-is/hail/pull/8186:216,Integrability,Depend,Dependent,216,Stacked on: https://github.com/hail-is/hail/pull/8179. This adds ModuleBuilder. A module is a collection of classes whose bytecode should be loaded together (like a function and some associated dependent functions). Dependent functions now add themselves to the module.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8186
https://github.com/hail-is/hail/pull/8186:141,Performance,load,loaded,141,Stacked on: https://github.com/hail-is/hail/pull/8179. This adds ModuleBuilder. A module is a collection of classes whose bytecode should be loaded together (like a function and some associated dependent functions). Dependent functions now add themselves to the module.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8186
https://github.com/hail-is/hail/pull/8189:45,Availability,failure,failure,45,Docker also reports transient DNS resolution failure as 500.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8189
https://github.com/hail-is/hail/pull/8190:31,Availability,error,error,31,"I want the function to exit in error if any step fails. I achieve this by; starting a sub-shell and setting -e inside that sub shell. That causes the; sub-shell to exit if any individual command fails. Previously, the `git clone` could fail but `clone` would still return exit code; zero.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8190
https://github.com/hail-is/hail/pull/8192:14,Availability,redundant,redundant,14,- remove some redundant project git ignores; - add a few more file types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8192
https://github.com/hail-is/hail/pull/8192:14,Safety,redund,redundant,14,- remove some redundant project git ignores; - add a few more file types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8192
https://github.com/hail-is/hail/pull/8193:733,Modifiability,config,config,733,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:18,Performance,race condition,race condition,18,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:181,Performance,cache,cache,181,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:250,Performance,race condition,race condition,250,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:272,Performance,cache,cache,272,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:1171,Performance,load,loads,1171,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:278,Safety,timeout,timeout,278,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:906,Safety,timeout,timeout,906,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8193:914,Safety,timeout,timeout,914,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193
https://github.com/hail-is/hail/pull/8194:23,Deployability,deploy,deployment,23,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:68,Deployability,install,installed,68,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:375,Deployability,install,install,375,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:453,Deployability,update,update-hail-repl,453,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:496,Deployability,update,updates,496,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:681,Deployability,deploy,deploys,681,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:335,Modifiability,variab,variable,335,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:641,Safety,risk,risk,641,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8194:39,Usability,simpl,simply,39,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194
https://github.com/hail-is/hail/pull/8196:91,Deployability,pipeline,pipeline,91,"It's not clear to me what the change that broke this was. Must have been when we added the pipeline docs, but I don't see what used to be calling `upload-docs`. Anyway, calling it now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8196
https://github.com/hail-is/hail/pull/8196:9,Usability,clear,clear,9,"It's not clear to me what the change that broke this was. Must have been when we added the pipeline docs, but I don't see what used to be calling `upload-docs`. Anyway, calling it now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8196
https://github.com/hail-is/hail/pull/8197:159,Testability,test,tests,159,"I use codec.py extensively for debugging the shuffler. Recently the Scala-side support for codec.py was deleted. This PR restores support and adds some simple tests. I need to thread the physical type back to the caller of compile, thus all the changes in CompileAndEvaluate and Backend. They should look better with whitespace changes hidden.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8197
https://github.com/hail-is/hail/pull/8197:152,Usability,simpl,simple,152,"I use codec.py extensively for debugging the shuffler. Recently the Scala-side support for codec.py was deleted. This PR restores support and adds some simple tests. I need to thread the physical type back to the caller of compile, thus all the changes in CompileAndEvaluate and Backend. They should look better with whitespace changes hidden.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8197
https://github.com/hail-is/hail/pull/8199:564,Testability,test,test,564,"~So this PR is me trying to switch just `TableFilter` over to the correct region memory management model we've decided on.~. ~I used this `usingFreshContext` method to pass a new context to the producer that filter is consuming from so that filter can clear unused rows.~. This PR is where I'm cleaning up region management. So far, I've decided the following:. Addressed: ; - `TableFilter`; - `TableHead` (via `rvd.head`); - `TableTail` (via `rvd.tail`). No change needed: ; - `TableMapRows`; - `TableMapGlobals`; - `TableRange`; - `TableLiteral`. I also added a test using the new `hl.zeros` that's intended to catch memory leaks in filter by going through lots of large rows. For some reason, this test is failing on CI with OOM despite seeming to use constant memory on my local machine, I am still looking into why this is. Mostly, I'm wondering if this is consistent with what y'all imagined correct Region management would look like before I go through and do the rest of them. ; @cseed @tpoterba @catoverdrive @patrick-schultz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8199
https://github.com/hail-is/hail/pull/8199:701,Testability,test,test,701,"~So this PR is me trying to switch just `TableFilter` over to the correct region memory management model we've decided on.~. ~I used this `usingFreshContext` method to pass a new context to the producer that filter is consuming from so that filter can clear unused rows.~. This PR is where I'm cleaning up region management. So far, I've decided the following:. Addressed: ; - `TableFilter`; - `TableHead` (via `rvd.head`); - `TableTail` (via `rvd.tail`). No change needed: ; - `TableMapRows`; - `TableMapGlobals`; - `TableRange`; - `TableLiteral`. I also added a test using the new `hl.zeros` that's intended to catch memory leaks in filter by going through lots of large rows. For some reason, this test is failing on CI with OOM despite seeming to use constant memory on my local machine, I am still looking into why this is. Mostly, I'm wondering if this is consistent with what y'all imagined correct Region management would look like before I go through and do the rest of them. ; @cseed @tpoterba @catoverdrive @patrick-schultz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8199
https://github.com/hail-is/hail/pull/8199:252,Usability,clear,clear,252,"~So this PR is me trying to switch just `TableFilter` over to the correct region memory management model we've decided on.~. ~I used this `usingFreshContext` method to pass a new context to the producer that filter is consuming from so that filter can clear unused rows.~. This PR is where I'm cleaning up region management. So far, I've decided the following:. Addressed: ; - `TableFilter`; - `TableHead` (via `rvd.head`); - `TableTail` (via `rvd.tail`). No change needed: ; - `TableMapRows`; - `TableMapGlobals`; - `TableRange`; - `TableLiteral`. I also added a test using the new `hl.zeros` that's intended to catch memory leaks in filter by going through lots of large rows. For some reason, this test is failing on CI with OOM despite seeming to use constant memory on my local machine, I am still looking into why this is. Mostly, I'm wondering if this is consistent with what y'all imagined correct Region management would look like before I go through and do the rest of them. ; @cseed @tpoterba @catoverdrive @patrick-schultz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8199
https://github.com/hail-is/hail/issues/8201:435,Modifiability,config,config,435,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8201
https://github.com/hail-is/hail/issues/8201:873,Performance,load,loads,873,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8201
https://github.com/hail-is/hail/issues/8201:608,Safety,timeout,timeout,608,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8201
https://github.com/hail-is/hail/issues/8201:616,Safety,timeout,timeout,616,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8201
https://github.com/hail-is/hail/pull/8203:744,Performance,queue,queue,744,"Some useful reading:; - [Phantom References in Java Reference Objects](http://www.kdgregory.com/index.php?page=java.refobj#PhantomReferences); - [java.lang.ref](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/package-summary.html#reachability); - [java.lang.ref.PhantomReference](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/PhantomReference.html). The structure is now this:; - `RegionPool` becomes `RegionPoolNativeMemoryOwner`; - a new `RegionPool` class is simply a (unique) reference to a `RegionPoolNativeMemoryOwner`; - `RegionPoolNativeMemoryFreer` is a `PhantomReference` to `RegionPool`; - There is just one instance of all three classes in memory at any given time.; - A single thread is blocking on the reference queue waiting for something to free. How it works:; - The reachability of `RegionPool` defines the reachability the native memory.; - When the `RegionPool` becomes unreachable, the JVM places the corresponding `RegionPoolNativeMemoryFreer` on its reference queue.; - The `cleaner` thread removes the phantom reference from the queue; - At this point, the `RegionPool` is effectively gone, there are no references to it anywhere. Because of that fact, the `RegionPoolNativeMemoryFreer` can free all the native memory by calling `RegionPoolNativeMemoryOwner.free`.; - The `cleaner` thread removes the `RegionPoolNativeMemoryFreer` from the `refs` set which allows the `RegionPoolNativeMemoryFreer` itself to be GC'ed. We do leak the cleaner thread. It will live for the entirety of the JVM process, which seems fine since once you start Hail, you probably want to keep using Hail. I can't find a reference for this, but I'm fairly certain you need to prevent the PhantomReference itself from being GC'ed. That's why I have that set of refs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203
https://github.com/hail-is/hail/pull/8203:1001,Performance,queue,queue,1001,"Some useful reading:; - [Phantom References in Java Reference Objects](http://www.kdgregory.com/index.php?page=java.refobj#PhantomReferences); - [java.lang.ref](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/package-summary.html#reachability); - [java.lang.ref.PhantomReference](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/PhantomReference.html). The structure is now this:; - `RegionPool` becomes `RegionPoolNativeMemoryOwner`; - a new `RegionPool` class is simply a (unique) reference to a `RegionPoolNativeMemoryOwner`; - `RegionPoolNativeMemoryFreer` is a `PhantomReference` to `RegionPool`; - There is just one instance of all three classes in memory at any given time.; - A single thread is blocking on the reference queue waiting for something to free. How it works:; - The reachability of `RegionPool` defines the reachability the native memory.; - When the `RegionPool` becomes unreachable, the JVM places the corresponding `RegionPoolNativeMemoryFreer` on its reference queue.; - The `cleaner` thread removes the phantom reference from the queue; - At this point, the `RegionPool` is effectively gone, there are no references to it anywhere. Because of that fact, the `RegionPoolNativeMemoryFreer` can free all the native memory by calling `RegionPoolNativeMemoryOwner.free`.; - The `cleaner` thread removes the `RegionPoolNativeMemoryFreer` from the `refs` set which allows the `RegionPoolNativeMemoryFreer` itself to be GC'ed. We do leak the cleaner thread. It will live for the entirety of the JVM process, which seems fine since once you start Hail, you probably want to keep using Hail. I can't find a reference for this, but I'm fairly certain you need to prevent the PhantomReference itself from being GC'ed. That's why I have that set of refs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203
https://github.com/hail-is/hail/pull/8203:1071,Performance,queue,queue,1071,"Some useful reading:; - [Phantom References in Java Reference Objects](http://www.kdgregory.com/index.php?page=java.refobj#PhantomReferences); - [java.lang.ref](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/package-summary.html#reachability); - [java.lang.ref.PhantomReference](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/PhantomReference.html). The structure is now this:; - `RegionPool` becomes `RegionPoolNativeMemoryOwner`; - a new `RegionPool` class is simply a (unique) reference to a `RegionPoolNativeMemoryOwner`; - `RegionPoolNativeMemoryFreer` is a `PhantomReference` to `RegionPool`; - There is just one instance of all three classes in memory at any given time.; - A single thread is blocking on the reference queue waiting for something to free. How it works:; - The reachability of `RegionPool` defines the reachability the native memory.; - When the `RegionPool` becomes unreachable, the JVM places the corresponding `RegionPoolNativeMemoryFreer` on its reference queue.; - The `cleaner` thread removes the phantom reference from the queue; - At this point, the `RegionPool` is effectively gone, there are no references to it anywhere. Because of that fact, the `RegionPoolNativeMemoryFreer` can free all the native memory by calling `RegionPoolNativeMemoryOwner.free`.; - The `cleaner` thread removes the `RegionPoolNativeMemoryFreer` from the `refs` set which allows the `RegionPoolNativeMemoryFreer` itself to be GC'ed. We do leak the cleaner thread. It will live for the entirety of the JVM process, which seems fine since once you start Hail, you probably want to keep using Hail. I can't find a reference for this, but I'm fairly certain you need to prevent the PhantomReference itself from being GC'ed. That's why I have that set of refs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203
https://github.com/hail-is/hail/pull/8203:480,Usability,simpl,simply,480,"Some useful reading:; - [Phantom References in Java Reference Objects](http://www.kdgregory.com/index.php?page=java.refobj#PhantomReferences); - [java.lang.ref](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/package-summary.html#reachability); - [java.lang.ref.PhantomReference](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/PhantomReference.html). The structure is now this:; - `RegionPool` becomes `RegionPoolNativeMemoryOwner`; - a new `RegionPool` class is simply a (unique) reference to a `RegionPoolNativeMemoryOwner`; - `RegionPoolNativeMemoryFreer` is a `PhantomReference` to `RegionPool`; - There is just one instance of all three classes in memory at any given time.; - A single thread is blocking on the reference queue waiting for something to free. How it works:; - The reachability of `RegionPool` defines the reachability the native memory.; - When the `RegionPool` becomes unreachable, the JVM places the corresponding `RegionPoolNativeMemoryFreer` on its reference queue.; - The `cleaner` thread removes the phantom reference from the queue; - At this point, the `RegionPool` is effectively gone, there are no references to it anywhere. Because of that fact, the `RegionPoolNativeMemoryFreer` can free all the native memory by calling `RegionPoolNativeMemoryOwner.free`.; - The `cleaner` thread removes the `RegionPoolNativeMemoryFreer` from the `refs` set which allows the `RegionPoolNativeMemoryFreer` itself to be GC'ed. We do leak the cleaner thread. It will live for the entirety of the JVM process, which seems fine since once you start Hail, you probably want to keep using Hail. I can't find a reference for this, but I'm fairly certain you need to prevent the PhantomReference itself from being GC'ed. That's why I have that set of refs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203
https://github.com/hail-is/hail/pull/8204:364,Performance,optimiz,optimizations,364,"~~Stacked on #8172~~. Implement emitters for StreamScan, RunAggScan, and StreamLeftJoinDistinct. These complete the handling in the new emitter for non-root stream nodes (i.e. those which take stream children). Thus it is now safe to delete non-root nodes from the previous EmitStream. This also implements length tracking in the new EmitStream, and adds back the optimizations that take advantage of knowing the length, plus some we weren't doing before, like in ArraySort and CollectDistributedArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8204
https://github.com/hail-is/hail/pull/8204:226,Safety,safe,safe,226,"~~Stacked on #8172~~. Implement emitters for StreamScan, RunAggScan, and StreamLeftJoinDistinct. These complete the handling in the new emitter for non-root stream nodes (i.e. those which take stream children). Thus it is now safe to delete non-root nodes from the previous EmitStream. This also implements length tracking in the new EmitStream, and adds back the optimizations that take advantage of knowing the length, plus some we weren't doing before, like in ArraySort and CollectDistributedArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8204
https://github.com/hail-is/hail/pull/8205:239,Availability,reliab,reliable,239,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8205:227,Performance,scalab,scalable,227,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8205:451,Performance,scalab,scalable,451,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8205:705,Performance,multi-thread,multi-threaded,705,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8205:1282,Performance,perform,performance,1282,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8205:1395,Performance,optimiz,optimizer,1395,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8205:1439,Performance,optimiz,optimizes,1439,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8205:1416,Testability,test,tests,1416,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205
https://github.com/hail-is/hail/pull/8213:659,Availability,down,downcasts,659,"- add PIndexableValue. This represents canonical array, set and dict values.; - use in ArrayRef; - newP{Local, FIeld} now has a `PV <: PValue` type parameter to eliminate some casting. Where I'm going:. There will be a abstract PValue with the interface for each virtual type (container/indexable, base struct, etc.) Each concrete PType will have a corresponding PValue implementation (in this case, PCanonicalArray is implemented by PCanonicalIndexableValue.) I think this will allow us to get rid of PArrayBackedContainer. Only primitive PValues will have a code method (since other types might be compound). The code generator should then dispatch through downcasts of PValues get access to the relevant methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8213
https://github.com/hail-is/hail/pull/8213:244,Integrability,interface,interface,244,"- add PIndexableValue. This represents canonical array, set and dict values.; - use in ArrayRef; - newP{Local, FIeld} now has a `PV <: PValue` type parameter to eliminate some casting. Where I'm going:. There will be a abstract PValue with the interface for each virtual type (container/indexable, base struct, etc.) Each concrete PType will have a corresponding PValue implementation (in this case, PCanonicalArray is implemented by PCanonicalIndexableValue.) I think this will allow us to get rid of PArrayBackedContainer. Only primitive PValues will have a code method (since other types might be compound). The code generator should then dispatch through downcasts of PValues get access to the relevant methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8213
https://github.com/hail-is/hail/pull/8213:684,Security,access,access,684,"- add PIndexableValue. This represents canonical array, set and dict values.; - use in ArrayRef; - newP{Local, FIeld} now has a `PV <: PValue` type parameter to eliminate some casting. Where I'm going:. There will be a abstract PValue with the interface for each virtual type (container/indexable, base struct, etc.) Each concrete PType will have a corresponding PValue implementation (in this case, PCanonicalArray is implemented by PCanonicalIndexableValue.) I think this will allow us to get rid of PArrayBackedContainer. Only primitive PValues will have a code method (since other types might be compound). The code generator should then dispatch through downcasts of PValues get access to the relevant methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8213
https://github.com/hail-is/hail/issues/8217:1022,Deployability,Install,Installation,1022,"What do we want a new Hail user to see / do, and in what order? This is what someone sees when they open the docs:. <img width=""1221"" alt=""Screen Shot 2020-03-02 at 3 10 09 PM"" src=""https://user-images.githubusercontent.com/13773586/75713528-fa74d800-5c97-11ea-9e55-01065f5f8c21.png"">. They get information from two places now. The side bar has some stuff, and this paragraph does as well. The first thing this paragraph recommends is the `Overview`, which is the wrong first step in my opinion, unless we rewrite the overview. The Overview is written in a very ""bottom up"" way, starting with talking about hail types. A geneticist does not want to arrive at `import_vcf`/`variant_qc`/`sample_qc` by first slowly walking through our weird version of lazy typed python. I think putting the tutorials first would be a good first step. Going through all the tutorials in order (even they we really mostly like the first tutorial) is more useful than jumping into hail types. . I also don't like the ordering on the sidebar. ""Installation"" is a fine start, but ""Hail on the Cloud"" is too soon. ""Datasets"" and ""Annotation Database"" are very experimental, those should be towards the very bottom, maybe just above ""Hail for Developers"". ""Cheat Sheets"" should be higher up. ""Cheat Sheets"" should also be advertised somewhere in the tutorials / intro paragraph, as I think getting the matrix table and hail table pictures in front of people is crucial to understanding. Finally, I want to pick a place in the hail new user flow where users find out hail is lazy. I frequently run into users who don't know it works this way, and I want to make that set of people smaller.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8217
https://github.com/hail-is/hail/issues/8217:506,Modifiability,rewrite,rewrite,506,"What do we want a new Hail user to see / do, and in what order? This is what someone sees when they open the docs:. <img width=""1221"" alt=""Screen Shot 2020-03-02 at 3 10 09 PM"" src=""https://user-images.githubusercontent.com/13773586/75713528-fa74d800-5c97-11ea-9e55-01065f5f8c21.png"">. They get information from two places now. The side bar has some stuff, and this paragraph does as well. The first thing this paragraph recommends is the `Overview`, which is the wrong first step in my opinion, unless we rewrite the overview. The Overview is written in a very ""bottom up"" way, starting with talking about hail types. A geneticist does not want to arrive at `import_vcf`/`variant_qc`/`sample_qc` by first slowly walking through our weird version of lazy typed python. I think putting the tutorials first would be a good first step. Going through all the tutorials in order (even they we really mostly like the first tutorial) is more useful than jumping into hail types. . I also don't like the ordering on the sidebar. ""Installation"" is a fine start, but ""Hail on the Cloud"" is too soon. ""Datasets"" and ""Annotation Database"" are very experimental, those should be towards the very bottom, maybe just above ""Hail for Developers"". ""Cheat Sheets"" should be higher up. ""Cheat Sheets"" should also be advertised somewhere in the tutorials / intro paragraph, as I think getting the matrix table and hail table pictures in front of people is crucial to understanding. Finally, I want to pick a place in the hail new user flow where users find out hail is lazy. I frequently run into users who don't know it works this way, and I want to make that set of people smaller.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8217
https://github.com/hail-is/hail/pull/8218:38,Availability,failure,failures,38,This should suppress many spurious CI failures.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8218
https://github.com/hail-is/hail/pull/8228:110,Integrability,wrap,wrapArgs,110,"In this PR:; - removed unused region arg for `fb.addLiteral`; - add non-type-serialization path for PLocus in wrapArgs; - port isAutosomal, … to not rely on RG serialization (does the implementation look reasonable?)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8228
https://github.com/hail-is/hail/pull/8229:56,Availability,Down,Downside,56,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8229
https://github.com/hail-is/hail/pull/8229:356,Availability,Down,Downside,356,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8229
https://github.com/hail-is/hail/pull/8229:78,Modifiability,extend,extends,78,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8229
https://github.com/hail-is/hail/pull/8229:208,Modifiability,polymorphi,polymorphic,208,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8229
https://github.com/hail-is/hail/pull/8229:650,Performance,load,load,650,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8229
https://github.com/hail-is/hail/pull/8229:749,Performance,load,load,749,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8229
https://github.com/hail-is/hail/pull/8229:682,Security,access,accessors,682,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8229
https://github.com/hail-is/hail/pull/8230:11,Availability,error,error,11,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:22,Deployability,deploy,deploy,22,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:864,Deployability,patch,patch,864,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:45,Testability,test,test,45,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:65,Testability,Test,Test,65,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:138,Testability,log,log,138,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:207,Testability,test,test,207,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:261,Testability,test,test,261,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:299,Testability,Test,Test,299,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:365,Testability,test,test,365,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:381,Testability,Test,Test,381,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:386,Testability,test,testMethod,386,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:581,Testability,log,log,581,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:929,Testability,log,login,929,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:1215,Testability,assert,assert,1215,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:1276,Testability,Assert,AssertionError,1276,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:1341,Testability,log,log,1341,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:1373,Testability,assert,assert,1373,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:1411,Testability,test,test,1411,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/pull/8230:1435,Testability,Assert,AssertionError,1435,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230
https://github.com/hail-is/hail/issues/8239:70,Availability,error,error,70,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:415,Availability,error,error,415,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:608,Availability,Error,Error,608,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:637,Availability,ERROR,ERROR,637,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:857,Availability,ERROR,ERROR,857,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:2363,Energy Efficiency,schedul,scheduler,2363,ask 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1840); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1833); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1833); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:2434,Energy Efficiency,schedul,scheduler,2434,ask 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1840); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1833); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1833); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:429,Integrability,message,message,429,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:484,Performance,concurren,concurrently,484,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:2715,Performance,concurren,concurrent,2715,ask 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1840); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1833); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1833); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:2799,Performance,concurren,concurrent,2799,ask 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1840); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1833); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1833); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8239:1927,Safety,unsafe,unsafeWriter,1927,ask 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1840); at is.hail.linalg.WriteBlocksRDD$$anonfun$63.apply(BlockMatrix.scala:1833); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1833); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8239
https://github.com/hail-is/hail/issues/8240:98,Availability,error,error,98,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:694,Availability,error,error,694,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:1175,Availability,down,downsampled,1175," pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:1264,Availability,down,downsample,1264,"-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:1779,Availability,down,downsampled,1779,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:1793,Availability,down,downsampled,1793,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:1826,Availability,down,downsampled,1826,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2147,Availability,down,downsampled,2147,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2176,Availability,down,downsampled,2176,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2190,Availability,down,downsampled,2190,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2221,Availability,down,downsampled,2221,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2247,Availability,down,downsampled,2247,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2273,Availability,down,downsampled,2273,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2287,Availability,down,downsampled,2287,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2317,Availability,down,downsampled,2317,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:48,Deployability,release,release,48,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:823,Deployability,pipeline,pipeline,823,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:2240,Modifiability,extend,extend,2240,"8); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,; ​; ); ),; unbinned=downsampled.unbinned,; ); ​; downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(); ​; return downsampled; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:473,Performance,load,loadBit,473,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:113,Safety,detect,detected,113,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8240:803,Testability,log,log,803,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240
https://github.com/hail-is/hail/issues/8241:114,Availability,down,downsample,114,We don't have any examples here: https://hail.is/docs/0.2/aggregators.html?highlight=filter#hail.expr.aggregators.downsample,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8241
https://github.com/hail-is/hail/issues/8244:500,Deployability,pipeline,pipeline,500,"In `TableMapRows.execute`, we have the following code at the top of a function that gets passed to `tableMapPartitions`:. ```; val globalRegion = ctx.freshRegion; val globals = if (rowIterationNeedsGlobals); globalsBc.value.readRegionValue(globalRegion); else 0; ```. The problem with the above is that even though the globals were only broadcasted once, they could be read into memory multiple times, and don't get cleaned up until the end of the partition where spark closes the context. Imagine a pipeline that alternated between calling map rows and filtering (the filterings prevent the adjacent maps from being simplified into one). Every time we called map, we'd run the above code, reading a new copy of the globals into a new `globalRegion` in a way that won't be cleaned up until the end of the partition. . As per Cotton: You can fix this by having `SerializedRegionValue` (which is the thing being broadcast in `globalsBC`) hold on to a pointer that is returned for each user, but has to get closed by the same callback that's clearing the contexts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8244
https://github.com/hail-is/hail/issues/8244:617,Usability,simpl,simplified,617,"In `TableMapRows.execute`, we have the following code at the top of a function that gets passed to `tableMapPartitions`:. ```; val globalRegion = ctx.freshRegion; val globals = if (rowIterationNeedsGlobals); globalsBc.value.readRegionValue(globalRegion); else 0; ```. The problem with the above is that even though the globals were only broadcasted once, they could be read into memory multiple times, and don't get cleaned up until the end of the partition where spark closes the context. Imagine a pipeline that alternated between calling map rows and filtering (the filterings prevent the adjacent maps from being simplified into one). Every time we called map, we'd run the above code, reading a new copy of the globals into a new `globalRegion` in a way that won't be cleaned up until the end of the partition. . As per Cotton: You can fix this by having `SerializedRegionValue` (which is the thing being broadcast in `globalsBC`) hold on to a pointer that is returned for each user, but has to get closed by the same callback that's clearing the contexts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8244
https://github.com/hail-is/hail/issues/8244:1039,Usability,clear,clearing,1039,"In `TableMapRows.execute`, we have the following code at the top of a function that gets passed to `tableMapPartitions`:. ```; val globalRegion = ctx.freshRegion; val globals = if (rowIterationNeedsGlobals); globalsBc.value.readRegionValue(globalRegion); else 0; ```. The problem with the above is that even though the globals were only broadcasted once, they could be read into memory multiple times, and don't get cleaned up until the end of the partition where spark closes the context. Imagine a pipeline that alternated between calling map rows and filtering (the filterings prevent the adjacent maps from being simplified into one). Every time we called map, we'd run the above code, reading a new copy of the globals into a new `globalRegion` in a way that won't be cleaned up until the end of the partition. . As per Cotton: You can fix this by having `SerializedRegionValue` (which is the thing being broadcast in `globalsBC`) hold on to a pointer that is returned for each user, but has to get closed by the same callback that's clearing the contexts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8244
https://github.com/hail-is/hail/pull/8247:833,Energy Efficiency,allocate,allocated,833,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:903,Energy Efficiency,allocate,allocateAndStore,903,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:1402,Integrability,depend,depending,1402,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:606,Performance,load,load,606,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:646,Performance,load,loadElement,646,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:661,Performance,load,loadField,661,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:687,Performance,load,loadElement,687,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:928,Performance,load,load,928,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:1066,Performance,load,load,1066,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:1266,Performance,load,load,1266,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:1283,Performance,load,load,1283,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/pull/8247:1506,Usability,clear,clears,1506,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8247
https://github.com/hail-is/hail/issues/8249:18,Deployability,install,install,18,"The tests need to install a previous version of Hail and verify that files written by the current version under test can be ready by older versions. That ""older version"" might be the change itself in the case that the file format version is being bumped. I'm not 100% sure how to specify that version because we won't know the hash on master until after merge. Hmm.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8249
https://github.com/hail-is/hail/issues/8249:327,Security,hash,hash,327,"The tests need to install a previous version of Hail and verify that files written by the current version under test can be ready by older versions. That ""older version"" might be the change itself in the case that the file format version is being bumped. I'm not 100% sure how to specify that version because we won't know the hash on master until after merge. Hmm.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8249
https://github.com/hail-is/hail/issues/8249:4,Testability,test,tests,4,"The tests need to install a previous version of Hail and verify that files written by the current version under test can be ready by older versions. That ""older version"" might be the change itself in the case that the file format version is being bumped. I'm not 100% sure how to specify that version because we won't know the hash on master until after merge. Hmm.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8249
https://github.com/hail-is/hail/issues/8249:112,Testability,test,test,112,"The tests need to install a previous version of Hail and verify that files written by the current version under test can be ready by older versions. That ""older version"" might be the change itself in the case that the file format version is being bumped. I'm not 100% sure how to specify that version because we won't know the hash on master until after merge. Hmm.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8249
https://github.com/hail-is/hail/pull/8254:1086,Deployability,update,update,1086,"This PR fixes a problem where the database state for the instance didn't match the in-memory state for the instance. For example, in-memory the state was 'inactive' while in the database it was 'deleted'. We're not sure why that happens yet. There are 4 possible states: pending, active, inactive, deleted. Rather than failing when this happens (causing an infinite retry loop), we check the return code from the database and act accordingly. . For `deactivate`, the return code will be non-zero only if the instance is inactive or deleted. I thought about making the in-memory state match the state in the database explicitly, but I think it's safer to keep the current behavior where the in-memory state is ""inactive"". The state will be fixed to deleted when the callers of deactivate eventually call `mark_deleted`. Likewise, `mark_deleted` expects the state to be inactive. I thought about handling the case where the db is pending or active and the in-memory state is inactive and realized that could never happen. Therefore, I just assert the db state must be already deleted and update the state appropriately.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8254
https://github.com/hail-is/hail/pull/8254:645,Safety,safe,safer,645,"This PR fixes a problem where the database state for the instance didn't match the in-memory state for the instance. For example, in-memory the state was 'inactive' while in the database it was 'deleted'. We're not sure why that happens yet. There are 4 possible states: pending, active, inactive, deleted. Rather than failing when this happens (causing an infinite retry loop), we check the return code from the database and act accordingly. . For `deactivate`, the return code will be non-zero only if the instance is inactive or deleted. I thought about making the in-memory state match the state in the database explicitly, but I think it's safer to keep the current behavior where the in-memory state is ""inactive"". The state will be fixed to deleted when the callers of deactivate eventually call `mark_deleted`. Likewise, `mark_deleted` expects the state to be inactive. I thought about handling the case where the db is pending or active and the in-memory state is inactive and realized that could never happen. Therefore, I just assert the db state must be already deleted and update the state appropriately.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8254
https://github.com/hail-is/hail/pull/8254:1038,Testability,assert,assert,1038,"This PR fixes a problem where the database state for the instance didn't match the in-memory state for the instance. For example, in-memory the state was 'inactive' while in the database it was 'deleted'. We're not sure why that happens yet. There are 4 possible states: pending, active, inactive, deleted. Rather than failing when this happens (causing an infinite retry loop), we check the return code from the database and act accordingly. . For `deactivate`, the return code will be non-zero only if the instance is inactive or deleted. I thought about making the in-memory state match the state in the database explicitly, but I think it's safer to keep the current behavior where the in-memory state is ""inactive"". The state will be fixed to deleted when the callers of deactivate eventually call `mark_deleted`. Likewise, `mark_deleted` expects the state to be inactive. I thought about handling the case where the db is pending or active and the in-memory state is inactive and realized that could never happen. Therefore, I just assert the db state must be already deleted and update the state appropriately.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8254
https://github.com/hail-is/hail/pull/8256:21,Testability,benchmark,benchmarking,21,Should be useful for benchmarking and other circumstances where; HailContexts are being started and stopped more often.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8256
https://github.com/hail-is/hail/pull/8258:146,Safety,timeout,timeout,146,Please check whether you like the behavior I added where I raise a NotImplementedError if you try and use the local backend with `always_run` or `timeout`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8258
https://github.com/hail-is/hail/pull/8263:454,Availability,down,downcast,454,"The picture is from asm4s:; - Value[T] is convertible to Code[T] and can be used multiple times: it is a primitive value (constant, variable ref, etc.); - Code[T] can be used once; - Settable[T] extends Value[T] and has a store operation. Changes:; - rename PValue => PCode; - add PValue which is multi-use, PSettable extends PValue; - rename EmitTriplet => EmitCode; - add EmitValue and EmitSettable; - Removed type parameter from PValue and introduced downcast operators. I'm not really happy with either option. Will revisit this again in the future. Changes that are coming:; - add EmitMethodBuilder.newEmit{Local, Field} that return EmitSettables; - Emit.E will become Env[EmitSettable]. The goal here is to rip out jointpoint and ParameterPack. EmitSettables will replace the funtionality of ParameterPack for TypedTriplets (which will go away in favor of EmitTriplet/EmitCode). Removing joinpoint will cause problems when Code[T] are reused, so the Value types must be pushed throughout the codebase. I will put out what infrastructure I can as separate PRs, but I'm having a hard time finding way to do this incrementally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8263
https://github.com/hail-is/hail/pull/8263:132,Modifiability,variab,variable,132,"The picture is from asm4s:; - Value[T] is convertible to Code[T] and can be used multiple times: it is a primitive value (constant, variable ref, etc.); - Code[T] can be used once; - Settable[T] extends Value[T] and has a store operation. Changes:; - rename PValue => PCode; - add PValue which is multi-use, PSettable extends PValue; - rename EmitTriplet => EmitCode; - add EmitValue and EmitSettable; - Removed type parameter from PValue and introduced downcast operators. I'm not really happy with either option. Will revisit this again in the future. Changes that are coming:; - add EmitMethodBuilder.newEmit{Local, Field} that return EmitSettables; - Emit.E will become Env[EmitSettable]. The goal here is to rip out jointpoint and ParameterPack. EmitSettables will replace the funtionality of ParameterPack for TypedTriplets (which will go away in favor of EmitTriplet/EmitCode). Removing joinpoint will cause problems when Code[T] are reused, so the Value types must be pushed throughout the codebase. I will put out what infrastructure I can as separate PRs, but I'm having a hard time finding way to do this incrementally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8263
https://github.com/hail-is/hail/pull/8263:195,Modifiability,extend,extends,195,"The picture is from asm4s:; - Value[T] is convertible to Code[T] and can be used multiple times: it is a primitive value (constant, variable ref, etc.); - Code[T] can be used once; - Settable[T] extends Value[T] and has a store operation. Changes:; - rename PValue => PCode; - add PValue which is multi-use, PSettable extends PValue; - rename EmitTriplet => EmitCode; - add EmitValue and EmitSettable; - Removed type parameter from PValue and introduced downcast operators. I'm not really happy with either option. Will revisit this again in the future. Changes that are coming:; - add EmitMethodBuilder.newEmit{Local, Field} that return EmitSettables; - Emit.E will become Env[EmitSettable]. The goal here is to rip out jointpoint and ParameterPack. EmitSettables will replace the funtionality of ParameterPack for TypedTriplets (which will go away in favor of EmitTriplet/EmitCode). Removing joinpoint will cause problems when Code[T] are reused, so the Value types must be pushed throughout the codebase. I will put out what infrastructure I can as separate PRs, but I'm having a hard time finding way to do this incrementally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8263
https://github.com/hail-is/hail/pull/8263:318,Modifiability,extend,extends,318,"The picture is from asm4s:; - Value[T] is convertible to Code[T] and can be used multiple times: it is a primitive value (constant, variable ref, etc.); - Code[T] can be used once; - Settable[T] extends Value[T] and has a store operation. Changes:; - rename PValue => PCode; - add PValue which is multi-use, PSettable extends PValue; - rename EmitTriplet => EmitCode; - add EmitValue and EmitSettable; - Removed type parameter from PValue and introduced downcast operators. I'm not really happy with either option. Will revisit this again in the future. Changes that are coming:; - add EmitMethodBuilder.newEmit{Local, Field} that return EmitSettables; - Emit.E will become Env[EmitSettable]. The goal here is to rip out jointpoint and ParameterPack. EmitSettables will replace the funtionality of ParameterPack for TypedTriplets (which will go away in favor of EmitTriplet/EmitCode). Removing joinpoint will cause problems when Code[T] are reused, so the Value types must be pushed throughout the codebase. I will put out what infrastructure I can as separate PRs, but I'm having a hard time finding way to do this incrementally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8263
https://github.com/hail-is/hail/pull/8268:249,Deployability,configurat,configuration,249,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268
https://github.com/hail-is/hail/pull/8268:249,Modifiability,config,configuration,249,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268
https://github.com/hail-is/hail/pull/8268:783,Modifiability,variab,variables,783,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268
https://github.com/hail-is/hail/pull/8268:923,Testability,test,test-dataproc,923,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268
https://github.com/hail-is/hail/pull/8268:941,Testability,test,test,941,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268
https://github.com/hail-is/hail/pull/8268:1049,Testability,test,testing,1049,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268
https://github.com/hail-is/hail/pull/8273:22,Usability,simpl,simply,22,`copyFrom` method was simply wrong. Also needed to fix memory management; in `result` function. Fixes #8240,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8273
https://github.com/hail-is/hail/pull/8274:14,Safety,safe,safe,14,"`boundary` is safe because after `it.next()` is called, the previous; value is no longer needed. Fixes #8163",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8274
https://github.com/hail-is/hail/issues/8277:43,Modifiability,variab,variable,43,Should maybe use `spark.local.dir` if that variable is set.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8277
https://github.com/hail-is/hail/issues/8278:202,Usability,guid,guide,202,"It is possible to export a field of a `MatrixTable` to a TSV using something like `mt.n.export('file_name')`. This is not easily determined from the docs. At minimum, maybe let's add this to the how to guide about exporting. Right now, we only talk about exporting VCFs. https://hail.is/docs/0.2/guides/basics.html?highlight=export%20matrix%20table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8278
https://github.com/hail-is/hail/issues/8278:296,Usability,guid,guides,296,"It is possible to export a field of a `MatrixTable` to a TSV using something like `mt.n.export('file_name')`. This is not easily determined from the docs. At minimum, maybe let's add this to the how to guide about exporting. Right now, we only talk about exporting VCFs. https://hail.is/docs/0.2/guides/basics.html?highlight=export%20matrix%20table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8278
https://github.com/hail-is/hail/pull/8280:303,Availability,error,error,303,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:413,Deployability,update,update,413,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:577,Deployability,update,updates,577,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:342,Performance,cache,cached,342,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:424,Performance,cache,cache,424,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:122,Safety,Timeout,TimeoutError,122,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:154,Safety,Timeout,TimeoutError,154,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:23,Testability,log,logs,23,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:235,Testability,log,logs,235,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:326,Testability,log,logs,326,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:370,Testability,log,logs,370,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8280:516,Testability,log,logs,516,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8280
https://github.com/hail-is/hail/pull/8283:244,Usability,simpl,simplify,244,"Turns out we never used the generality provided by `ContextRDD`. This forces the context type to be `RVDContext`, and the context factory `mkc` to be `RVDContext.default`, both of which were the only values ever actually used. This change will simplify further work migrating region management to the new model.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8283
https://github.com/hail-is/hail/pull/8287:430,Usability,clear,clearly,430,"ContextRDD registered a TaskCompletionListener with Spark to close its context no matter why the task ends. But if the region in the context points to RegionMemory that is shared (has refcount > 1), this won't actually free the memory. Plus, this doesn't free any regions not created by an RVDContext. This PR removes the TaskCompletionListener from ContextRDD, and instead adds a single one to each thread-local RegionPool. This clearly ensures all off-heap memory is freed at the end of the task.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8287
https://github.com/hail-is/hail/pull/8288:131,Deployability,pipeline,pipelines,131,"~Stacked on #8283~. We were creating fresh regions to pass as the `partitionRegion` to compiled functions deep within `ContextRDD` pipelines. Doing it that way, there's no clear owner responsible for freeing those regions. We're currently relying on Spark to clean them up. This PR adds a `partitionRegion` field to `RVDContext`. This way, the root consumer is responsible for creating the partition region before running the iterator, and for freeing it after. This is a step towards clarifying the structure of region ownership.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8288
https://github.com/hail-is/hail/pull/8288:172,Usability,clear,clear,172,"~Stacked on #8283~. We were creating fresh regions to pass as the `partitionRegion` to compiled functions deep within `ContextRDD` pipelines. Doing it that way, there's no clear owner responsible for freeing those regions. We're currently relying on Spark to clean them up. This PR adds a `partitionRegion` field to `RVDContext`. This way, the root consumer is responsible for creating the partition region before running the iterator, and for freeing it after. This is a step towards clarifying the structure of region ownership.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8288
https://github.com/hail-is/hail/pull/8289:203,Deployability,release,release,203,"Now there are `hail-uk-vep` and `hail-eu-vep` buckets. Both are requester pays. This PR adds mappings from regions in those areas to the corresponding VEP data replicates. Once this goes in, we can do a release, after which we can change the permissions on the current hail vep folder to be not public (eventually we will delete, but maybe there's something in there I haven't noticed yet that people need / want).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8289
https://github.com/hail-is/hail/pull/8290:160,Usability,clear,clear,160,"Stacked on #8283. This is WIP that converts the `ContextRDD` in `RVD` to a `ContextRDD[Long]`. This clarifies the region ownership semantics. Before, it wasn't clear what relation there was between the context region and the `RegionValue` region.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8290
https://github.com/hail-is/hail/pull/8291:41,Testability,assert,assertBMEvalsTo,41,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8291:90,Testability,assert,assertEvalsTo,90,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8291:129,Testability,test,testing,129,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8291:146,Testability,test,tests,146,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8291:268,Testability,test,tests,268,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8291:277,Testability,test,test,277,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8291:462,Testability,test,test,462,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8291:533,Testability,test,test,533,"The main point here was to implement an `assertBMEvalsTo` function that works a lot like `assertEvalsTo` for IR---currently only testing existing tests through interpret and BlockMatrix, but as we move lowering things over we can also add eval strategies for existing tests to test them through the lowerer as well. I also removed a lot of the explicit BlockMatrix construction and conversions since we can handle them through the IR. (also removed the lowering test in favor of adding the lowered execStrategy to the BlockMatrixDot test as it's essentially a duplicate of that)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8291
https://github.com/hail-is/hail/pull/8292:96,Performance,perform,performance,96,"I listed everything I'm aware of, though it's possible that some internal changes may have made performance impacts I don't know about.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8292
https://github.com/hail-is/hail/pull/8293:46,Energy Efficiency,reduce,reduce,46,"More replicates and fewer iterations seems to reduce variance, and; finishes faster as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8293
https://github.com/hail-is/hail/pull/8307:141,Availability,ERROR,ERROR,141,"@danking I remember you wanted raise web.Response() here for a reason, so I'll let you decide if this is correct or not. ```; {""levelname"": ""ERROR"", ""asctime"": ""2020-03-13 15:53:52,139"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 635, in insert; jobs_args); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 172, in execute_many; return await cursor.executemany(sql, args_array); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 283, in executemany; self._get_db().encoding)); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:270,Availability,Error,Error,270,"@danking I remember you wanted raise web.Response() here for a reason, so I'll let you decide if this is correct or not. ```; {""levelname"": ""ERROR"", ""asctime"": ""2020-03-13 15:53:52,139"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 635, in insert; jobs_args); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 172, in execute_many; return await cursor.executemany(sql, args_array); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 283, in executemany; self._get_db().encoding)); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:1865,Availability,error,errorclass,1865," File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 57, in wrapped; return await fun(request, userdata, *args, **kwargs);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:259,Integrability,message,message,259,"@danking I remember you wanted raise web.Response() here for a reason, so I'll let you decide if this is correct or not. ```; {""levelname"": ""ERROR"", ""asctime"": ""2020-03-13 15:53:52,139"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 635, in insert; jobs_args); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 172, in execute_many; return await cursor.executemany(sql, args_array); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 283, in executemany; self._get_db().encoding)); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:1679,Integrability,protocol,protocol,1679," 283, in executemany; self._get_db().encoding)); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 318, in _do_execute_many; r = await self.execute(sql + postfix); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 239, in execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:2701,Integrability,wrap,wrapped,2701,"t = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 57, in wrapped; return await fun(request, userdata, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 681, in create_jobs; await insert() # pylint: disable=no-value-for-parameter; File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 24, in wrapper; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 40, in wrapper; return await fun(tx, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 640, in insert; raise web.Response(); TypeError: exceptions must derive from BaseException"", ""hail_log"": 1}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:2797,Integrability,wrap,wrapped,2797,"t = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 57, in wrapped; return await fun(request, userdata, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 681, in create_jobs; await insert() # pylint: disable=no-value-for-parameter; File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 24, in wrapper; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 40, in wrapper; return await fun(tx, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 640, in insert; raise web.Response(); TypeError: exceptions must derive from BaseException"", ""hail_log"": 1}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:3099,Integrability,wrap,wrapper,3099,"t = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 57, in wrapped; return await fun(request, userdata, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 681, in create_jobs; await insert() # pylint: disable=no-value-for-parameter; File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 24, in wrapper; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 40, in wrapper; return await fun(tx, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 640, in insert; raise web.Response(); TypeError: exceptions must derive from BaseException"", ""hail_log"": 1}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:3219,Integrability,wrap,wrapper,3219,"t = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 57, in wrapped; return await fun(request, userdata, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 681, in create_jobs; await insert() # pylint: disable=no-value-for-parameter; File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 24, in wrapper; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/gear/database.py\"", line 40, in wrapper; return await fun(tx, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", line 640, in insert; raise web.Response(); TypeError: exceptions must derive from BaseException"", ""hail_log"": 1}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8307:1904,Security,Integrity,IntegrityError,1904,"execute; await self._query(query); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py\"", line 457, in _query; await conn.query(q); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 622, in _read_query_result; await result.read(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 1105, in read; first_packet = await self.connection._read_packet(); File \""/usr/local/lib/python3.6/dist-packages/aiomysql/connection.py\"", line 593, in _read_packet; packet.check_error(); File \""/usr/local/lib/python3.6/dist-packages/pymysql/protocol.py\"", line 220, in check_error; err.raise_mysql_exception(self._data); File \""/usr/local/lib/python3.6/dist-packages/pymysql/err.py\"", line 109, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.IntegrityError: (1062, \""Duplicate entry '27-122310' for key 'PRIMARY'\""). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start; resp = await task; File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle; resp = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl; return await handler(request); File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory; response = await handler(request); File \""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py\"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 57, in wrapped; return await fun(request, userdata, *args, **kwargs); File \""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py\"", lin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8307
https://github.com/hail-is/hail/pull/8310:4,Testability,test,testing,4,For testing. Will peel off and PR pieces.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8310
https://github.com/hail-is/hail/pull/8312:531,Integrability,rout,routines,531,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8312
https://github.com/hail-is/hail/pull/8312:102,Modifiability,variab,variable,102,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8312
https://github.com/hail-is/hail/pull/8312:170,Modifiability,variab,variables,170,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8312
https://github.com/hail-is/hail/pull/8312:815,Performance,load,load,815,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8312
https://github.com/hail-is/hail/pull/8312:1106,Performance,optimiz,optimize,1106,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8312
https://github.com/hail-is/hail/pull/8315:142,Usability,simpl,simplifies,142,"So this PR is mostly Patrick ripping out `RegionValues` in a lot of places in favor of just using a `Long`. I think this is useful because it simplifies the memory management situation by ensuring that the only regions to worry about at a particular `TableIR.execute` are the one on the `RegionContext` and any regions created in that `execute`. . One exception to this is that currently we have methods `toCRDDRegionValue` and `toCRDDPtr` to switch back to a `RegionValue` based CRDD for compatibility with all the join stuff (really, anything that uses `OrderedRVIterator`). In addition, this PR goes through and tries to systematically fix places where we were not manging regions correctly. This makes it somewhat subtle and hard to review. Some things I'd focus on are: . - Anywhere a boundary was removed.; - `TableKeyByAndAggregate` and `TableAggregateByKey` (noted below. I haven't done anything aggregator related prior to this, don't fully understand them); - Spot check of any of the places I listed under ""Addressed"" below.; - The implementations of `toCRDDRegionValue` and `toCRDDPtr`. . Addressed:. ```; - TableFilter; - TableSubset (just the parent of head and tail); - TableHead (via rvd.head); - TableTail (via rvd.tail); - TableExplode; - LinearRegression; ```. No change needed:. ```; - TableMapGlobals; - TableRange; - TableLiteral; - TableOrderBy; - TableDistinct; - TableKeyBy; - TableRename; - TableUnion; - TableParallelize; - TableRead; ```. Didn't / Minimally Changed, but wasn't really sure about:. ```; - TableAggregateByKey; - TableKeyByAndAggregate; - TableMapRows (specifically, the bit about computing `scanPartitionAggs); ```. cc @patrick-schultz @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315
https://github.com/hail-is/hail/issues/8316:208,Deployability,pipeline,pipeline,208,"```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_rows(maf_flag = hl.empty_array('bool')); counts = mt.aggregate_rows(hl.agg.array_agg(lambda x: hl.agg.counter(x), mt.maf_flag)); ```. The above pipeline fails with `KeyError: 'va'`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8316
https://github.com/hail-is/hail/pull/8317:36,Modifiability,variab,variable,36,"Remove some (now) unnecessary local variable initializations. newEmit{Local, Field}: don't store missingness variable for required types. Split up zip cases assert same length and extend na. Otherwise, the same length code ended up assigning a missing element to a required field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8317
https://github.com/hail-is/hail/pull/8317:109,Modifiability,variab,variable,109,"Remove some (now) unnecessary local variable initializations. newEmit{Local, Field}: don't store missingness variable for required types. Split up zip cases assert same length and extend na. Otherwise, the same length code ended up assigning a missing element to a required field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8317
https://github.com/hail-is/hail/pull/8317:180,Modifiability,extend,extend,180,"Remove some (now) unnecessary local variable initializations. newEmit{Local, Field}: don't store missingness variable for required types. Split up zip cases assert same length and extend na. Otherwise, the same length code ended up assigning a missing element to a required field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8317
https://github.com/hail-is/hail/pull/8317:157,Testability,assert,assert,157,"Remove some (now) unnecessary local variable initializations. newEmit{Local, Field}: don't store missingness variable for required types. Split up zip cases assert same length and extend na. Otherwise, the same length code ended up assigning a missing element to a required field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8317
https://github.com/hail-is/hail/issues/8318:18,Deployability,update,update,18,"Is it possible to update the function ; `hl.plot.histogram`; to not require legend, and/or to use explicit legend field as mention in BokehJS warning. * Hail version 0.2.32-a5876a0a2853; * BokehJS 2.0.0; * Sniplet; ```py; p = hl.plot.histogram(; ht.meta.age,; range=(0, 100),; bins=10,; legend='age',; title='Age distribution'; ); show(p); ```; * Output. ```sh; # If no `legend` argument; ValueError: Bad 'legend' parameter value: None; ```; ```sh; # If `legend` argument prvided; BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8318
https://github.com/hail-is/hail/pull/8322:114,Testability,test,tests,114,"Fixes #8316 . I honestly don't know really know how this env/agg_env stuff works, I just know that this makes the tests pass. It's possible that this is an improvement but not a fully correct substitution rule, would appreciate if you could check it / tell me how you figured out what it's supposed to be.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8322
https://github.com/hail-is/hail/issues/8325:1507,Availability,error,error,1507,"mt = mt.annotate_cols(**{f""PC{i + 1}"":hl.rand_unif(0, 10) for i in range(10)}); mt = mt.annotate_entries(x = hl.rand_unif(0, 3)); mt = mt.key_rows_by(mt.gene_set, mt.consequence_category); mt = mt.key_cols_by(mt.col_id); return mt. def permute_phenotypes(np_pheno, n_perms):. np_pheno[np_pheno == None] = 2; np_pheno = np_pheno.astype(int); np_pheno_mat = np.repeat(np_pheno, n_perms).reshape(np_pheno.size, n_perms); for i in range(np_pheno_mat.shape[1]):; np.random.shuffle(np_pheno_mat[:,i]). return(np_pheno_mat). def run_regressions_perm(mt, phenotypes, n_perms):. mt = mt.add_col_index(). init = True; for phenotype in phenotypes:. mt = mt.annotate_globals(; pheno_perm = permute_phenotypes(np.array(mt.phenotype_boolean[phenotype].collect()), n_perms)); mt = mt.annotate_globals(; pheno_perm = mt.pheno_perm.map(; lambda x: { hl.array([False, True, hl.null(hl.tbool)])[hl.int(x)] }; )); return mt. mt = make_fake_data(); res = run_regressions_perm(mt, phenotypes_BP, 100); res.show(); ```. error:. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:331); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:2321,Integrability,Wrap,WrappedArray,2321,"p(; lambda x: { hl.array([False, True, hl.null(hl.tbool)])[hl.int(x)] }; )); return mt. mt = make_fake_data(); res = run_regressions_perm(mt, phenotypes_BP, 100); res.show(); ```. error:. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:331); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:2342,Integrability,Wrap,WrappedArray,2342,"l.array([False, True, hl.null(hl.tbool)])[hl.int(x)] }; )); return mt. mt = make_fake_data(); res = run_regressions_perm(mt, phenotypes_BP, 100); res.show(); ```. error:. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:331); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.Traversabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:3270,Integrability,Wrap,WrappedArray,3270,mized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.coll,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:3291,Integrability,Wrap,WrappedArray,3291,at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.Traversabl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:4219,Integrability,Wrap,WrappedArray,4219,mized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.coll,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:4240,Integrability,Wrap,WrappedArray,4240,at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.Traversabl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:5168,Integrability,Wrap,WrappedArray,5168,ap(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:5189,Integrability,Wrap,WrappedArray,5189,ap(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:333); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:1530,Testability,Assert,AssertionError,1530,"rand_unif(0, 10) for i in range(10)}); mt = mt.annotate_entries(x = hl.rand_unif(0, 3)); mt = mt.key_rows_by(mt.gene_set, mt.consequence_category); mt = mt.key_cols_by(mt.col_id); return mt. def permute_phenotypes(np_pheno, n_perms):. np_pheno[np_pheno == None] = 2; np_pheno = np_pheno.astype(int); np_pheno_mat = np.repeat(np_pheno, n_perms).reshape(np_pheno.size, n_perms); for i in range(np_pheno_mat.shape[1]):; np.random.shuffle(np_pheno_mat[:,i]). return(np_pheno_mat). def run_regressions_perm(mt, phenotypes, n_perms):. mt = mt.add_col_index(). init = True; for phenotype in phenotypes:. mt = mt.annotate_globals(; pheno_perm = permute_phenotypes(np.array(mt.phenotype_boolean[phenotype].collect()), n_perms)); mt = mt.annotate_globals(; pheno_perm = mt.pheno_perm.map(; lambda x: { hl.array([False, True, hl.null(hl.tbool)])[hl.int(x)] }; )); return mt. mt = make_fake_data(); res = run_regressions_perm(mt, phenotypes_BP, 100); res.show(); ```. error:. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:331); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:1546,Testability,assert,assertion,1546,"rand_unif(0, 10) for i in range(10)}); mt = mt.annotate_entries(x = hl.rand_unif(0, 3)); mt = mt.key_rows_by(mt.gene_set, mt.consequence_category); mt = mt.key_cols_by(mt.col_id); return mt. def permute_phenotypes(np_pheno, n_perms):. np_pheno[np_pheno == None] = 2; np_pheno = np_pheno.astype(int); np_pheno_mat = np.repeat(np_pheno, n_perms).reshape(np_pheno.size, n_perms); for i in range(np_pheno_mat.shape[1]):; np.random.shuffle(np_pheno_mat[:,i]). return(np_pheno_mat). def run_regressions_perm(mt, phenotypes, n_perms):. mt = mt.add_col_index(). init = True; for phenotype in phenotypes:. mt = mt.annotate_globals(; pheno_perm = permute_phenotypes(np.array(mt.phenotype_boolean[phenotype].collect()), n_perms)); mt = mt.annotate_globals(; pheno_perm = mt.pheno_perm.map(; lambda x: { hl.array([False, True, hl.null(hl.tbool)])[hl.int(x)] }; )); return mt. mt = make_fake_data(); res = run_regressions_perm(mt, phenotypes_BP, 100); res.show(); ```. error:. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:331); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/issues/8325:1581,Testability,assert,assert,1581,"; mt = mt.annotate_entries(x = hl.rand_unif(0, 3)); mt = mt.key_rows_by(mt.gene_set, mt.consequence_category); mt = mt.key_cols_by(mt.col_id); return mt. def permute_phenotypes(np_pheno, n_perms):. np_pheno[np_pheno == None] = 2; np_pheno = np_pheno.astype(int); np_pheno_mat = np.repeat(np_pheno, n_perms).reshape(np_pheno.size, n_perms); for i in range(np_pheno_mat.shape[1]):; np.random.shuffle(np_pheno_mat[:,i]). return(np_pheno_mat). def run_regressions_perm(mt, phenotypes, n_perms):. mt = mt.add_col_index(). init = True; for phenotype in phenotypes:. mt = mt.annotate_globals(; pheno_perm = permute_phenotypes(np.array(mt.phenotype_boolean[phenotype].collect()), n_perms)); mt = mt.annotate_globals(; pheno_perm = mt.pheno_perm.map(; lambda x: { hl.array([False, True, hl.null(hl.tbool)])[hl.int(x)] }; )); return mt. mt = make_fake_data(); res = run_regressions_perm(mt, phenotypes_BP, 100); res.show(); ```. error:. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.agg.Extract$.extract(Extract.scala:331); at is.hail.expr.ir.agg.Extract$.is$hail$expr$ir$agg$Extract$$extract$1(Extract.scala:215); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.agg.Extract$$anonfun$extract$3.apply(Extract.scala:333); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:8); at is.hail.expr.ir.MapIR$$anonfun$apply$1.apply(MapIR.scala:7); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.MapIR$.apply(MapIR.scala:7); at is.hail.expr.ir.agg.Extract$.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325
https://github.com/hail-is/hail/pull/8331:56,Integrability,wrap,wrap,56,This is thrown by the connector which always uses it to wrap an OSError,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8331
https://github.com/hail-is/hail/pull/8333:38,Integrability,wrap,wrapping,38,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8333
https://github.com/hail-is/hail/pull/8333:593,Performance,perform,performance,593,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8333
https://github.com/hail-is/hail/pull/8333:265,Security,access,accesses,265,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8333
https://github.com/hail-is/hail/pull/8333:537,Testability,test,testing,537,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8333
https://github.com/hail-is/hail/pull/8333:605,Testability,test,test,605,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8333
https://github.com/hail-is/hail/pull/8333:128,Usability,Simpl,SimplifyControl,128,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8333
https://github.com/hail-is/hail/pull/8335:20,Integrability,interface,interface,20,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:895,Integrability,interface,interface,895,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:923,Integrability,Wrap,Wrapped,923,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:969,Integrability,Wrap,WrappedClassBuilder,969,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1014,Integrability,Wrap,WrappedModuleBuilder,1014,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1074,Integrability,interface,interface,1074,"lass, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1268,Integrability,Wrap,WrappedMethodBuilder,1268,"This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class Funct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1328,Integrability,interface,interfaces,1328,"}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBui",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1892,Integrability,rout,routines,1892,".g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1992,Integrability,Wrap,WrappedModuleBuilder,1992,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2069,Integrability,Wrap,WrappedModuleBuilder,2069,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2097,Integrability,Wrap,WrappedClassBuilder,2097,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2177,Integrability,Wrap,WrappedClassBuilder,2177,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2207,Integrability,Wrap,WrappedMethodBuilder,2207,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2288,Integrability,Wrap,WrappedMethodBuilder,2288,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2381,Integrability,Wrap,WrappedModuleBuilder,2381,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2409,Integrability,Wrap,WrappedEmitModuleBuilder,2409,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2442,Integrability,Wrap,WrappedModuleBuilder,2442,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2528,Integrability,Wrap,WrappedEmitModuleBuilder,2528,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2558,Integrability,Wrap,WrappedClassBuilder,2558,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2588,Integrability,Wrap,WrappedEmitClassBuilder,2588,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2623,Integrability,Wrap,WrappedModuleBuilder,2623,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2649,Integrability,Wrap,WrappedClassBuilder,2649,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2735,Integrability,Wrap,WrappedClassBuilder,2735,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2763,Integrability,Wrap,WrappedMethodBuilder,2763,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2794,Integrability,Wrap,WrappedEmitMethodBuilder,2794,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2830,Integrability,Wrap,WrappedClassBuilder,2830,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2858,Integrability,Wrap,WrappedMethodBuilder,2858,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2948,Integrability,Wrap,WrappedEmitMethodBuilder,2948,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:961,Modifiability,extend,extends,961,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1006,Modifiability,extend,extends,1006,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:1260,Modifiability,extend,extends,1260,"This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class Funct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2061,Modifiability,extend,extends,2061,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2169,Modifiability,extend,extends,2169,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2280,Modifiability,extend,extends,2280,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2373,Modifiability,extend,extends,2373,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2434,Modifiability,extend,extends,2434,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2520,Modifiability,extend,extends,2520,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2615,Modifiability,extend,extends,2615,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2727,Modifiability,extend,extends,2727,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2822,Modifiability,extend,extends,2822,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8335:2940,Modifiability,extend,extends,2940,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335
https://github.com/hail-is/hail/pull/8336:113,Deployability,deploy,deploy,113,Base image builds in tests are taking a long time. I think the root image changed. This empty PR will force a re-deploy and the :latest image will get updated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8336
https://github.com/hail-is/hail/pull/8336:151,Deployability,update,updated,151,Base image builds in tests are taking a long time. I think the root image changed. This empty PR will force a re-deploy and the :latest image will get updated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8336
https://github.com/hail-is/hail/pull/8336:21,Testability,test,tests,21,Base image builds in tests are taking a long time. I think the root image changed. This empty PR will force a re-deploy and the :latest image will get updated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8336
https://github.com/hail-is/hail/pull/8337:202,Deployability,deploy,deploy,202,"CI gets the logs for every failing job in the batch and sends it with the deploy_status response. This performs O(n_jobs) work for a single request, but this API is developers only and used only by the deploy test which has a small batch. We can revisit if this becomes a problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8337
https://github.com/hail-is/hail/pull/8337:103,Performance,perform,performs,103,"CI gets the logs for every failing job in the batch and sends it with the deploy_status response. This performs O(n_jobs) work for a single request, but this API is developers only and used only by the deploy test which has a small batch. We can revisit if this becomes a problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8337
https://github.com/hail-is/hail/pull/8337:12,Testability,log,logs,12,"CI gets the logs for every failing job in the batch and sends it with the deploy_status response. This performs O(n_jobs) work for a single request, but this API is developers only and used only by the deploy test which has a small batch. We can revisit if this becomes a problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8337
https://github.com/hail-is/hail/pull/8337:209,Testability,test,test,209,"CI gets the logs for every failing job in the batch and sends it with the deploy_status response. This performs O(n_jobs) work for a single request, but this API is developers only and used only by the deploy test which has a small batch. We can revisit if this becomes a problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8337
https://github.com/hail-is/hail/issues/8338:1734,Availability,checkpoint,checkpoint,1734,"leles'); return mt. def mwzj_hts_by_tree(all_hts, temp_dir, globals_for_col_key, debug=False, inner_mode = 'overwrite',; repartition_final: int = None):; chunk_size = int(len(all_hts) ** 0.5) + 1; outer_hts = []. checkpoint_kwargs = {inner_mode: True}; if repartition_final is not None:; intervals = get_n_even_intervals(repartition_final); checkpoint_kwargs['_intervals'] = intervals. if debug: print(f'Running chunk size {chunk_size}...'); for i in range(chunk_size):; if i * chunk_size >= len(all_hts): break; hts = all_hts[i * chunk_size:(i + 1) * chunk_size]; if debug: print(f'Going from {i * chunk_size} to {(i + 1) * chunk_size} ({len(hts)} HTs)...'); try:; if isinstance(hts[0], str):; hts = list(map(lambda x: hl.read_table(x), hts)); ht = hl.Table.multi_way_zip_join(hts, 'row_field_name', 'global_field_name'); except:; if debug:; print(f'problem in range {i * chunk_size}-{i * chunk_size + chunk_size}'); _ = [ht.describe() for ht in hts]; raise; outer_hts.append(ht.checkpoint(f'{temp_dir}/temp_output_{i}.ht', **checkpoint_kwargs)); ht = hl.Table.multi_way_zip_join(outer_hts, 'row_field_name_outer', 'global_field_name_outer'); ht = ht.transmute(inner_row=hl.flatmap(lambda i:; hl.cond(hl.is_missing(ht.row_field_name_outer[i].row_field_name),; hl.range(0, hl.len(ht.global_field_name_outer[i].global_field_name)); .map(lambda _: hl.null(ht.row_field_name_outer[i].row_field_name.dtype.element_type)),; ht.row_field_name_outer[i].row_field_name),; hl.range(hl.len(ht.global_field_name_outer)))); ht = ht.transmute_globals(inner_global=hl.flatmap(lambda x: x.global_field_name, ht.global_field_name_outer)); mt = ht._unlocalize_entries('inner_row', 'inner_global', globals_for_col_key); return mt. all_hts = list(map(lambda x: unify_saige_ht_schema(hl.read_table(x)), all_variant_outputs)); mt = join_pheno_hts_to_mt(all_hts, row_keys, col_keys, pheno_dict, f'{temp_bucket}/{pop}/variant',; inner_mode=inner_mode, repartition_final=20000); mt = mt.annotate_cols(saige_heritability=heri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:2999,Availability,error,error,2999,"e_outer[i].row_field_name),; hl.range(0, hl.len(ht.global_field_name_outer[i].global_field_name)); .map(lambda _: hl.null(ht.row_field_name_outer[i].row_field_name.dtype.element_type)),; ht.row_field_name_outer[i].row_field_name),; hl.range(hl.len(ht.global_field_name_outer)))); ht = ht.transmute_globals(inner_global=hl.flatmap(lambda x: x.global_field_name, ht.global_field_name_outer)); mt = ht._unlocalize_entries('inner_row', 'inner_global', globals_for_col_key); return mt. all_hts = list(map(lambda x: unify_saige_ht_schema(hl.read_table(x)), all_variant_outputs)); mt = join_pheno_hts_to_mt(all_hts, row_keys, col_keys, pheno_dict, f'{temp_bucket}/{pop}/variant',; inner_mode=inner_mode, repartition_final=20000); mt = mt.annotate_cols(saige_heritability=heritability_dict[mt.col_key]); mt = mt.filter_cols(mt.pheno != """").drop('varT', 'varTstar', 'Is.SPA.converge'); mt.key_rows_by('locus', 'alleles').write(get_variant_results_path(pop, 'mt'), overwrite=args.overwrite); ```; I'm getting the following error:; ```; hail.utils.java.FatalError: RuntimeException: found inconsistent agg or scan environments:; left: true, true; right: false, false. Java stack trace:; java.lang.RuntimeException: found inconsistent agg or scan environments:; left: true, true; right: false, false; at is.hail.expr.ir.BindingEnv.merge(Env.scala:68); at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$2.apply(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$2.apply(FreeVariables.scala:38); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8150,Integrability,Wrap,WrappedArray,8150,$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8171,Integrability,Wrap,WrappedArray,8171,$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.co,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9118,Integrability,Wrap,WrappedArray,9118, scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9139,Integrability,Wrap,WrappedArray,9139,.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.co,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10086,Integrability,Wrap,WrappedArray,10086, scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.colle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10107,Integrability,Wrap,WrappedArray,10107,.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.Wra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11107,Integrability,Wrap,WrappedArray,11107,dArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11128,Integrability,Wrap,WrappedArray,11128, at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7175,Modifiability,rewrite,rewriteMatrixNode,7175,; at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(Wrapp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11774,Performance,Optimiz,Optimize,11774,pply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.bac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11830,Performance,Optimiz,Optimize,11830,a.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Back,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11869,Performance,Optimiz,Optimize,11869,ke.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11925,Performance,Optimiz,Optimize,11925,on.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11964,Performance,Optimiz,Optimize,11964,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11998,Performance,Optimiz,Optimize,11998,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12025,Performance,Optimiz,Optimize,12025,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12064,Performance,Optimiz,Optimize,12064,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12098,Performance,Optimiz,Optimize,12098,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12125,Performance,Optimiz,Optimize,12125,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12227,Performance,Optimiz,Optimize,12227,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12253,Performance,Optimiz,Optimize,12253,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12272,Performance,Optimiz,Optimize,12272,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12311,Performance,Optimiz,Optimize,12311,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12350,Performance,Optimiz,Optimize,12350,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12389,Performance,Optimiz,Optimize,12389,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12421,Performance,Optimiz,Optimize,12421,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12460,Performance,Optimiz,Optimize,12460,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12492,Performance,Optimiz,Optimize,12492,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12594,Performance,Optimiz,Optimize,12594,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:12610,Performance,Optimiz,Optimize,12610,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:6839,Usability,Simpl,Simplify,6839,(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$1.apply(FreeVariables.scala:27); at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$1.apply(FreeVariables.scala:24); at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:6883,Usability,Simpl,Simplify,6883,at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$1.apply(FreeVariables.scala:27); at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$1.apply(FreeVariables.scala:24); at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:6923,Usability,Simpl,Simplify,6923,FreeVariables$$compute$1$1.apply(FreeVariables.scala:27); at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$1.apply(FreeVariables.scala:24); at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:6967,Usability,Simpl,Simplify,6967,.apply(FreeVariables.scala:27); at is.hail.expr.ir.FreeVariables$$anonfun$is$hail$expr$ir$FreeVariables$$compute$1$1.apply(FreeVariables.scala:24); at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.colle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7139,Usability,Simpl,Simplify,7139,non$11.next(Iterator.scala:410); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7165,Usability,Simpl,Simplify,7165,; at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(Wrapp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7193,Usability,Simpl,Simplify,7193,; at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(Wrapp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7232,Usability,Simpl,Simplify,7232,bstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7266,Usability,Simpl,Simplify,7266,bstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7276,Usability,simpl,simplifyMatrix,7276,bstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7299,Usability,Simpl,Simplify,7299,:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTrav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7338,Usability,Simpl,Simplify,7338,leOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7372,Usability,Simpl,Simplify,7372,leOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7382,Usability,simpl,simplifyMatrix,7382,leOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7405,Usability,Simpl,Simplify,7405,.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7444,Usability,Simpl,Simplify,7444,334); at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$v,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7470,Usability,Simpl,Simplify,7470,raversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7490,Usability,Simpl,Simplify,7490,raversableOnce$class.fold(TraversableOnce.scala:212); at scala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7529,Usability,Simpl,Simplify,7529,cala.collection.AbstractIterator.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7555,Usability,Simpl,Simplify,7555,or.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7565,Usability,simpl,simplifyMatrix,7565,or.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7580,Usability,Simpl,Simplify,7580,or.fold(Iterator.scala:1334); at is.hail.expr.ir.FreeVariables$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7619,Usability,Simpl,Simplify,7619,es$.is$hail$expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7635,Usability,Simpl,Simplify,7635,expr$ir$FreeVariables$$compute$1(FreeVariables.scala:38); at is.hail.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7674,Usability,Simpl,Simplify,7674,.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7708,Usability,Simpl,Simplify,7708,.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7718,Usability,simpl,simplifyMatrix,7718,.expr.ir.FreeVariables$.apply(FreeVariables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7741,Usability,Simpl,Simplify,7741,ables.scala:42); at is.hail.expr.ir.Mentions$.inAggOrScan(Mentions.scala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7780,Usability,Simpl,Simplify,7780,cala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7814,Usability,Simpl,Simplify,7814,cala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7824,Usability,simpl,simplifyMatrix,7824,cala:10); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:7847,Usability,Simpl,Simplify,7847,$anonfun$matrixRules$1.applyOrElse(Simplify.scala:893); at is.hail.expr.ir.Simplify$$anonfun$matrixRules$1.applyOrElse(Simplify.scala:828); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$rewriteMatrixNode(Simplify.scala:68); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$3.apply(Simplify.scala:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.colle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8412,Usability,Simpl,Simplify,8412,la:50); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$v,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8438,Usability,Simpl,Simplify,8438,Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8458,Usability,Simpl,Simplify,8458,Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8497,Usability,Simpl,Simplify,8497,la:31); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8523,Usability,Simpl,Simplify,8523,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8533,Usability,simpl,simplifyMatrix,8533,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8548,Usability,Simpl,Simplify,8548,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8587,Usability,Simpl,Simplify,8587,la:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8603,Usability,Simpl,Simplify,8603,s.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8642,Usability,Simpl,Simplify,8642,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8676,Usability,Simpl,Simplify,8676,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8686,Usability,simpl,simplifyMatrix,8686,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8709,Usability,Simpl,Simplify,8709,mplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8748,Usability,Simpl,Simplify,8748,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8782,Usability,Simpl,Simplify,8782,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8792,Usability,simpl,simplifyMatrix,8792,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:8815,Usability,Simpl,Simplify,8815,mplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.colle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9380,Usability,Simpl,Simplify,9380,la:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$v,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9406,Usability,Simpl,Simplify,9406,Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9426,Usability,Simpl,Simplify,9426,Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9465,Usability,Simpl,Simplify,9465,la:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9491,Usability,Simpl,Simplify,9491,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9501,Usability,simpl,simplifyMatrix,9501,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9516,Usability,Simpl,Simplify,9516,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:5,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9555,Usability,Simpl,Simplify,9555,la:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9571,Usability,Simpl,Simplify,9571,s.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9610,Usability,Simpl,Simplify,9610,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9644,Usability,Simpl,Simplify,9644,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9654,Usability,simpl,simplifyMatrix,9654,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9677,Usability,Simpl,Simplify,9677,mplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$si,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9716,Usability,Simpl,Simplify,9716,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9750,Usability,Simpl,Simplify,9750,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9760,Usability,simpl,simplifyMatrix,9760,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:9783,Usability,Simpl,Simplify,9783,mplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$sim,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10348,Usability,Simpl,Simplify,10348,la:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10374,Usability,Simpl,Simplify,10374,Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10394,Usability,Simpl,Simplify,10394,Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10433,Usability,Simpl,Simplify,10433,la:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10459,Usability,Simpl,Simplify,10459,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10469,Usability,simpl,simplifyMatrix,10469,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10484,Usability,Simpl,Simplify,10484,plify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10523,Usability,Simpl,Simplify,10523,la:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10539,Usability,Simpl,Simplify,10539,s.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10578,Usability,Simpl,Simplify,10578,la:21); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10594,Usability,Simpl,Simplify,10594,s.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Sim,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10633,Usability,Simpl,Simplify,10633,atrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10667,Usability,Simpl,Simplify,10667,atrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10677,Usability,simpl,simplifyValue,10677,atrix$2.apply(Simplify.scala:49); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10699,Usability,Simpl,Simplify,10699,is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyMatrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10738,Usability,Simpl,Simplify,10738,Matrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10772,Usability,Simpl,Simplify,10772,Matrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10782,Usability,simpl,simplifyValue,10782,Matrix$2.apply(Simplify.scala:49); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:10804,Usability,Simpl,Simplify,10804, scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11369,Usability,Simpl,Simplify,11369,Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11395,Usability,Simpl,Simplify,11395,y.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11415,Usability,Simpl,Simplify,11415,y.scala:30); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$simplifyMatrix(Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11454,Usability,Simpl,Simplify,11454,Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11488,Usability,Simpl,Simplify,11488,Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11498,Usability,simpl,simplifyValue,11498,Simplify.scala:52); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11520,Usability,Simpl,Simplify,11520,r.Simplify$.apply(Simplify.scala:21); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11559,Usability,Simpl,Simplify,11559,pply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11593,Usability,Simpl,Simplify,11593,pply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11603,Usability,simpl,simplifyValue,11603,pply(Simplify.scala:11); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11625,Usability,Simpl,Simplify,11625,xpr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11664,Usability,Simpl,Simplify,11664,mplify$$simplifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11680,Usability,Simpl,Simplify,11680,lifyValue$1.apply(Simplify.scala:36); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11719,Usability,Simpl,Simplify,11719,.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/issues/8338:11735,Usability,Simpl,Simplify,11735,r.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$1.apply(Simplify.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAn,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8338
https://github.com/hail-is/hail/pull/8342:9,Usability,feedback,feedback,9,"Based on feedback from Maryam, this helps users use VEP correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8342
https://github.com/hail-is/hail/issues/8343:112,Availability,error,errors,112,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:614,Availability,avail,available,614,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:2152,Availability,Error,Error,2152,"python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:7598,Availability,Error,Error,7598,"ype$lzycompute(TextTableReader.scala:347); at is.hail.expr.ir.TextTableReader.fullType(TextTableReader.scala:347); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1231); at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1205); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1664); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1674); at is.hail.expr.ir.IRParser.parse_table_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-2684f0214a05; Error summary: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; ```. This is caused by a backwards incompatible change introduced in Google's Hadoop connector in version 2.1.0 https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/323. As of 2.1.0 Google's Hadoop connector relies on Hadoop 2.8.3. Unfortunately, there are no Spark releases with Hadoop 2.8.3 yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:2298,Deployability,Configurat,Configuration,2298," 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:2438,Deployability,Configurat,Configuration,2438," ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(Goo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:7655,Deployability,Configurat,Configuration,7655,"ype$lzycompute(TextTableReader.scala:347); at is.hail.expr.ir.TextTableReader.fullType(TextTableReader.scala:347); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1231); at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1205); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1664); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1674); at is.hail.expr.ir.IRParser.parse_table_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-2684f0214a05; Error summary: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; ```. This is caused by a backwards incompatible change introduced in Google's Hadoop connector in version 2.1.0 https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/323. As of 2.1.0 Google's Hadoop connector relies on Hadoop 2.8.3. Unfortunately, there are no Spark releases with Hadoop 2.8.3 yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:8001,Deployability,release,releases,8001,"ype$lzycompute(TextTableReader.scala:347); at is.hail.expr.ir.TextTableReader.fullType(TextTableReader.scala:347); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1231); at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1205); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1664); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1674); at is.hail.expr.ir.IRParser.parse_table_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-2684f0214a05; Error summary: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; ```. This is caused by a backwards incompatible change introduced in Google's Hadoop connector in version 2.1.0 https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/323. As of 2.1.0 Google's Hadoop connector relies on Hadoop 2.8.3. Unfortunately, there are no Spark releases with Hadoop 2.8.3 yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:1206,Integrability,wrap,wrapper,1206,":00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:2298,Modifiability,Config,Configuration,2298," 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:2438,Modifiability,Config,Configuration,2438," ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/hail/python/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;. Java stack trace:; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(Goo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:3432,Modifiability,config,configure,3432,n.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:7655,Modifiability,Config,Configuration,7655,"ype$lzycompute(TextTableReader.scala:347); at is.hail.expr.ir.TextTableReader.fullType(TextTableReader.scala:347); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1231); at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1205); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1664); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1674); at is.hail.expr.ir.IRParser.parse_table_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-2684f0214a05; Error summary: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; ```. This is caused by a backwards incompatible change introduced in Google's Hadoop connector in version 2.1.0 https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/323. As of 2.1.0 Google's Hadoop connector relies on Hadoop 2.8.3. Unfortunately, there are no Spark releases with Hadoop 2.8.3 yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:286,Performance,load,load,286,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:3878,Performance,Cache,Cache,3878,e.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:3954,Performance,Cache,Cache,3954,figurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:31,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:3811,Security,access,access,3811,; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collect,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:395,Testability,log,log,395,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:427,Testability,log,logging,427,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:757,Testability,LOG,LOGGING,757,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/issues/8343:817,Testability,log,log,817,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8343
https://github.com/hail-is/hail/pull/8344:262,Availability,down,downgrade,262,"Fixes #8343. The Google Storage Hadoop connector introduced; a backwards incompatible change in 2.1.0 which relies on; a new method in Hadoop 2.8.3 that is not present in Hadoop; 2.7.3. There are no Spark releases that include Hadoop 2.8.3; yet, so we choose to downgrade to the last compatible; connector library version. Randomly picked a hail query person.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8344
https://github.com/hail-is/hail/pull/8344:205,Deployability,release,releases,205,"Fixes #8343. The Google Storage Hadoop connector introduced; a backwards incompatible change in 2.1.0 which relies on; a new method in Hadoop 2.8.3 that is not present in Hadoop; 2.7.3. There are no Spark releases that include Hadoop 2.8.3; yet, so we choose to downgrade to the last compatible; connector library version. Randomly picked a hail query person.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8344
https://github.com/hail-is/hail/pull/8350:36,Testability,log,logic,36,"Fixes #8325. Got rid of dummy_table logic, which is unnecessary with `parallelize`. Agg/Scan envs were being mishandled in Extract/liftScan.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8350
https://github.com/hail-is/hail/pull/8356:161,Availability,error,error,161,This was implicitly converted to a pair. SBT complained that; this was probably not what I intended. Not sure why gradle; does not report the same warning as an error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8356
https://github.com/hail-is/hail/pull/8357:344,Deployability,deploy,deployment,344,"It's obvious that I'm the only one using SBT 😉 . I removed this spark helper thing that data bricks has abandoned. It's not hard to specify the right spark dependencies manually. In fact, we do that in `build.gradle` already. I don't know what the deal with hadoopClient, but it didn't seem necessary for my tests to pass. We don't use SBT for deployment, so I'm not worried. I'm not sure how all the http4s and json4s stuff got pulled in. They're not present in grade, so I removed them. I also bumped the SBT version for no particular reason. 🤷‍♀ . It works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8357
https://github.com/hail-is/hail/pull/8357:156,Integrability,depend,dependencies,156,"It's obvious that I'm the only one using SBT 😉 . I removed this spark helper thing that data bricks has abandoned. It's not hard to specify the right spark dependencies manually. In fact, we do that in `build.gradle` already. I don't know what the deal with hadoopClient, but it didn't seem necessary for my tests to pass. We don't use SBT for deployment, so I'm not worried. I'm not sure how all the http4s and json4s stuff got pulled in. They're not present in grade, so I removed them. I also bumped the SBT version for no particular reason. 🤷‍♀ . It works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8357
https://github.com/hail-is/hail/pull/8357:308,Testability,test,tests,308,"It's obvious that I'm the only one using SBT 😉 . I removed this spark helper thing that data bricks has abandoned. It's not hard to specify the right spark dependencies manually. In fact, we do that in `build.gradle` already. I don't know what the deal with hadoopClient, but it didn't seem necessary for my tests to pass. We don't use SBT for deployment, so I'm not worried. I'm not sure how all the http4s and json4s stuff got pulled in. They're not present in grade, so I removed them. I also bumped the SBT version for no particular reason. 🤷‍♀ . It works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8357
https://github.com/hail-is/hail/issues/8359:875,Availability,error,errors,875,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:147,Integrability,interface,interfaces,147,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:53,Modifiability,inherit,inheritance,53,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:94,Testability,Test,TestNGSuite,94,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:122,Testability,Assert,Assertions,122,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:161,Testability,Assert,Assertions,161,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:179,Testability,assert,assertThrows,179,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:257,Testability,test,testng,257,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:264,Testability,Test,TestNGSuite,264,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:281,Testability,assert,assertThrows,281,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:414,Testability,Assert,Assertion,414,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:487,Testability,Test,TestUtils,487,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:533,Testability,assert,assertThrows,533,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:691,Testability,Test,TestUtils,691,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:701,Testability,assert,assertThrows,701,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:798,Testability,Assert,Assertions,798,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:809,Testability,assert,assertThrows,809,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:970,Testability,assert,assertThrows,970,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:997,Testability,Test,TestUtils,997,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/issues/8359:1007,Testability,assert,assertThrows,1007,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8359
https://github.com/hail-is/hail/pull/8361:614,Integrability,wrap,wraps,614,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:758,Integrability,interface,interfaces,758,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:357,Modifiability,config,configurable,357,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:568,Modifiability,config,configure,568,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:1694,Performance,perform,performance,1694,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:329,Security,secur,secured,329,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:838,Security,secur,secure,838,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:952,Security,certificate,certificate,952,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/pull/8361:1528,Testability,test,test,1528,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361
https://github.com/hail-is/hail/issues/8366:10,Testability,benchmark,benchmarks,10,"Right now benchmarks only work if your project is set to `hail-vdc`. We should either check project in the Makefile and fail, or just always submit to `hail-vdc` project.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8366
https://github.com/hail-is/hail/pull/8371:2810,Availability,down,downcast,2810,"d generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:156,Integrability,Wrap,WrappedMethodBuilder,156,"It might not look like it, but I promise, I'm trying to keep these as small as possible. Summary of changes:; - EmitMethodBuilder, etc. no longer implement WrappedMethodBuilder, etc. but they do proxy the necessary functions.; - Emit method params and return values are now statically either: an Code[T] with type TypeInfo[T], or an EmitCode, with type PType.; - Added ParamType to wrap the two parameter type options, and Param to wrap the two code options, with implicit conversions.; - A bunch of methods now have emit and code variants: getEmitParam, getCodeParam, invokeCode, invokeEmit (where code, emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:382,Integrability,wrap,wrap,382,"It might not look like it, but I promise, I'm trying to keep these as small as possible. Summary of changes:; - EmitMethodBuilder, etc. no longer implement WrappedMethodBuilder, etc. but they do proxy the necessary functions.; - Emit method params and return values are now statically either: an Code[T] with type TypeInfo[T], or an EmitCode, with type PType.; - Added ParamType to wrap the two parameter type options, and Param to wrap the two code options, with implicit conversions.; - A bunch of methods now have emit and code variants: getEmitParam, getCodeParam, invokeCode, invokeEmit (where code, emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:432,Integrability,wrap,wrap,432,"It might not look like it, but I promise, I'm trying to keep these as small as possible. Summary of changes:; - EmitMethodBuilder, etc. no longer implement WrappedMethodBuilder, etc. but they do proxy the necessary functions.; - Emit method params and return values are now statically either: an Code[T] with type TypeInfo[T], or an EmitCode, with type PType.; - Added ParamType to wrap the two parameter type options, and Param to wrap the two code options, with implicit conversions.; - A bunch of methods now have emit and code variants: getEmitParam, getCodeParam, invokeCode, invokeEmit (where code, emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1351,Integrability,wrap,wrap,1351,"to wrap the two parameter type options, and Param to wrap the two code options, with implicit conversions.; - A bunch of methods now have emit and code variants: getEmitParam, getCodeParam, invokeCode, invokeEmit (where code, emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1463,Integrability,depend,dependent,1463," of methods now have emit and code variants: getEmitParam, getCodeParam, invokeCode, invokeEmit (where code, emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1482,Integrability,interface,interface,1482," of methods now have emit and code variants: getEmitParam, getCodeParam, invokeCode, invokeEmit (where code, emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1550,Integrability,Depend,DependentFunctionBuilder,1550,"emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1605,Integrability,Depend,DependentMethodBuilder,1605,"emit refer to the return value).; - I made the minimal changes to get things working again. Some code still code parameters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1663,Integrability,Depend,DependentMethodBuilder,1663,"meters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1686,Integrability,wrap,wraps,1686,"meters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1727,Integrability,wrap,wraps,1727,"meters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:1735,Integrability,Depend,DependentMethodBuilder,1735,"meters to pass emit values. I will fix these in a later PR.; - Where possible, make the class implemented by Compile concrete rather than generic. I think this can be pushed through the entire code base and we can remove the option to build generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:2144,Integrability,interface,interfaces,2144,"d generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:2592,Testability,assert,assert,2592,"d generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8371:2950,Testability,assert,assertions,2950,"d generic methods. We should have done this a long time ago.; - ModuleBuilder can now create type-specialized tuple types. This is used for EmitCode return values. I'm not sure if this is actually used yet.; - Require RVDType rowType to be required. Require TypeValue global type to be required. Fix lots of places to make this true. In a few spots (e.g. TableMap{Rows, Globals}), I had to wrap the IR being compiled in a Coalesce with a Die to make sure the return type is required.; - Cleaned up the dependent function interface to be closer to what we have now with MethodBuilder, etc. DependentFunctionBuilder is now just an `apply_method: DependentMethodBuilder`, EmitFunctionBuilder analogously. DependentMethodBuilder wraps a MethodBuilder, EmitMethodBuilder wraps a DependentMethodBuilder and an EmitMethodBuilder.; - Add equality comparison to TypeInfo[_]; - Add methods to convert IndexedSeq[Code[_]] to/from PCode and EmitCode. These are used to pass EmitCode as arguments to method invocation. If an emit parameter is required, the missingness boolean is omitted, otherwise it is present. Furthermore, this change also adds requiredness to many things and improves ptype interfaces:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371
https://github.com/hail-is/hail/pull/8372:124,Integrability,depend,dependency,124,"This fixes the ""Unknown source"" issue in asm stack traces in IntelliJ. Also:; - Exclude conflicting asm version from hadoop dependency. I'm not 100% sure this is right, but it seems to work.; - Remove unused jacoco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8372
https://github.com/hail-is/hail/pull/8378:10,Testability,log,logging,10,"Made some logging improvements to the combiner in the process of; understanding the various components. Benchmarks, docs, perf.; improvements are next. Simple example of merging three GVCFs:. ```; 2020-03-27 15:17:58 Hail: INFO: GVCF combiner plan:; Branch factor: 2; Batch size: 2; Combining 3 input files in 2 phases with 2 total jobs.; Phase 1: 1 job corresponding to 2 intermediate output files.; Phase 2: 1 job corresponding to 1 final output file. 2020-03-27 15:17:58 Hail: INFO: Starting phase 1/2, merging 3 input GVCFs in 1 job.; 2020-03-27 15:17:58 Hail: INFO: Starting job 1/1 to create 2 merged files, corresponding to ~50.0% of total I/O.; 2020-03-27 15:21:20 Hail: INFO: Finished job 1/1, 50.0% of total I/O finished.; 2020-03-27 15:21:20 Hail: INFO: Finished phase 1/2.; 2020-03-27 15:21:20 Hail: INFO: Starting phase 2/2, merging 2 intermediate sparse matrix tables in 1 job.; 2020-03-27 15:21:27 Hail: INFO: Starting job 1/1 to create 1 merged file, corresponding to ~50.0% of total I/O.; 2020-03-27 15:24:47 Hail: INFO: wrote matrix table with 47031230 rows and 3 columns in 33 partitions to combiner_out.mt; 2020-03-27 15:24:47 Hail: INFO: Finished job 1/1, 100.0% of total I/O finished.; 2020-03-27 15:24:47 Hail: INFO: Finished phase 2/2.; 2020-03-27 15:24:47 Hail: INFO: Finished!; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8378
https://github.com/hail-is/hail/pull/8378:104,Testability,Benchmark,Benchmarks,104,"Made some logging improvements to the combiner in the process of; understanding the various components. Benchmarks, docs, perf.; improvements are next. Simple example of merging three GVCFs:. ```; 2020-03-27 15:17:58 Hail: INFO: GVCF combiner plan:; Branch factor: 2; Batch size: 2; Combining 3 input files in 2 phases with 2 total jobs.; Phase 1: 1 job corresponding to 2 intermediate output files.; Phase 2: 1 job corresponding to 1 final output file. 2020-03-27 15:17:58 Hail: INFO: Starting phase 1/2, merging 3 input GVCFs in 1 job.; 2020-03-27 15:17:58 Hail: INFO: Starting job 1/1 to create 2 merged files, corresponding to ~50.0% of total I/O.; 2020-03-27 15:21:20 Hail: INFO: Finished job 1/1, 50.0% of total I/O finished.; 2020-03-27 15:21:20 Hail: INFO: Finished phase 1/2.; 2020-03-27 15:21:20 Hail: INFO: Starting phase 2/2, merging 2 intermediate sparse matrix tables in 1 job.; 2020-03-27 15:21:27 Hail: INFO: Starting job 1/1 to create 1 merged file, corresponding to ~50.0% of total I/O.; 2020-03-27 15:24:47 Hail: INFO: wrote matrix table with 47031230 rows and 3 columns in 33 partitions to combiner_out.mt; 2020-03-27 15:24:47 Hail: INFO: Finished job 1/1, 100.0% of total I/O finished.; 2020-03-27 15:24:47 Hail: INFO: Finished phase 2/2.; 2020-03-27 15:24:47 Hail: INFO: Finished!; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8378
https://github.com/hail-is/hail/pull/8378:152,Usability,Simpl,Simple,152,"Made some logging improvements to the combiner in the process of; understanding the various components. Benchmarks, docs, perf.; improvements are next. Simple example of merging three GVCFs:. ```; 2020-03-27 15:17:58 Hail: INFO: GVCF combiner plan:; Branch factor: 2; Batch size: 2; Combining 3 input files in 2 phases with 2 total jobs.; Phase 1: 1 job corresponding to 2 intermediate output files.; Phase 2: 1 job corresponding to 1 final output file. 2020-03-27 15:17:58 Hail: INFO: Starting phase 1/2, merging 3 input GVCFs in 1 job.; 2020-03-27 15:17:58 Hail: INFO: Starting job 1/1 to create 2 merged files, corresponding to ~50.0% of total I/O.; 2020-03-27 15:21:20 Hail: INFO: Finished job 1/1, 50.0% of total I/O finished.; 2020-03-27 15:21:20 Hail: INFO: Finished phase 1/2.; 2020-03-27 15:21:20 Hail: INFO: Starting phase 2/2, merging 2 intermediate sparse matrix tables in 1 job.; 2020-03-27 15:21:27 Hail: INFO: Starting job 1/1 to create 1 merged file, corresponding to ~50.0% of total I/O.; 2020-03-27 15:24:47 Hail: INFO: wrote matrix table with 47031230 rows and 3 columns in 33 partitions to combiner_out.mt; 2020-03-27 15:24:47 Hail: INFO: Finished job 1/1, 100.0% of total I/O finished.; 2020-03-27 15:24:47 Hail: INFO: Finished phase 2/2.; 2020-03-27 15:24:47 Hail: INFO: Finished!; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8378
https://github.com/hail-is/hail/pull/8383:9,Performance,perform,performance,9,Improves performance of GVCF import significantly:; ```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; import_gvcf_force_count 81.2% 68.737 55.833; import_and_transform_gvcf 79.9% 75.692 60.464; ----------------------; Harmonic mean: 80.5%; Geometric mean: 80.6%; Arithmetic mean: 80.6%; Median: 80.6%; ```. Stacked on #8382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8383
https://github.com/hail-is/hail/pull/8383:57,Testability,Benchmark,Benchmark,57,Improves performance of GVCF import significantly:; ```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; import_gvcf_force_count 81.2% 68.737 55.833; import_and_transform_gvcf 79.9% 75.692 60.464; ----------------------; Harmonic mean: 80.5%; Geometric mean: 80.6%; Arithmetic mean: 80.6%; Median: 80.6%; ```. Stacked on #8382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8383
https://github.com/hail-is/hail/pull/8386:220,Integrability,interface,interface,220,"Summary of changes:; - Added index_bgen to Python Backend; - Move most Backend functions to SparkBackend. Everything about the current code assumes a single user, but the ServiceBackend will have a different, mutli-user interface.; - Added Backend and FS to ExecuteContext.; - renamed Backend clear => stop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8386
https://github.com/hail-is/hail/pull/8386:293,Usability,clear,clear,293,"Summary of changes:; - Added index_bgen to Python Backend; - Move most Backend functions to SparkBackend. Everything about the current code assumes a single user, but the ServiceBackend will have a different, mutli-user interface.; - Added Backend and FS to ExecuteContext.; - renamed Backend clear => stop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8386
https://github.com/hail-is/hail/pull/8388:41,Integrability,interface,interface,41,"It was too tightly coupled to the Hadoop interface (still is). Remove fs.{FilePath, FileSystem}.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8388
https://github.com/hail-is/hail/issues/8390:1517,Availability,down,download,1517,"hon3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1612,Availability,down,download,1612,"lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1706,Availability,down,download,1706,"b/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.leng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1843,Availability,down,download,1843,"ne 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:2089,Availability,down,download,2089,""", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhoste",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:2283,Availability,down,download,2283,"/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.; The command '/bin/sh -c python3 -m pip install --no-cache-dir -r requirements.txt -r dev-requirements.txt' returned a non-zero code: 2; [0m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1148,Deployability,install,install,1148,"ponse.py"", line 302, in _error_catcher; yield; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 384, in read; data = self._fp.read(amt); File ""/usr/lib/python3.6/http/client.py"", line 459, in read; n = self.readinto(b); File ""/usr/lib/python3.6/http/client.py"", line 503, in readinto; n = self.fp.readinto(b); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:3118,Deployability,install,install,3118,"/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.; The command '/bin/sh -c python3 -m pip install --no-cache-dir -r requirements.txt -r dev-requirements.txt' returned a non-zero code: 2; [0m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:3131,Performance,cache,cache-dir,3131,"/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.; The command '/bin/sh -c python3 -m pip install --no-cache-dir -r requirements.txt -r dev-requirements.txt' returned a non-zero code: 2; [0m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:843,Safety,timeout,timeout,843,"```; Traceback (most recent call last):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 302, in _error_catcher; yield; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 384, in read; data = self._fp.read(amt); File ""/usr/lib/python3.6/http/client.py"", line 459, in read; n = self.readinto(b); File ""/usr/lib/python3.6/http/client.py"", line 503, in readinto; n = self.fp.readinto(b); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1460,Security,hash,hashes,1460,"hon3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1467,Security,hash,hashes,1467,"hon3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1556,Security,hash,hashes,1556,"lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1563,Security,hash,hashes,1563,"lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1656,Security,hash,hashes,1656,"b/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.leng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1793,Security,hash,hashes,1793,"ne 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1885,Security,hash,hashes,1885,"imed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"",",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/issues/8390:1980,Security,hash,hashes,1980," occurred:. Traceback (most recent call last):; File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 342, in run; requirement_set.prepare_files(finder); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, Non",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8390
https://github.com/hail-is/hail/pull/8391:42,Availability,error,errors,42,It was previously hard to retry transient errors from synchronous libraries like; requests because `hailtop` lacked a synchronous retry wrapper. This PR; implements such a function and uses it in every place that hail imports; `requests`. I also finally addressed the 1kg download issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8391
https://github.com/hail-is/hail/pull/8391:272,Availability,down,download,272,It was previously hard to retry transient errors from synchronous libraries like; requests because `hailtop` lacked a synchronous retry wrapper. This PR; implements such a function and uses it in every place that hail imports; `requests`. I also finally addressed the 1kg download issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8391
https://github.com/hail-is/hail/pull/8391:136,Integrability,wrap,wrapper,136,It was previously hard to retry transient errors from synchronous libraries like; requests because `hailtop` lacked a synchronous retry wrapper. This PR; implements such a function and uses it in every place that hail imports; `requests`. I also finally addressed the 1kg download issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8391
https://github.com/hail-is/hail/issues/8395:157,Availability,error,error,157,"So I just had the following situation: saved an Excel doc from a collaborator as CSV to the import it into hail. Running `hl.import_table` didn't return any error... but somehow the header was both correctly assigned as column names but also added as the first line. After some poking around, it turns out that the file encoding was UTF8 with BOM and that it somehow tripped `hl.import_table`. Same file works after re-encoding it as plain UTF8. . Please include the full Hail version and as much detail as possible.; version 0.2.34-914bd8a10ca2; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8395
https://github.com/hail-is/hail/pull/8396:67,Testability,test,tests,67,"Also some changes to make it possible to run through Python. These tests are obviously not sufficient, but they are a good start.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8396
https://github.com/hail-is/hail/issues/8397:289,Availability,error,error,289,```; + date; Mon Mar 30 22:11:05 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.3H7wTmq0R2; + git clone https://github.com/danking/hail.git /tmp/tmp.3H7wTmq0R2; Cloning into '/tmp/tmp.3H7wTmq0R2'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.3H7wTmq0R2. real	0m3.373s; user	0m0.006s; sys	0m0.025s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8397
https://github.com/hail-is/hail/issues/8397:328,Availability,error,error,328,```; + date; Mon Mar 30 22:11:05 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.3H7wTmq0R2; + git clone https://github.com/danking/hail.git /tmp/tmp.3H7wTmq0R2; Cloning into '/tmp/tmp.3H7wTmq0R2'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.3H7wTmq0R2. real	0m3.373s; user	0m0.006s; sys	0m0.025s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8397
https://github.com/hail-is/hail/issues/8397:341,Availability,Error,Error,341,```; + date; Mon Mar 30 22:11:05 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.3H7wTmq0R2; + git clone https://github.com/danking/hail.git /tmp/tmp.3H7wTmq0R2; Cloning into '/tmp/tmp.3H7wTmq0R2'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.3H7wTmq0R2. real	0m3.373s; user	0m0.006s; sys	0m0.025s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8397
https://github.com/hail-is/hail/issues/8397:538,Modifiability,config,config,538,```; + date; Mon Mar 30 22:11:05 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.3H7wTmq0R2; + git clone https://github.com/danking/hail.git /tmp/tmp.3H7wTmq0R2; Cloning into '/tmp/tmp.3H7wTmq0R2'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.3H7wTmq0R2. real	0m3.373s; user	0m0.006s; sys	0m0.025s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8397
https://github.com/hail-is/hail/pull/8398:1111,Deployability,update,update,1111,"cc: @tpoterba ; ![Screen Shot 2020-03-30 at 6 53 49 PM](https://user-images.githubusercontent.com/106194/77969485-cd780d00-72b7-11ea-96fc-4f529297c830.png). Since my mind was already thinking about CI to address other issues, I thought it prudent to implement this longstanding request. The key idea is that each batch now has a target sha, source sha, and an attempt id. When we refresh state from batch, we look for the oldest batch with the newest attempt_id. We increment attempt_id whenever we `_start_build` on a batch. If the source sha changes, we set the attempt id to zero. It will be incremented to one the next time we build. In this manner, we prevent CI from refreshing from batch and blowing away a retried batch even though it has a higher batch id (and we always prefer lower ids). Retry is implemented in the standard a PR is noted as needing a retry and the watched branch is informed that its state has changed. The heal method of a PR checks for the retry flag, cancels any existing batch, and triggers a new build. Miscellaneous changes/fixes:; - a developers only endpoint to force CI to update right now (convenient for dev deploy where the GitHub triggers are not configured); - don't try to clean up a database that wasn't created; - correct URL for CI in a couple places",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8398
https://github.com/hail-is/hail/pull/8398:1148,Deployability,deploy,deploy,1148,"cc: @tpoterba ; ![Screen Shot 2020-03-30 at 6 53 49 PM](https://user-images.githubusercontent.com/106194/77969485-cd780d00-72b7-11ea-96fc-4f529297c830.png). Since my mind was already thinking about CI to address other issues, I thought it prudent to implement this longstanding request. The key idea is that each batch now has a target sha, source sha, and an attempt id. When we refresh state from batch, we look for the oldest batch with the newest attempt_id. We increment attempt_id whenever we `_start_build` on a batch. If the source sha changes, we set the attempt id to zero. It will be incremented to one the next time we build. In this manner, we prevent CI from refreshing from batch and blowing away a retried batch even though it has a higher batch id (and we always prefer lower ids). Retry is implemented in the standard a PR is noted as needing a retry and the watched branch is informed that its state has changed. The heal method of a PR checks for the retry flag, cancels any existing batch, and triggers a new build. Miscellaneous changes/fixes:; - a developers only endpoint to force CI to update right now (convenient for dev deploy where the GitHub triggers are not configured); - don't try to clean up a database that wasn't created; - correct URL for CI in a couple places",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8398
https://github.com/hail-is/hail/pull/8398:1189,Modifiability,config,configured,1189,"cc: @tpoterba ; ![Screen Shot 2020-03-30 at 6 53 49 PM](https://user-images.githubusercontent.com/106194/77969485-cd780d00-72b7-11ea-96fc-4f529297c830.png). Since my mind was already thinking about CI to address other issues, I thought it prudent to implement this longstanding request. The key idea is that each batch now has a target sha, source sha, and an attempt id. When we refresh state from batch, we look for the oldest batch with the newest attempt_id. We increment attempt_id whenever we `_start_build` on a batch. If the source sha changes, we set the attempt id to zero. It will be incremented to one the next time we build. In this manner, we prevent CI from refreshing from batch and blowing away a retried batch even though it has a higher batch id (and we always prefer lower ids). Retry is implemented in the standard a PR is noted as needing a retry and the watched branch is informed that its state has changed. The heal method of a PR checks for the retry flag, cancels any existing batch, and triggers a new build. Miscellaneous changes/fixes:; - a developers only endpoint to force CI to update right now (convenient for dev deploy where the GitHub triggers are not configured); - don't try to clean up a database that wasn't created; - correct URL for CI in a couple places",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8398
https://github.com/hail-is/hail/pull/8400:31,Integrability,message,message,31,"Prints the default in the help message, way nicer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8400
https://github.com/hail-is/hail/pull/8402:272,Availability,error,error,272,"I never tested that PR that got merged (whoops!) and CI tests are insufficient; to catch this case (we should beef those up, asana task added). The issue was that I thought the method to issue an HTTP get request was `get`,; but it was `getitem`. This PR fixes that. This error occured during `update` and; thus prevented all forward progress of CI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402
https://github.com/hail-is/hail/pull/8402:294,Deployability,update,update,294,"I never tested that PR that got merged (whoops!) and CI tests are insufficient; to catch this case (we should beef those up, asana task added). The issue was that I thought the method to issue an HTTP get request was `get`,; but it was `getitem`. This PR fixes that. This error occured during `update` and; thus prevented all forward progress of CI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402
https://github.com/hail-is/hail/pull/8402:8,Testability,test,tested,8,"I never tested that PR that got merged (whoops!) and CI tests are insufficient; to catch this case (we should beef those up, asana task added). The issue was that I thought the method to issue an HTTP get request was `get`,; but it was `getitem`. This PR fixes that. This error occured during `update` and; thus prevented all forward progress of CI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402
https://github.com/hail-is/hail/pull/8402:56,Testability,test,tests,56,"I never tested that PR that got merged (whoops!) and CI tests are insufficient; to catch this case (we should beef those up, asana task added). The issue was that I thought the method to issue an HTTP get request was `get`,; but it was `getitem`. This PR fixes that. This error occured during `update` and; thus prevented all forward progress of CI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402
https://github.com/hail-is/hail/pull/8403:0,Availability,Error,Error,0,Error is rather obvious as is fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8403
https://github.com/hail-is/hail/pull/8404:66,Testability,test,testing,66,"This works for importing the single-dog sample vcf that I've been testing on. It looks like there's a couple of different coding conventions for contig names, but this is the one that matches the fasta file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8404
https://github.com/hail-is/hail/pull/8405:239,Availability,error,errors,239,"Move installed location of hail-all-spark.jar to from hail/ to; hail/backend/. We were not finding the jar properly with pkg_resources, and so were not; setting the paths appropriately for pip installs, causing 'JavaPackage; not callable' errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8405
https://github.com/hail-is/hail/pull/8405:5,Deployability,install,installed,5,"Move installed location of hail-all-spark.jar to from hail/ to; hail/backend/. We were not finding the jar properly with pkg_resources, and so were not; setting the paths appropriately for pip installs, causing 'JavaPackage; not callable' errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8405
https://github.com/hail-is/hail/pull/8405:193,Deployability,install,installs,193,"Move installed location of hail-all-spark.jar to from hail/ to; hail/backend/. We were not finding the jar properly with pkg_resources, and so were not; setting the paths appropriately for pip installs, causing 'JavaPackage; not callable' errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8405
https://github.com/hail-is/hail/pull/8406:23,Deployability,deploy,deploys,23,"- Fix statuses for dev deploys.; - Fix zulip notify for dev deploys (not currently used).; - Fix HTTPFound for dev deploys and PRs. We need to prefix the redirect with the base path; for this current CI instance. In my dev deploy case,; this prepends: `dking/ci` from which the web browser can; correctly redirect to: `https://internal.hail.is/dking/ci/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8406
https://github.com/hail-is/hail/pull/8406:60,Deployability,deploy,deploys,60,"- Fix statuses for dev deploys.; - Fix zulip notify for dev deploys (not currently used).; - Fix HTTPFound for dev deploys and PRs. We need to prefix the redirect with the base path; for this current CI instance. In my dev deploy case,; this prepends: `dking/ci` from which the web browser can; correctly redirect to: `https://internal.hail.is/dking/ci/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8406
https://github.com/hail-is/hail/pull/8406:115,Deployability,deploy,deploys,115,"- Fix statuses for dev deploys.; - Fix zulip notify for dev deploys (not currently used).; - Fix HTTPFound for dev deploys and PRs. We need to prefix the redirect with the base path; for this current CI instance. In my dev deploy case,; this prepends: `dking/ci` from which the web browser can; correctly redirect to: `https://internal.hail.is/dking/ci/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8406
https://github.com/hail-is/hail/pull/8406:223,Deployability,deploy,deploy,223,"- Fix statuses for dev deploys.; - Fix zulip notify for dev deploys (not currently used).; - Fix HTTPFound for dev deploys and PRs. We need to prefix the redirect with the base path; for this current CI instance. In my dev deploy case,; this prepends: `dking/ci` from which the web browser can; correctly redirect to: `https://internal.hail.is/dking/ci/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8406
https://github.com/hail-is/hail/pull/8407:165,Deployability,deploy,deploys,165,"Databases that are not created fail currently without this change; because they try to connect to a database called None. This; is only visible in PR builds and dev deploys. PR builds do; not test this behavior, so I do not think anyone else; has observed it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8407
https://github.com/hail-is/hail/pull/8407:192,Testability,test,test,192,"Databases that are not created fail currently without this change; because they try to connect to a database called None. This; is only visible in PR builds and dev deploys. PR builds do; not test this behavior, so I do not think anyone else; has observed it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8407
https://github.com/hail-is/hail/pull/8408:27,Security,authenticat,authenticated,27,"This makes it easy to send authenticated HTTP requests to our; infrastructure. When developing a new service, you might not have an; existing (or working) client yet and want to explore the HTTP endpoints with; cURL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8408
https://github.com/hail-is/hail/pull/8409:52,Deployability,deploy,deploy,52,This is developer only and helpful primarily in dev deploy.; I think its safe as long as we trust hail devs not to; accidentally hit this endpoint.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8409
https://github.com/hail-is/hail/pull/8409:73,Safety,safe,safe,73,This is developer only and helpful primarily in dev deploy.; I think its safe as long as we trust hail devs not to; accidentally hit this endpoint.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8409
https://github.com/hail-is/hail/pull/8410:120,Usability,clear,clear,120,The latest changes to CI introduced a bug where an old build status; can be reported to GitHub because we do not always clear the; intended_github_status when we change the build_state. This PR; fixes that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8410
https://github.com/hail-is/hail/issues/8411:1379,Availability,Error,Error,1379,"The code as written doesn't seem to allow this to happen. Did someone else bind to that port? It looks like it can happen if an unhandled exception occurs during docker stop or delete, in which case we free the port even though the container might still have the port open. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 354, in run; start_container, self.container); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 94, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.6/asyncio/tasks.py"", line 358, in wait_for; return fut.result(); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 142, in start_container; return await container.start(); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 170, in start; data=kwargs,; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'driver failed programming external connectivity on endpoint batch-20376-job-59-main (8a971634c54c03a1e7df1b4255814137c92e10d310b3d47a1fe6cb7432222ed0): Error starting userland proxy: listen tcp 0.0.0.0:46572: bind: address already in use'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8411
https://github.com/hail-is/hail/issues/8411:505,Integrability,wrap,wrapper,505,"The code as written doesn't seem to allow this to happen. Did someone else bind to that port? It looks like it can happen if an unhandled exception occurs during docker stop or delete, in which case we free the port even though the container might still have the port open. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 354, in run; start_container, self.container); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 94, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.6/asyncio/tasks.py"", line 358, in wait_for; return fut.result(); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 142, in start_container; return await container.start(); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 170, in start; data=kwargs,; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'driver failed programming external connectivity on endpoint batch-20376-job-59-main (8a971634c54c03a1e7df1b4255814137c92e10d310b3d47a1fe6cb7432222ed0): Error starting userland proxy: listen tcp 0.0.0.0:46572: bind: address already in use'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8411
https://github.com/hail-is/hail/issues/8411:1146,Performance,load,loads,1146,"The code as written doesn't seem to allow this to happen. Did someone else bind to that port? It looks like it can happen if an unhandled exception occurs during docker stop or delete, in which case we free the port even though the container might still have the port open. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 354, in run; start_container, self.container); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 94, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.6/asyncio/tasks.py"", line 358, in wait_for; return fut.result(); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 142, in start_container; return await container.start(); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 170, in start; data=kwargs,; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'driver failed programming external connectivity on endpoint batch-20376-job-59-main (8a971634c54c03a1e7df1b4255814137c92e10d310b3d47a1fe6cb7432222ed0): Error starting userland proxy: listen tcp 0.0.0.0:46572: bind: address already in use'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8411
https://github.com/hail-is/hail/issues/8411:564,Safety,timeout,timeout,564,"The code as written doesn't seem to allow this to happen. Did someone else bind to that port? It looks like it can happen if an unhandled exception occurs during docker stop or delete, in which case we free the port even though the container might still have the port open. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 354, in run; start_container, self.container); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 94, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.6/asyncio/tasks.py"", line 358, in wait_for; return fut.result(); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 142, in start_container; return await container.start(); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 170, in start; data=kwargs,; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'driver failed programming external connectivity on endpoint batch-20376-job-59-main (8a971634c54c03a1e7df1b4255814137c92e10d310b3d47a1fe6cb7432222ed0): Error starting userland proxy: listen tcp 0.0.0.0:46572: bind: address already in use'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8411
https://github.com/hail-is/hail/pull/8414:108,Integrability,interface,interface,108,"Summary of changes:; - more FS de-Hadoopification; - make the FS core minimal, and implement as much of the interface in terms of that core; - add GoogleStorageFS file system; - add general FSSuite and two implementations: Hadoop and gs://; - google storage tests don't run by default because you need a bucket and key. I set those in the test_hail_java build step.; - There is more stuff to clean up, the Seekable stuff specifically. I'd like to get to the point where only HadoopFS imports hadoop. I don't think we're that far off.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8414
https://github.com/hail-is/hail/pull/8414:258,Testability,test,tests,258,"Summary of changes:; - more FS de-Hadoopification; - make the FS core minimal, and implement as much of the interface in terms of that core; - add GoogleStorageFS file system; - add general FSSuite and two implementations: Hadoop and gs://; - google storage tests don't run by default because you need a bucket and key. I set those in the test_hail_java build step.; - There is more stuff to clean up, the Seekable stuff specifically. I'd like to get to the point where only HadoopFS imports hadoop. I don't think we're that far off.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8414
https://github.com/hail-is/hail/pull/8416:73,Deployability,configurat,configuration,73,"Soon, I will add certificates and keys to the secrets and I want; to add configuration parameters that specify the paths to those; certificates and keys. Therefore, the mount locations of the; secrets must be the same everywhere so the paths are valid.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8416
https://github.com/hail-is/hail/pull/8416:73,Modifiability,config,configuration,73,"Soon, I will add certificates and keys to the secrets and I want; to add configuration parameters that specify the paths to those; certificates and keys. Therefore, the mount locations of the; secrets must be the same everywhere so the paths are valid.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8416
https://github.com/hail-is/hail/pull/8416:17,Security,certificate,certificates,17,"Soon, I will add certificates and keys to the secrets and I want; to add configuration parameters that specify the paths to those; certificates and keys. Therefore, the mount locations of the; secrets must be the same everywhere so the paths are valid.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8416
https://github.com/hail-is/hail/pull/8416:131,Security,certificate,certificates,131,"Soon, I will add certificates and keys to the secrets and I want; to add configuration parameters that specify the paths to those; certificates and keys. Therefore, the mount locations of the; secrets must be the same everywhere so the paths are valid.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8416
https://github.com/hail-is/hail/pull/8417:479,Deployability,deploy,deployed,479,"Changes:; - mostly boilerplate; - moved Dockerfile.hail-base from apiserver/ to hail/; - move apiserver => query; - I'm going to call the Hail query service query; - remove legacy apiserver stuff; - add a dummy Java ServiceBackend with a single dummy method `request` that returns 5.; - query spins up py4j without using Spark; - has a single /request REST endpoint that just returns `{'value': 5}` via the Java ServiceBackend; - add query to letsencrypt, routers, etc.; - I had deployed all of this via the Makefile and it is working fine. Just one point that might not be obvious: the Python ServiceBackend talks to the query service. The query service talks to the Java ServiceBackend but won't itself spin up a Python backend. Plan:; - I will start adding multi-user functionality to this after the GoogleStorageFS PR lands; - It will soon be possible to connect lowered stuff and the shuffler to this. Things are about to get real. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8417
https://github.com/hail-is/hail/pull/8417:456,Integrability,rout,routers,456,"Changes:; - mostly boilerplate; - moved Dockerfile.hail-base from apiserver/ to hail/; - move apiserver => query; - I'm going to call the Hail query service query; - remove legacy apiserver stuff; - add a dummy Java ServiceBackend with a single dummy method `request` that returns 5.; - query spins up py4j without using Spark; - has a single /request REST endpoint that just returns `{'value': 5}` via the Java ServiceBackend; - add query to letsencrypt, routers, etc.; - I had deployed all of this via the Makefile and it is working fine. Just one point that might not be obvious: the Python ServiceBackend talks to the query service. The query service talks to the Java ServiceBackend but won't itself spin up a Python backend. Plan:; - I will start adding multi-user functionality to this after the GoogleStorageFS PR lands; - It will soon be possible to connect lowered stuff and the shuffler to this. Things are about to get real. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8417
https://github.com/hail-is/hail/pull/8420:249,Availability,error,error,249,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8420
https://github.com/hail-is/hail/pull/8420:427,Availability,error,error,427,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8420
https://github.com/hail-is/hail/pull/8420:433,Integrability,message,message,433,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8420
https://github.com/hail-is/hail/pull/8420:214,Testability,test,tests,214,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8420
https://github.com/hail-is/hail/pull/8420:270,Testability,test,tests,270,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8420
https://github.com/hail-is/hail/pull/8420:282,Testability,test,test,282,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8420
https://github.com/hail-is/hail/pull/8420:496,Testability,test,test,496,"Changes:; - correct the interpretation of less and greater.; - improve the formatting and verbiage of the docs,; - expand upon the statistical definition alluded to previously in only the less; case,; - add python tests which would have caught this error,; - add python tests which test against `scipy`,; - deprecate the use of `'two.sided'`, an R-ism, document the preferred use of; `'two-sided'`, a Python-ism, and; - fix an error message in Scala that used yet another naming of the two sided test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8420
https://github.com/hail-is/hail/issues/8423:130,Availability,error,error,130,"After I set-up my spark context on my spark cluster, I ran hail.init() and ran into the following: ; `py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([null, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423
https://github.com/hail-is/hail/issues/8423:107,Integrability,protocol,protocol,107,"After I set-up my spark context on my spark cluster, I ran hail.init() and ran into the following: ; `py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([null, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423
https://github.com/hail-is/hail/pull/8428:8,Availability,failure,failure,8,A merge failure creates a funny batch that doesn't have an id. This handles that properly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8428
https://github.com/hail-is/hail/pull/8431:264,Usability,simpl,simply,264,"PBetterLocus is represented as a Long. The high 32 bits are the index of; the contig in its reference genome. The low 32 bits are the position. A key assumption here is that there will never be more than INT_MAX; contigs in any reference genome. With that, we can simply compare; PBetterLocus with Long comparision as they are all nonnegative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8431
https://github.com/hail-is/hail/pull/8433:991,Availability,error,errors,991,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:60,Deployability,configurat,configuration,60,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:485,Deployability,update,update,485,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:1092,Deployability,update,updated,1092,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:1206,Deployability,update,updated,1206,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:60,Modifiability,config,configuration,60,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:262,Modifiability,config,config,262,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:563,Modifiability,config,config,563,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:1260,Modifiability,config,config,1260,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:736,Security,certificate,certificate,736,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:832,Security,certificate,certificate,832,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8433:195,Usability,simpl,simply,195,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433
https://github.com/hail-is/hail/pull/8434:932,Modifiability,extend,extend,932,"This changes allow `hl.init()` to run against the query service without starting JVM/Spark on the client:. ```; $ python3; Python 3.7.3 (default, Oct 7 2019, 12:56:13) ; [GCC 8.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; >>> hl.init(_backend=hl.backend.ServiceBackend()); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-0ef20f14e0c1; LOGGING: writing to /home/cotton/hail/hail-20200402-0120-0.2.34-0ef20f14e0c1.log; ```. Summary of changes:; - Move initialization of Java HailContext from init to SparkBackend ctor. There is no JVM or Java HailContext when using the service backend.; - Env no longer carries the gateway. (Next: jvm); - make Java HailContext.tmpDir construction lazy. It requires a fs, but HailContext will only carry an fs for the SparkBackend. This will have to get rethought.; - Make Java ServiceBackend extend Backend.; - Construct a HailContext in the query service.; - Implement /references/get in query backend which is needed by hl.init to get the builtin reference genomes on startup.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8434
https://github.com/hail-is/hail/pull/8434:442,Testability,LOG,LOGGING,442,"This changes allow `hl.init()` to run against the query service without starting JVM/Spark on the client:. ```; $ python3; Python 3.7.3 (default, Oct 7 2019, 12:56:13) ; [GCC 8.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; >>> hl.init(_backend=hl.backend.ServiceBackend()); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-0ef20f14e0c1; LOGGING: writing to /home/cotton/hail/hail-20200402-0120-0.2.34-0ef20f14e0c1.log; ```. Summary of changes:; - Move initialization of Java HailContext from init to SparkBackend ctor. There is no JVM or Java HailContext when using the service backend.; - Env no longer carries the gateway. (Next: jvm); - make Java HailContext.tmpDir construction lazy. It requires a fs, but HailContext will only carry an fs for the SparkBackend. This will have to get rethought.; - Make Java ServiceBackend extend Backend.; - Construct a HailContext in the query service.; - Implement /references/get in query backend which is needed by hl.init to get the builtin reference genomes on startup.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8434
https://github.com/hail-is/hail/pull/8434:519,Testability,log,log,519,"This changes allow `hl.init()` to run against the query service without starting JVM/Spark on the client:. ```; $ python3; Python 3.7.3 (default, Oct 7 2019, 12:56:13) ; [GCC 8.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; >>> hl.init(_backend=hl.backend.ServiceBackend()); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-0ef20f14e0c1; LOGGING: writing to /home/cotton/hail/hail-20200402-0120-0.2.34-0ef20f14e0c1.log; ```. Summary of changes:; - Move initialization of Java HailContext from init to SparkBackend ctor. There is no JVM or Java HailContext when using the service backend.; - Env no longer carries the gateway. (Next: jvm); - make Java HailContext.tmpDir construction lazy. It requires a fs, but HailContext will only carry an fs for the SparkBackend. This will have to get rethought.; - Make Java ServiceBackend extend Backend.; - Construct a HailContext in the query service.; - Implement /references/get in query backend which is needed by hl.init to get the builtin reference genomes on startup.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8434
https://github.com/hail-is/hail/pull/8444:39,Integrability,Depend,DependentEmitFunction,39,WIP. Need to figure out addLiteral for DependentEmitFunction.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8444
https://github.com/hail-is/hail/pull/8445:29,Deployability,deploy,deploy,29,I need to test this with dev deploy and make sure it actually works. But would appreciate feedback on the design before I start doing that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8445
https://github.com/hail-is/hail/pull/8445:10,Testability,test,test,10,I need to test this with dev deploy and make sure it actually works. But would appreciate feedback on the design before I start doing that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8445
https://github.com/hail-is/hail/pull/8445:90,Usability,feedback,feedback,90,I need to test this with dev deploy and make sure it actually works. But would appreciate feedback on the design before I start doing that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8445
https://github.com/hail-is/hail/pull/8448:12,Testability,test,testng-build,12,There is a `testng-build.xml` that is used by build.yaml and tests the google storage fs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448
https://github.com/hail-is/hail/pull/8448:61,Testability,test,tests,61,There is a `testng-build.xml` that is used by build.yaml and tests the google storage fs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448
https://github.com/hail-is/hail/pull/8449:192,Modifiability,refactor,refactorings,192,"Emit was taking two MethodBuilder arguments, one directly and one embedded in an EmitRegion. This PR replaces the EmitRegion argument with a Value[Region]. It also makes a couple other simple refactorings that were personally helpful in understanding the structure of Emit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8449
https://github.com/hail-is/hail/pull/8449:185,Usability,simpl,simple,185,"Emit was taking two MethodBuilder arguments, one directly and one embedded in an EmitRegion. This PR replaces the EmitRegion argument with a Value[Region]. It also makes a couple other simple refactorings that were personally helpful in understanding the structure of Emit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8449
https://github.com/hail-is/hail/pull/8452:98,Usability,guid,guides,98,"I added libraries to the sidebar, added a libraries page, and moved up ""cheat sheets"" and ""how to guides"" about ""Datasets"" and ""Annotation Database"". I figure documentation should be together, as opposed to split up by datasets. Looks like: ; <img width=""1579"" alt=""Screen Shot 2020-04-03 at 2 30 41 PM"" src=""https://user-images.githubusercontent.com/13773586/78395546-aa9d6f80-75bb-11ea-8235-ec18db91c541.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8452
https://github.com/hail-is/hail/pull/8453:45,Deployability,pipeline,pipeline,45,"I still need to figure out where to move the pipeline tests to and whether to rename the BatchBackend to BatchServiceBackend. Making a PR now so I can see if there are any other bugs. Also, I didn't rename where the docs path is in the header navbar yet. This needs to be done when we release a new Hail release after this PR goes in. @johnc1231 @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453
https://github.com/hail-is/hail/pull/8453:285,Deployability,release,release,285,"I still need to figure out where to move the pipeline tests to and whether to rename the BatchBackend to BatchServiceBackend. Making a PR now so I can see if there are any other bugs. Also, I didn't rename where the docs path is in the header navbar yet. This needs to be done when we release a new Hail release after this PR goes in. @johnc1231 @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453
https://github.com/hail-is/hail/pull/8453:304,Deployability,release,release,304,"I still need to figure out where to move the pipeline tests to and whether to rename the BatchBackend to BatchServiceBackend. Making a PR now so I can see if there are any other bugs. Also, I didn't rename where the docs path is in the header navbar yet. This needs to be done when we release a new Hail release after this PR goes in. @johnc1231 @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453
https://github.com/hail-is/hail/pull/8453:54,Testability,test,tests,54,"I still need to figure out where to move the pipeline tests to and whether to rename the BatchBackend to BatchServiceBackend. Making a PR now so I can see if there are any other bugs. Also, I didn't rename where the docs path is in the header navbar yet. This needs to be done when we release a new Hail release after this PR goes in. @johnc1231 @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453
https://github.com/hail-is/hail/pull/8455:0,Energy Efficiency,Reduce,Reduces,0,"Reduces necessary template for a new page to:. ```xslt; <?xml version=""1.0"" encoding=""ISO-8859-15""?>; <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">. <xsl:import href=""template.xslt""/>. <xsl:template name=""page-title"">Foo Bar</xsl:template>; <xsl:template name=""meta-description"">; <meta name=""description"" content=""Hail Foo Bar Baz""/>; </xsl:template>. </xsl:stylesheet>. ```. also moves scripts around to reduce blocking html loading. https://developers.google.com/speed/docs/insights/BlockingJS. cc @mkveerapen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8455
https://github.com/hail-is/hail/pull/8455:440,Energy Efficiency,reduce,reduce,440,"Reduces necessary template for a new page to:. ```xslt; <?xml version=""1.0"" encoding=""ISO-8859-15""?>; <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">. <xsl:import href=""template.xslt""/>. <xsl:template name=""page-title"">Foo Bar</xsl:template>; <xsl:template name=""meta-description"">; <meta name=""description"" content=""Hail Foo Bar Baz""/>; </xsl:template>. </xsl:stylesheet>. ```. also moves scripts around to reduce blocking html loading. https://developers.google.com/speed/docs/insights/BlockingJS. cc @mkveerapen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8455
https://github.com/hail-is/hail/pull/8455:461,Performance,load,loading,461,"Reduces necessary template for a new page to:. ```xslt; <?xml version=""1.0"" encoding=""ISO-8859-15""?>; <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">. <xsl:import href=""template.xslt""/>. <xsl:template name=""page-title"">Foo Bar</xsl:template>; <xsl:template name=""meta-description"">; <meta name=""description"" content=""Hail Foo Bar Baz""/>; </xsl:template>. </xsl:stylesheet>. ```. also moves scripts around to reduce blocking html loading. https://developers.google.com/speed/docs/insights/BlockingJS. cc @mkveerapen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8455
https://github.com/hail-is/hail/pull/8464:18,Deployability,update,updated,18,They had not been updated when the module structure changed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8464
https://github.com/hail-is/hail/pull/8467:480,Integrability,depend,depending,480,"This fixes one mistake (right join does not set fields in the right table to missing), and makes the join table to be more precise. Keys are arrays of columns/fields of the table. They need to be present (they must be of same length and type for a join to be performed). The values are what are considered. The description also doesn't give a clear idea that left/right join is really about returning all rows from left/right table, and then joining the right/left table's fields depending on matches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8467
https://github.com/hail-is/hail/pull/8467:259,Performance,perform,performed,259,"This fixes one mistake (right join does not set fields in the right table to missing), and makes the join table to be more precise. Keys are arrays of columns/fields of the table. They need to be present (they must be of same length and type for a join to be performed). The values are what are considered. The description also doesn't give a clear idea that left/right join is really about returning all rows from left/right table, and then joining the right/left table's fields depending on matches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8467
https://github.com/hail-is/hail/pull/8467:343,Usability,clear,clear,343,"This fixes one mistake (right join does not set fields in the right table to missing), and makes the join table to be more precise. Keys are arrays of columns/fields of the table. They need to be present (they must be of same length and type for a join to be performed). The values are what are considered. The description also doesn't give a clear idea that left/right join is really about returning all rows from left/right table, and then joining the right/left table's fields depending on matches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8467
https://github.com/hail-is/hail/pull/8468:86,Availability,down,down,86,"It is able to execute a trivial pipeline without the JVM on the client. The countdown down to a fully functional Hail Query service begins now. I will start running the Python tests against the service to benchmark our progress. The main blockers are:; - Table lowering @tpoterba @patrick-schultz @catoverdrive ; - The shuffle service @tpoterba @danking ; - Reading, writing and threading the (per-user, for the query service) filesystem through execution. I'll be working on this.; - A Batch backend for distributed execution. I will do this once there is enough functionality to execute something big/interesting. It's happening!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8468
https://github.com/hail-is/hail/pull/8468:32,Deployability,pipeline,pipeline,32,"It is able to execute a trivial pipeline without the JVM on the client. The countdown down to a fully functional Hail Query service begins now. I will start running the Python tests against the service to benchmark our progress. The main blockers are:; - Table lowering @tpoterba @patrick-schultz @catoverdrive ; - The shuffle service @tpoterba @danking ; - Reading, writing and threading the (per-user, for the query service) filesystem through execution. I'll be working on this.; - A Batch backend for distributed execution. I will do this once there is enough functionality to execute something big/interesting. It's happening!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8468
https://github.com/hail-is/hail/pull/8468:176,Testability,test,tests,176,"It is able to execute a trivial pipeline without the JVM on the client. The countdown down to a fully functional Hail Query service begins now. I will start running the Python tests against the service to benchmark our progress. The main blockers are:; - Table lowering @tpoterba @patrick-schultz @catoverdrive ; - The shuffle service @tpoterba @danking ; - Reading, writing and threading the (per-user, for the query service) filesystem through execution. I'll be working on this.; - A Batch backend for distributed execution. I will do this once there is enough functionality to execute something big/interesting. It's happening!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8468
https://github.com/hail-is/hail/pull/8468:205,Testability,benchmark,benchmark,205,"It is able to execute a trivial pipeline without the JVM on the client. The countdown down to a fully functional Hail Query service begins now. I will start running the Python tests against the service to benchmark our progress. The main blockers are:; - Table lowering @tpoterba @patrick-schultz @catoverdrive ; - The shuffle service @tpoterba @danking ; - Reading, writing and threading the (per-user, for the query service) filesystem through execution. I'll be working on this.; - A Batch backend for distributed execution. I will do this once there is enough functionality to execute something big/interesting. It's happening!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8468
https://github.com/hail-is/hail/issues/8469:2605,Availability,recover,recover-dangling-heads,2605,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:2643,Availability,recover,recover-dangling-branches,2643,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:2765,Availability,error,error-correct-kmers,2765,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:2866,Availability,error,error-correction,2866,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:3384,Availability,failure,failure-bam,3384,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:3404,Availability,error,error-correct-reads,3404,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:7859,Availability,down,downsampled,7859," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:12086,Availability,error,error,12086,"ALLED,Number=1,Type=Integer,Description=""1 for variants that were called after phasing via splitting the bam into its component haplotypes and calling variants in haploid mode"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=BX,Number=.,Type=String,Description=""Barcodes and Associated Qual-Scores Supporting Alleles"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""ID of Phase Set for Variant"">; ##FORMAT=<ID=PQ,Number=1,Type=Integer,Description=""Phred QV indicating probability at this variant is incorrectly phased"">; ##FORMAT=<ID=JQ,Number=1,Type=Integer,Description=""Phred QV indicating probability of a phasing switch error in gap prior to this variant"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FILTER=<ID=UNSUPPORTED_GENOTYPE,Description=""If genotype field contains '.' we assume that this is due to making a single sample vcf from a multiple sample vcf in which this sample does not contain the variant."">; ##FILTER=<ID=10X_RESCUED_MOLECULE_HIGH_DIVERSITY,Description=""Set if true: (((RESCUED+NOT_RESCUED) > 0 & RESCUED/(RESCUED+NOT_RESCUED) > 0.1) & (MMD == -1 | MMD >= 3.0))"">; ##FILTER=<ID=10X_QUAL_FILTER,Description=""Set if true: (%QUAL <= 15 || (AF[0] > 0.5 && %QUAL < 50))"">; ##FILTER=<ID=10X_ALLELE_FRACTION_FILTER,Description=""Set if true: (AO[0] < 2 || AO[0]/(AO[0] + RO) < 0.15)"">; ##FILTER=<ID=10X_PHASING_INCONSISTENT,Description=""Uses haplotype information from the fragments and the alleles to filter some variants that are not consistent with phasing."">; ##FILTER=<ID=10X_H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:23292,Availability,ERROR,ERROR,23292,"e information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --disable-tool-default-annotations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-rea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:36022,Availability,error,error,36022,"1_70_65,GAAGTCTCATCCCATC-1_45;AACGTCAGTGTTTCAG-1_74:1:19:255	0/1:2,4:6:60:101,0,60:.:.:.:.; chr1	12198	rs62635282	G	C	69.6	.	BaseQRankSum=0;ClippingRankSum=0;DB;ExcessHet=3.0103;FS=4.771;MQ=42;MQRankSum=-0.967;QD=23.2;ReadPosRankSum=0.967;SOR=2.225;CSQ=C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000450305|transcribed_unprocessed_pseudogene|2/6||||68|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000456328|processed_transcript|1/3||||330|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|downstream_gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinva",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:36742,Availability,avail,available,36742,"gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:46860,Availability,ERROR,ERROR,46860,"s: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'xpos': int64; 'xstart': int64; 'xstop': int64; ----------------------------------------; Entry fields:; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'PL': array<int32>; 'BX': array<str>; 'PS': int32; 'PQ': int32; 'JQ': int32; 'MIN_DP': int32; 'PGT': call; 'PID': str; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 4:===================================================> (480 + 20) / 500]2020-04-05 14:09:48 Hail: INFO: Coerced almost-sorted dataset; [Stage 5:======================================================>(498 + 2) / 500]2020-04-05 14:09:50 Hail: INFO: Coerced almost-sorted dataset; [Stage 7:> (0 + 108) / 500]ERROR: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) failed SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:48786,Availability,Error,Error,48786,"eqr_loading.py"", line 54, in run; self.read_vcf_write_mt(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 84, in read_vcf_write_mt; mt.write(self.output().path, stage_locally=True, overwrite=True); File ""<decorator-gen-1092>"", line 2, in write; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:49038,Availability,error,error,49038,"File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:77); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:51113,Availability,failure,failure,51113,"at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:51172,Availability,failure,failure,51172,"s.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:51422,Availability,Error,ErrorHandling,51422,"ethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:146); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:51448,Availability,Error,ErrorHandling,51448,"voke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:146); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:87675,Availability,Error,ErrorHandling,87675,scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:146); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:87701,Availability,Error,ErrorHandling,87701,t.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:146); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$19.apply(ContextRD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:118368,Availability,Error,Error,118368,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:13,Deployability,pipeline,pipeline,13,"Running this pipeline ; https://github.com/macarthur-lab/hail-elasticsearch-pipelines/blob/master/luigi_pipeline/seqr_loading.py#L39; on a non-Broad VCF with this header and several example variants:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GATKCommandLine=<ID=HaplotypeCaller,CommandLine=""HaplotypeCaller --genotyping-mode DISCOVERY --output /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/CALL_SNPINDELS/fork0/chnk00-u77951d7808/files/default.vcf --intervals /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/CALL_SNPINDELS/fork0/chnk00-u77951d7808/files/default.vcf.bed --input /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_LINKED_READS_ALIGNER/MERGE_POS_BAM/fork0/join-u77951d1e3c/files/pos_sorted_bam.bam --reference /home/fgc4/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --emit-ref-confidence NONE --gvcf-gq-bands 1 --gvcf-gq-bands 2 --gvcf-gq-bands 3 --gvcf-gq-bands 4 --gvcf-gq-bands 5 --gvcf-gq-bands 6 --gvcf-gq-bands 7 --gvcf-gq-bands 8 --gvcf-gq-bands 9 --gvcf-gq-bands 10 --gvcf-gq-bands 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvcf-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --gvcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:76,Deployability,pipeline,pipelines,76,"Running this pipeline ; https://github.com/macarthur-lab/hail-elasticsearch-pipelines/blob/master/luigi_pipeline/seqr_loading.py#L39; on a non-Broad VCF with this header and several example variants:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GATKCommandLine=<ID=HaplotypeCaller,CommandLine=""HaplotypeCaller --genotyping-mode DISCOVERY --output /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/CALL_SNPINDELS/fork0/chnk00-u77951d7808/files/default.vcf --intervals /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/CALL_SNPINDELS/fork0/chnk00-u77951d7808/files/default.vcf.bed --input /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_LINKED_READS_ALIGNER/MERGE_POS_BAM/fork0/join-u77951d1e3c/files/pos_sorted_bam.bam --reference /home/fgc4/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --emit-ref-confidence NONE --gvcf-gq-bands 1 --gvcf-gq-bands 2 --gvcf-gq-bands 3 --gvcf-gq-bands 4 --gvcf-gq-bands 5 --gvcf-gq-bands 6 --gvcf-gq-bands 7 --gvcf-gq-bands 8 --gvcf-gq-bands 9 --gvcf-gq-bands 10 --gvcf-gq-bands 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvcf-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --gvcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:4329,Deployability,update,updates,4329,"false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genotype-filtered-alleles false --contamination-fraction-to-filter 0.0 --output-mode EMIT_VARIANTS_ONLY --all-site-pls false --min-assembly-region-size 50 --max-assembly-region-size 300 --assembly-region-padding 100 --max-reads-per-alignment-start 50 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotations false"",Version=4.0.7.0,Date=""December 21, 2018 6:32:37 PM EST"">; ##source=HaplotypeCaller; ##source=10X/pipelines/stages/snpindels/attach_bcs_snpindels 2.2.2; ##source=10X/pipelines/stages/snpindels/phase_snpindel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:5122,Deployability,pipeline,pipelines,5122,"ity-threshold 0.002 --max-prob-propagation-distance 50 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotations false"",Version=4.0.7.0,Date=""December 21, 2018 6:32:37 PM EST"">; ##source=HaplotypeCaller; ##source=10X/pipelines/stages/snpindels/attach_bcs_snpindels 2.2.2; ##source=10X/pipelines/stages/snpindels/phase_snpindels 2.2.2; ##bcftools_filterVersion=1.1-3-g9058fce+htslib-1.1-1-g03a4427; ##bcftools_filterCommand=filter canonicalized.vcftmp.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_RESCUED_MOLECULE_HIGH_DIVERSITY -e '(((RESCUED+NOT_RESCUED) > 0 & RESCUED/(RESCUED+NOT_RESCUED) > 0.1) & (MMD == -1 | MMD >= 3.0)) ' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951dd300/files/default.vcf.gztmp2.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_QUAL_FILTER -e '(%QUAL <= 15 || (AF[0] > 0.5 && %QUAL < 50))' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951dd300/files/default.vcf.gztmp2.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_ALLELE_FRACTION_FILTER -e '(AO[0] < 2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:5190,Deployability,pipeline,pipelines,5190," 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotations false"",Version=4.0.7.0,Date=""December 21, 2018 6:32:37 PM EST"">; ##source=HaplotypeCaller; ##source=10X/pipelines/stages/snpindels/attach_bcs_snpindels 2.2.2; ##source=10X/pipelines/stages/snpindels/phase_snpindels 2.2.2; ##bcftools_filterVersion=1.1-3-g9058fce+htslib-1.1-1-g03a4427; ##bcftools_filterCommand=filter canonicalized.vcftmp.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_RESCUED_MOLECULE_HIGH_DIVERSITY -e '(((RESCUED+NOT_RESCUED) > 0 & RESCUED/(RESCUED+NOT_RESCUED) > 0.1) & (MMD == -1 | MMD >= 3.0)) ' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951dd300/files/default.vcf.gztmp2.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_QUAL_FILTER -e '(%QUAL <= 15 || (AF[0] > 0.5 && %QUAL < 50))' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951dd300/files/default.vcf.gztmp2.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_ALLELE_FRACTION_FILTER -e '(AO[0] < 2 || AO[0]/(AO[0] + RO) < 0.15)' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:23806,Deployability,update,updates,23806,"ditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --disable-tool-default-annotations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false"",Version=4.0.1.2,Date=""March 22, 2018 1:12:03 AM EDT"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:81524,Energy Efficiency,schedul,scheduler,81524,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:81596,Energy Efficiency,schedul,scheduler,81596,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82121,Energy Efficiency,schedul,scheduler,82121,stractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82161,Energy Efficiency,schedul,scheduler,82161,.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82260,Energy Efficiency,schedul,scheduler,82260,he.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82358,Energy Efficiency,schedul,scheduler,82358,ntext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82612,Energy Efficiency,schedul,scheduler,82612,sk.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82693,Energy Efficiency,schedul,scheduler,82693,y(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82799,Energy Efficiency,schedul,scheduler,82799,he.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82949,Energy Efficiency,schedul,scheduler,82949,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:83038,Energy Efficiency,schedul,scheduler,83038,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:83136,Energy Efficiency,schedul,scheduler,83136,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:989); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:83232,Energy Efficiency,schedul,scheduler,83232,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:989); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:936); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:83397,Energy Efficiency,schedul,scheduler,83397,.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:989); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:936); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:117777,Energy Efficiency,schedul,scheduler,117777,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:117849,Energy Efficiency,schedul,scheduler,117849,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:36028,Integrability,message,message,36028,"1_70_65,GAAGTCTCATCCCATC-1_45;AACGTCAGTGTTTCAG-1_74:1:19:255	0/1:2,4:6:60:101,0,60:.:.:.:.; chr1	12198	rs62635282	G	C	69.6	.	BaseQRankSum=0;ClippingRankSum=0;DB;ExcessHet=3.0103;FS=4.771;MQ=42;MQRankSum=-0.967;QD=23.2;ReadPosRankSum=0.967;SOR=2.225;CSQ=C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000450305|transcribed_unprocessed_pseudogene|2/6||||68|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000456328|processed_transcript|1/3||||330|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|downstream_gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinva",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
