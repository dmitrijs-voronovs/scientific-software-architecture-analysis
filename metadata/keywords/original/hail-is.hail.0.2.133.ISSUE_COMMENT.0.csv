id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/2#issuecomment-152268753:162,Testability,test,tests,162,"_From @jbloom22 on August 26, 2015 21:12_. I've fixed the variant type Boolean methods, added a variantType method which returns the enumerated type, and written tests for all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2#issuecomment-152268753
https://github.com/hail-is/hail/issues/12#issuecomment-152269033:75,Testability,test,testing,75,"_From @alexb-3 on August 26, 2015 16:10_. This represents implementing and testing the chi squared version. See also https://github.com/cseed/k3/issues/21",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12#issuecomment-152269033
https://github.com/hail-is/hail/issues/16#issuecomment-156226289:152,Security,access,access,152,"First cut: https://github.com/broadinstitute/hail/commit/f5e93963844656449259ad893ec3ce7ddcef2f3c. Still needed: testing, implicit option manipulation, access to INFO field and QC results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/16#issuecomment-156226289
https://github.com/hail-is/hail/issues/16#issuecomment-156226289:113,Testability,test,testing,113,"First cut: https://github.com/broadinstitute/hail/commit/f5e93963844656449259ad893ec3ce7ddcef2f3c. Still needed: testing, implicit option manipulation, access to INFO field and QC results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/16#issuecomment-156226289
https://github.com/hail-is/hail/issues/16#issuecomment-158250176:0,Deployability,Update,Update,0,Update: Tested: https://github.com/broadinstitute/hail/commit/736b61a3b4f576963dc78c913a6596adeb7cc65e. Jon is working on implicit option manipulation. Tim is working on INFO support and annotation infrastructure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/16#issuecomment-158250176
https://github.com/hail-is/hail/issues/16#issuecomment-158250176:8,Testability,Test,Tested,8,Update: Tested: https://github.com/broadinstitute/hail/commit/736b61a3b4f576963dc78c913a6596adeb7cc65e. Jon is working on implicit option manipulation. Tim is working on INFO support and annotation infrastructure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/16#issuecomment-158250176
https://github.com/hail-is/hail/issues/19#issuecomment-152269229:145,Deployability,upgrade,upgrade,145,"This should include performance experiments we use to drive design decisions (e.g., Array vs. Vector) so we know what assumptions change when we upgrade Scala/Spark/JVM or underlying abstractions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/19#issuecomment-152269229
https://github.com/hail-is/hail/issues/19#issuecomment-152269229:20,Performance,perform,performance,20,"This should include performance experiments we use to drive design decisions (e.g., Array vs. Vector) so we know what assumptions change when we upgrade Scala/Spark/JVM or underlying abstractions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/19#issuecomment-152269229
https://github.com/hail-is/hail/issues/23#issuecomment-152270045:84,Testability,test,testing,84,"_From @jbloom22 on August 27, 2015 1:13_. Looks great! One small change to code and testing, then go ahead and merge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/23#issuecomment-152270045
https://github.com/hail-is/hail/issues/25#issuecomment-179404081:19,Modifiability,rewrite,rewrite,19,Done in scalacheck rewrite.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/25#issuecomment-179404081
https://github.com/hail-is/hail/issues/28#issuecomment-279514823:18,Deployability,integrat,integration,18,"We now have latex integration for formulas, used pervasively in docs along with references to literature. Not sure if this issue calls for more?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/28#issuecomment-279514823
https://github.com/hail-is/hail/issues/28#issuecomment-279514823:18,Integrability,integrat,integration,18,"We now have latex integration for formulas, used pervasively in docs along with references to literature. Not sure if this issue calls for more?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/28#issuecomment-279514823
https://github.com/hail-is/hail/issues/32#issuecomment-279513351:36,Testability,test,tests,36,Balding-Nichols is used in `lmmreg` tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/32#issuecomment-279513351
https://github.com/hail-is/hail/issues/35#issuecomment-208692497:86,Deployability,integrat,integrate,86,"Nobody has ever asked for this. I'm tabling it for now. If it does come up, we should integrate with Hadoop-BAM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/35#issuecomment-208692497
https://github.com/hail-is/hail/issues/35#issuecomment-208692497:86,Integrability,integrat,integrate,86,"Nobody has ever asked for this. I'm tabling it for now. If it does come up, we should integrate with Hadoop-BAM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/35#issuecomment-208692497
https://github.com/hail-is/hail/issues/39#issuecomment-235158444:10,Usability,simpl,simpleAssert,10,We added `simpleAssert`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/39#issuecomment-235158444
https://github.com/hail-is/hail/issues/41#issuecomment-152272292:126,Integrability,message,messages,126,Partially done: https://github.com/cseed/k3/commit/3cfd6b226ec1bd885faa1fae96965435dd78130b. There is still a small number of messages I haven't figured out how to disable.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/41#issuecomment-152272292
https://github.com/hail-is/hail/issues/46#issuecomment-316222854:39,Testability,test,testing,39,I think we're generally happy with TSV testing now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/46#issuecomment-316222854
https://github.com/hail-is/hail/issues/47#issuecomment-152273044:136,Deployability,install,installation,136,"_From @alexb-3 on September 30, 2015 22:4_. pro-tip: To add references, first open `bibfile.bib` in BibDesk (comes with standard MacTeX installation). Then find your paper in Google Scholar, click the `cite` button, select `BibTeX` and paste into BibDesk. You may need to reformat slightly; I remove irrelevant fields, and proper names in the title requiring capital letters should go in braces, e.g. {Hardy}--{W}einberg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/47#issuecomment-152273044
https://github.com/hail-is/hail/issues/49#issuecomment-422364141:11,Integrability,interface,interface,11,this whole interface is gone,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/49#issuecomment-422364141
https://github.com/hail-is/hail/issues/50#issuecomment-152273680:60,Testability,test,test,60,"_From @jbloom22 on October 14, 2015 1:26_. core statistical test, though i reworked the formulas:; https://en.wikipedia.org/wiki/Simple_linear_regression#Normality_assumption",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273680
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:348,Deployability,install,install,348,"_From @jbloom22 on October 28, 2015 21:2_. Hail runtime for linreg with 10 PCs on profile.vds: 56s, 59s, 58s.; Hail runtime for variantqc on profile.vds: 35s, 34s, 35s.; Plink runtime for linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:546,Deployability,install,install,546,"_From @jbloom22 on October 28, 2015 21:2_. Hail runtime for linreg with 10 PCs on profile.vds: 56s, 59s, 58s.; Hail runtime for variantqc on profile.vds: 35s, 34s, 35s.; Plink runtime for linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1360,Performance,load,loaded,1360,"or linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/plinkTest.assoc.linear ... done. real 0m13.167s; user 0m13.071s; sys 0m0.080s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1433,Performance,load,loaded,1433,"or linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/plinkTest.assoc.linear ... done. real 0m13.167s; user 0m13.071s; sys 0m0.080s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1804,Performance,load,loaded,1804,"or linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/plinkTest.assoc.linear ... done. real 0m13.167s; user 0m13.071s; sys 0m0.080s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1296,Safety,detect,detected,1296,"or linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/plinkTest.assoc.linear ... done. real 0m13.167s; user 0m13.071s; sys 0m0.080s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1055,Testability,Log,Logging,1055,"8, 2015 21:2_. Hail runtime for linreg with 10 PCs on profile.vds: 56s, 59s, 58s.; Hail runtime for variantqc on profile.vds: 35s, 34s, 35s.; Plink runtime for linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1092,Testability,log,log,1092," 56s, 59s, 58s.; Hail runtime for variantqc on profile.vds: 35s, 34s, 35s.; Plink runtime for linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684
https://github.com/hail-is/hail/issues/55#issuecomment-316223325:49,Integrability,interface,interface,49,I think this is subsumed by the generic genotype interface + ongoing unsafe work.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/55#issuecomment-316223325
https://github.com/hail-is/hail/issues/55#issuecomment-316223325:69,Safety,unsafe,unsafe,69,I think this is subsumed by the generic genotype interface + ongoing unsafe work.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/55#issuecomment-316223325
https://github.com/hail-is/hail/pull/68#issuecomment-154742941:6,Testability,test,test-passing,6,"First test-passing version with Array[Array[GenotypeType]] drops time to 1m37s, not as much as expected. I'll look more closely to see if there are obvious inefficiencies, and may try unboxing GenotypeType to Int. If you look, let me know if you see something obvious. Output looks good. But there are only 6 complete trios in profile225, so after filtering out samples not in complete trios in .filterSamples(isTrioSample) , there is very little data to process. Here is the contents of profile225.fmendel:. ```; FID PAT MAT CHLD N; VN049 HG02026 HG02025 1 2009; SH074 HG00656 HG00657 1 5669; m009 NA19679 NA19678 1 3953; m008 NA19661 NA19660 1 5240; Y117 NA19239 NA19238 1 6499; PR05 HG00731 HG00732 1 1506; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-154742941
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:382,Availability,error,error,382,"I've done some extensive remodeling of Pedigree and MendelErrors, shorter and conceptually cleaner now, got to delete a bunch of code. But I'm having a serialization issue, which may be related to changing MendelError to include the CompleteTrio rather than the sample. For example, if I replace ""implicatedSample"" by pasting the body in the closure instead, then the serialization error at that point goes away. but there are a bunch of other ones from the toLine below. ```; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:945,Testability,test,test,945,"I've done some extensive remodeling of Pedigree and MendelErrors, shorter and conceptually cleaner now, got to delete a bunch of code. But I'm having a serialization issue, which may be related to changing MendelError to include the CompleteTrio rather than the sample. For example, if I replace ""implicatedSample"" by pasting the body in the closure instead, then the serialization error at that point goes away. but there are a bunch of other ones from the toLine below. ```; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1275,Testability,test,testng,1275,"ple, if I replace ""implicatedSample"" by pasting the body in the closure instead, then the serialization error at that point goes away. but there are a bunch of other ones from the toLine below. ```; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.ru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1367,Testability,test,testng,1367,rialization error at that point goes away. but there are a bunch of other ones from the toLine below. ```; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1430,Testability,test,testng,1430,h of other ones from the toLine below. ```; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.R,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1497,Testability,test,testng,1497,xception: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.refle,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1566,Testability,test,testng,1566,ner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.Na,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1582,Testability,Test,TestMethodWorker,1582,le(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1617,Testability,Test,TestMethodWorker,1617,6); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAcces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1652,Testability,test,testng,1652,aner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.Delega,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1668,Testability,Test,TestMethodWorker,1668,eaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1689,Testability,Test,TestMethodWorker,1689,t org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegating,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1724,Testability,test,testng,1724,clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1731,Testability,Test,TestRunner,1731,rkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1753,Testability,Test,TestRunner,1753,622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1782,Testability,test,testng,1782,D.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1789,Testability,Test,TestRunner,1789,.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.r,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1804,Testability,Test,TestRunner,1804,at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1833,Testability,test,testng,1833,thods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1890,Testability,test,testng,1890,t org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTarg,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1955,Testability,test,testng,1955,rorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Nati,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2015,Testability,test,testng,2015,.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(N,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2068,Testability,test,testng,2068,ccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.Dele,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2137,Testability,test,testng,2137,DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2201,Testability,test,testng,2201,.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2208,Testability,Test,TestNG,2208,3); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2237,Testability,Test,TestNG,2237,eflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.Ser,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2263,Testability,test,testng,2263,; at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassM,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2270,Testability,Test,TestNG,2270,g.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2294,Testability,Test,TestNG,2294,l.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldVal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2320,Testability,test,testng,2320,hod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldValues$extension(SerializationDebugger,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2327,Testability,Test,TestNG,2327,hodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldValues$extension(SerializationDebugger.scala:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2338,Testability,Test,TestNG,2338,tionHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldValues$extension(SerializationDebugger.scala:240); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2364,Testability,test,testng,2364,testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldValues$extension(SerializationDebugger.scala:240); at org.apache.spark.serializer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2426,Testability,test,testng,2426,.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldValues$extension(SerializationDebugger.scala:240); at org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializable(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880
https://github.com/hail-is/hail/pull/68#issuecomment-156268875:119,Testability,test,test,119,"The reason you didn't see those others fixed before is that I had mistakenly not committed all changes (that's why the test failed too). I am using Array.groupBy, repushed, back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-156268875
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:9,Availability,Down,Downloads,9,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:187,Availability,Down,Downloads,187,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:47,Testability,test,test,47,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:171,Testability,test,test,171,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:224,Testability,test,test,224,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:264,Testability,test,test,264,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:310,Testability,test,testplink,310,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-155571444:408,Testability,test,test,408,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444
https://github.com/hail-is/hail/pull/72#issuecomment-158671102:259,Testability,test,tests,259,"Tim,. Overall, the code looks great! I think I read through all the code except the PLINK and BGEN parsers themselves. I'm going to stop here and let you address the current comments before I make another pass. Couple of things:; - You need to rebase and the tests are failing. If there are big files involved, let's talk about that.; - You should write `importbgen` and `importplink` commands (and rename `import` `importvcf`). The reason for the different names is that I think each one will have options specific to the format (e.g. choice of VCF parser).; - A lot of comments I made at one point apply to lots of placing in the code (e.g. `: Unit =`, braces around a single statement, don't use Java file IO). When you make a fix (bug fix, style improvement, whatever), it is a good mental habit to think about other places in the code that might have the same problem. Applied consistently, this will lead to massive improvements in code quality over time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-158671102
https://github.com/hail-is/hail/pull/73#issuecomment-156199705:37,Deployability,install,install,37,Timing:. Command: `time ~/hail/build/install/hail/bin/hail read -i ~/profile225.vds variantqc -o ~/variantqc.tsv`. With HWE:. timing: ; read: 1.449s; variantqc: 2m15.3s. real 2m19.420s; user 2m54.741s; sys 0m5.010s. Without HWE:. timing: ; read: 1.775s; variantqc: 2m11.8s. real 2m16.133s; user 2m51.038s; sys 0m5.092s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/73#issuecomment-156199705
https://github.com/hail-is/hail/pull/76#issuecomment-156434811:0,Testability,Test,Tests,0,Tests are failing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/76#issuecomment-156434811
https://github.com/hail-is/hail/issues/78#issuecomment-316223517:52,Availability,error,error,52,I'm not aware of an existing problem. We produce an error if sample IDs are not unique and user can rename as needed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/78#issuecomment-316223517
https://github.com/hail-is/hail/pull/79#issuecomment-161677715:192,Availability,error,error,192,"Cotton -- I fixed the changes you suggested and it should be ready to be merged. For the multiarray of size 0, I tested that you can create the object, but using the apply for (0,0) throws an error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/79#issuecomment-161677715
https://github.com/hail-is/hail/pull/79#issuecomment-161677715:113,Testability,test,tested,113,"Cotton -- I fixed the changes you suggested and it should be ready to be merged. For the multiarray of size 0, I tested that you can create the object, but using the apply for (0,0) throws an error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/79#issuecomment-161677715
https://github.com/hail-is/hail/pull/80#issuecomment-161827387:65,Testability,test,tests,65,Addressed all comments except for the addition of property-based tests. Will loop back around when adding those to all my test suites soon if that's OK.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/80#issuecomment-161827387
https://github.com/hail-is/hail/pull/80#issuecomment-161827387:122,Testability,test,test,122,Addressed all comments except for the addition of property-based tests. Will loop back around when adding those to all my test suites soon if that's OK.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/80#issuecomment-161827387
https://github.com/hail-is/hail/pull/82#issuecomment-159316557:35,Performance,Load,LoadVCF,35,"Keep deleting! You can get rid of `LoadVCF` `readerBuilder` argument and, in fact, all the `ReaderBuilder`s and `AbstractRecordReader`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/82#issuecomment-159316557
https://github.com/hail-is/hail/pull/83#issuecomment-160690390:52,Modifiability,extend,extend,52,"Fixed. Two outstanding issues:; 1. `RichVDS` cannot extend `AnyVal` due to implementation restrictions on value extensions nesting with other implicit conversions. Right now it is a standalone class.; 2. What do with `VSMSuite`? This seemed like it was designed to make sure all the implementations were synced, but that is no longer necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160690390
https://github.com/hail-is/hail/pull/83#issuecomment-160692682:265,Deployability,upgrade,upgraded,265,"- This problem was fixed in Scala 2.11. I ran into elsewhere, like RichVector in Utils. I left this comment:. // FIXME AnyVal in Scala 2.11. We're using Scala 2.10 because the Spark/Intel cluster is running an old version of, well, everything. It should be getting upgraded tomorrow. Then hopefully we can switch to 2.11 permanently. For now, I'd just put a similar comment.; - I'd remove testSingletonVariants. testFilterSamples is still good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160692682
https://github.com/hail-is/hail/pull/83#issuecomment-160692682:389,Testability,test,testSingletonVariants,389,"- This problem was fixed in Scala 2.11. I ran into elsewhere, like RichVector in Utils. I left this comment:. // FIXME AnyVal in Scala 2.11. We're using Scala 2.10 because the Spark/Intel cluster is running an old version of, well, everything. It should be getting upgraded tomorrow. Then hopefully we can switch to 2.11 permanently. For now, I'd just put a similar comment.; - I'd remove testSingletonVariants. testFilterSamples is still good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160692682
https://github.com/hail-is/hail/pull/83#issuecomment-160692682:412,Testability,test,testFilterSamples,412,"- This problem was fixed in Scala 2.11. I ran into elsewhere, like RichVector in Utils. I left this comment:. // FIXME AnyVal in Scala 2.11. We're using Scala 2.10 because the Spark/Intel cluster is running an old version of, well, everything. It should be getting upgraded tomorrow. Then hopefully we can switch to 2.11 permanently. For now, I'd just put a similar comment.; - I'd remove testSingletonVariants. testFilterSamples is still good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160692682
https://github.com/hail-is/hail/issues/90#issuecomment-256329242:44,Testability,test,test,44,"```; wm9f1-8cf:hail tpoterba$ hail read src/test/resources/sample2.vcf; hail: info: running: read src/test/resources/sample2.vcf; hail: fatal: read: input path ending in `.vds' required, found `file:/Users/tpoterba/hail/src/test/resources/sample2.vcf'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/90#issuecomment-256329242
https://github.com/hail-is/hail/issues/90#issuecomment-256329242:102,Testability,test,test,102,"```; wm9f1-8cf:hail tpoterba$ hail read src/test/resources/sample2.vcf; hail: info: running: read src/test/resources/sample2.vcf; hail: fatal: read: input path ending in `.vds' required, found `file:/Users/tpoterba/hail/src/test/resources/sample2.vcf'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/90#issuecomment-256329242
https://github.com/hail-is/hail/issues/90#issuecomment-256329242:224,Testability,test,test,224,"```; wm9f1-8cf:hail tpoterba$ hail read src/test/resources/sample2.vcf; hail: info: running: read src/test/resources/sample2.vcf; hail: fatal: read: input path ending in `.vds' required, found `file:/Users/tpoterba/hail/src/test/resources/sample2.vcf'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/90#issuecomment-256329242
https://github.com/hail-is/hail/issues/91#issuecomment-235397652:122,Availability,error,errors,122,"Yes, PR #137 treats the hemizygous Y case as well. As with Plink, I do not consider hets at homozygous sites to be Mendel errors, but rather sequencing errors. Users should deal with them separately as they see fit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/91#issuecomment-235397652
https://github.com/hail-is/hail/issues/91#issuecomment-235397652:152,Availability,error,errors,152,"Yes, PR #137 treats the hemizygous Y case as well. As with Plink, I do not consider hets at homozygous sites to be Mendel errors, but rather sequencing errors. Users should deal with them separately as they see fit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/91#issuecomment-235397652
https://github.com/hail-is/hail/pull/93#issuecomment-163000784:39,Deployability,install,install,39,Timing on sampleqc:. Command: `./build/install/hail/bin/hail read -i profile225.vds sampleqc -o sampleqc.tsv`. cs_fastsqc:; read: 1.410s; sampleqc: 1m49.8s. master:; read: 1.449s; sampleqc: 2m48.9s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/93#issuecomment-163000784
https://github.com/hail-is/hail/issues/104#issuecomment-279858875:138,Deployability,integrat,integrated,138,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875
https://github.com/hail-is/hail/issues/104#issuecomment-279858875:138,Integrability,integrat,integrated,138,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875
https://github.com/hail-is/hail/issues/104#issuecomment-279858875:71,Usability,clear,clearly,71,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875
https://github.com/hail-is/hail/issues/106#issuecomment-240001202:46,Deployability,install,installed,46,We've switched to Jenkins. Required tools are installed there.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/106#issuecomment-240001202
https://github.com/hail-is/hail/pull/109#issuecomment-169135201:119,Availability,error,errors,119,"Just a few comments. Looking good! Looking forward to tests. Also, need to abstract out the code you share with Mendel errors as we discussed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/109#issuecomment-169135201
https://github.com/hail-is/hail/pull/109#issuecomment-169135201:54,Testability,test,tests,54,"Just a few comments. Looking good! Looking forward to tests. Also, need to abstract out the code you share with Mendel errors as we discussed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/109#issuecomment-169135201
https://github.com/hail-is/hail/issues/112#issuecomment-169052284:48,Usability,simpl,simply,48,"Added in #115. We did not add AN, since this is simply `2 * nCalled`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/112#issuecomment-169052284
https://github.com/hail-is/hail/issues/118#issuecomment-208690564:0,Availability,Down,Down,0,Down with Travis. Up with Jenkins!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/118#issuecomment-208690564
https://github.com/hail-is/hail/pull/119#issuecomment-169488520:125,Performance,queue,queue,125,"I'm happy with this, ready for review. I haven't yet added the `-o ""-""` option yet to write to stdout, but I'll put it in my queue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/119#issuecomment-169488520
https://github.com/hail-is/hail/issues/130#issuecomment-172673947:102,Testability,test,tests,102,Done: https://github.com/broadinstitute/hail/commit/5a3981eec849e5b2046d62fb867cfc18eb22952c. Can run tests with gradle -Dhail.master=<master> or with spark-submit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/130#issuecomment-172673947
https://github.com/hail-is/hail/issues/131#issuecomment-235240333:82,Performance,optimiz,optimizer,82,"We do the parsing. We don't do file checking, but this will come later in a query optimizer",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/131#issuecomment-235240333
https://github.com/hail-is/hail/issues/133#issuecomment-249706530:96,Modifiability,extend,extend,96,"We now have logistic Wald, LRT, and Score via #585 . I'll be adding Firth soon. We also need to extend these to subset to non-missing genotypes rather than mean impute.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/133#issuecomment-249706530
https://github.com/hail-is/hail/issues/133#issuecomment-249706530:12,Testability,log,logistic,12,"We now have logistic Wald, LRT, and Score via #585 . I'll be adding Firth soon. We also need to extend these to subset to non-missing genotypes rather than mean impute.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/133#issuecomment-249706530
https://github.com/hail-is/hail/issues/133#issuecomment-279516849:100,Testability,log,logistic,100,"And we have linear mixed regression. So I consider this closed as further single-variant linear and logistic methods will be by demand, not by EPACTS support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/133#issuecomment-279516849
https://github.com/hail-is/hail/pull/134#issuecomment-226613696:76,Integrability,interface,interface,76,This will need to be revisited in the context of the DataFrame-style Matrix interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/134#issuecomment-226613696
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:217,Testability,test,testClasses,217,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:242,Testability,test,test,242,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:552,Testability,test,test,552,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:612,Testability,test,test,612,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:647,Testability,test,test,647,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:706,Testability,test,test,706,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:741,Testability,test,test,741,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:790,Testability,test,testCall,790,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:829,Testability,test,test,829,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:885,Testability,test,test,885,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:920,Testability,test,test,920,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:971,Testability,test,test,971,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1006,Testability,test,test,1006,```; $ gradle check; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1060,Testability,test,testSameAsOrig,1060,s UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSui,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1105,Testability,test,test,1105,s UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSui,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1159,Testability,test,testSorted,1159,ocessTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldan,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1200,Testability,test,test,1200,ocessTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test; objc[30146]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldan,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1290,Testability,test,test,1290,hHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1382,Testability,test,test,1382,ome/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.Leve,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1483,Testability,test,test,1483,rument.dylib. One of the two will be used. Which one is undefined. Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneH,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1537,Testability,test,test,1537,rg.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1572,Testability,test,test,1572,rg.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1631,Testability,test,testGenotypeStream,1631,adinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1680,Testability,test,test,1680,adinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1733,Testability,test,testGenotype,1733,titute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Sui,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1776,Testability,test,test,1776,titute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Sui,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1824,Testability,test,test,1824,stitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1859,Testability,test,test,1859,stitute.hail.methods.ExportPlinkSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1916,Testability,test,test,1916,titute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:1951,Testability,test,test,1951,titute.hail.methods.ExportSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2000,Testability,test,test,2000,nstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2035,Testability,test,test,2035,nstitute.hail.methods.ExportVcfSuite.testSameAsOrig PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2136,Testability,test,test,2136,adinstitute.hail.methods.ExportVcfSuite.testSorted PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlus,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2231,Testability,test,test,2231,nstitute.hail.methods.FilterSuite.evalTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2326,Testability,test,test,2326,ute.hail.methods.FilterSuite.filterTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2420,Testability,test,test,2420,e.hail.methods.FilterSuite.treeTransformerTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2519,Testability,test,test,2519,itute.hail.methods.GQByDPBinSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2580,Testability,test,test,2580,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2615,Testability,test,test,2615,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2672,Testability,test,test,2672,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2707,Testability,test,test,2707,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2763,Testability,test,test,2763,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2798,Testability,test,test,2798,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2851,Testability,test,test,2851,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2886,Testability,test,test,2886,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:2972,Testability,test,test,2972,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3063,Testability,test,test,3063,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3111,Testability,test,testFlushDouble,3111,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3157,Testability,test,test,3157,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3209,Testability,test,testFilterSamples,3209,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3257,Testability,test,test,3257,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3309,Testability,test,testSame,3309,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3348,Testability,test,test,3348,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3400,Testability,test,testVariant,3400,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3442,Testability,test,test,3442,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-171361971:3496,Testability,test,test,3496,e test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL. Total time: 3 mins 10.058 secs; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-171361971
https://github.com/hail-is/hail/pull/136#issuecomment-173715371:119,Testability,TEST,TESTING,119,"TIMING:. ```; timing:; read: 5.549s; exportvcf: 1m39.7s; ```. ```; timing:; read: 5.304s; exportplink: 49.167s; ```. **TESTING**; for chr22 on the Dataflow cluster, ran import vcf / export vcf and import vcf / export plink, then plink --vcf to read the VCF written by hail (plink does not process multiallelic files with the desired behavior). I then ran concordance on these two bed/bim/fam file sets, and got 100% concordance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-173715371
https://github.com/hail-is/hail/pull/136#issuecomment-184022844:191,Performance,perform,performance,191,Exporting profile225 on my laptop:; exportvcf (.bgz) -- 4m45s; exportplink (before changes) -- 34s; exportplink (after changes) -- 46s. Weird behavior going on there... I'd have thought that performance would go up.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-184022844
https://github.com/hail-is/hail/pull/136#issuecomment-184148300:63,Performance,optimiz,optimizing,63,"More common case is to first do some QC, that's the case I was optimizing for. What about:. `import ... filtergenotypes --remove -c 'g.gq < 20' variantqc filtervariants --remove -c 'va.qc.callRate < 0.2' exportplink ...`. Still, I thought it might be better, too. Must be the double persist.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-184148300
https://github.com/hail-is/hail/pull/136#issuecomment-184786953:46,Testability,test,tests,46,"Back to you, merged to master and passing all tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/136#issuecomment-184786953
https://github.com/hail-is/hail/issues/145#issuecomment-319495949:0,Testability,Test,Tested,0,Tested in `Genotype.testGenotype`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/145#issuecomment-319495949
https://github.com/hail-is/hail/issues/145#issuecomment-319495949:20,Testability,test,testGenotype,20,Tested in `Genotype.testGenotype`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/145#issuecomment-319495949
https://github.com/hail-is/hail/issues/147#issuecomment-172043157:20,Availability,error,error,20,"actually, given the error output, it looks like `apply` isn't converting to `fApply` at all -- we wouldn't see this error if it were converting correctly",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/147#issuecomment-172043157
https://github.com/hail-is/hail/issues/147#issuecomment-172043157:116,Availability,error,error,116,"actually, given the error output, it looks like `apply` isn't converting to `fApply` at all -- we wouldn't see this error if it were converting correctly",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/147#issuecomment-172043157
https://github.com/hail-is/hail/issues/148#issuecomment-240001350:7,Availability,error,errors,7,"Mendel errors should stay a separate pass for various reasons. Perhaps this can be revisited when we start working on a higher-level, plink-like tool.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/148#issuecomment-240001350
https://github.com/hail-is/hail/pull/153#issuecomment-176331224:15,Modifiability,variab,variable,15,"Just renamed a variable, now really back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/153#issuecomment-176331224
https://github.com/hail-is/hail/pull/155#issuecomment-182922376:130,Performance,load,load,130,"The `CovarianceData` reordering stuff looks right, but makes me a little nervous. Can you add some tests to verify the data? Just load a cov file and verify a few entries by hand. Also, verify the entries are sorted after loading. Otherwise, looks good!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/155#issuecomment-182922376
https://github.com/hail-is/hail/pull/155#issuecomment-182922376:222,Performance,load,loading,222,"The `CovarianceData` reordering stuff looks right, but makes me a little nervous. Can you add some tests to verify the data? Just load a cov file and verify a few entries by hand. Also, verify the entries are sorted after loading. Otherwise, looks good!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/155#issuecomment-182922376
https://github.com/hail-is/hail/pull/155#issuecomment-182922376:99,Testability,test,tests,99,"The `CovarianceData` reordering stuff looks right, but makes me a little nervous. Can you add some tests to verify the data? Just load a cov file and verify a few entries by hand. Also, verify the entries are sorted after loading. Otherwise, looks good!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/155#issuecomment-182922376
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:27,Testability,test,test,27,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:87,Testability,test,test,87,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:122,Testability,test,test,122,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:181,Testability,test,test,181,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:216,Testability,test,test,216,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:265,Testability,test,testCall,265,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:304,Testability,test,test,304,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:355,Testability,test,test,355,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:390,Testability,test,test,390,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:444,Testability,test,testSameAsOrigBGzip,444,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:494,Testability,test,test,494,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:548,Testability,test,testSameAsOrigNoCompression,548,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:606,Testability,test,test,606,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:660,Testability,test,testSorted,660,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:701,Testability,test,test,701,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:791,Testability,test,test,791,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:883,Testability,test,test,883,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:984,Testability,test,test,984,```; Gradle suite > Gradle test > org.broadinstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gra,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1038,Testability,test,test,1038,nstitute.hail.annotations.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1073,Testability,test,test,1073,ns.AnnotationsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1132,Testability,test,testGenotypeStream,1132,te.hail.io.compress.BGzipCodecSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1181,Testability,test,test,1181,Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1234,Testability,test,testGenotype,1234,il.variant.CallSuite.testCall PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.metho,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1277,Testability,test,test,1277,ll PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1325,Testability,test,test,1325,ail.methods.ExportSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.m,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1360,Testability,test,test,1360,.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1417,Testability,test,test,1417,thods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.metho,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1452,Testability,test,test,1452,stSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testSt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1501,Testability,test,test,1501,oadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broad,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1536,Testability,test,test,1536,ds.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.Spli,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1637,Testability,test,test,1637,.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1732,Testability,test,test,1732,.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1827,Testability,test,test,1827,.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFil,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1921,Testability,test,test,1921,ethods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2020,Testability,test,test,2020,.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVaria,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2081,Testability,test,test,2081,adinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.g,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2116,Testability,test,test,2116,t.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2173,Testability,test,test,2173,"e test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; -",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2208,Testability,test,test,2208,"tute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2264,Testability,test,test,2264,"Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2299,Testability,test,test,2299,"institute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2352,Testability,test,test,2352,"e test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2387,Testability,test,test,2387,"tute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, bes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2440,Testability,test,testStoreAfterFilter,2440,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2491,Testability,test,test,2491,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2577,Testability,test,test,2577,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2668,Testability,test,test,2668,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2716,Testability,test,testFlushDouble,2716,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2762,Testability,test,test,2762,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2814,Testability,test,testFilterSamples,2814,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2862,Testability,test,test,2862,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2914,Testability,test,testSame,2914,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2953,Testability,test,test,2953,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:3005,Testability,test,testVariant,3005,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:3047,Testability,test,test,3047,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/pull/158#issuecomment-173700450:3101,Testability,test,test,3101,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450
https://github.com/hail-is/hail/issues/174#issuecomment-279517149:12,Deployability,update,update,12,@andgan any update on this issue? should it remain open?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/174#issuecomment-279517149
https://github.com/hail-is/hail/issues/174#issuecomment-279574525:195,Deployability,update,update,195,"Hi, I think this is outdated given the new annotation database.; cheers,. > On Feb 13, 2017, at 3:47 PM, jbloom22 <notifications@github.com> wrote:; > ; > @andgan <https://github.com/andgan> any update on this issue? should it remain open?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/hail-is/hail/issues/174#issuecomment-279517149>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADIkAnlW6KE-f6entdPPA6wzrnTBTrz6ks5rcMF1gaJpZM4HLmN9>.; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/174#issuecomment-279574525
https://github.com/hail-is/hail/issues/200#issuecomment-279521097:0,Integrability,Depend,Depends,0,Depends -- we can extend the HWE aggregator to incorporate sample phenotype information or we can leave it to the user to filter out male samples.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/200#issuecomment-279521097
https://github.com/hail-is/hail/issues/200#issuecomment-279521097:18,Modifiability,extend,extend,18,Depends -- we can extend the HWE aggregator to incorporate sample phenotype information or we can leave it to the user to filter out male samples.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/200#issuecomment-279521097
https://github.com/hail-is/hail/pull/202#issuecomment-188772599:38,Testability,test,tests,38,"Back to you. Looks great! Still needs tests. Sounds like you have independent python code: put that in `src/test/resources` along with the output on a non-trivial example with missingness and compare against that. Finally, it would be good to have a graphical comparison with PLINK after these changes a la Kyle's plots from before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/202#issuecomment-188772599
https://github.com/hail-is/hail/pull/202#issuecomment-188772599:108,Testability,test,test,108,"Back to you. Looks great! Still needs tests. Sounds like you have independent python code: put that in `src/test/resources` along with the output on a non-trivial example with missingness and compare against that. Finally, it would be good to have a graphical comparison with PLINK after these changes a la Kyle's plots from before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/202#issuecomment-188772599
https://github.com/hail-is/hail/pull/202#issuecomment-189526505:38,Testability,test,tests,38,Thanks! Back to you. Still working on tests and agreed on graphical comparison with PLINK.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/202#issuecomment-189526505
https://github.com/hail-is/hail/issues/207#issuecomment-301788879:13,Performance,perform,performance,13,Non specific performance issue,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/207#issuecomment-301788879
https://github.com/hail-is/hail/pull/210#issuecomment-192798627:59,Testability,test,tests,59,"Made all discussed changes and then some, except HailCheck tests. Added symbolic variant check / filter, but we should still talk about the PL(GT) != 0 genotypes in GoT2D.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/210#issuecomment-192798627
https://github.com/hail-is/hail/issues/216#issuecomment-279518768:4,Availability,error,error,4,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768
https://github.com/hail-is/hail/issues/216#issuecomment-279518768:10,Integrability,message,message,10,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768
https://github.com/hail-is/hail/issues/216#issuecomment-279518768:54,Usability,clear,clearer,54,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768
https://github.com/hail-is/hail/issues/230#issuecomment-208688839:20,Testability,test,testTypePretty,20,Done: see ExprSuite.testTypePretty.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/230#issuecomment-208688839
https://github.com/hail-is/hail/pull/233#issuecomment-204477417:34,Performance,load,loadvcf,34,"Back to you. Couldn't resolve the loadvcf vcf report stuff, but everything else should be good to go",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/233#issuecomment-204477417
https://github.com/hail-is/hail/pull/234#issuecomment-204439315:82,Availability,error,error,82,"Back to you. I've re-pushed to accommodate space-delimited .fam again (will throw error if some sample name has space too). While you look at the rest, I'll work in another branch on .fam importer to annotations which will give user a delimiter option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/234#issuecomment-204439315
https://github.com/hail-is/hail/issues/238#issuecomment-256328775:67,Performance,optimiz,optimizer,67,Only if you're doing something like write or PCA. Definitely query optimizer territory,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/238#issuecomment-256328775
https://github.com/hail-is/hail/issues/240#issuecomment-317567372:15,Performance,perform,performance,15,The problem is performance. You wanted to have an allocation-free text importer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/240#issuecomment-317567372
https://github.com/hail-is/hail/pull/242#issuecomment-208516279:166,Availability,error,error,166,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279
https://github.com/hail-is/hail/pull/242#issuecomment-208516279:1778,Availability,error,error,1778,". I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed? Should I increase the service queue size?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279
https://github.com/hail-is/hail/pull/242#issuecomment-208516279:20,Deployability,install,installed,20,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279
https://github.com/hail-is/hail/pull/242#issuecomment-208516279:1934,Performance,queue,queue,1934,". I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed? Should I increase the service queue size?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279
https://github.com/hail-is/hail/pull/242#issuecomment-208516279:2033,Performance,queue,queue,2033,". I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed? Should I increase the service queue size?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279
https://github.com/hail-is/hail/pull/242#issuecomment-208516279:446,Safety,timeout,timeout,446,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279
https://github.com/hail-is/hail/pull/242#issuecomment-208516279:1578,Testability,log,logs,1578,". I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed? Should I increase the service queue size?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:865,Availability,error,error,865,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:2490,Availability,error,error,2490,"er file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505))). In the Kudu logs, I'm seeing tons of:. W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS; a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet; 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable:; CreateTablet request on kudu.tserver.TabletServerAdminService from; 69.173.65.227:42904 dropped due to backpressure. The service queue is; full; it has 50 items. Suggestions on how to proceed? Should I increase the service queue size?. —; You are receiving this because you authored the thread.; Reply to this email directly or view it on GitHub; https://github.com/broadinstitute/hail/pull/242#issuecomment-208516279",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:719,Deployability,install,installed,719,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:201,Modifiability,plugin,plugins,201,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:2648,Performance,queue,queue,2648,"er file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505))). In the Kudu logs, I'm seeing tons of:. W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS; a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet; 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable:; CreateTablet request on kudu.tserver.TabletServerAdminService from; 69.173.65.227:42904 dropped due to backpressure. The service queue is; full; it has 50 items. Suggestions on how to proceed? Should I increase the service queue size?. —; You are receiving this because you authored the thread.; Reply to this email directly or view it on GitHub; https://github.com/broadinstitute/hail/pull/242#issuecomment-208516279",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:2742,Performance,queue,queue,2742,"er file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505))). In the Kudu logs, I'm seeing tons of:. W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS; a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet; 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable:; CreateTablet request on kudu.tserver.TabletServerAdminService from; 69.173.65.227:42904 dropped due to backpressure. The service queue is; full; it has 50 items. Suggestions on how to proceed? Should I increase the service queue size?. —; You are receiving this because you authored the thread.; Reply to this email directly or view it on GitHub; https://github.com/broadinstitute/hail/pull/242#issuecomment-208516279",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:1144,Safety,timeout,timeout,1144,"et that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-208722298:2293,Testability,log,logs,2293,"er file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505))). In the Kudu logs, I'm seeing tons of:. W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS; a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet; 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable:; CreateTablet request on kudu.tserver.TabletServerAdminService from; 69.173.65.227:42904 dropped due to backpressure. The service queue is; full; it has 50 items. Suggestions on how to proceed? Should I increase the service queue size?. —; You are receiving this because you authored the thread.; Reply to this email directly or view it on GitHub; https://github.com/broadinstitute/hail/pull/242#issuecomment-208516279",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298
https://github.com/hail-is/hail/pull/242#issuecomment-213006430:160,Performance,race condition,race condition,160,"@cseed I managed to reproduce the problem you were seeing on a cluster. It's not a problem with the size of the table (as I previously suspected), but rather a race condition caused by dropping a table then recreating it immediately. Adding a sleep avoids the problem for the time being. There are stil some merge conflicts due to the way that the metadata is stored. The best thing would be to factor out the code to do that so it can be shared by the Parquet and Kudu code paths. I'll have a go at doing that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-213006430
https://github.com/hail-is/hail/pull/242#issuecomment-213006430:249,Safety,avoid,avoids,249,"@cseed I managed to reproduce the problem you were seeing on a cluster. It's not a problem with the size of the table (as I previously suspected), but rather a race condition caused by dropping a table then recreating it immediately. Adding a sleep avoids the problem for the time being. There are stil some merge conflicts due to the way that the metadata is stored. The best thing would be to factor out the code to do that so it can be shared by the Parquet and Kudu code paths. I'll have a go at doing that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-213006430
https://github.com/hail-is/hail/pull/242#issuecomment-220609757:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220609757
https://github.com/hail-is/hail/pull/242#issuecomment-220610690:307,Deployability,update,updated,307,"I've now addressed the sequence dictionary point made by @laserson, as well as rebasing on master. . Persisting annotations is a challenge as it looks like the schema is dynamic - is that right? I'm not sure how that will work with Kudu, which expects a fixed schema at table-creation time. . BTW here's an updated link to the Kudu issue I mentioned earlier: https://issues.apache.org/jira/browse/KUDU-383. Note that it's not a blocker for this PR as there's a workaround.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220610690
https://github.com/hail-is/hail/pull/242#issuecomment-220667612:181,Availability,down,down,181,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612
https://github.com/hail-is/hail/pull/242#issuecomment-220667612:257,Deployability,patch,patch,257,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612
https://github.com/hail-is/hail/pull/242#issuecomment-220667612:753,Usability,simpl,simple,753,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612
https://github.com/hail-is/hail/pull/242#issuecomment-226535225:212,Availability,down,down,212,"Kudu 0.9.0 was released a few days ago, and it has a re-written Spark library so we don't need the `org.kududb.spark` package any more. It also fixes bugs, like the one @cseed saw with the context not being shut down. Annotations still don't work though - is there a way to get their schema early on so we can create a database table for them?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-226535225
https://github.com/hail-is/hail/pull/242#issuecomment-226535225:15,Deployability,release,released,15,"Kudu 0.9.0 was released a few days ago, and it has a re-written Spark library so we don't need the `org.kududb.spark` package any more. It also fixes bugs, like the one @cseed saw with the context not being shut down. Annotations still don't work though - is there a way to get their schema early on so we can create a database table for them?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-226535225
https://github.com/hail-is/hail/pull/243#issuecomment-215215199:88,Testability,test,tests,88,@tpoterba: I fixed everything except the null variant in bgen block reader. The Jenkins tests are running now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-215215199
https://github.com/hail-is/hail/pull/243#issuecomment-218212906:1235,Availability,error,error,1235,"@cseed: It would be great if we could merge this into master soon -- there's a lot of changes here!. Highlight of major changes:; 1. Dosage is implemented in Genotype.scala; - A user can get either dosages `.dosage` or PLs `.pl`; - To go from PLs to Dosages: rescale each PL (10^(-PL/10)), take the sum of the rescaled numbers, then divide by the sum. This is assuming equal weights prior (can incorporate alternate prior later); - To go from Dosages to PLs: same transformation as before; 2. INFO score is implemented in variantqc; - No tests for info score yet as still uncertain which method to use; - My computation agrees with SNPTEST but not QCTOOL; 3. `importgen` and `exportgen` are now implemented; 4. SplitMulti will split dosages correctly except for the setting of false ref. If the original dosage with N genotypes had more than one maximum value [ex: (0.2, 0.2, 0.1, 0.1, 0.1, 0.3)], then the original genotype is -1. But after combining dosages, then there is one unique maximum value. The fakeref flag is not set in this case, but the genotype is > 0.; 5. A randomly generated genotype can have two values very close together (0.4035, 0.4036, 0.2...) that when read back in via gen file or bgen file will have rounding error (0.4035, 0.4035, 0.2...) so there is no maximal genotype anymore (gt = -1). I don't think this is a huge concern as it can only happen if the max dosage is <= 0.5, and these will get filtered out by most users anyways. **To-Do:; 1. Finalize INFO score calculation and write tests; 2. Fix null variant in PLINK code (want to do this in separate branch); 3. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 4. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 5. Update the readFam function in `importplink` to utilize functionality Jon wrote already",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-218212906
https://github.com/hail-is/hail/pull/243#issuecomment-218212906:1839,Deployability,Update,Update,1839,"@cseed: It would be great if we could merge this into master soon -- there's a lot of changes here!. Highlight of major changes:; 1. Dosage is implemented in Genotype.scala; - A user can get either dosages `.dosage` or PLs `.pl`; - To go from PLs to Dosages: rescale each PL (10^(-PL/10)), take the sum of the rescaled numbers, then divide by the sum. This is assuming equal weights prior (can incorporate alternate prior later); - To go from Dosages to PLs: same transformation as before; 2. INFO score is implemented in variantqc; - No tests for info score yet as still uncertain which method to use; - My computation agrees with SNPTEST but not QCTOOL; 3. `importgen` and `exportgen` are now implemented; 4. SplitMulti will split dosages correctly except for the setting of false ref. If the original dosage with N genotypes had more than one maximum value [ex: (0.2, 0.2, 0.1, 0.1, 0.1, 0.3)], then the original genotype is -1. But after combining dosages, then there is one unique maximum value. The fakeref flag is not set in this case, but the genotype is > 0.; 5. A randomly generated genotype can have two values very close together (0.4035, 0.4036, 0.2...) that when read back in via gen file or bgen file will have rounding error (0.4035, 0.4035, 0.2...) so there is no maximal genotype anymore (gt = -1). I don't think this is a huge concern as it can only happen if the max dosage is <= 0.5, and these will get filtered out by most users anyways. **To-Do:; 1. Finalize INFO score calculation and write tests; 2. Fix null variant in PLINK code (want to do this in separate branch); 3. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 4. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 5. Update the readFam function in `importplink` to utilize functionality Jon wrote already",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-218212906
https://github.com/hail-is/hail/pull/243#issuecomment-218212906:538,Testability,test,tests,538,"@cseed: It would be great if we could merge this into master soon -- there's a lot of changes here!. Highlight of major changes:; 1. Dosage is implemented in Genotype.scala; - A user can get either dosages `.dosage` or PLs `.pl`; - To go from PLs to Dosages: rescale each PL (10^(-PL/10)), take the sum of the rescaled numbers, then divide by the sum. This is assuming equal weights prior (can incorporate alternate prior later); - To go from Dosages to PLs: same transformation as before; 2. INFO score is implemented in variantqc; - No tests for info score yet as still uncertain which method to use; - My computation agrees with SNPTEST but not QCTOOL; 3. `importgen` and `exportgen` are now implemented; 4. SplitMulti will split dosages correctly except for the setting of false ref. If the original dosage with N genotypes had more than one maximum value [ex: (0.2, 0.2, 0.1, 0.1, 0.1, 0.3)], then the original genotype is -1. But after combining dosages, then there is one unique maximum value. The fakeref flag is not set in this case, but the genotype is > 0.; 5. A randomly generated genotype can have two values very close together (0.4035, 0.4036, 0.2...) that when read back in via gen file or bgen file will have rounding error (0.4035, 0.4035, 0.2...) so there is no maximal genotype anymore (gt = -1). I don't think this is a huge concern as it can only happen if the max dosage is <= 0.5, and these will get filtered out by most users anyways. **To-Do:; 1. Finalize INFO score calculation and write tests; 2. Fix null variant in PLINK code (want to do this in separate branch); 3. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 4. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 5. Update the readFam function in `importplink` to utilize functionality Jon wrote already",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-218212906
https://github.com/hail-is/hail/pull/243#issuecomment-218212906:1515,Testability,test,tests,1515,"@cseed: It would be great if we could merge this into master soon -- there's a lot of changes here!. Highlight of major changes:; 1. Dosage is implemented in Genotype.scala; - A user can get either dosages `.dosage` or PLs `.pl`; - To go from PLs to Dosages: rescale each PL (10^(-PL/10)), take the sum of the rescaled numbers, then divide by the sum. This is assuming equal weights prior (can incorporate alternate prior later); - To go from Dosages to PLs: same transformation as before; 2. INFO score is implemented in variantqc; - No tests for info score yet as still uncertain which method to use; - My computation agrees with SNPTEST but not QCTOOL; 3. `importgen` and `exportgen` are now implemented; 4. SplitMulti will split dosages correctly except for the setting of false ref. If the original dosage with N genotypes had more than one maximum value [ex: (0.2, 0.2, 0.1, 0.1, 0.1, 0.3)], then the original genotype is -1. But after combining dosages, then there is one unique maximum value. The fakeref flag is not set in this case, but the genotype is > 0.; 5. A randomly generated genotype can have two values very close together (0.4035, 0.4036, 0.2...) that when read back in via gen file or bgen file will have rounding error (0.4035, 0.4035, 0.2...) so there is no maximal genotype anymore (gt = -1). I don't think this is a huge concern as it can only happen if the max dosage is <= 0.5, and these will get filtered out by most users anyways. **To-Do:; 1. Finalize INFO score calculation and write tests; 2. Fix null variant in PLINK code (want to do this in separate branch); 3. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 4. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 5. Update the readFam function in `importplink` to utilize functionality Jon wrote already",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-218212906
https://github.com/hail-is/hail/pull/243#issuecomment-233953902:99,Deployability,pipeline,pipelines,99,"Hi, Is there a plan to merge this branch in soon? We are testing out HAIL for some of our in-house pipelines and an ability to import bgens would be really handy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-233953902
https://github.com/hail-is/hail/pull/243#issuecomment-233953902:57,Testability,test,testing,57,"Hi, Is there a plan to merge this branch in soon? We are testing out HAIL for some of our in-house pipelines and an ability to import bgens would be really handy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-233953902
https://github.com/hail-is/hail/issues/258#issuecomment-301789487:51,Availability,error,errors,51,nobody's complained about this in a long time. our errors are pretty good here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/258#issuecomment-301789487
https://github.com/hail-is/hail/issues/262#issuecomment-206416460:254,Availability,error,error,254,"That's a good point about which fatal is called. I'm OK with removing it. On Wed, Apr 6, 2016 at 10:56 AM, cseed notifications@github.com wrote:. > I argue fatalIf(p, msg) is less readable than if (p) fatal(msg) and not; > any shorter. It also causes an error since you can't control which fatal to; > call, e.g., Utils.fatal vs Line.fatal. @tpoterba; > https://github.com/tpoterba Thoughts?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/issues/262",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/262#issuecomment-206416460
https://github.com/hail-is/hail/issues/263#issuecomment-214538990:417,Testability,log,log,417,"This afternoon I tried to run a series of commands that began with:. hail-new read -i /user/satterst/DBS_v2.4/temp3.vds \; filtervariants --keep -c 'va.kyle.lof == ""HC""' \; filtervariants --remove -c /user/satterst/exac.variant_list. and I got output that said:; hail: info: running: filtervariants --remove -c /user/satterst/exac.variant_list; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space. log here:; /humgen/atgu1/fs03/satterst/DBS_v2.4/hail.heapspace.log. Has the fix been incorporated into the jar on the cluster?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/263#issuecomment-214538990
https://github.com/hail-is/hail/issues/263#issuecomment-214538990:480,Testability,log,log,480,"This afternoon I tried to run a series of commands that began with:. hail-new read -i /user/satterst/DBS_v2.4/temp3.vds \; filtervariants --keep -c 'va.kyle.lof == ""HC""' \; filtervariants --remove -c /user/satterst/exac.variant_list. and I got output that said:; hail: info: running: filtervariants --remove -c /user/satterst/exac.variant_list; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space. log here:; /humgen/atgu1/fs03/satterst/DBS_v2.4/hail.heapspace.log. Has the fix been incorporated into the jar on the cluster?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/263#issuecomment-214538990
https://github.com/hail-is/hail/issues/263#issuecomment-214545407:646,Testability,log,log,646,"Not in the default script yet. Try running with this:. /psych/genetics_data/working/cseed/hail-inst/bin/hail. Cotton. On Mon, Apr 25, 2016 at 5:53 PM, ksatterstrom notifications@github.com; wrote:. > This afternoon I tried to run a series of commands that began with:; > ; > hail-new read -i /user/satterst/DBS_v2.4/temp3.vds \; > filtervariants --keep -c 'va.kyle.lof == ""HC""' \; > filtervariants --remove -c /user/satterst/exac.variant_list; > ; > and I got output that said:; > hail: info: running: filtervariants --remove -c; > /user/satterst/exac.variant_list; > Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; > ; > log here:; > /humgen/atgu1/fs03/satterst/DBS_v2.4/hail.heapspace.log; > ; > Has the fix been incorporated into the jar on the cluster?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/issues/263#issuecomment-214538990",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/263#issuecomment-214545407
https://github.com/hail-is/hail/issues/263#issuecomment-214545407:711,Testability,log,log,711,"Not in the default script yet. Try running with this:. /psych/genetics_data/working/cseed/hail-inst/bin/hail. Cotton. On Mon, Apr 25, 2016 at 5:53 PM, ksatterstrom notifications@github.com; wrote:. > This afternoon I tried to run a series of commands that began with:; > ; > hail-new read -i /user/satterst/DBS_v2.4/temp3.vds \; > filtervariants --keep -c 'va.kyle.lof == ""HC""' \; > filtervariants --remove -c /user/satterst/exac.variant_list; > ; > and I got output that said:; > hail: info: running: filtervariants --remove -c; > /user/satterst/exac.variant_list; > Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; > ; > log here:; > /humgen/atgu1/fs03/satterst/DBS_v2.4/hail.heapspace.log; > ; > Has the fix been incorporated into the jar on the cluster?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/issues/263#issuecomment-214538990",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/263#issuecomment-214545407
https://github.com/hail-is/hail/pull/265#issuecomment-206544748:22,Testability,test,tests,22,"Reopened, passing all tests and fixed a few things.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/265#issuecomment-206544748
https://github.com/hail-is/hail/pull/282#issuecomment-208341414:24,Testability,test,testing,24,What should we do about testing Cassandra? We need an `annotatevariants cass` as well I think. This stuff looks good though (awesome that it can be so simple!),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208341414
https://github.com/hail-is/hail/pull/282#issuecomment-208341414:151,Usability,simpl,simple,151,What should we do about testing Cassandra? We need an `annotatevariants cass` as well I think. This stuff looks good though (awesome that it can be so simple!),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208341414
https://github.com/hail-is/hail/pull/282#issuecomment-208368921:242,Deployability,install,installed,242,"Once we can draw from Cassandra (with the likes of `annotatevariants cass`, we can write/read/compare in the usual way. We need a Cassandra cluster for testing. There are three options: an embedded server as part of Hail, assume the user has installed Cassandra locally, or run against a fixed server. I set up a single-node Cassandra install on hail-ci. In the spirit of small commits, I want to be able to test in experimental/untested/in progress work so we don't get so much divergence. We need a way to mark it. I will investigate the options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208368921
https://github.com/hail-is/hail/pull/282#issuecomment-208368921:335,Deployability,install,install,335,"Once we can draw from Cassandra (with the likes of `annotatevariants cass`, we can write/read/compare in the usual way. We need a Cassandra cluster for testing. There are three options: an embedded server as part of Hail, assume the user has installed Cassandra locally, or run against a fixed server. I set up a single-node Cassandra install on hail-ci. In the spirit of small commits, I want to be able to test in experimental/untested/in progress work so we don't get so much divergence. We need a way to mark it. I will investigate the options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208368921
https://github.com/hail-is/hail/pull/282#issuecomment-208368921:152,Testability,test,testing,152,"Once we can draw from Cassandra (with the likes of `annotatevariants cass`, we can write/read/compare in the usual way. We need a Cassandra cluster for testing. There are three options: an embedded server as part of Hail, assume the user has installed Cassandra locally, or run against a fixed server. I set up a single-node Cassandra install on hail-ci. In the spirit of small commits, I want to be able to test in experimental/untested/in progress work so we don't get so much divergence. We need a way to mark it. I will investigate the options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208368921
https://github.com/hail-is/hail/pull/282#issuecomment-208368921:408,Testability,test,test,408,"Once we can draw from Cassandra (with the likes of `annotatevariants cass`, we can write/read/compare in the usual way. We need a Cassandra cluster for testing. There are three options: an embedded server as part of Hail, assume the user has installed Cassandra locally, or run against a fixed server. I set up a single-node Cassandra install on hail-ci. In the spirit of small commits, I want to be able to test in experimental/untested/in progress work so we don't get so much divergence. We need a way to mark it. I will investigate the options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208368921
https://github.com/hail-is/hail/pull/291#issuecomment-210106396:20,Testability,test,testing,20,"I've got some weird testing issues I'm working out. Once those are fixed, I'll kick it back to Jackie (probably today / early tomorrow)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210106396
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:232,Performance,Load,LoadBgenSuite,232,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:317,Performance,Load,LoadBgenSuite,317,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:398,Performance,Load,LoadBgenSuite,398,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:479,Performance,Load,LoadBgenSuite,479,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:34,Testability,test,tests,34,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:168,Testability,test,tested,168,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:198,Testability,test,test,198,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:246,Testability,test,testBgenImportRandom,246,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:364,Testability,test,test,364,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:412,Testability,test,testGavinExample,412,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:525,Testability,test,test,525,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210432089:583,Testability,test,test,583,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089
https://github.com/hail-is/hail/pull/291#issuecomment-210435597:73,Testability,test,test,73,does jenkins have qctool? . I've also noticed that the import plink spec test occasionally fails due to the fatal(partitions greater than filesize) but I haven't changed any of that code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210435597
https://github.com/hail-is/hail/pull/291#issuecomment-210482932:37,Performance,load,load,37,The 'details' page for Jenkins won't load. This branch passed on my local computer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210482932
https://github.com/hail-is/hail/pull/291#issuecomment-210483887:299,Performance,load,load,299,"You can see the log of the test here:; http://hail-ci:8080/job/Hail%20-%20Test%20All%20Branches/52/console. It is ""Console Output"" on the left hand side on the build page. Cotton. On Fri, Apr 15, 2016 at 10:27 AM, Tim Poterba notifications@github.com; wrote:. > The 'details' page for Jenkins won't load. This branch passed on my local; > computer.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/pull/291#issuecomment-210482932",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210483887
https://github.com/hail-is/hail/pull/291#issuecomment-210483887:16,Testability,log,log,16,"You can see the log of the test here:; http://hail-ci:8080/job/Hail%20-%20Test%20All%20Branches/52/console. It is ""Console Output"" on the left hand side on the build page. Cotton. On Fri, Apr 15, 2016 at 10:27 AM, Tim Poterba notifications@github.com; wrote:. > The 'details' page for Jenkins won't load. This branch passed on my local; > computer.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/pull/291#issuecomment-210482932",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210483887
https://github.com/hail-is/hail/pull/291#issuecomment-210483887:27,Testability,test,test,27,"You can see the log of the test here:; http://hail-ci:8080/job/Hail%20-%20Test%20All%20Branches/52/console. It is ""Console Output"" on the left hand side on the build page. Cotton. On Fri, Apr 15, 2016 at 10:27 AM, Tim Poterba notifications@github.com; wrote:. > The 'details' page for Jenkins won't load. This branch passed on my local; > computer.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/pull/291#issuecomment-210482932",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210483887
https://github.com/hail-is/hail/pull/291#issuecomment-210554186:4,Testability,test,test,4,The test that failed is this one:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59. Accepting the changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210554186
https://github.com/hail-is/hail/pull/291#issuecomment-210554186:57,Testability,test,test,57,The test that failed is this one:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59. Accepting the changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210554186
https://github.com/hail-is/hail/pull/291#issuecomment-210554186:115,Testability,test,test,115,The test that failed is this one:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59. Accepting the changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210554186
https://github.com/hail-is/hail/pull/292#issuecomment-212196570:104,Deployability,update,updates,104,"I made these changes and a few others in LinearRegressionCommand (flatMap instead of Array.concat), and updates the docs to reflect removal of --output. Remerged with master and pushed. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/292#issuecomment-212196570
https://github.com/hail-is/hail/pull/294#issuecomment-210115731:43,Testability,test,testing,43,"Don't merge it yet, I want to sort out the testing issues even though I think that's a local problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/294#issuecomment-210115731
https://github.com/hail-is/hail/pull/300#issuecomment-210843547:15,Deployability,install,install,15,```; $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.860s; filtervariants: 47.061ms; exportvariants: 1m32.1s. $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtersamples --remove --all filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.766s; filtersamples: 1.671ms; filtervariants: 39.474ms; exportvariants: 1m31.5s. $ ./build/install/hail/bin/hail --master 'local[1]' read --skip-genotypes -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.844s; filtervariants: 44.972ms; exportvariants: 3.336s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/300#issuecomment-210843547
https://github.com/hail-is/hail/pull/300#issuecomment-210843547:265,Deployability,install,install,265,```; $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.860s; filtervariants: 47.061ms; exportvariants: 1m32.1s. $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtersamples --remove --all filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.766s; filtersamples: 1.671ms; filtervariants: 39.474ms; exportvariants: 1m31.5s. $ ./build/install/hail/bin/hail --master 'local[1]' read --skip-genotypes -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.844s; filtervariants: 44.972ms; exportvariants: 3.336s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/300#issuecomment-210843547
https://github.com/hail-is/hail/pull/300#issuecomment-210843547:568,Deployability,install,install,568,```; $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.860s; filtervariants: 47.061ms; exportvariants: 1m32.1s. $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtersamples --remove --all filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.766s; filtersamples: 1.671ms; filtervariants: 39.474ms; exportvariants: 1m31.5s. $ ./build/install/hail/bin/hail --master 'local[1]' read --skip-genotypes -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.844s; filtervariants: 44.972ms; exportvariants: 3.336s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/300#issuecomment-210843547
https://github.com/hail-is/hail/pull/300#issuecomment-210845452:15,Deployability,install,install,15,```; $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtersamples --remove -c true filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.789s; filtersamples: 48.752ms; filtervariants: 20.286ms; exportvariants: 1m33.0s; ```. Surprised `filtersamples --remove --all` wasn't significantly faster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/300#issuecomment-210845452
https://github.com/hail-is/hail/issues/301#issuecomment-210901646:131,Usability,clear,clear,131,I think this is due to HDFS filling up. Possibly related to the fact Tim was creating a copy of ExAC. I will retry the job once we clear up some space.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301#issuecomment-210901646
https://github.com/hail-is/hail/issues/301#issuecomment-210901896:334,Usability,clear,clear,334,"ok, let me know when I can try again. I’m now importing another large WES study, hope doesn’t have the same problem.; cheers,. > On Apr 16, 2016, at 5:08 PM, cseed notifications@github.com wrote:; > ; > I think this is due to HDFS filling up. Possibly related to the fact Tim was creating a copy of ExAC. I will retry the job once we clear up some space.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub https://github.com/broadinstitute/hail/issues/301#issuecomment-210901646",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301#issuecomment-210901896
https://github.com/hail-is/hail/issues/302#issuecomment-210903100:293,Availability,error,errors,293,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100
https://github.com/hail-is/hail/issues/302#issuecomment-210903100:138,Deployability,upgrade,upgrade,138,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100
https://github.com/hail-is/hail/issues/302#issuecomment-210903100:159,Deployability,install,installed,159,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100
https://github.com/hail-is/hail/issues/302#issuecomment-210903100:24,Energy Efficiency,schedul,scheduler,24,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100
https://github.com/hail-is/hail/issues/302#issuecomment-210941562:118,Modifiability,config,config,118,I tried again:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 1000 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds. It's taking wayyyy too long. log here: /humgen/atgu1/fs03/satterst/hail.log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210941562
https://github.com/hail-is/hail/issues/302#issuecomment-210941562:253,Testability,log,log,253,I tried again:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 1000 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds. It's taking wayyyy too long. log here: /humgen/atgu1/fs03/satterst/hail.log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210941562
https://github.com/hail-is/hail/issues/302#issuecomment-210941562:296,Testability,log,log,296,I tried again:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 1000 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds. It's taking wayyyy too long. log here: /humgen/atgu1/fs03/satterst/hail.log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210941562
https://github.com/hail-is/hail/issues/302#issuecomment-210947424:16,Testability,log,log,16,I looked at the log. Looks like it is running fine. Is it running slowly? How long do you expect it to take? How many variants in the DILI controls?. VEP is never going to run fast in this form. We're going to merge pre-computed annotations for SNPs and only run VEP on indels. That should make it about 20x faster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210947424
https://github.com/hail-is/hail/issues/302#issuecomment-211046143:93,Availability,error,error,93,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143
https://github.com/hail-is/hail/issues/302#issuecomment-211046143:256,Availability,failure,failures,256,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143
https://github.com/hail-is/hail/issues/302#issuecomment-211046143:371,Availability,error,error,371,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143
https://github.com/hail-is/hail/issues/302#issuecomment-211046143:646,Availability,failure,failures,646,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143
https://github.com/hail-is/hail/issues/302#issuecomment-211046143:99,Integrability,message,message,99,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143
https://github.com/hail-is/hail/issues/302#issuecomment-211046143:377,Integrability,message,message,377,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143
https://github.com/hail-is/hail/issues/303#issuecomment-211167105:16,Testability,log,log,16,Is the attached log file correct? It is for a job that never got assigned cores on the cluster. I don't see anything about NoClassDefFoundError in it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303#issuecomment-211167105
https://github.com/hail-is/hail/issues/303#issuecomment-211658957:14,Testability,log,log,14,"Hi,; here the log, it failed again (after ~12h running).; cheers,. > > On Apr 17, 2016, at 11:03 PM, cseed <notifications@github.com <mailto:notifications@github.com>> wrote:; > > ; > > Is the attached log file correct? It is for a job that never got assigned cores on the cluster. I don't see anything about NoClassDefFoundError in it.; > > ; > > —; > > You are receiving this because you authored the thread.; > > Reply to this email directly or view it on GitHub https://github.com/broadinstitute/hail/issues/303#issuecomment-211167105",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303#issuecomment-211658957
https://github.com/hail-is/hail/issues/303#issuecomment-211658957:202,Testability,log,log,202,"Hi,; here the log, it failed again (after ~12h running).; cheers,. > > On Apr 17, 2016, at 11:03 PM, cseed <notifications@github.com <mailto:notifications@github.com>> wrote:; > > ; > > Is the attached log file correct? It is for a job that never got assigned cores on the cluster. I don't see anything about NoClassDefFoundError in it.; > > ; > > —; > > You are receiving this because you authored the thread.; > > Reply to this email directly or view it on GitHub https://github.com/broadinstitute/hail/issues/303#issuecomment-211167105",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303#issuecomment-211658957
https://github.com/hail-is/hail/issues/303#issuecomment-211716001:250,Availability,down,down,250,"First, this actually succeeded:. ```; $ hdfs dfs -ls /user/aganna/CANCER.vep.vds/rdd.parquet/_SUCCESS; -rw-r--r-- 2 aganna supergroup 0 2016-04-18 20:14 /user/aganna/CANCER.vep.vds/rdd.parquet/_SUCCESS; ```. It crashed while the program was shutting down. I know what caused it and it has been fixed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303#issuecomment-211716001
https://github.com/hail-is/hail/issues/304#issuecomment-211168558:145,Safety,avoid,avoid,145,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558
https://github.com/hail-is/hail/issues/304#issuecomment-211168558:31,Testability,log,log,31,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558
https://github.com/hail-is/hail/issues/304#issuecomment-211168558:105,Testability,log,log,105,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558
https://github.com/hail-is/hail/issues/304#issuecomment-211168558:123,Testability,log,log,123,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558
https://github.com/hail-is/hail/issues/304#issuecomment-211168558:167,Testability,log,log,167,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558
https://github.com/hail-is/hail/issues/304#issuecomment-211168558:205,Testability,log,log,205,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558
https://github.com/hail-is/hail/issues/304#issuecomment-211168558:266,Testability,log,log,266,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558
https://github.com/hail-is/hail/issues/308#issuecomment-211685443:353,Safety,avoid,avoid,353,"Actually, going to re-open this. We determined that these were filtered because the sites each had an allele that exceeded the maximum length of 150 (set at https://github.com/samtools/htsjdk/blob/master/src/java/htsjdk/variant/variantcontext/VariantContext.java). Jon suggests that Hail should filter using isSymbolic() rather than isSymbolicOrSV() to avoid filtering these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/308#issuecomment-211685443
https://github.com/hail-is/hail/issues/309#issuecomment-211989236:38,Availability,error,error,38,Did this finish today? It is the same error as the last issue: it actually succeeded and shouldn't happen again for new jobs. ```; -rw-r--r-- 2 aganna supergroup 0 2016-04-19 00:00 /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds/rdd.parquet/_SUCCESS; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309#issuecomment-211989236
https://github.com/hail-is/hail/pull/310#issuecomment-212577261:38,Availability,error,error,38,"I have some reorganization and better error checking I want to do, but I'll accept this and make that in a separate pull request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/310#issuecomment-212577261
https://github.com/hail-is/hail/issues/313#issuecomment-212074652:14,Usability,clear,clearly,14,And we should clearly document what we've implemented so there is no ambiguity.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/313#issuecomment-212074652
https://github.com/hail-is/hail/issues/317#issuecomment-212477402:293,Availability,error,error,293,"Maryam,; I ran this command in Unix:. ```; gunzip -c <file> | cut -f4 | sort | uniq -c; 20709505 A; 20934670 C; 20968049 G; 20693812 T; 25 alt; ```. I think the problem is that the headers from all the files were included in the one file. I'm running another grep now to be sure. I'll fix the error message though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/317#issuecomment-212477402
https://github.com/hail-is/hail/issues/317#issuecomment-212477402:299,Integrability,message,message,299,"Maryam,; I ran this command in Unix:. ```; gunzip -c <file> | cut -f4 | sort | uniq -c; 20709505 A; 20934670 C; 20968049 G; 20693812 T; 25 alt; ```. I think the problem is that the headers from all the files were included in the one file. I'm running another grep now to be sure. I'll fix the error message though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/317#issuecomment-212477402
https://github.com/hail-is/hail/issues/319#issuecomment-212160076:7,Deployability,update,updated,7,"When I updated the interval_list to this format: `1:1-10000`, it worked.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/319#issuecomment-212160076
https://github.com/hail-is/hail/issues/321#issuecomment-212885679:222,Availability,error,error,222,"I let it go at the end of a long string of commands overnight and it looked to get stuck in the same place, still at (0 + 25) / 25 after what I estimate was about three hours on the grm. A glance at the log shows the same error. I killed it to free up the cluster. . Log here: humgen/atgu1/fs03/satterst/DBS_v2.3/hail.kryo.log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321#issuecomment-212885679
https://github.com/hail-is/hail/issues/321#issuecomment-212885679:203,Testability,log,log,203,"I let it go at the end of a long string of commands overnight and it looked to get stuck in the same place, still at (0 + 25) / 25 after what I estimate was about three hours on the grm. A glance at the log shows the same error. I killed it to free up the cluster. . Log here: humgen/atgu1/fs03/satterst/DBS_v2.3/hail.kryo.log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321#issuecomment-212885679
https://github.com/hail-is/hail/issues/321#issuecomment-212885679:267,Testability,Log,Log,267,"I let it go at the end of a long string of commands overnight and it looked to get stuck in the same place, still at (0 + 25) / 25 after what I estimate was about three hours on the grm. A glance at the log shows the same error. I killed it to free up the cluster. . Log here: humgen/atgu1/fs03/satterst/DBS_v2.3/hail.kryo.log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321#issuecomment-212885679
https://github.com/hail-is/hail/issues/321#issuecomment-212885679:323,Testability,log,log,323,"I let it go at the end of a long string of commands overnight and it looked to get stuck in the same place, still at (0 + 25) / 25 after what I estimate was about three hours on the grm. A glance at the log shows the same error. I killed it to free up the cluster. . Log here: humgen/atgu1/fs03/satterst/DBS_v2.3/hail.kryo.log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321#issuecomment-212885679
https://github.com/hail-is/hail/issues/322#issuecomment-291943984:59,Deployability,update,update,59,"Is this still an issue without command line? If so, can we update the issue to python terms?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/322#issuecomment-291943984
https://github.com/hail-is/hail/issues/323#issuecomment-279525226:43,Deployability,update,update,43,@johnc1231 has started to do this and will update soon here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/323#issuecomment-279525226
https://github.com/hail-is/hail/pull/324#issuecomment-213010784:127,Integrability,message,message,127,"ready for a look. Also, the bug in annotatevariants tsv (the length checking on string instead of split) was only in the fatal message, not the actual check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/324#issuecomment-213010784
https://github.com/hail-is/hail/pull/326#issuecomment-213803970:80,Deployability,update,update,80,Great. Just address the localSize == 0 thing (fix or tell me why I'm wrong) and update to master and I'll merge.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/326#issuecomment-213803970
https://github.com/hail-is/hail/issues/332#issuecomment-422364559:58,Integrability,interface,interface,58,I'm convinced this is entirely possible in the 0.2 python interface,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/332#issuecomment-422364559
https://github.com/hail-is/hail/issues/335#issuecomment-214377125:164,Availability,error,errors,164,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125
https://github.com/hail-is/hail/issues/335#issuecomment-214377125:299,Availability,error,error,299,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125
https://github.com/hail-is/hail/issues/335#issuecomment-214377125:29,Deployability,configurat,configuration,29,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125
https://github.com/hail-is/hail/issues/335#issuecomment-214377125:305,Integrability,message,messages,305,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125
https://github.com/hail-is/hail/issues/335#issuecomment-214377125:29,Modifiability,config,configuration,29,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125
https://github.com/hail-is/hail/issues/335#issuecomment-214377125:258,Testability,test,tests,258,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125
https://github.com/hail-is/hail/issues/335#issuecomment-215453804:94,Testability,log,log,94,See the random seed specification in #357 . Gradle will say the random seed at the top of the log if tests are run with gradle. Spec.check will also print it out (for use in IntelliJ). You can either specify using Spec.check(seed = Option(5)) or with the Java properties:; -Dhail.seed=5; -Dhail.randomize=true,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-215453804
https://github.com/hail-is/hail/issues/335#issuecomment-215453804:101,Testability,test,tests,101,See the random seed specification in #357 . Gradle will say the random seed at the top of the log if tests are run with gradle. Spec.check will also print it out (for use in IntelliJ). You can either specify using Spec.check(seed = Option(5)) or with the Java properties:; -Dhail.seed=5; -Dhail.randomize=true,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-215453804
https://github.com/hail-is/hail/issues/347#issuecomment-214553591:89,Deployability,update,update,89,This was a problem with .crc files. The solution is to delete the .crc files. We need to update the renamesamples documentation explain this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/347#issuecomment-214553591
https://github.com/hail-is/hail/issues/353#issuecomment-240550514:107,Deployability,pipeline,pipeline,107,"Hail has a set of commands that can be strung together by a user on the command line to create an analysis pipeline. We have a few users with development backgrounds who have started to build their own commands. It would be great if they could just throw those in their CLASSPATH and then run them directly from the command line by name. We'd have to pick a shell-friendly syntax, so something like, but not:. `$ hail importvcf ... $com.company.CustomLifeSavingAnalysis ...`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/353#issuecomment-240550514
https://github.com/hail-is/hail/pull/356#issuecomment-215227188:149,Testability,test,test,149,I think the code looks fine. Somehow a setting on Jenkins got messed up and new branches weren't being built. It should be working now. Wait for the test results before merging with master.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/356#issuecomment-215227188
https://github.com/hail-is/hail/issues/361#issuecomment-216535838:75,Availability,error,error,75,"If it's an important VCF, it shouldn't be corrupted... My solution to this error message will be to add something like `requirement failed: ref was equal to alt` or something like that",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-216535838
https://github.com/hail-is/hail/issues/361#issuecomment-216535838:81,Integrability,message,message,81,"If it's an important VCF, it shouldn't be corrupted... My solution to this error message will be to add something like `requirement failed: ref was equal to alt` or something like that",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-216535838
https://github.com/hail-is/hail/issues/361#issuecomment-216535908:48,Availability,error,errors,48,"We can talk more about how to handle lines with errors, but that won't be part of this fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-216535908
https://github.com/hail-is/hail/issues/361#issuecomment-218033798:6,Availability,error,error,6,"Fixed error message in 4d38cca, new issue supercedes in #376",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-218033798
https://github.com/hail-is/hail/issues/361#issuecomment-218033798:12,Integrability,message,message,12,"Fixed error message in 4d38cca, new issue supercedes in #376",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-218033798
https://github.com/hail-is/hail/issues/363#issuecomment-248430677:20,Testability,test,test,20,sample mmskat group test: . /humgen/gsa-hphome1/sek/akihiro/Software/EPACTS-3.2.6/bin/epacts group --groupf ${GROUP}.txt \; --vcf ${VCF_dir}/FinEst.WGS.QCed_Lipids.chr1.vcf.gz \; --ped ${Phen_dir}/EstoniaOnlyLipids.ped --max-maf 0.01 \; --kin ${Kin_dir}/FinEstKinship.kinf --sepchr --pheno ${PHENO} \; --cov AGE --cov FAST10 --cov SEX \; --cov LCSET.7048 --cov LCSET.7049 --cov LCSET.7123 --cov LCSET.7125 --cov LCSET.7130 --cov LCSET.7131 --cov LCSET.7132 --cov LCSET.7148 --cov LCSET.7162 --cov LCSET.7174 --cov LCSET.7175 --cov LCSET.7176 --cov LCSET.7257 --cov LCSET.7263 --cov LCSET.7308 --cov LCSET.7331 --cov LCSET.7334 --cov LCSET.7339 --cov LCSET.7340 --cov LCSET.7341 --cov LCSET.7353 --cov LCSET.7354 --cov LCSET.7355 --cov LCSET.7356 --cov LCSET.7357 --cov LCSET.7419 --cov LCSET.7420 --cov LCSET.7473 --cov LCSET.7494 \; --test mmskat --out ${OUT}.ESTonly --run 4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/363#issuecomment-248430677
https://github.com/hail-is/hail/issues/363#issuecomment-248430677:836,Testability,test,test,836,sample mmskat group test: . /humgen/gsa-hphome1/sek/akihiro/Software/EPACTS-3.2.6/bin/epacts group --groupf ${GROUP}.txt \; --vcf ${VCF_dir}/FinEst.WGS.QCed_Lipids.chr1.vcf.gz \; --ped ${Phen_dir}/EstoniaOnlyLipids.ped --max-maf 0.01 \; --kin ${Kin_dir}/FinEstKinship.kinf --sepchr --pheno ${PHENO} \; --cov AGE --cov FAST10 --cov SEX \; --cov LCSET.7048 --cov LCSET.7049 --cov LCSET.7123 --cov LCSET.7125 --cov LCSET.7130 --cov LCSET.7131 --cov LCSET.7132 --cov LCSET.7148 --cov LCSET.7162 --cov LCSET.7174 --cov LCSET.7175 --cov LCSET.7176 --cov LCSET.7257 --cov LCSET.7263 --cov LCSET.7308 --cov LCSET.7331 --cov LCSET.7334 --cov LCSET.7339 --cov LCSET.7340 --cov LCSET.7341 --cov LCSET.7353 --cov LCSET.7354 --cov LCSET.7355 --cov LCSET.7356 --cov LCSET.7357 --cov LCSET.7419 --cov LCSET.7420 --cov LCSET.7473 --cov LCSET.7494 \; --test mmskat --out ${OUT}.ESTonly --run 4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/363#issuecomment-248430677
https://github.com/hail-is/hail/issues/368#issuecomment-236621515:0,Deployability,Configurat,Configuration,0,Configuration can now be stored in the metadata top-level JSON (e.g. split).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/368#issuecomment-236621515
https://github.com/hail-is/hail/issues/368#issuecomment-236621515:0,Modifiability,Config,Configuration,0,Configuration can now be stored in the metadata top-level JSON (e.g. split).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/368#issuecomment-236621515
https://github.com/hail-is/hail/pull/370#issuecomment-217297231:0,Testability,test,testing,0,testing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/370#issuecomment-217297231
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:311,Availability,error,error,311,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:875,Availability,reliab,reliably,875,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:1056,Availability,reliab,reliable,1056,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:1212,Availability,reliab,reliable,1212,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:363,Deployability,pipeline,pipeline,363,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:511,Deployability,pipeline,pipeline,511,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:317,Integrability,message,messages,317,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:866,Integrability,message,messages,866,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-240550289:31,Security,integrity,integrity,31,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289
https://github.com/hail-is/hail/issues/371#issuecomment-319497230:152,Availability,error,error,152,report/accumulators have been removed: https://github.com/hail-is/hail/pull/2024. No longer relevant. .... although better input integrity checking and error reporting would still be nice.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-319497230
https://github.com/hail-is/hail/issues/371#issuecomment-319497230:129,Security,integrity,integrity,129,report/accumulators have been removed: https://github.com/hail-is/hail/pull/2024. No longer relevant. .... although better input integrity checking and error reporting would still be nice.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-319497230
https://github.com/hail-is/hail/issues/377#issuecomment-240002560:19,Testability,log,log,19,I prefer just hail.log.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/377#issuecomment-240002560
https://github.com/hail-is/hail/pull/386#issuecomment-224895254:18,Safety,avoid,avoid,18,"I think we should avoid the term 'gender' here, and replace it with 'sex'. From Wikipedia:. > The distinction between sex and gender differentiates sex (the anatomy of an individual's reproductive system, and secondary sex characteristics) from gender, which can refer to either social roles based on the sex of the person (gender role) or personal identification of one's own gender based on an internal awareness (gender identity).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/386#issuecomment-224895254
https://github.com/hail-is/hail/issues/388#issuecomment-219845067:480,Availability,error,error,480,Here is the problematic command:. `annotateglobal table \; -i file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/genelists/all_scores.scores \; -r global.all_scores \; annotateglobal expr -c 'global.GWAS_height = global.all_scores.filter(x => x.GWAS_HEIGHT == '1').map(x => x.V1)' \; annotatevariants expr -c 'va.andrea.test = global.GWAS_height.toSet.contains(va.andrea.genename)' \`. The shell was eliding the single quote and we were comparing a String and an Int. That should be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/388#issuecomment-219845067
https://github.com/hail-is/hail/issues/388#issuecomment-219845067:314,Testability,test,test,314,Here is the problematic command:. `annotateglobal table \; -i file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/genelists/all_scores.scores \; -r global.all_scores \; annotateglobal expr -c 'global.GWAS_height = global.all_scores.filter(x => x.GWAS_HEIGHT == '1').map(x => x.V1)' \; annotatevariants expr -c 'va.andrea.test = global.GWAS_height.toSet.contains(va.andrea.genename)' \`. The shell was eliding the single quote and we were comparing a String and an Int. That should be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/388#issuecomment-219845067
https://github.com/hail-is/hail/pull/398#issuecomment-220695888:160,Testability,test,tests,160,"Also added fix for issue #389 . Removed withScope function from implementation of leftOuterJoin. Not sure what the effect of leaving this function out, but the tests worked fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/398#issuecomment-220695888
https://github.com/hail-is/hail/pull/398#issuecomment-223023873:177,Safety,avoid,avoid,177,"I noticed that. This isn't ideal. In general, when you're tempted to do this, you should: (1) see if there is a way to restructure the code to use the existing abstractions and avoid the code duplication, or (2) generalize the abstraction to handle your use case and the previous ones without duplication. I need to study the code a bit more to see how'd I proceed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/398#issuecomment-223023873
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:1436,Availability,error,error,1436,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:2174,Availability,error,error,2174,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:2180,Integrability,message,message,2180,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:2144,Modifiability,variab,variables,2144,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:664,Performance,perform,performs,664,"A few observations:; - `Prop` is essentially a named `Gen[Unit]`; - The type of `Prop.forAll` is `Gen[T] -> (U -> Boolean) -> Prop`; - The first two observations suggest: `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Par",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:2112,Security,access,access,2112,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:284,Testability,test,test,284,"A few observations:; - `Prop` is essentially a named `Gen[Unit]`; - The type of `Prop.forAll` is `Gen[T] -> (U -> Boolean) -> Prop`; - The first two observations suggest: `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Par",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:971,Testability,test,tests,971,"A few observations:; - `Prop` is essentially a named `Gen[Unit]`; - The type of `Prop.forAll` is `Gen[T] -> (U -> Boolean) -> Prop`; - The first two observations suggest: `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Par",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:1059,Testability,assert,assert,1059,"` is essentially a named `Gen[Unit]`; - The type of `Prop.forAll` is `Gen[T] -> (U -> Boolean) -> Prop`; - The first two observations suggest: `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: G",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:1250,Testability,test,tests,1250,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238901220:1338,Testability,assert,assert,1338,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220
https://github.com/hail-is/hail/issues/400#issuecomment-238904307:303,Modifiability,variab,variable,303,"This isn't quite right. The `check` method will have no way to inspect the context of a sampled value (particularly, the sampled boolean from a proposition). I think `Gen[T]` should probably be a `Reader[Parameters, (Seq[Any], T)]` where the `Seq[Any]` are the witnesses for each universally quantified variable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238904307
https://github.com/hail-is/hail/issues/400#issuecomment-244517801:974,Availability,failure,failure,974,"Options for nested `forAll`:. ``` scala; toProp(for (; j <- forAll(Gen.choose(0, 10000));; k <- forAll(Gen.choose(0, 10000));; ) yield {; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }).check(); ```. ``` scala; forAll(Gen.choose(0, 10000)) { (j: Int) =>; forAll(Gen.choose(0, 10000)) { (k: Int) =>; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }; }.check(); ```. I think I can ditch the `toProp` on the do notation with an implicit conversion. I might be able to support either syntax in a unified way, but I haven't found the time to think about it. There's a little bit of weirdness because you only want `check` to be callable on things that are `Boolean`-valued. The difference between this monad and the `Gen[T]` monad is that this one is a reader monad, collecting a stack of ""read"" variables that can be used by the inner most `forAll` to generate a useful check-failure message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-244517801
https://github.com/hail-is/hail/issues/400#issuecomment-244517801:982,Integrability,message,message,982,"Options for nested `forAll`:. ``` scala; toProp(for (; j <- forAll(Gen.choose(0, 10000));; k <- forAll(Gen.choose(0, 10000));; ) yield {; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }).check(); ```. ``` scala; forAll(Gen.choose(0, 10000)) { (j: Int) =>; forAll(Gen.choose(0, 10000)) { (k: Int) =>; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }; }.check(); ```. I think I can ditch the `toProp` on the do notation with an implicit conversion. I might be able to support either syntax in a unified way, but I haven't found the time to think about it. There's a little bit of weirdness because you only want `check` to be callable on things that are `Boolean`-valued. The difference between this monad and the `Gen[T]` monad is that this one is a reader monad, collecting a stack of ""read"" variables that can be used by the inner most `forAll` to generate a useful check-failure message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-244517801
https://github.com/hail-is/hail/issues/400#issuecomment-244517801:893,Modifiability,variab,variables,893,"Options for nested `forAll`:. ``` scala; toProp(for (; j <- forAll(Gen.choose(0, 10000));; k <- forAll(Gen.choose(0, 10000));; ) yield {; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }).check(); ```. ``` scala; forAll(Gen.choose(0, 10000)) { (j: Int) =>; forAll(Gen.choose(0, 10000)) { (k: Int) =>; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }; }.check(); ```. I think I can ditch the `toProp` on the do notation with an implicit conversion. I might be able to support either syntax in a unified way, but I haven't found the time to think about it. There's a little bit of weirdness because you only want `check` to be callable on things that are `Boolean`-valued. The difference between this monad and the `Gen[T]` monad is that this one is a reader monad, collecting a stack of ""read"" variables that can be used by the inner most `forAll` to generate a useful check-failure message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-244517801
https://github.com/hail-is/hail/pull/406#issuecomment-232000412:83,Integrability,depend,dependency,83,Will split out the disk representation aspect once the other part goes in (one-way dependency),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/406#issuecomment-232000412
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:53,Deployability,install,install,53,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:477,Deployability,install,install,477,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:617,Deployability,install,install,617,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:1087,Deployability,install,install,1087,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:132,Modifiability,config,config,132,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:555,Modifiability,config,config,555,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:741,Modifiability,config,config,741,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:4,Testability,test,tested,4,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224702021:1184,Testability,test,test,1184,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021
https://github.com/hail-is/hail/pull/414#issuecomment-224707193:0,Deployability,Update,Updated,0,Updated. Script now needs `--force` on the ab initio VEP calls.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224707193
https://github.com/hail-is/hail/pull/414#issuecomment-224777755:19,Integrability,message,message,19,I removed the info message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224777755
https://github.com/hail-is/hail/pull/422#issuecomment-235642374:405,Integrability,depend,dependencies,405,"Couple of things:. You left in a bunch of commented out blocks that need to get cleaned up. You need to squash the history. That might be hard. The solution is to merge the current master, create a diff between the resulting version and master, and apply that to a fresh copy of master and then PR against that (or force this branch to the resulting commit). Finally, I'd like you to break out the gradle dependencies as a separate PR first. I need that for the seqr stuff ASAP. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/422#issuecomment-235642374
https://github.com/hail-is/hail/pull/423#issuecomment-226914280:92,Modifiability,refactor,refactoring,92,"I have the FET in the expr language done. Also in this branch is the linear regression code refactoring. I removed the docs for the group tests, but left the code for creating groups and the FET and linear regression group tests in this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/423#issuecomment-226914280
https://github.com/hail-is/hail/pull/423#issuecomment-226914280:138,Testability,test,tests,138,"I have the FET in the expr language done. Also in this branch is the linear regression code refactoring. I removed the docs for the group tests, but left the code for creating groups and the FET and linear regression group tests in this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/423#issuecomment-226914280
https://github.com/hail-is/hail/pull/423#issuecomment-226914280:223,Testability,test,tests,223,"I have the FET in the expr language done. Also in this branch is the linear regression code refactoring. I removed the docs for the group tests, but left the code for creating groups and the FET and linear regression group tests in this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/423#issuecomment-226914280
https://github.com/hail-is/hail/pull/426#issuecomment-226536643:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/426#issuecomment-226536643
https://github.com/hail-is/hail/pull/426#issuecomment-229917065:5,Deployability,update,updated,5,"I've updated this with the code from Hadoop-BAM (in 7.6.0), so it's ready for review. Can you take a look please @cseed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/426#issuecomment-229917065
https://github.com/hail-is/hail/pull/438#issuecomment-227626931:154,Modifiability,refactor,refactor,154,"@cseed: I feel somewhat guilty about duplicating a large amount of `AnnotateVariantsTable` code here, almost all of it. However, it isn't totally easy to refactor. I'm also happy to rename ""GenomicIndex"" to something better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/438#issuecomment-227626931
https://github.com/hail-is/hail/pull/438#issuecomment-232252468:147,Modifiability,rewrite,rewrite,147,"This guy will be totally retooled with the table changes, so I'll wait for that to go in too. The GenomicIndex here actually predated the interval rewrite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/438#issuecomment-232252468
https://github.com/hail-is/hail/issues/439#issuecomment-227984589:0,Integrability,Depend,Depends,0,Depends on #406,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/439#issuecomment-227984589
https://github.com/hail-is/hail/issues/439#issuecomment-316240428:161,Testability,test,tested,161,We have removed Kudu support in the 0.2 branch pending Kudu support for arrays (see https://issues.apache.org/jira/browse/KUDU-1261) and some support to have it tested/maintained.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/439#issuecomment-316240428
https://github.com/hail-is/hail/issues/440#issuecomment-316240418:161,Testability,test,tested,161,We have removed Kudu support in the 0.2 branch pending Kudu support for arrays (see https://issues.apache.org/jira/browse/KUDU-1261) and some support to have it tested/maintained.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/440#issuecomment-316240418
https://github.com/hail-is/hail/pull/441#issuecomment-231229379:8,Testability,benchmark,benchmarking,8,"I did a benchmarking experiment.; My inputs:; **VDS**: vds created from `seq 1 100 300000000` mapped to ""1 $i A T NA"", and imported with `hail importannotations`. This is a 3M variant sites vds.; **interval file**: exome capture regions provided by Monkol. Seems like pretty random non-overlapping interval over chr1. 39K variants fall in these intervals, slightly over 1%. My cmd line:; `hail read -i <vds> filtervariants intervals --keep -i <interval list> count`, with the read/filter/count repeated 10 times. I ran this on the current master and this branch on one core on my laptop. I then calculated the time per iteration. **Benchmark results**; current master: 4.876s, with a std dev of 0.18; this branch: 4.716s, with a std dev of 0.18; mean ratio: 0.967. **Conclusion**: this branch is faster than master for interval queries by a minimum of 3-4%",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/441#issuecomment-231229379
https://github.com/hail-is/hail/pull/441#issuecomment-231229379:632,Testability,Benchmark,Benchmark,632,"I did a benchmarking experiment.; My inputs:; **VDS**: vds created from `seq 1 100 300000000` mapped to ""1 $i A T NA"", and imported with `hail importannotations`. This is a 3M variant sites vds.; **interval file**: exome capture regions provided by Monkol. Seems like pretty random non-overlapping interval over chr1. 39K variants fall in these intervals, slightly over 1%. My cmd line:; `hail read -i <vds> filtervariants intervals --keep -i <interval list> count`, with the read/filter/count repeated 10 times. I ran this on the current master and this branch on one core on my laptop. I then calculated the time per iteration. **Benchmark results**; current master: 4.876s, with a std dev of 0.18; this branch: 4.716s, with a std dev of 0.18; mean ratio: 0.967. **Conclusion**: this branch is faster than master for interval queries by a minimum of 3-4%",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/441#issuecomment-231229379
https://github.com/hail-is/hail/issues/442#issuecomment-279578856:34,Deployability,update,update,34,"relates to this (which we need to update to Python, I made a separate issue):; http://discuss.hail.is/t/save-pcs-for-projection/46. There are several issues requesting more flexible PCA at high abstraction level. We should make a game plan.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-279578856
https://github.com/hail-is/hail/issues/442#issuecomment-279578856:173,Modifiability,flexible,flexible,173,"relates to this (which we need to update to Python, I made a separate issue):; http://discuss.hail.is/t/save-pcs-for-projection/46. There are several issues requesting more flexible PCA at high abstraction level. We should make a game plan.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-279578856
https://github.com/hail-is/hail/issues/442#issuecomment-587074535:330,Performance,load,loadings,330,"Out of curiosity, what was the reason that this kind of functionality shouldn't be a core method? . I saw [pc_project](https://github.com/macarthur-lab/gnomad_hail/blob/537cb9dd19c4a854a9ec7f29e552129081598399/utils/generic.py#L105) (mentioned in this [thread](https://discuss.hail.is/t/pca-to-output-allele-frequencies-alongside-loadings/439/4)) and that function stands out to me amongst all the gnomad other utilities as being particularly useful for many applications. Separately, where do most contrib functions end up? Other than gnomad, would you recommend any good collections of generic functionality?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-587074535
https://github.com/hail-is/hail/issues/442#issuecomment-613650798:21,Energy Efficiency,schedul,scheduled,21,Moved to Asana to be scheduled.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-613650798
https://github.com/hail-is/hail/issues/445#issuecomment-422365142:54,Security,expose,exposed,54,"Yes I agree, this is something the user should not be exposed. . > On Sep 18, 2018, at 8:00 AM, Tim Poterba <notifications@github.com> wrote:; > ; > I think we need to hide partitioning and do more automatic resizing / partition combining.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub <https://github.com/hail-is/hail/issues/445#issuecomment-422364795>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADIkAiuXQNQQSrESG44JjWYXH62FVg0Fks5ucOB2gaJpZM4I995P>.; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/445#issuecomment-422365142
https://github.com/hail-is/hail/pull/446#issuecomment-228498489:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-228498489
https://github.com/hail-is/hail/pull/446#issuecomment-228735638:148,Performance,perform,performance,148,"Another possible worry -- the partitioning stays the same as before `head`, right? Having thousands of empty partitions around is going to have bad performance characteristics for other modules, I think. Why does Spark make this so hard?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-228735638
https://github.com/hail-is/hail/pull/446#issuecomment-234642054:498,Availability,error,error,498,"Hey @cseed,. I tried running it as I need a test version of the 5.5K WGS data but it fails:; `hail-spark-lf read -i MacArthur_Merck_Finns.vds head --keep 10000 write -o MacArthur_Merck_Finns.head.vds; hail: info: running: read -i MacArthur_Merck_Finns.vds; [Stage 0:======================================================>(134 + 1) / 135]hail: info: running: head --keep 10000; hail: info: running: write -o MacArthur_Merck_Finns.head.vds; hail: write: caught exception: Job aborted.`. Got the same error on both dataflow and Cray. Also, my implementation somehow fails on Cray (different error) but not on dataflow....yay!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642054
https://github.com/hail-is/hail/pull/446#issuecomment-234642054:588,Availability,error,error,588,"Hey @cseed,. I tried running it as I need a test version of the 5.5K WGS data but it fails:; `hail-spark-lf read -i MacArthur_Merck_Finns.vds head --keep 10000 write -o MacArthur_Merck_Finns.head.vds; hail: info: running: read -i MacArthur_Merck_Finns.vds; [Stage 0:======================================================>(134 + 1) / 135]hail: info: running: head --keep 10000; hail: info: running: write -o MacArthur_Merck_Finns.head.vds; hail: write: caught exception: Job aborted.`. Got the same error on both dataflow and Cray. Also, my implementation somehow fails on Cray (different error) but not on dataflow....yay!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642054
https://github.com/hail-is/hail/pull/446#issuecomment-234642054:474,Safety,abort,aborted,474,"Hey @cseed,. I tried running it as I need a test version of the 5.5K WGS data but it fails:; `hail-spark-lf read -i MacArthur_Merck_Finns.vds head --keep 10000 write -o MacArthur_Merck_Finns.head.vds; hail: info: running: read -i MacArthur_Merck_Finns.vds; [Stage 0:======================================================>(134 + 1) / 135]hail: info: running: head --keep 10000; hail: info: running: write -o MacArthur_Merck_Finns.head.vds; hail: write: caught exception: Job aborted.`. Got the same error on both dataflow and Cray. Also, my implementation somehow fails on Cray (different error) but not on dataflow....yay!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642054
https://github.com/hail-is/hail/pull/446#issuecomment-234642054:44,Testability,test,test,44,"Hey @cseed,. I tried running it as I need a test version of the 5.5K WGS data but it fails:; `hail-spark-lf read -i MacArthur_Merck_Finns.vds head --keep 10000 write -o MacArthur_Merck_Finns.head.vds; hail: info: running: read -i MacArthur_Merck_Finns.vds; [Stage 0:======================================================>(134 + 1) / 135]hail: info: running: head --keep 10000; hail: info: running: write -o MacArthur_Merck_Finns.head.vds; hail: write: caught exception: Job aborted.`. Got the same error on both dataflow and Cray. Also, my implementation somehow fails on Cray (different error) but not on dataflow....yay!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642054
https://github.com/hail-is/hail/pull/446#issuecomment-234642057:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642057
https://github.com/hail-is/hail/pull/446#issuecomment-234642726:16,Testability,log,log,16,Do you have the log handy?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642726
https://github.com/hail-is/hail/pull/446#issuecomment-234979777:32,Testability,log,log,32,"Just re-ran it on the Cray. The log is here:; /home/users/lfran/hail.head.log. On Fri, Jul 22, 2016 at 4:05 PM, cseed notifications@github.com wrote:. > Do you have the log handy?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/446#issuecomment-234642726,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgUdytKyjwJ5r3pp3-yEam3Xc6-xQks5qYSJ4gaJpZM4I-Oz9; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234979777
https://github.com/hail-is/hail/pull/446#issuecomment-234979777:74,Testability,log,log,74,"Just re-ran it on the Cray. The log is here:; /home/users/lfran/hail.head.log. On Fri, Jul 22, 2016 at 4:05 PM, cseed notifications@github.com wrote:. > Do you have the log handy?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/446#issuecomment-234642726,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgUdytKyjwJ5r3pp3-yEam3Xc6-xQks5qYSJ4gaJpZM4I-Oz9; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234979777
https://github.com/hail-is/hail/pull/446#issuecomment-234979777:169,Testability,log,log,169,"Just re-ran it on the Cray. The log is here:; /home/users/lfran/hail.head.log. On Fri, Jul 22, 2016 at 4:05 PM, cseed notifications@github.com wrote:. > Do you have the log handy?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/446#issuecomment-234642726,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgUdytKyjwJ5r3pp3-yEam3Xc6-xQks5qYSJ4gaJpZM4I-Oz9; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234979777
https://github.com/hail-is/hail/pull/446#issuecomment-235344742:557,Testability,test,tests,557,"@cseed, so I looked into it this morning and somehow your implementation failed as prev.partitions always returns null in this context (not sure why). I modified the implementation of HeadRDD and hopefully you'll like it: It can now either return an approximately the number of variants based on the number of variants in the 1st partition or an exact number of variants with a pass to compute the number of variants (in parallel and using approximation from the 1st partition -- so pretty fast). In any case, it never goes through the whole dataset and my tests indicate that it's pretty fast (see below for ExAC on Cray). Also, it retains the same partitioning as the parent RDD. Approximate version:; nSamples 60,706; nVariants 11,804; hail: info: timing:; read: 7.579s; head: 5.413s; count: 47.792s. Exact version:; nSamples 60,706; nVariants 10,000; hail: info: timing:; read: 4.472s; head: 5.283s; count: 54.490s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-235344742
https://github.com/hail-is/hail/pull/446#issuecomment-237269843:211,Testability,test,tests,211,"I made some mostly minor, stylistic comments. Back to you!. Also, there are conflicts so you will need to rebase against the current master. You'll also need to do that to pick up ""Jenkinsfile"" so the automated tests get run. Finally, we're trying to keep the git history clean, so single conceptual changes should go in as single commits. Can you squash all these commits into a single one? (Use `git rebase -i` and squash.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-237269843
https://github.com/hail-is/hail/pull/450#issuecomment-229020804:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/450#issuecomment-229020804
https://github.com/hail-is/hail/pull/450#issuecomment-232474916:102,Integrability,interface,interface,102,"@tomwhite I rebased and modified the code to use the recently added `AnnotationImpex` (import/export) interface. I ran the tests again the quickstart and it looks good. If you're happy with the changes, I'll merge it in. Is there plan for arrays in Kudu? We might consider serializing as JSON, say, rather than the fixed array size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/450#issuecomment-232474916
https://github.com/hail-is/hail/pull/450#issuecomment-232474916:123,Testability,test,tests,123,"@tomwhite I rebased and modified the code to use the recently added `AnnotationImpex` (import/export) interface. I ran the tests again the quickstart and it looks good. If you're happy with the changes, I'll merge it in. Is there plan for arrays in Kudu? We might consider serializing as JSON, say, rather than the fixed array size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/450#issuecomment-232474916
https://github.com/hail-is/hail/issues/453#issuecomment-229750270:25,Availability,error,error,25,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270
https://github.com/hail-is/hail/issues/453#issuecomment-229750270:221,Deployability,Install,Install,221,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270
https://github.com/hail-is/hail/issues/453#issuecomment-229750270:244,Deployability,install,install,244,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270
https://github.com/hail-is/hail/issues/453#issuecomment-229750270:261,Deployability,Install,Install,261,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270
https://github.com/hail-is/hail/issues/453#issuecomment-229750270:1308,Deployability,install,installDist,1308,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270
https://github.com/hail-is/hail/issues/453#issuecomment-229750270:862,Security,hash,hash,862,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270
https://github.com/hail-is/hail/issues/453#issuecomment-229752430:105,Availability,error,error,105,"I just tried cloned locally and `gradle installDist` worked on the first try. We've gotten this compiler error sporadically for a few months, and every time it's been resolved by rebuilding after `gradle clean`. I'll continue to investigate, and see if I can find the source of the problem (could be a compiler bug).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229752430
https://github.com/hail-is/hail/issues/453#issuecomment-229752430:40,Deployability,install,installDist,40,"I just tried cloned locally and `gradle installDist` worked on the first try. We've gotten this compiler error sporadically for a few months, and every time it's been resolved by rebuilding after `gradle clean`. I'll continue to investigate, and see if I can find the source of the problem (could be a compiler bug).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229752430
https://github.com/hail-is/hail/issues/453#issuecomment-229753452:38,Deployability,update,updated,38,"Hi, thanks for the quick response!. I updated the original post with more info to reproduce.; Please let me know if I can get you more information about the Scala environment!. All the best,; Michael",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229753452
https://github.com/hail-is/hail/pull/455#issuecomment-232711401:44,Testability,test,test,44,"Back to you. Overall, looks great, esp. the test vs R.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/455#issuecomment-232711401
https://github.com/hail-is/hail/issues/457#issuecomment-229943619:83,Deployability,install,install,83,"These three tests call out to [Plink](https://www.cog-genomics.org/plink2). If you install it from that link and add it to your global path, those tests will pass. If that sounds annoying, you can trust that these tests pass on our end!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-229943619
https://github.com/hail-is/hail/issues/457#issuecomment-229943619:12,Testability,test,tests,12,"These three tests call out to [Plink](https://www.cog-genomics.org/plink2). If you install it from that link and add it to your global path, those tests will pass. If that sounds annoying, you can trust that these tests pass on our end!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-229943619
https://github.com/hail-is/hail/issues/457#issuecomment-229943619:147,Testability,test,tests,147,"These three tests call out to [Plink](https://www.cog-genomics.org/plink2). If you install it from that link and add it to your global path, those tests will pass. If that sounds annoying, you can trust that these tests pass on our end!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-229943619
https://github.com/hail-is/hail/issues/457#issuecomment-229943619:214,Testability,test,tests,214,"These three tests call out to [Plink](https://www.cog-genomics.org/plink2). If you install it from that link and add it to your global path, those tests will pass. If that sounds annoying, you can trust that these tests pass on our end!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-229943619
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:878,Availability,error,errors,878,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:23,Deployability,install,installed,23,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:200,Testability,test,test,200,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:909,Testability,test,test,909,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:965,Testability,test,testBiallelic,965,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1055,Testability,test,test,1055,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1061,Testability,Test,Test,1061,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1073,Testability,test,test,1073,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1146,Testability,test,test,1146,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1193,Testability,test,test,1193,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1266,Testability,test,test,1266,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1272,Testability,Test,Test,1272,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1284,Testability,test,testGenotypeStream,1284,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1379,Testability,test,test,1379,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1433,Testability,test,testImputeSexPlinkVersion,1433,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1533,Testability,test,test,1533,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1539,Testability,Test,Test,1539,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230191234:1551,Testability,test,test,1551,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234
https://github.com/hail-is/hail/issues/457#issuecomment-230192783:65,Availability,error,error,65,"Hi,; When I remove plink from my path I get a bit of a different error. Can you rerun `gradle check` with the `--info` argument? It'll vomit a bunch of details, but the output from those tests should tell us what's going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230192783
https://github.com/hail-is/hail/issues/457#issuecomment-230192783:187,Testability,test,tests,187,"Hi,; When I remove plink from my path I get a bit of a different error. Can you rerun `gradle check` with the `--info` argument? It'll vomit a bunch of details, but the output from those tests should tell us what's going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230192783
https://github.com/hail-is/hail/issues/457#issuecomment-230213155:159,Availability,error,error,159,"@tpoterba . Hi Tim ,Thank you for answering , I run the ""gradle check --info"" again , there is a lot of infomation, so I save the standard output and standard error in ""check_info_1.txt"",as attached. Thank you for your help. [check_info_1.txt](https://github.com/broadinstitute/hail/files/345679/check_info_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230213155
https://github.com/hail-is/hail/issues/457#issuecomment-230288836:109,Availability,fault,fault,109,"I think I know the problem! We are testing against Plink 1.9, but you have the old version 1.07 (which is my fault for linking the plink base page). Install it from the link below and please try again:. [https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230288836
https://github.com/hail-is/hail/issues/457#issuecomment-230288836:149,Deployability,Install,Install,149,"I think I know the problem! We are testing against Plink 1.9, but you have the old version 1.07 (which is my fault for linking the plink base page). Install it from the link below and please try again:. [https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230288836
https://github.com/hail-is/hail/issues/457#issuecomment-230288836:35,Testability,test,testing,35,"I think I know the problem! We are testing against Plink 1.9, but you have the old version 1.07 (which is my fault for linking the plink base page). Install it from the link below and please try again:. [https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230288836
https://github.com/hail-is/hail/issues/457#issuecomment-230289474:574,Performance,cache,cached,574,"For reference, here was the plink output from one of those tests:. ```. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); Recent cached web-check found...Problem connecting to web. Writing this text to log file [ /tmp/hail.3ouc7OzAKpSQ/plink.00001.log ]; Analysis started: Mon Jul 4 11:38:41 2016. ** Unused command line option: --vcf; ** Unused command line option: src/test/resources/sample.vcf; ** Unused command line option: --const-fid; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230289474
https://github.com/hail-is/hail/issues/457#issuecomment-230289474:59,Testability,test,tests,59,"For reference, here was the plink output from one of those tests:. ```. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); Recent cached web-check found...Problem connecting to web. Writing this text to log file [ /tmp/hail.3ouc7OzAKpSQ/plink.00001.log ]; Analysis started: Mon Jul 4 11:38:41 2016. ** Unused command line option: --vcf; ** Unused command line option: src/test/resources/sample.vcf; ** Unused command line option: --const-fid; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230289474
https://github.com/hail-is/hail/issues/457#issuecomment-230289474:647,Testability,log,log,647,"For reference, here was the plink output from one of those tests:. ```. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); Recent cached web-check found...Problem connecting to web. Writing this text to log file [ /tmp/hail.3ouc7OzAKpSQ/plink.00001.log ]; Analysis started: Mon Jul 4 11:38:41 2016. ** Unused command line option: --vcf; ** Unused command line option: src/test/resources/sample.vcf; ** Unused command line option: --const-fid; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230289474
https://github.com/hail-is/hail/issues/457#issuecomment-230289474:693,Testability,log,log,693,"For reference, here was the plink output from one of those tests:. ```. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); Recent cached web-check found...Problem connecting to web. Writing this text to log file [ /tmp/hail.3ouc7OzAKpSQ/plink.00001.log ]; Analysis started: Mon Jul 4 11:38:41 2016. ** Unused command line option: --vcf; ** Unused command line option: src/test/resources/sample.vcf; ** Unused command line option: --const-fid; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230289474
https://github.com/hail-is/hail/issues/457#issuecomment-230289474:816,Testability,test,test,816,"For reference, here was the plink output from one of those tests:. ```. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); Recent cached web-check found...Problem connecting to web. Writing this text to log file [ /tmp/hail.3ouc7OzAKpSQ/plink.00001.log ]; Analysis started: Mon Jul 4 11:38:41 2016. ** Unused command line option: --vcf; ** Unused command line option: src/test/resources/sample.vcf; ** Unused command line option: --const-fid; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230289474
https://github.com/hail-is/hail/issues/457#issuecomment-230429438:962,Availability,echo,echo,962,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438
https://github.com/hail-is/hail/issues/457#issuecomment-230429438:302,Deployability,install,install,302,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438
https://github.com/hail-is/hail/issues/457#issuecomment-230429438:522,Performance,load,load,522,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438
https://github.com/hail-is/hail/issues/457#issuecomment-230429438:338,Testability,test,test,338,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438
https://github.com/hail-is/hail/issues/457#issuecomment-230429438:428,Testability,test,test,428,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438
https://github.com/hail-is/hail/issues/457#issuecomment-230429438:611,Testability,log,logger,611,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438
https://github.com/hail-is/hail/issues/457#issuecomment-230429438:767,Testability,test,test,767,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438
https://github.com/hail-is/hail/issues/457#issuecomment-230431412:72,Deployability,deploy,deployment,72,"This SLF4J warning is nothing to be worried about -- we see that in our deployment here, and it doesn't affect the running or results in any way. I'll create an issue to resolve this though. Another note about running Hail: if you're running on one machine, using the script in `hail/build/install/hail/bin/hail` created by `gradle installDist` is the thing to do. But if you're deploying on a spark cluster, you're going to need to do the following:. `gradle shadowJar` : this builds a jar in `hail/build/libs/hail-all-spark.jar`, which can be run in parallel using the `spark-submit` command as below:. ```; /usr/bin/spark-submit --executor-memory 48g --executor-cores 24 --class org.broadinstitute.hail.driver.Main ~/hail/build/libs/hail-all-spark.jar --master yarn-client <hail commands here>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230431412
https://github.com/hail-is/hail/issues/457#issuecomment-230431412:290,Deployability,install,install,290,"This SLF4J warning is nothing to be worried about -- we see that in our deployment here, and it doesn't affect the running or results in any way. I'll create an issue to resolve this though. Another note about running Hail: if you're running on one machine, using the script in `hail/build/install/hail/bin/hail` created by `gradle installDist` is the thing to do. But if you're deploying on a spark cluster, you're going to need to do the following:. `gradle shadowJar` : this builds a jar in `hail/build/libs/hail-all-spark.jar`, which can be run in parallel using the `spark-submit` command as below:. ```; /usr/bin/spark-submit --executor-memory 48g --executor-cores 24 --class org.broadinstitute.hail.driver.Main ~/hail/build/libs/hail-all-spark.jar --master yarn-client <hail commands here>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230431412
https://github.com/hail-is/hail/issues/457#issuecomment-230431412:332,Deployability,install,installDist,332,"This SLF4J warning is nothing to be worried about -- we see that in our deployment here, and it doesn't affect the running or results in any way. I'll create an issue to resolve this though. Another note about running Hail: if you're running on one machine, using the script in `hail/build/install/hail/bin/hail` created by `gradle installDist` is the thing to do. But if you're deploying on a spark cluster, you're going to need to do the following:. `gradle shadowJar` : this builds a jar in `hail/build/libs/hail-all-spark.jar`, which can be run in parallel using the `spark-submit` command as below:. ```; /usr/bin/spark-submit --executor-memory 48g --executor-cores 24 --class org.broadinstitute.hail.driver.Main ~/hail/build/libs/hail-all-spark.jar --master yarn-client <hail commands here>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230431412
https://github.com/hail-is/hail/issues/457#issuecomment-230431412:379,Deployability,deploy,deploying,379,"This SLF4J warning is nothing to be worried about -- we see that in our deployment here, and it doesn't affect the running or results in any way. I'll create an issue to resolve this though. Another note about running Hail: if you're running on one machine, using the script in `hail/build/install/hail/bin/hail` created by `gradle installDist` is the thing to do. But if you're deploying on a spark cluster, you're going to need to do the following:. `gradle shadowJar` : this builds a jar in `hail/build/libs/hail-all-spark.jar`, which can be run in parallel using the `spark-submit` command as below:. ```; /usr/bin/spark-submit --executor-memory 48g --executor-cores 24 --class org.broadinstitute.hail.driver.Main ~/hail/build/libs/hail-all-spark.jar --master yarn-client <hail commands here>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230431412
https://github.com/hail-is/hail/pull/462#issuecomment-232252890:243,Integrability,interface,interface,243,"I haven't looked at this yet, I have a new use case in seqr for the table code. I need to be able to load data as `RDD[Annotation]` without pulling out a sample/variant key (or pull out a totally custom key). Can I do that easily with the new interface?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232252890
https://github.com/hail-is/hail/pull/462#issuecomment-232252890:101,Performance,load,load,101,"I haven't looked at this yet, I have a new use case in seqr for the table code. I need to be able to load data as `RDD[Annotation]` without pulling out a sample/variant key (or pull out a totally custom key). Can I do that easily with the new interface?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232252890
https://github.com/hail-is/hail/pull/462#issuecomment-232253067:435,Integrability,interface,interface,435,"Yes totally. You'd just call ParsedLine.getValue. getKey would return an; empty array, since you passed nothing in. On Jul 13, 2016 12:21 AM, ""cseed"" notifications@github.com wrote:. > I haven't looked at this yet, I have a new use case in seqr for the table; > code. I need to be able to load data as RDD[Annotation] without pulling; > out a sample/variant key (or pull out a totally custom key). Can I do that; > easily with the new interface?; > ; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/462#issuecomment-232252890,; > or mute the thread; > https://github.com/notifications/unsubscribe/AKEs6uAzeg--b5QgvgWITjuIpY51afyEks5qVGesgaJpZM4JGtqM; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232253067
https://github.com/hail-is/hail/pull/462#issuecomment-232253067:289,Performance,load,load,289,"Yes totally. You'd just call ParsedLine.getValue. getKey would return an; empty array, since you passed nothing in. On Jul 13, 2016 12:21 AM, ""cseed"" notifications@github.com wrote:. > I haven't looked at this yet, I have a new use case in seqr for the table; > code. I need to be able to load data as RDD[Annotation] without pulling; > out a sample/variant key (or pull out a totally custom key). Can I do that; > easily with the new interface?; > ; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/462#issuecomment-232252890,; > or mute the thread; > https://github.com/notifications/unsubscribe/AKEs6uAzeg--b5QgvgWITjuIpY51afyEks5qVGesgaJpZM4JGtqM; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232253067
https://github.com/hail-is/hail/pull/462#issuecomment-232471446:0,Deployability,Update,Updated,0,Updated and ready for review,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232471446
https://github.com/hail-is/hail/pull/462#issuecomment-232740494:9,Integrability,interface,interface,9,"Proposed interface changes:. class TextTableConfiguration. class TextTableReader. TextTableReader(conf); TextTableReader(delimiter = ""#"", ...). // only read fields ; TextTableReader.read(columnTypes: Map[String, Type], path: String, [select]): (TStruct, RDD[Annotation]). (and JSON). for JSON:. JSONReader.read(t: Type, path: String): RDD[Annotation]. in expr language:. support. Variant(""chr:pos:ref:alt1,...,altN"") . (so Variant(v.toString) == v) and. Variant(chr: String, pos: Int, ref: String, alts: Array[String]). Then we can do:. annotatevariants table -v 'Variant(Chrom, Pos, Ref, Alts.split("",""))'. annotatevariants table -v 'Variant(Variant)'. To get this behavior, you'll have to build the EvalContext from the table type. Add. TStruct.filter(predicate: (Field) => Boolean): (TStruct, Filterer). where. type Filterer = (Annotation) => Annotation. This should make implementating importvariants table simple and elegant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232740494
https://github.com/hail-is/hail/pull/462#issuecomment-232740494:911,Usability,simpl,simple,911,"Proposed interface changes:. class TextTableConfiguration. class TextTableReader. TextTableReader(conf); TextTableReader(delimiter = ""#"", ...). // only read fields ; TextTableReader.read(columnTypes: Map[String, Type], path: String, [select]): (TStruct, RDD[Annotation]). (and JSON). for JSON:. JSONReader.read(t: Type, path: String): RDD[Annotation]. in expr language:. support. Variant(""chr:pos:ref:alt1,...,altN"") . (so Variant(v.toString) == v) and. Variant(chr: String, pos: Int, ref: String, alts: Array[String]). Then we can do:. annotatevariants table -v 'Variant(Chrom, Pos, Ref, Alts.split("",""))'. annotatevariants table -v 'Variant(Variant)'. To get this behavior, you'll have to build the EvalContext from the table type. Add. TStruct.filter(predicate: (Field) => Boolean): (TStruct, Filterer). where. type Filterer = (Annotation) => Annotation. This should make implementating importvariants table simple and elegant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232740494
https://github.com/hail-is/hail/pull/462#issuecomment-232824524:121,Availability,error,error,121,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524
https://github.com/hail-is/hail/pull/462#issuecomment-232824524:450,Availability,error,error,450,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524
https://github.com/hail-is/hail/pull/462#issuecomment-232824524:36,Integrability,interface,interface,36,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524
https://github.com/hail-is/hail/pull/462#issuecomment-232824524:127,Integrability,message,messages,127,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524
https://github.com/hail-is/hail/pull/462#issuecomment-232824524:231,Integrability,interface,interface,231,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524
https://github.com/hail-is/hail/pull/462#issuecomment-232825352:117,Integrability,interface,interface,117,"I also think a fixed result type of RDD will be a problem. My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I think there are a lot of strengths of generating a `(String) => Annotation` parse function, and then letting the client code use that however it likes. Including `sc.textFileLine(...)` in one read function prevents only a little bit of code duplication, and causes major headaches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232825352
https://github.com/hail-is/hail/pull/462#issuecomment-233005833:242,Availability,error,error,242,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833
https://github.com/hail-is/hail/pull/462#issuecomment-233005833:263,Availability,error,error,263,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833
https://github.com/hail-is/hail/pull/462#issuecomment-233005833:269,Integrability,message,message,269,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833
https://github.com/hail-is/hail/pull/462#issuecomment-233005833:453,Integrability,interface,interface,453,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833
https://github.com/hail-is/hail/pull/462#issuecomment-233005833:161,Security,validat,validation,161,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833
https://github.com/hail-is/hail/pull/462#issuecomment-233007740:89,Availability,error,error-catching,89,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740
https://github.com/hail-is/hail/pull/462#issuecomment-233007740:104,Integrability,wrap,wrapper,104,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740
https://github.com/hail-is/hail/pull/462#issuecomment-233007740:318,Modifiability,config,config,318,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740
https://github.com/hail-is/hail/pull/462#issuecomment-233007740:20,Security,validat,validation,20,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740
https://github.com/hail-is/hail/pull/462#issuecomment-233012302:655,Availability,error,error,655,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302
https://github.com/hail-is/hail/pull/462#issuecomment-233012302:19,Integrability,interface,interface,19,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302
https://github.com/hail-is/hail/pull/462#issuecomment-233012302:254,Integrability,wrap,wrapError,254,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302
https://github.com/hail-is/hail/pull/462#issuecomment-233012302:311,Integrability,wrap,wrapError,311,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302
https://github.com/hail-is/hail/pull/462#issuecomment-233012302:448,Integrability,wrap,wrapError,448,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302
https://github.com/hail-is/hail/pull/462#issuecomment-233012302:421,Modifiability,extend,extends,421,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302
https://github.com/hail-is/hail/pull/462#issuecomment-233012302:651,Testability,log,log,651,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:86,Availability,error,error,86,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:700,Availability,error,error-catching,700,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:841,Availability,error,error,841,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:871,Availability,error,errors,871,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:967,Availability,error,error,967,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:1061,Availability,error,error,1061,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:1130,Availability,error,errors,1130,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:1420,Availability,error,errors,1420,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:92,Integrability,message,messages,92,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:261,Integrability,interface,interface,261,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:323,Integrability,interface,interface,323,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:715,Integrability,wrap,wrapper,715,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:759,Integrability,interface,interface,759,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:505,Performance,load,load,505,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:1100,Performance,concurren,concurrency,1100,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:2,Security,validat,validation,2,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233041581:1295,Usability,clear,clear,1295,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
https://github.com/hail-is/hail/pull/462#issuecomment-233967833:23,Deployability,update,updates,23,"I'm now happy with the updates, ready for another look",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233967833
https://github.com/hail-is/hail/pull/468#issuecomment-234586874:58,Availability,error,error,58,"Back to you. I didn't do the octal changes or the desired error handling, but I made an issue to do that when I have a little more time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/468#issuecomment-234586874
https://github.com/hail-is/hail/pull/480#issuecomment-233942160:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-233942160
https://github.com/hail-is/hail/pull/480#issuecomment-234027310:1776,Integrability,interoperab,interoperability,1776,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310
https://github.com/hail-is/hail/pull/480#issuecomment-234027310:1667,Performance,load,loading,1667,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310
https://github.com/hail-is/hail/pull/480#issuecomment-234027310:786,Usability,simpl,simple,786,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310
https://github.com/hail-is/hail/pull/480#issuecomment-234498769:778,Performance,optimiz,optimized,778,"> Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. I don't think there is a Hive equivalent unfortunately. > It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. In Hive a UDTF (user-defined table function; ImpEx naming is growing on me :) would allow you to return a genotype per row. Impala doesn't support UDTFs, but it should be possible to write a UDF that returns an array of genotypes. The UDF to do this could be written in Java so that Hive or Impala can use it; it's also possible to write an optimized version in C++ for Impala. On standard Parquet storage: thanks for the numbers. I don't have a good idea of how to close the gap. I ran some experiments with a standard Parquet representation (in Quince) but have now come to the conclusion that custom encoding of the genotypes is a pragmatic way of achieving the best performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234498769
https://github.com/hail-is/hail/pull/480#issuecomment-234498769:1107,Performance,perform,performance,1107,"> Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. I don't think there is a Hive equivalent unfortunately. > It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. In Hive a UDTF (user-defined table function; ImpEx naming is growing on me :) would allow you to return a genotype per row. Impala doesn't support UDTFs, but it should be possible to write a UDF that returns an array of genotypes. The UDF to do this could be written in Java so that Hive or Impala can use it; it's also possible to write an optimized version in C++ for Impala. On standard Parquet storage: thanks for the numbers. I don't have a good idea of how to close the gap. I ran some experiments with a standard Parquet representation (in Quince) but have now come to the conclusion that custom encoding of the genotypes is a pragmatic way of achieving the best performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234498769
https://github.com/hail-is/hail/pull/483#issuecomment-235277403:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/483#issuecomment-235277403
https://github.com/hail-is/hail/pull/484#issuecomment-234568962:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/484#issuecomment-234568962
https://github.com/hail-is/hail/pull/487#issuecomment-234624008:34,Deployability,patch,patch,34,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/487#issuecomment-234624008
https://github.com/hail-is/hail/pull/488#issuecomment-235089575:301,Testability,test,test,301,"Few things:. If we get rid of annotatevariants json, how do we deal with _0 problem? (Related, but not directly, to this PR.). To use this to export/reimport annotations, it seems not quite trivial to get the annotations back in the right place. What's the intended command line? Can we get that as a test case?. Otherwise, back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/488#issuecomment-235089575
https://github.com/hail-is/hail/pull/488#issuecomment-235098604:4,Deployability,pipeline,pipeline,4,The pipeline I posted doesn't leave _1 lying around -- `va = va._1` overwrites `va`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/488#issuecomment-235098604
https://github.com/hail-is/hail/issues/493#issuecomment-235044203:53,Availability,error,error,53,Also add test case for unterminated string with nice error message (use `interceptFatal`).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/493#issuecomment-235044203
https://github.com/hail-is/hail/issues/493#issuecomment-235044203:59,Integrability,message,message,59,Also add test case for unterminated string with nice error message (use `interceptFatal`).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/493#issuecomment-235044203
https://github.com/hail-is/hail/issues/493#issuecomment-235044203:9,Testability,test,test,9,Also add test case for unterminated string with nice error message (use `interceptFatal`).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/493#issuecomment-235044203
https://github.com/hail-is/hail/pull/499#issuecomment-235430971:44,Testability,log,logical,44,some arguments in favor:; 1) symmetry (more logical from dev standpoint); 2) future proofing (i'd bet there are labs focused on structural variation in Y that already phase in PAR); 3) this way we are very explicit about our definition of PAR on Y. Who knows how it was defined for any given data set (though our current settings are consistent with GATK).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235430971
https://github.com/hail-is/hail/pull/499#issuecomment-235602871:86,Integrability,interface,interface,86,I don't know why you'd ever check the position and not the chromosome. Leave the same interface but have them check the chromosome.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235602871
https://github.com/hail-is/hail/pull/499#issuecomment-235629724:400,Safety,avoid,avoid,400,"New version. In fact both uses in the code check on the chromosome and NOT in the PAR position, namely inNonParX (which is different from !inParX). And I think conceptually users like Kyle want both inParX and inNonParX in the expr langauge, rather than having to write `!inParX && contig == X` for the latter. I agree that inParXPos won't be used except through inParX and inNonParX but I wanted to avoid duplicating the intervals. as discussed in hail-dev, I also think we should change ""23"" to ""X"" on Plink import so that we are consistent on ""v.contig = X"" being the proper check throughout Hail. Then in ImputeSexPlink we can change:. ```; if (!includePar); (v.contig == ""X"" || v.contig == ""23"") && !v.inParXPos; else; v.contig == ""X"" || v.contig == ""23""; ```. to. ```; if (!includePar); v.inNonParX; else; v.contig == ""X""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235629724
https://github.com/hail-is/hail/pull/499#issuecomment-235660516:355,Deployability,update,update,355,"I don't like `inNonParX`. It will confuse people (it confused me). What about `inHemiX`? You're already using that in CopyState. So `(v.contig == ""X"" || v.contig == ""23"") == (inParX || inHemiX)`. Check the plink chromosome numbers in Variant until the reference fix goes in. The Pos versions are fine. When you change the expression language, you have to update the docs, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235660516
https://github.com/hail-is/hail/pull/499#issuecomment-235665809:10,Integrability,depend,depends,10,"`inHemiX` depends on sex. How about `inXPar` and `inXNonPar`, so that Non clearly just refers to Par? This reads as ""in the X pseudo-autosomal region"" or ""in the X non-pseudo-autosomal-region""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235665809
https://github.com/hail-is/hail/pull/499#issuecomment-235665809:74,Usability,clear,clearly,74,"`inHemiX` depends on sex. How about `inXPar` and `inXNonPar`, so that Non clearly just refers to Par? This reads as ""in the X pseudo-autosomal region"" or ""in the X non-pseudo-autosomal-region""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235665809
https://github.com/hail-is/hail/issues/509#issuecomment-237703639:202,Usability,simpl,simple,202,"Well, fraction reads supporting that alternate allele (`1-Ref` for the bi-allelic case, but not necessarily for the multi-allelic) - but that's possibly a separate discussion. GQ histogram I think is a simple application here. For each variant, a histogram of GQs - I think `gs.hist()` is the right idea though yes (this is an idea, does not exist yet, right?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/509#issuecomment-237703639
https://github.com/hail-is/hail/pull/511#issuecomment-236266169:37,Modifiability,extend,extend,37,"If/when the need is pressing, we can extend parsing to deal with both unnamed and named and optional args in full generality. With sort and sortBy, I handled the possibilities more directly and think the documentation is clear as is. I also assume the Boolean parameter is a constant rather than an expression handling null etc, but I think that covers the use cases of these functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236266169
https://github.com/hail-is/hail/pull/511#issuecomment-236266169:221,Usability,clear,clear,221,"If/when the need is pressing, we can extend parsing to deal with both unnamed and named and optional args in full generality. With sort and sortBy, I handled the possibilities more directly and think the documentation is clear as is. I also assume the Boolean parameter is a constant rather than an expression handling null etc, but I think that covers the use cases of these functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236266169
https://github.com/hail-is/hail/pull/511#issuecomment-236389770:39,Availability,error,error,39,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770
https://github.com/hail-is/hail/pull/511#issuecomment-236389770:45,Integrability,message,messages,45,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770
https://github.com/hail-is/hail/pull/511#issuecomment-236389770:24,Usability,simpl,simplified,24,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770
https://github.com/hail-is/hail/pull/517#issuecomment-236385122:289,Integrability,interface,interfaces,289,"I deleted the LinearRegressionFromHcsCommand and associated tests as it'd fallen out of sync with how hcs evolved for T2D, it's independent of the rest of the PR and I can add this functionality back later (at which point I imagine there will be other changes both to hcs and to the stats interfaces more generally).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/517#issuecomment-236385122
https://github.com/hail-is/hail/pull/517#issuecomment-236385122:106,Modifiability,evolve,evolved,106,"I deleted the LinearRegressionFromHcsCommand and associated tests as it'd fallen out of sync with how hcs evolved for T2D, it's independent of the rest of the PR and I can add this functionality back later (at which point I imagine there will be other changes both to hcs and to the stats interfaces more generally).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/517#issuecomment-236385122
https://github.com/hail-is/hail/pull/517#issuecomment-236385122:60,Testability,test,tests,60,"I deleted the LinearRegressionFromHcsCommand and associated tests as it'd fallen out of sync with how hcs evolved for T2D, it's independent of the rest of the PR and I can add this functionality back later (at which point I imagine there will be other changes both to hcs and to the stats interfaces more generally).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/517#issuecomment-236385122
https://github.com/hail-is/hail/pull/517#issuecomment-236386986:42,Testability,test,test,42,This is failing because the server in the test tries to use the same port (8080) as Jenkins. I'm going to move Jenkins.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/517#issuecomment-236386986
https://github.com/hail-is/hail/issues/520#issuecomment-236401286:242,Availability,error,error,242,"I think this issue is going to arise often with type imputation. As Tim wrote, you can also use --impute and say str(contig) or give it type string which will overwrite the imputation...but it's going to confuse folks. We could expand on the error message, or maybe we should implicitly convert Int to String for contig in the Variant constructor.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236401286
https://github.com/hail-is/hail/issues/520#issuecomment-236401286:248,Integrability,message,message,248,"I think this issue is going to arise often with type imputation. As Tim wrote, you can also use --impute and say str(contig) or give it type string which will overwrite the imputation...but it's going to confuse folks. We could expand on the error message, or maybe we should implicitly convert Int to String for contig in the Variant constructor.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236401286
https://github.com/hail-is/hail/issues/520#issuecomment-236406850:241,Availability,error,errors,241,"I don't think it makes sense to check explicitly for ""Chromosome"" and not impute that as `Int`. I also don't particularly like implicitly converting `Int` to `String` in the variant constructor -- this could lead to much more indecipherable errors like ""NumberFormatException: cannot convert 'X' to Int in column Chr"". I'm not sure what to do here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236406850
https://github.com/hail-is/hail/pull/536#issuecomment-238384250:26,Testability,test,test,26,The value of my minimum R test is dubious. The bigger indicator of a problem will that the test will OOM or take a very long period of time. Perhaps there's a better test of linearity? Or maybe I need a lot more points to see non-linearity.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/536#issuecomment-238384250
https://github.com/hail-is/hail/pull/536#issuecomment-238384250:91,Testability,test,test,91,The value of my minimum R test is dubious. The bigger indicator of a problem will that the test will OOM or take a very long period of time. Perhaps there's a better test of linearity? Or maybe I need a lot more points to see non-linearity.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/536#issuecomment-238384250
https://github.com/hail-is/hail/pull/536#issuecomment-238384250:166,Testability,test,test,166,The value of my minimum R test is dubious. The bigger indicator of a problem will that the test will OOM or take a very long period of time. Perhaps there's a better test of linearity? Or maybe I need a lot more points to see non-linearity.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/536#issuecomment-238384250
https://github.com/hail-is/hail/pull/536#issuecomment-238687799:32,Integrability,message,message,32,And here's a squashed commit w/ message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/536#issuecomment-238687799
https://github.com/hail-is/hail/pull/540#issuecomment-237322879:53,Availability,error,error,53,"Don't commit this yet. I intentionally introduced an error to make sure it fails. Although it failed for a different reason, ugh.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/540#issuecomment-237322879
https://github.com/hail-is/hail/pull/541#issuecomment-248439671:50,Deployability,integrat,integrate,50,"Of course. Cotton also had a few comments, I will integrate too. On Tuesday, September 20, 2016, Tim Poterba notifications@github.com; wrote:. > Let's get this in. Can you rebase?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/541#issuecomment-248438901, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgSLomjBSxoQFMUqrmHl1RdXl_mD3ks5qsE7igaJpZM4Jb_3Y; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/541#issuecomment-248439671
https://github.com/hail-is/hail/pull/541#issuecomment-248439671:50,Integrability,integrat,integrate,50,"Of course. Cotton also had a few comments, I will integrate too. On Tuesday, September 20, 2016, Tim Poterba notifications@github.com; wrote:. > Let's get this in. Can you rebase?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/541#issuecomment-248438901, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgSLomjBSxoQFMUqrmHl1RdXl_mD3ks5qsE7igaJpZM4Jb_3Y; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/541#issuecomment-248439671
https://github.com/hail-is/hail/issues/549#issuecomment-240003377:52,Usability,clear,clear,52,"You're right, they don't. @tpoterba Let's make this clear in the documentation. Although it is on the roadmap, it is not easy to support nested aggregators in full generality (what amounts to subqueries in SQL). I propose adding a `oneHot(x, arr)` where `x: Whatever`, `arr: Array[Whatever]` and `oneHot` returns the one hot encoding of `x` with respect to the ordered values in `arr` as an `Array[Int]` (and missing if `x` is not in `arr`). Then, using Tim's new changes that support aggregating over arrays (https://github.com/broadinstitute/hail/pull/584, still being reviewed but very close), you can write the above example as:. ```; annotateglobal expr -c 'global.pops = [""AFR"", ""NFE""]'; annotateglobals expr -c 'global.pop_counts = samples.sum(oneHot(sa.meta_test.POP, global.pops))'; ```. Reasonable?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/549#issuecomment-240003377
https://github.com/hail-is/hail/issues/551#issuecomment-240216457:517,Deployability,update,update,517,"This is primarily a condensation and formatting of what you have above. Is there more to document?. ### `filteralleles`. #### Usage; - `-c | --condition <expr>`—a hail language expression, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | variant |; | `va` | variant annotations |; | `aIndex` | allele index |; - `--keep/--remove`—keep or remove the allele if the expression is true; - `-a | --annotation <expr>`—a hail language expression which may update the variant annotations based on the removed alleles, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | the _new_ variant |; | `va` | the _old_ variant annotations |; | `aIndices` | an array of the old indices (such that `aIndices[newIndex] = oldIndex`) |. _NB:_ the allele indices are zero indexed and the zeroth index contains the reference. The `condition` expression will be executed for all alleles with index greater than zero. _NB2:_ if all alternate alleles are filtered the entire variant is filtered. #### Examples. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(oldIndex => va.info.AC[oldIndex]), va.info.AN = ...'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240216457
https://github.com/hail-is/hail/issues/551#issuecomment-240216457:223,Modifiability,variab,variables,223,"This is primarily a condensation and formatting of what you have above. Is there more to document?. ### `filteralleles`. #### Usage; - `-c | --condition <expr>`—a hail language expression, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | variant |; | `va` | variant annotations |; | `aIndex` | allele index |; - `--keep/--remove`—keep or remove the allele if the expression is true; - `-a | --annotation <expr>`—a hail language expression which may update the variant annotations based on the removed alleles, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | the _new_ variant |; | `va` | the _old_ variant annotations |; | `aIndices` | an array of the old indices (such that `aIndices[newIndex] = oldIndex`) |. _NB:_ the allele indices are zero indexed and the zeroth index contains the reference. The `condition` expression will be executed for all alleles with index greater than zero. _NB2:_ if all alternate alleles are filtered the entire variant is filtered. #### Examples. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(oldIndex => va.info.AC[oldIndex]), va.info.AN = ...'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240216457
https://github.com/hail-is/hail/issues/551#issuecomment-240216457:612,Modifiability,variab,variables,612,"This is primarily a condensation and formatting of what you have above. Is there more to document?. ### `filteralleles`. #### Usage; - `-c | --condition <expr>`—a hail language expression, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | variant |; | `va` | variant annotations |; | `aIndex` | allele index |; - `--keep/--remove`—keep or remove the allele if the expression is true; - `-a | --annotation <expr>`—a hail language expression which may update the variant annotations based on the removed alleles, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | the _new_ variant |; | `va` | the _old_ variant annotations |; | `aIndices` | an array of the old indices (such that `aIndices[newIndex] = oldIndex`) |. _NB:_ the allele indices are zero indexed and the zeroth index contains the reference. The `condition` expression will be executed for all alleles with index greater than zero. _NB2:_ if all alternate alleles are filtered the entire variant is filtered. #### Examples. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(oldIndex => va.info.AC[oldIndex]), va.info.AN = ...'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240216457
https://github.com/hail-is/hail/issues/551#issuecomment-240494452:31,Usability,clear,clear,31,"Two small comments:; - make it clear the name/description tables are describing the scope of the corresponding expression. We might need to do this elsewhere in the documentation, too.; - I'd write `aIndices[newIndex] = oldIndex` just to make it clear you're talking about the indices, and not the alleles.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240494452
https://github.com/hail-is/hail/issues/551#issuecomment-240494452:246,Usability,clear,clear,246,"Two small comments:; - make it clear the name/description tables are describing the scope of the corresponding expression. We might need to do this elsewhere in the documentation, too.; - I'd write `aIndices[newIndex] = oldIndex` just to make it clear you're talking about the indices, and not the alleles.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240494452
https://github.com/hail-is/hail/issues/551#issuecomment-240788265:333,Deployability,a/b,a/b,333,"During implementation, I noticed some behaviors that were not fully specified. If an allele is filtered, we must address the genotypes that reference that allele. A genotype consists of five parts. | Part | Description | Action |; | --- | --- | --- |; | GT | the hard call | if the filtered allele is `a` then `forall b.` `b/a` and `a/b` are converted, respectively, to `b/0` and `0/b` |; | AD | allele depth | the filtered allele's column is eliminated, e.g. filtering allele 1 transforms `[25,5,20]` to `[25,20]` |; | DP | number of informative reads | no change |; | PL | Phred-likelihoods for each allele pair | convert the allele-pair & likelihood pairs (e.g. `(0/1, 10)`) according to the GT rule. This yields a bag of (possibly duplicated) allele-likelihood pairs. We reduce back to unique allele-pairs by taking the `min` likelihood for each allele-pair |; | GQ | genotype quality | set to the second lowest value in the modified PL |. I'm a tad uneasy about the actions for AD, DP, and PL. For AD, should we shift the depth to the reference? For DP, should we subtract the removed depths? For PL, `min` should be ok when the values have differing orders of magnitude, but if two values are similar, should we convert to probabilities, sum, and convert back?. @cseed @monkollek @konradjk",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240788265
https://github.com/hail-is/hail/issues/551#issuecomment-240788265:775,Energy Efficiency,reduce,reduce,775,"During implementation, I noticed some behaviors that were not fully specified. If an allele is filtered, we must address the genotypes that reference that allele. A genotype consists of five parts. | Part | Description | Action |; | --- | --- | --- |; | GT | the hard call | if the filtered allele is `a` then `forall b.` `b/a` and `a/b` are converted, respectively, to `b/0` and `0/b` |; | AD | allele depth | the filtered allele's column is eliminated, e.g. filtering allele 1 transforms `[25,5,20]` to `[25,20]` |; | DP | number of informative reads | no change |; | PL | Phred-likelihoods for each allele pair | convert the allele-pair & likelihood pairs (e.g. `(0/1, 10)`) according to the GT rule. This yields a bag of (possibly duplicated) allele-likelihood pairs. We reduce back to unique allele-pairs by taking the `min` likelihood for each allele-pair |; | GQ | genotype quality | set to the second lowest value in the modified PL |. I'm a tad uneasy about the actions for AD, DP, and PL. For AD, should we shift the depth to the reference? For DP, should we subtract the removed depths? For PL, `min` should be ok when the values have differing orders of magnitude, but if two values are similar, should we convert to probabilities, sum, and convert back?. @cseed @monkollek @konradjk",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240788265
https://github.com/hail-is/hail/issues/551#issuecomment-240823343:278,Availability,down,downcode,278,"Consider three alleles. ```; GT: 1/2; GQ: 10. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2; ```. Want to preserve:; - PL(GT) = 0. ### Algorithms. Suppose we remove allele 2. There are two options:; - (_minning_) in the PL array convert occurences of 2 to 0 (""downcode to ref"") and take minimums where there are multiple likelihoods for a single genotype; also downcode GT to ref; - (_subsetting_) subset the PL array (i.e. remove entries with 2's); set GT to the genotype with the minimum likelihood. ### Interpretations. The qualitative interpretation of _minning_ is a belief that the alternate is real but we want to shift the probability mass to 0 (thus changing our interpretation of 0 from ""reference"" to ""reference or something not listed""). The qualitative interpretation of _subsetting_ is a belief that the alternate is not-real and we want to discard any probability mass associated with the alternate. ### Results. The _minning_ algorithm produces. ```; GT: 0/1; GQ: 10. 0 | 20; 1 | 0 10; +-----------; 0 1; ```. The _subsetting_ algorithm produces. ```; GT: 1/1; GQ: 990. 0 | 990; 1 | 990 0; +----------; 0 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240823343
https://github.com/hail-is/hail/issues/551#issuecomment-240823343:379,Availability,down,downcode,379,"Consider three alleles. ```; GT: 1/2; GQ: 10. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2; ```. Want to preserve:; - PL(GT) = 0. ### Algorithms. Suppose we remove allele 2. There are two options:; - (_minning_) in the PL array convert occurences of 2 to 0 (""downcode to ref"") and take minimums where there are multiple likelihoods for a single genotype; also downcode GT to ref; - (_subsetting_) subset the PL array (i.e. remove entries with 2's); set GT to the genotype with the minimum likelihood. ### Interpretations. The qualitative interpretation of _minning_ is a belief that the alternate is real but we want to shift the probability mass to 0 (thus changing our interpretation of 0 from ""reference"" to ""reference or something not listed""). The qualitative interpretation of _subsetting_ is a belief that the alternate is not-real and we want to discard any probability mass associated with the alternate. ### Results. The _minning_ algorithm produces. ```; GT: 0/1; GQ: 10. 0 | 20; 1 | 0 10; +-----------; 0 1; ```. The _subsetting_ algorithm produces. ```; GT: 1/1; GQ: 990. 0 | 990; 1 | 990 0; +----------; 0 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240823343
https://github.com/hail-is/hail/issues/551#issuecomment-240825234:10,Deployability,update,update,10,"Here's an update to the action table. | Part | Description | Action |; | --- | --- | --- |; | GT | the hard call | _minning_ or _subsetting_ |; | AD | allele depth | the filtered allele's column is eliminated, e.g. filtering allele 1 transforms `[25,5,20]` to `[25,20]` |; | DP | number of informative reads | no change |; | PL | Phred-likelihoods for each allele pair | _minning_ or _subsetting_ |; | GQ | genotype quality | increasing-sort PL and take `PL[1] - PL[0]` |. We choose either _minning_ or _subsetting_ consistently for all parts. I now feel:; - when _minning_ (i.e. _believe real_) we should move AD value to reference.; - when _subsetting_ (i.e. _believe not-real_) we should subtract removed depth from DP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240825234
https://github.com/hail-is/hail/issues/551#issuecomment-240847998:401,Safety,avoid,avoid,401,"I mostly agree with the last idea, though I would suggest defaulting to (or really only implementing) subsetting rather than minning: minning is highly inconsistent with what's done by GATK. I don't think it's particularly valid to take reads assigned to another haplotype and call them reference (ESPECIALLY for GT - this is likely to lead to some pretty gnarly reference bias that most will want to avoid). And to confirm, what does subset mean in terms of GT? Say for `1/2` where 2 is filtered - will it be `./1`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240847998
https://github.com/hail-is/hail/issues/551#issuecomment-240850923:81,Testability,log,log-likelihood,81,"When subsetting, the GT will be set to whatever genotype has the lowest negative-log-likelihood in the PL after subsetting. This maintains the `PL(GT) = 0` invariant, though I could also see an argument for `./.`. AFAIK, we cannot represent half-NA cases like `./1`. @cseed, we had planned to implement _minning_, should we not do that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240850923
https://github.com/hail-is/hail/pull/555#issuecomment-238118077:146,Availability,down,down,146,"Things that remain to be done:; - overriding repartition. This isn't super trivial because if you have a huge pipeline that ends in a repartition down to 10 partitions, you're going to get 10 cores for the whole job.; - better ordering process on VCF import; - speed up joins by skipping ahead if the 'right' is behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/555#issuecomment-238118077
https://github.com/hail-is/hail/pull/555#issuecomment-238118077:110,Deployability,pipeline,pipeline,110,"Things that remain to be done:; - overriding repartition. This isn't super trivial because if you have a huge pipeline that ends in a repartition down to 10 partitions, you're going to get 10 cores for the whole job.; - better ordering process on VCF import; - speed up joins by skipping ahead if the 'right' is behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/555#issuecomment-238118077
https://github.com/hail-is/hail/pull/555#issuecomment-239675545:26,Testability,test,tests,26,"Rebased/squashed, passing tests, ready for a look @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/555#issuecomment-239675545
https://github.com/hail-is/hail/pull/558#issuecomment-238264351:51,Usability,simpl,simpler,51,@cseed Merging this PR will make the diff for #536 simpler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/558#issuecomment-238264351
https://github.com/hail-is/hail/issues/561#issuecomment-238502640:20,Availability,error,error,20,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640
https://github.com/hail-is/hail/issues/561#issuecomment-238502640:63,Deployability,update,updated,63,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640
https://github.com/hail-is/hail/issues/561#issuecomment-238502640:107,Deployability,update,update,107,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640
https://github.com/hail-is/hail/issues/561#issuecomment-238502640:26,Integrability,message,message,26,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640
https://github.com/hail-is/hail/issues/561#issuecomment-238502640:366,Testability,test,test,366,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640
https://github.com/hail-is/hail/issues/561#issuecomment-238502640:389,Testability,test,test,389,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640
https://github.com/hail-is/hail/issues/561#issuecomment-238502640:486,Testability,test,test,486,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640
https://github.com/hail-is/hail/issues/563#issuecomment-238641595:101,Testability,Benchmark,Benchmark,101,Pradeep was seeing 2x larger output on VCF export after filtering/QC. This could explain some of it. Benchmark the improvement from this change and investigate further if this doesn't explain it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/563#issuecomment-238641595
https://github.com/hail-is/hail/issues/565#issuecomment-238879581:770,Deployability,install,installed,770,"This means that hail (or something on which hail depends) is trying to call into BLAS. BLAS is a Fortran library and is often shipped with C bindings. The symbol `cblas_dgemv` is a C function. Your machine is likely missing `libcblas`. Can you post the output of these commands:; - `nm -g /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so` (this may say that the file doesn't exist, in which case skip the next command; - `objdump -TC /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so`. Can you also answer these questions:; - What distribution are you using?; - What version of that distribution do you have?; - What package management tool do you use?. If you're on Ubuntu, can you tell me what version of `libatlas-dev` you have installed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-238879581
https://github.com/hail-is/hail/issues/565#issuecomment-238879581:49,Integrability,depend,depends,49,"This means that hail (or something on which hail depends) is trying to call into BLAS. BLAS is a Fortran library and is often shipped with C bindings. The symbol `cblas_dgemv` is a C function. Your machine is likely missing `libcblas`. Can you post the output of these commands:; - `nm -g /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so` (this may say that the file doesn't exist, in which case skip the next command; - `objdump -TC /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so`. Can you also answer these questions:; - What distribution are you using?; - What version of that distribution do you have?; - What package management tool do you use?. If you're on Ubuntu, can you tell me what version of `libatlas-dev` you have installed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-238879581
https://github.com/hail-is/hail/issues/565#issuecomment-238881210:63,Deployability,install,installing,63,"And in the off chance that you're using ArchLinux, can you try installing `openblas-lapack` from the AUR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-238881210
https://github.com/hail-is/hail/issues/565#issuecomment-239495713:85,Deployability,install,installed,85,"Can you post the results of. ```; yum info atlas-devel; ```. If `atlas-devel` is not installed, can you try installing it?. ```; yum install atlas-devel; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239495713
https://github.com/hail-is/hail/issues/565#issuecomment-239495713:108,Deployability,install,installing,108,"Can you post the results of. ```; yum info atlas-devel; ```. If `atlas-devel` is not installed, can you try installing it?. ```; yum install atlas-devel; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239495713
https://github.com/hail-is/hail/issues/565#issuecomment-239495713:133,Deployability,install,install,133,"Can you post the results of. ```; yum info atlas-devel; ```. If `atlas-devel` is not installed, can you try installing it?. ```; yum install atlas-devel; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239495713
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:583,Availability,Avail,Available,583,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1990,Availability,error,error,1990,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:2067,Availability,error,error,2067,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:2176,Availability,FAILURE,FAILURE,2176,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:160,Deployability,update,updates,160,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:351,Deployability,update,updates,351,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:544,Deployability,update,updates,544,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:654,Deployability,Release,Release,654,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1000,Deployability,Release,Release,1000,"m info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still app",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1301,Deployability,install,installed,1301,"/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1356,Deployability,install,install,1356," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1553,Deployability,update,updates,1553,"rs.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1692,Deployability,install,installed,1692,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1813,Deployability,Install,Installed,1813,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1873,Deployability,Install,Installed,1873,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1591,Integrability,Depend,Dependencies,1591,"el; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1718,Integrability,Depend,Dependency,1718,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1862,Integrability,Depend,Dependency,1862,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:73,Modifiability,plugin,plugins,73,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1384,Modifiability,plugin,plugins,1384," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:66,Performance,Load,Loaded,66,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:399,Performance,Load,Loading,399,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:426,Performance,cache,cached,426,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:915,Performance,Tune,Tuned,915,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1261,Performance,Tune,Tuned,1261,"up_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an excepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1377,Performance,Load,Loaded,1377," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1419,Performance,Load,Loading,1419," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1446,Performance,cache,cached,1446," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:2266,Testability,test,test,2266,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:2294,Testability,Test,Test,2294,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-239729893:2452,Testability,log,log,2452,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:97,Availability,down,downloaded,97,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:286,Availability,down,downloaded,286,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:382,Availability,down,download,382,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:398,Availability,down,downloaded,398,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:681,Availability,error,errors,681,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:249,Deployability,install,installed,249,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:498,Deployability,install,installDist,498,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:1232,Deployability,release,release,1232,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:1248,Deployability,release,release-,1248,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:1705,Deployability,install,installed,1705,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:1723,Deployability,install,installed-packages,1723,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:1799,Deployability,install,installed-packages,1799,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/issues/565#issuecomment-240446097:615,Testability,test,tests,615,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097
https://github.com/hail-is/hail/pull/572#issuecomment-239253597:83,Testability,test,tests,83,could you run gen/count like 10x and paste the dimensions here? Until we have meta-tests that'll have to do,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/572#issuecomment-239253597
https://github.com/hail-is/hail/pull/572#issuecomment-239462375:3273,Testability,log,log-log,3273, gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 2]hail: info: count:; nSamples 1; nVariants 398; nCalled 195; callRate 48.995%; hail: info: timing:; gen: 1.126s; count: 638.722ms; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 6]hail: info: count:; nSamples 147; nVariants 3; nCalled 229; callRate 51.927%; hail: info: timing:; gen: 1.624s; count: 1.126s; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 6]hail: info: count:; nSamples 5; nVariants 2; nCalled 5; callRate 50.000%; hail: info: timing:; gen: 1.062s; count: 614.361ms; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 9]hail: info: count:; nSamples 6; nVariants 27; nCalled 84; callRate 51.852%; hail: info: timing:; gen: 1.280s; count: 990.637ms. ```. This one sampled the rectangle 900x1. The 0 count for variants is caused by `buildableOf` and friends returning an empty sequence with probability `1/(fuel+1)`: 50% in this case. The 900 fuel units happened to yield 820 partitions which each shared the 900 fuel. Many of the sample ids were single digits in this run (due to low fuel). ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 10]hail: info: count:; nSamples 820; nVariants 0; nCalled 0; callRate NA; hail: info: timing:; gen: 952.424ms; count: 569.784ms; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 5]hail: info: count:; nSamples 28; nVariants 22; nCalled 328; callRate 53.247%; hail: info: timing:; gen: 1.169s; count: 1.355s; ```. And here's the above pairs plotted with samples on the x and variants on the y. Second one is log-log.; ![figure_2](https://cloud.githubusercontent.com/assets/106194/17625714/05957f52-6078-11e6-98cb-7f41c7970b83.png); ![figure_1](https://cloud.githubusercontent.com/assets/106194/17625713/05928482-6078-11e6-9154-a364bb739e16.png),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/572#issuecomment-239462375
https://github.com/hail-is/hail/pull/574#issuecomment-239036784:108,Availability,error,error,108,"okay, now ready. In addition to the bugfix and expr array features, fixed a lingering stupid problem in our error traversal method",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/574#issuecomment-239036784
https://github.com/hail-is/hail/pull/574#issuecomment-240128679:8,Usability,simpl,simplified,8,Greatly simplified the evaluation matching.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/574#issuecomment-240128679
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:249,Deployability,install,install,249,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:479,Testability,log,logreg,479,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:610,Testability,log,logreg,610,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:636,Testability,log,logreg,636,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:663,Testability,log,logreg,663,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:692,Testability,log,logreg,692,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:721,Testability,log,logreg,721,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:754,Testability,log,logreg,754,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:790,Testability,log,logreg,790,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239628493:1094,Testability,log,logreg,1094,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493
https://github.com/hail-is/hail/pull/585#issuecomment-239686067:44,Testability,log,logistic,44,"Looks great. Any chance you can add Firth's logistic regression?. Thanks,. Manny. On Sun, Aug 14, 2016 at 12:58 PM, jbloom22 notifications@github.com wrote:. > jenkins; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/585#issuecomment-239684133,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABPl2UH8kO3OSrp-6XTXAAacpxxSI9Z2ks5qf0kdgaJpZM4Jje3f; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686067
https://github.com/hail-is/hail/pull/585#issuecomment-239686959:887,Availability,error,error,887,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959
https://github.com/hail-is/hail/pull/585#issuecomment-239686959:84,Deployability,update,update,84,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959
https://github.com/hail-is/hail/pull/585#issuecomment-239686959:369,Testability,test,tests,369,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959
https://github.com/hail-is/hail/pull/585#issuecomment-239686959:1018,Testability,Log,LogisticRegression,1018,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:1211,Energy Efficiency,reduce,reduced,1211,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:1556,Energy Efficiency,reduce,reduced,1556,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:1314,Integrability,wrap,wrap,1314,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:303,Performance,perform,performance,303,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:121,Testability,test,test,121,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:152,Testability,Test,Test,152,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:173,Testability,Test,TestResult,173,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:325,Testability,Log,LogisticRegressionNullFit,325,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:506,Testability,Log,LogisticRegressionFit,506,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:578,Testability,test,test,578,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:714,Testability,test,test,714,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:837,Testability,Log,LogisticRegressionFit,837,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:970,Testability,Log,LogisticRegressionNullFit,970,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-241153168:279,Usability,feedback,feedback,279,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:0,Deployability,Update,Updated,0,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:57,Deployability,install,install,57,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:274,Testability,log,logreg,274,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:389,Testability,log,logreg,389,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:416,Testability,log,logreg,416,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:443,Testability,log,logreg,443,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:471,Testability,log,logreg,471,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:504,Testability,log,logreg,504,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:540,Testability,log,logreg,540,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/585#issuecomment-242580502:706,Testability,log,logreg,706,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502
https://github.com/hail-is/hail/pull/586#issuecomment-239802203:5,Testability,test,testVSMGenIsLinearSpaceInSizeParameter,5,Re: `testVSMGenIsLinearSpaceInSizeParameter` I agree and I also feel that it's a bit flaky.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586#issuecomment-239802203
https://github.com/hail-is/hail/pull/591#issuecomment-240544764:0,Modifiability,refactor,refactored,0,refactored with flattenOrNull in Utils. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/591#issuecomment-240544764
https://github.com/hail-is/hail/pull/593#issuecomment-248638035:41,Testability,test,testing,41,Closing this so we don't have to keep re-testing. @alexb-3 resubmit after you're reviewed it and you are ready to get it in to the master.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/593#issuecomment-248638035
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:308,Availability,Down,Download,308,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:396,Availability,down,download,396,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:454,Availability,down,download,454,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:40,Deployability,Install,Install,40,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:256,Deployability,update,update,256,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:277,Deployability,install,install,277,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:424,Deployability,Release,Release,424,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:643,Deployability,release,released,643,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:745,Deployability,update,update,745,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240306249:679,Testability,test,tested,679,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249
https://github.com/hail-is/hail/issues/594#issuecomment-240309173:114,Availability,down,download,114,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173
https://github.com/hail-is/hail/issues/594#issuecomment-240309173:296,Deployability,install,installDist,296,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173
https://github.com/hail-is/hail/issues/594#issuecomment-240309173:70,Integrability,wrap,wrapper,70,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173
https://github.com/hail-is/hail/issues/594#issuecomment-240309173:183,Integrability,wrap,wrapper,183,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:324,Availability,error,error,324,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1165,Availability,error,errors,1165,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1201,Availability,FAILURE,FAILURE,1201,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1459,Availability,down,down,1459,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:128,Deployability,install,installDist,128,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1605,Deployability,install,install,1605,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1645,Deployability,install,install,1645,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1258,Integrability,depend,dependency,1258,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1468,Integrability,depend,dependencies,1468,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:173,Testability,test,tests,173,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1507,Testability,test,testing,1507,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120
https://github.com/hail-is/hail/issues/594#issuecomment-240384398:266,Availability,avail,available,266,"Hi Jerome, this `AnnotationPathException` issue is something we've seen before. It seems to be caused sporadically by gradle's build caching, and can usually be fixed by running `gradle clean`. The tests that failed are probably the ones that require external tools available on the command line:; FisherExactSuite (requires Rscript); ImportPlinkSuite (requires plink 1.9); ExportPlinkSuite (requires plink 1.9); LoadBgenSuite (requires qctool)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240384398
https://github.com/hail-is/hail/issues/594#issuecomment-240384398:413,Performance,Load,LoadBgenSuite,413,"Hi Jerome, this `AnnotationPathException` issue is something we've seen before. It seems to be caused sporadically by gradle's build caching, and can usually be fixed by running `gradle clean`. The tests that failed are probably the ones that require external tools available on the command line:; FisherExactSuite (requires Rscript); ImportPlinkSuite (requires plink 1.9); ExportPlinkSuite (requires plink 1.9); LoadBgenSuite (requires qctool)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240384398
https://github.com/hail-is/hail/issues/594#issuecomment-240384398:198,Testability,test,tests,198,"Hi Jerome, this `AnnotationPathException` issue is something we've seen before. It seems to be caused sporadically by gradle's build caching, and can usually be fixed by running `gradle clean`. The tests that failed are probably the ones that require external tools available on the command line:; FisherExactSuite (requires Rscript); ImportPlinkSuite (requires plink 1.9); ExportPlinkSuite (requires plink 1.9); LoadBgenSuite (requires qctool)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240384398
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:78,Availability,failure,failures,78,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:488,Performance,Load,LoadBgenSuite,488,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:73,Testability,test,test,73,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:167,Testability,test,test,167,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:218,Testability,test,testBiallelic,218,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:262,Testability,test,test,262,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:309,Testability,test,test,309,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:344,Testability,test,test,344,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:398,Testability,test,testImputeSexPlinkVersion,398,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:454,Testability,test,test,454,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240395647:502,Testability,test,testBgenImportRandom,502,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647
https://github.com/hail-is/hail/issues/594#issuecomment-240399174:153,Deployability,install,installed,153,"Hi Jerome, yup, the first three require plink 1.9 and the fourth requires qctool. I'm surprised FisherExactSuite didn't fail as well, perhaps you have R installed or pulled Hail before that went into master. Thanks for the feedback, super helpful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240399174
https://github.com/hail-is/hail/issues/594#issuecomment-240399174:223,Usability,feedback,feedback,223,"Hi Jerome, yup, the first three require plink 1.9 and the fourth requires qctool. I'm surprised FisherExactSuite didn't fail as well, perhaps you have R installed or pulled Hail before that went into master. Thanks for the feedback, super helpful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240399174
https://github.com/hail-is/hail/issues/594#issuecomment-244471032:26,Integrability,depend,dependencies,26,"We added the list of test dependencies to the ""Running the tests"" subsection of [Getting Started](https://hail.is/docs/devel/index.html#GettingStarted) so I'm going to mark this as closed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-244471032
https://github.com/hail-is/hail/issues/594#issuecomment-244471032:21,Testability,test,test,21,"We added the list of test dependencies to the ""Running the tests"" subsection of [Getting Started](https://hail.is/docs/devel/index.html#GettingStarted) so I'm going to mark this as closed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-244471032
https://github.com/hail-is/hail/issues/594#issuecomment-244471032:59,Testability,test,tests,59,"We added the list of test dependencies to the ""Running the tests"" subsection of [Getting Started](https://hail.is/docs/devel/index.html#GettingStarted) so I'm going to mark this as closed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-244471032
https://github.com/hail-is/hail/issues/597#issuecomment-240548537:80,Availability,error,error,80,"Proposed approach: annotate variants with Map[Int, Set[String]], which maps the error code to the set of trios (child string) with error of that code at that variant. We should still annotate samples with counts that are harder to pull out of this, like count per nuclear family and for the individual overall.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597#issuecomment-240548537
https://github.com/hail-is/hail/issues/597#issuecomment-240548537:131,Availability,error,error,131,"Proposed approach: annotate variants with Map[Int, Set[String]], which maps the error code to the set of trios (child string) with error of that code at that variant. We should still annotate samples with counts that are harder to pull out of this, like count per nuclear family and for the individual overall.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597#issuecomment-240548537
https://github.com/hail-is/hail/issues/598#issuecomment-284758059:274,Safety,safe,safe,274,"Reviving this as would be great to have! I haven't added it myself since we discussed using the python way using `{}` (e.g. `{ 1 , 2 ,3}`). When I looked at implementing it based on the Array constructor, I saw some of the compiler things @danking added, so wasn't sure how safe it was to just mimic it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/598#issuecomment-284758059
https://github.com/hail-is/hail/issues/606#issuecomment-240906565:72,Availability,error,errors,72,@cseed: Is this issue for the files that are known to exist but get 404 errors (due to capitalization with the importbgen etc. files) or all 404 errors? I don't think we can remove the 404 errors from the console for any command that doesn't have a corresponding markdown file in the commands directory. See this link: http://stackoverflow.com/questions/7035466/check-if-file-exists-but-prevent-404-error-in-console-from-showing-up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/606#issuecomment-240906565
https://github.com/hail-is/hail/issues/606#issuecomment-240906565:145,Availability,error,errors,145,@cseed: Is this issue for the files that are known to exist but get 404 errors (due to capitalization with the importbgen etc. files) or all 404 errors? I don't think we can remove the 404 errors from the console for any command that doesn't have a corresponding markdown file in the commands directory. See this link: http://stackoverflow.com/questions/7035466/check-if-file-exists-but-prevent-404-error-in-console-from-showing-up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/606#issuecomment-240906565
https://github.com/hail-is/hail/issues/606#issuecomment-240906565:189,Availability,error,errors,189,@cseed: Is this issue for the files that are known to exist but get 404 errors (due to capitalization with the importbgen etc. files) or all 404 errors? I don't think we can remove the 404 errors from the console for any command that doesn't have a corresponding markdown file in the commands directory. See this link: http://stackoverflow.com/questions/7035466/check-if-file-exists-but-prevent-404-error-in-console-from-showing-up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/606#issuecomment-240906565
https://github.com/hail-is/hail/issues/606#issuecomment-240906565:399,Availability,error,error-in-console-from-showing-up,399,@cseed: Is this issue for the files that are known to exist but get 404 errors (due to capitalization with the importbgen etc. files) or all 404 errors? I don't think we can remove the 404 errors from the console for any command that doesn't have a corresponding markdown file in the commands directory. See this link: http://stackoverflow.com/questions/7035466/check-if-file-exists-but-prevent-404-error-in-console-from-showing-up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/606#issuecomment-240906565
https://github.com/hail-is/hail/issues/614#issuecomment-279583150:48,Modifiability,evolve,evolved,48,@cseed am I right that the thinking on this has evolved with move to Python?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/614#issuecomment-279583150
https://github.com/hail-is/hail/issues/620#issuecomment-295316093:100,Performance,perform,performance,100,"#1655 addresses the linear regression case with a fixed sample set, where there is the largest-fold performance gain to be had per variant. There are other situations we should return to as they become priorities for users.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/620#issuecomment-295316093
https://github.com/hail-is/hail/pull/622#issuecomment-240831961:252,Modifiability,inherit,inheritance,252,"I would prefer that there was some way to abstract over this behavior, but I don't see any ""mutually exclusive"" tag in Args4j. A common `FilterOptions` class might help, but we'd still have to call a ""check options"" method in each `Command` and linear inheritance limits the scalability of this approach to other common options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/622#issuecomment-240831961
https://github.com/hail-is/hail/pull/622#issuecomment-240831961:275,Performance,scalab,scalability,275,"I would prefer that there was some way to abstract over this behavior, but I don't see any ""mutually exclusive"" tag in Args4j. A common `FilterOptions` class might help, but we'd still have to call a ""check options"" method in each `Command` and linear inheritance limits the scalability of this approach to other common options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/622#issuecomment-240831961
https://github.com/hail-is/hail/issues/626#issuecomment-317568168:17,Performance,perform,perform,17,We can also just perform the checks in python code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/626#issuecomment-317568168
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1098,Performance,load,loading,1098,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1233,Performance,load,loaded,1233,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1287,Performance,load,loads,1287,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1305,Performance,load,load,1305,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1401,Performance,load,loads,1401,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1500,Performance,load,load,1500,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1543,Performance,load,load,1543,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1560,Performance,load,load,1560,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1876,Performance,load,load,1876,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/pull/630#issuecomment-240919692:1881,Performance,perform,performance,1881,"For https://github.com/broadinstitute/hail/issues/604, I don't like this fix. I did some research on recommended line lengths, for example:. http://graphicdesign.stackexchange.com/questions/13724/recommended-column-width-for-text-reading-digital-vs-printed. and the consensus seems that 50-80 is good, and maybe as high as 95. With 45em, the documentation text had a width of around 88 characters. I think that's basically perfect. I think the complaint is about information density and other parts of the layout, not the blocks of text (which are relatively rare). For example, even with this change, tables (see Hadoop Glob Patterns) are quite narrow. The TOC is narrow and takes up a huge amount of vertical space. Etc. It might be reasonable to make the code examples wider than the rest of the text. Let's address the other layout issues and then see if this is still a problem/complaint. For https://github.com/broadinstitute/hail/issues/605, genius observation, I think you're totally right. I did a little research on defer. It seems like the script runs after the DOM is parsed. But since loading the HTML pages are themselves asynchronous, I don't think that's enough of a guarantee that the content with equations will be loaded. Potential solution: call a callback after the loads complete to load the JSON and MathJax. Here's an example on using `$.when` to run a callback after multiple loads:. http://stackoverflow.com/questions/17609084/jquery-callback-after-multiple-functions. Then load the JSON, build the docs, and finally load MathJax. To load JS from JS, use jQuery `$.getScript` or build a DOM script element directly, for example:. http://stackoverflow.com/questions/950087/how-to-include-a-javascript-file-in-another-javascript-file. I think eventually we'll want to include the HTML fragments into a single page at build time. That will also improve load performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630#issuecomment-240919692
https://github.com/hail-is/hail/issues/634#issuecomment-349719025:13,Integrability,interface,interface,13,"Hail2 python interface makes this impossible, so no longer worrying about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/634#issuecomment-349719025
https://github.com/hail-is/hail/pull/637#issuecomment-241093024:207,Testability,test,test,207,"Added another commit. This should fix the math rendering problem. Works for me on all Safari, Firefox and Chrome now. Since it was hard to reproduce, not linking to the issue, we should get the community to test it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/637#issuecomment-241093024
https://github.com/hail-is/hail/pull/643#issuecomment-241108325:34,Availability,down,down,34,"@cseed, this carries anchor style down to `code` elements contained within them. I don't know if this is ideal, but it's at least consistent with anchors and informs the user that they can click the command names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/643#issuecomment-241108325
https://github.com/hail-is/hail/pull/644#issuecomment-241179566:2,Deployability,integrat,integrated,2,"I integrated these changes into https://github.com/broadinstitute/hail/pull/652 except I used block instead of block-inline. I also fixed the line height. (Problem was not using display: block, padding was per-line.) With the line-height fixed, font-size: 0.8em feels too small. How do you think it looks now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/644#issuecomment-241179566
https://github.com/hail-is/hail/pull/644#issuecomment-241179566:2,Integrability,integrat,integrated,2,"I integrated these changes into https://github.com/broadinstitute/hail/pull/652 except I used block instead of block-inline. I also fixed the line height. (Problem was not using display: block, padding was per-line.) With the line-height fixed, font-size: 0.8em feels too small. How do you think it looks now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/644#issuecomment-241179566
https://github.com/hail-is/hail/pull/645#issuecomment-241120586:66,Testability,log,logreg,66,Ready for review. This also brings the documentation in line with logreg.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/645#issuecomment-241120586
https://github.com/hail-is/hail/pull/649#issuecomment-241138084:46,Testability,test,test,46,"Oh, man. I think we should have a new kind of test suite, PropertySuite (and SparkPropertySuite, or a Spark mixin) which just declares a bunch of properties to verify, like the current check.Properties (which can maybe go away). Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241138084
https://github.com/hail-is/hail/pull/649#issuecomment-241139977:81,Availability,error,error,81,I particularly like that a `PropertySuite` would reduce the possibility of human error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241139977
https://github.com/hail-is/hail/pull/649#issuecomment-241139977:49,Energy Efficiency,reduce,reduce,49,I particularly like that a `PropertySuite` would reduce the possibility of human error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241139977
https://github.com/hail-is/hail/pull/649#issuecomment-241846815:73,Availability,error,errors,73,"I had to choose rather small upper bounds to avoid a variety of overflow errors in `Genotype`. I imagine these are practically not a problem, but I didn't take a particularly principled approach to choosing upper bounds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241846815
https://github.com/hail-is/hail/pull/649#issuecomment-241846815:45,Safety,avoid,avoid,45,"I had to choose rather small upper bounds to avoid a variety of overflow errors in `Genotype`. I imagine these are practically not a problem, but I didn't take a particularly principled approach to choosing upper bounds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241846815
https://github.com/hail-is/hail/pull/654#issuecomment-242075913:27,Testability,test,test,27,"No, we're setting up a new test server and things are broken at the moment. We should get things sorted today, hopefully.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/654#issuecomment-242075913
https://github.com/hail-is/hail/issues/655#issuecomment-241418469:478,Deployability,update,updates,478,"On April 2016, this exact issue was reported: https://issues.jenkins-ci.org/browse/JENKINS-34177. This issue results in Jenkins using arbitrary amounts of disk space (until builds fail due to a full disk). JENKINS-34177 was closed as a duplicate of: https://issues.jenkins-ci.org/browse/JENKINS-2111 which has been open since 2008. The thread of conversation seems to be about how difficult it would be to ensure that all files are deleted across all slaves. There have been no updates to that thread since October of 2015.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/655#issuecomment-241418469
https://github.com/hail-is/hail/issues/655#issuecomment-241545086:82,Testability,test,test,82,We now have TeamCity running along side Jenkins at http://ci.hail.is:8111 . We'll test it out for a couple days and see if it represents a concrete improvement over Jenkins.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/655#issuecomment-241545086
https://github.com/hail-is/hail/pull/658#issuecomment-241537793:16,Testability,test,test,16,"I'm not able to test this against real-sized data yet due to UNIX permissions issues, but I want to start getting code critique.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-241537793
https://github.com/hail-is/hail/pull/658#issuecomment-242136648:28,Availability,error,error,28,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648
https://github.com/hail-is/hail/pull/658#issuecomment-242136648:34,Integrability,message,message,34,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648
https://github.com/hail-is/hail/pull/658#issuecomment-242136648:69,Security,access,access,69,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648
https://github.com/hail-is/hail/pull/658#issuecomment-242136648:255,Security,access,accessing,255,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:2376,Energy Efficiency,schedul,scheduler,2376,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:2447,Energy Efficiency,schedul,scheduler,2447,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:1781,Performance,Cache,CacheManager,1781,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:1812,Performance,Cache,CacheManager,1812,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:1857,Performance,Cache,CacheManager,1857,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:1883,Performance,Cache,CacheManager,1883,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:2569,Performance,concurren,concurrent,2569,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241800222:2653,Performance,concurren,concurrent,2653,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222
https://github.com/hail-is/hail/issues/660#issuecomment-241806708:20,Availability,error,error,20,I can reproduce the error in the current master with:. ```; ./build/install/hail/bin/hail importvcf ~/sample2.vcf splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241806708
https://github.com/hail-is/hail/issues/660#issuecomment-241806708:68,Deployability,install,install,68,I can reproduce the error in the current master with:. ```; ./build/install/hail/bin/hail importvcf ~/sample2.vcf splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241806708
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:18,Availability,error,error,18,"This still causes error messages. `hail -l /mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed.Chr${num}.QC.vds.test.log importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr${num}/TopMed_8k.853.vcf.bgz splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count`. `2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@716: Client environment:host.name=nid00014; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@723: Client environment:os.name=Linux; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-431.el6_1.0000.9051-cray_ari_athena_c_cos; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Thu Jan 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:3329,Availability,failure,failure,3329,"8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:3387,Availability,failure,failure,3387,"6' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:8972,Deployability,deploy,deploy,8972,nfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9009,Deployability,deploy,deploy,9009,titute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9081,Deployability,deploy,deploy,9081,.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqO,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9157,Deployability,deploy,deploy,9157,institute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(I,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9228,Deployability,deploy,deploy,9228,; at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9297,Deployability,deploy,deploy,9297,ized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Ann,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5487,Energy Efficiency,schedul,scheduler,5487,roadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5558,Energy Efficiency,schedul,scheduler,5558,eVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5917,Energy Efficiency,schedul,scheduler,5917,.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5957,Energy Efficiency,schedul,scheduler,5957,extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6055,Energy Efficiency,schedul,scheduler,6055,or$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6152,Energy Efficiency,schedul,scheduler,6152,:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6403,Energy Efficiency,schedul,scheduler,6403,kContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6483,Energy Efficiency,schedul,scheduler,6483,cheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6588,Energy Efficiency,schedul,scheduler,6588,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6736,Energy Efficiency,schedul,scheduler,6736,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:23,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6824,Energy Efficiency,schedul,scheduler,6824,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinst,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6921,Energy Efficiency,schedul,scheduler,6921,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinsti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:7016,Energy Efficiency,schedul,scheduler,7016,.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinst,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:7179,Energy Efficiency,schedul,scheduler,7179,abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Ma,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:11371,Energy Efficiency,schedul,scheduler,11371,nnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); I0824 16:44:07.061986 9121 sched.cpp:1771] Asked to stop the driver; I0824 16:44:07.062144 8743 sched.cpp:1040] Stopping framework '0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:11442,Energy Efficiency,schedul,scheduler,11442,nnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); I0824 16:44:07.061986 9121 sched.cpp:1771] Asked to stop the driver; I0824 16:44:07.062144 8743 sched.cpp:1040] Stopping framework '0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:24,Integrability,message,messages,24,"This still causes error messages. `hail -l /mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed.Chr${num}.QC.vds.test.log importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr${num}/TopMed_8k.853.vcf.bgz splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count`. `2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@716: Client environment:host.name=nid00014; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@723: Client environment:os.name=Linux; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-431.el6_1.0000.9051-cray_ari_athena_c_cos; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Thu Jan 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2086,Performance,queue,queue,2086,"r.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmult",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5680,Performance,concurren,concurrent,5680,a:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5764,Performance,concurren,concurrent,5764,iantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:11564,Performance,concurren,concurrent,11564,nnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); I0824 16:44:07.061986 9121 sched.cpp:1771] Asked to stop the driver; I0824 16:44:07.062144 8743 sched.cpp:1040] Stopping framework '0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:11648,Performance,concurren,concurrent,11648,nnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); I0824 16:44:07.061986 9121 sched.cpp:1771] Asked to stop the driver; I0824 16:44:07.062144 8743 sched.cpp:1040] Stopping framework '0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:1893,Safety,timeout,timeout,1893,"an 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2248,Safety,detect,detector,2248,"s/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2266,Safety,Detect,Detected,2266,"7,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2427,Safety,detect,detector,2427,"181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2500,Safety,detect,detected,2500,"text=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cann",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2563,Safety,detect,detected,2563,"ched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadins",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:3308,Safety,abort,aborted,3308,"8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6087,Safety,abort,abortStage,6087,tor.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6184,Safety,abort,abortStage,6184,spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6426,Safety,abort,abortStage,6426,ly(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2698,Security,authenticat,authentication,2698,"r [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AS",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:113,Testability,test,test,113,"This still causes error messages. `hail -l /mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed.Chr${num}.QC.vds.test.log importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr${num}/TopMed_8k.853.vcf.bgz splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count`. `2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@716: Client environment:host.name=nid00014; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@723: Client environment:os.name=Linux; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-431.el6_1.0000.9051-cray_ari_athena_c_cos; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Thu Jan 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/issues/660#issuecomment-242218633:118,Testability,log,log,118,"This still causes error messages. `hail -l /mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed.Chr${num}.QC.vds.test.log importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr${num}/TopMed_8k.853.vcf.bgz splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count`. `2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@716: Client environment:host.name=nid00014; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@723: Client environment:os.name=Linux; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-431.el6_1.0000.9051-cray_ari_athena_c_cos; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Thu Jan 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633
https://github.com/hail-is/hail/pull/663#issuecomment-242074915:260,Availability,error,error,260,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915
https://github.com/hail-is/hail/pull/663#issuecomment-242074915:0,Deployability,Update,Update,0,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915
https://github.com/hail-is/hail/pull/663#issuecomment-242074915:253,Deployability,Update,Update,253,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915
https://github.com/hail-is/hail/pull/663#issuecomment-242074915:266,Integrability,message,message,266,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915
https://github.com/hail-is/hail/pull/663#issuecomment-242074915:302,Security,access,accessing,302,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915
https://github.com/hail-is/hail/pull/663#issuecomment-242153148:13,Availability,error,error,13,Just fix the error message in AST and I'm happy with it!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242153148
https://github.com/hail-is/hail/pull/663#issuecomment-242153148:19,Integrability,message,message,19,Just fix the error message in AST and I'm happy with it!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242153148
https://github.com/hail-is/hail/issues/667#issuecomment-242106481:125,Availability,error,errors,125,"PartitionedParquetRelation currently does not compile on spark 1.6.0. If compiled with spark 1.5.0 and run on 1.6.0, cryptic errors appear from changed method signatures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/667#issuecomment-242106481
https://github.com/hail-is/hail/issues/669#issuecomment-242106016:22,Deployability,deploy,deploying,22,This is an issue with deploying on spark 1.6.0. Subsumed by #667,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669#issuecomment-242106016
https://github.com/hail-is/hail/issues/674#issuecomment-242212918:137,Deployability,install,install,137,"When TeamCity is using SSL, we can enable GitHub to push notify TeamCity of commits. We'll need to create a TeamCity user for github and install the credentials in the TeamCity Service hook for the broadinstitute/hail project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-242212918
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1246,Availability,Avail,Available,1246,"odified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1318,Availability,error,error,1318,"VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1452,Availability,Error,ErrorLog,1452,"ins.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1479,Availability,error,error,1479,"ins.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1580,Availability,avail,available,1580,"me directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/sub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1873,Availability,avail,available,1873,"s Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2,Deployability,update,updated,2,"I updated the HTTP to HTTPS redirect in `/etc/apache2/sites-enabled/000-default.conf` to preserve the sub-domains:. ``` apache; RewriteEngine on; RewriteCond %{HTTPS} off; RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [END,QSA,R=permanent]; ```. and I modified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsenc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1550,Deployability,configurat,configuration,1550,"me directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/sub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1768,Deployability,configurat,configuration,1768,"xt of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:81",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:128,Modifiability,Rewrite,RewriteEngine,128,"I updated the HTTP to HTTPS redirect in `/etc/apache2/sites-enabled/000-default.conf` to preserve the sub-domains:. ``` apache; RewriteEngine on; RewriteCond %{HTTPS} off; RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [END,QSA,R=permanent]; ```. and I modified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsenc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:146,Modifiability,Rewrite,RewriteCond,146,"I updated the HTTP to HTTPS redirect in `/etc/apache2/sites-enabled/000-default.conf` to preserve the sub-domains:. ``` apache; RewriteEngine on; RewriteCond %{HTTPS} off; RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [END,QSA,R=permanent]; ```. and I modified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsenc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:172,Modifiability,Rewrite,RewriteRule,172,"I updated the HTTP to HTTPS redirect in `/etc/apache2/sites-enabled/000-default.conf` to preserve the sub-domains:. ``` apache; RewriteEngine on; RewriteCond %{HTTPS} off; RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [END,QSA,R=permanent]; ```. and I modified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsenc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1371,Modifiability,config,configure,1371,"e-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadMo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1550,Modifiability,config,configuration,1550,"me directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/sub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1768,Modifiability,config,configuration,1768,"xt of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:81",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2175,Performance,Load,LoadModule,2175,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2238,Performance,Load,LoadModule,2238,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2311,Performance,Load,LoadModule,2311,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2378,Performance,Load,LoadModule,2378,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2589,Safety,timeout,timeout,2589,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2735,Safety,timeout,timeout,2735,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1518,Security,access,access,1518,"74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; Proxy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1256,Testability,log,loglevels,1256,"odified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1385,Testability,log,loglevel,1385,"e-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadMo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1428,Testability,Log,LogLevel,1428,"iki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1485,Testability,log,log,1485,"74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; Proxy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1525,Testability,log,log,1525,"mCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170
https://github.com/hail-is/hail/issues/675#issuecomment-245383790:2307,Availability,echo,echo,2307,"; drwxr-xr-x 8 teamcity www-data 4096 Sep 7 18:55 ./; drwxr-xr-x 4 root root 4096 Sep 7 18:16 ../; drwxrwxr-x 6 teamcity teamcity 4096 Sep 7 18:33 .BuildServer/; drwxr-xr-x 5 teamcity www-data 4096 Sep 7 18:55 .gradle/; drwxr-xr-x 13 teamcity teamcity 4096 Aug 22 19:22 TeamCity/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent1/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent2/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent3/; ```. ### Update the `init.d` scripts. #### `/etc/init.d/teamcity`. ```; #!/bin/sh; ### BEGIN INIT INFO; # Provides: teamcity ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcity ; # Description: teamcity build server; ### END INIT INFO; # /etc/init.d/teamcity - startup script for teamcity; export TEAMCITY_DATA_PATH=""/home/teamcity/.BuildServer""; export TEAMCITY_SERVER_OPTS=-Djava.awt.headless=true # Configure TeamCity for use on a headless OS. case $1 in; start); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh start; ;;. stop); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh stop; ;;. esac. exit 0; ```. #### `/etc/init.d/teamcityAgents`. ```; #!/bin/bash; ### BEGIN INIT INFO; # Provides: teamcityAgents ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcityAgents ; # Description: TeamCity build agents ; ### END INIT INFO. USER=""teamcity""; AGENTS=(TeamCityAgent1 TeamCityAgent2 TeamCityAgent3). case ""$1"" in; start); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh start; done; ;;; stop); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh stop; done; ;;; *); echo ""usage start/stop""; exit 1; ;;. esac. exit 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383790
https://github.com/hail-is/hail/issues/675#issuecomment-245383790:849,Deployability,Update,Update,849,"### Create a User. ```; sudo adduser --system teamcity --ingroup www-data; sudo groupadd teamcity; sudo adduser teamcity teamcity; ```. (for some reason it didn't create the `teamcity` group on the first line). ### Move the Binaries. I moved the TeamCity Server and Agent binaries into `/home/teamcity`:. ```; ubuntu@ip-172-31-8-190:/home/teamcity$ ll; total 32; drwxr-xr-x 8 teamcity www-data 4096 Sep 7 18:55 ./; drwxr-xr-x 4 root root 4096 Sep 7 18:16 ../; drwxrwxr-x 6 teamcity teamcity 4096 Sep 7 18:33 .BuildServer/; drwxr-xr-x 5 teamcity www-data 4096 Sep 7 18:55 .gradle/; drwxr-xr-x 13 teamcity teamcity 4096 Aug 22 19:22 TeamCity/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent1/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent2/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent3/; ```. ### Update the `init.d` scripts. #### `/etc/init.d/teamcity`. ```; #!/bin/sh; ### BEGIN INIT INFO; # Provides: teamcity ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcity ; # Description: teamcity build server; ### END INIT INFO; # /etc/init.d/teamcity - startup script for teamcity; export TEAMCITY_DATA_PATH=""/home/teamcity/.BuildServer""; export TEAMCITY_SERVER_OPTS=-Djava.awt.headless=true # Configure TeamCity for use on a headless OS. case $1 in; start); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh start; ;;. stop); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh stop; ;;. esac. exit 0; ```. #### `/etc/init.d/teamcityAgents`. ```; #!/bin/bash; ### BEGIN INIT INFO; # Provides: teamcityAgents ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcityAgents ; # Description: TeamCity build agents ; ### END INIT INFO. USER=""teamcity""; AGENTS=(TeamCityAgent1 TeamCityAgent2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383790
https://github.com/hail-is/hail/issues/675#issuecomment-245383790:1337,Modifiability,Config,Configure,1337,"; total 32; drwxr-xr-x 8 teamcity www-data 4096 Sep 7 18:55 ./; drwxr-xr-x 4 root root 4096 Sep 7 18:16 ../; drwxrwxr-x 6 teamcity teamcity 4096 Sep 7 18:33 .BuildServer/; drwxr-xr-x 5 teamcity www-data 4096 Sep 7 18:55 .gradle/; drwxr-xr-x 13 teamcity teamcity 4096 Aug 22 19:22 TeamCity/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent1/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent2/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent3/; ```. ### Update the `init.d` scripts. #### `/etc/init.d/teamcity`. ```; #!/bin/sh; ### BEGIN INIT INFO; # Provides: teamcity ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcity ; # Description: teamcity build server; ### END INIT INFO; # /etc/init.d/teamcity - startup script for teamcity; export TEAMCITY_DATA_PATH=""/home/teamcity/.BuildServer""; export TEAMCITY_SERVER_OPTS=-Djava.awt.headless=true # Configure TeamCity for use on a headless OS. case $1 in; start); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh start; ;;. stop); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh stop; ;;. esac. exit 0; ```. #### `/etc/init.d/teamcityAgents`. ```; #!/bin/bash; ### BEGIN INIT INFO; # Provides: teamcityAgents ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcityAgents ; # Description: TeamCity build agents ; ### END INIT INFO. USER=""teamcity""; AGENTS=(TeamCityAgent1 TeamCityAgent2 TeamCityAgent3). case ""$1"" in; start); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh start; done; ;;; stop); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh stop; done; ;;; *); echo ""usage start/stop""; exit 1; ;;. esac. e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383790
https://github.com/hail-is/hail/issues/675#issuecomment-245383880:40,Security,Secur,Security,40,There were no other open holes from the Security Notes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383880
https://github.com/hail-is/hail/issues/683#issuecomment-292305976:99,Safety,Safe,Safe,99,"It seems we dropped the ball on this, but I think this is old enough now to be no longer relevant. Safe to close?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683#issuecomment-292305976
https://github.com/hail-is/hail/issues/683#issuecomment-296671361:57,Integrability,interface,interface,57,"Going to close this since we no longer have Command Line interface and Hail has changed considerably. If you still have problems @lv-xy11 , please chat with us on Gitter here https://gitter.im/hail-is/hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683#issuecomment-296671361
https://github.com/hail-is/hail/issues/685#issuecomment-251701348:27,Testability,test,tested,27,"@tpoterba @cseed @jigold I tested this commend in dataflow, but this does not work.; Script; `hail importbgen -s impv1.hail.sample chr21impv1.bgen \; variantqc \; annotatevariants expr -c 'va.infoMetrc = gs.infoScore()' \; exportvariants -c 'Chrom=v.contig,rsID = va.rsid,info=infoScore,Pos=v.start,Ref=v.ref,Alt=v.alt,MAF=va.qc.AF' -o file:///medpop/afib/schoi/projects/ukbb/Result/QC/variantQC.tsv`. Log; `hail: info: running: importbgen -s impv1.hail.sample chr21impv1.bgen; hail: info: Number of BGEN files parsed: 1; hail: info: Number of samples in BGEN files: 152249; hail: info: Number of variants across all BGEN files: 982854; [Stage 0:======================================================>(155 + 1) / 156]hail: info: Coerced almost-sorted dataset; hail: info: running: variantqc; hail: info: running: annotatevariants expr -c 'va.infoMetrc = gs.infoScore()'; hail: fatal: annotatevariants expr: 'no matching signature for 'infoScore()' on 'Aggregable[Genotype]'; <input>:1:va.infoMetrc = gs.infoScore()`. Thank you",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/685#issuecomment-251701348
https://github.com/hail-is/hail/issues/685#issuecomment-251701348:402,Testability,Log,Log,402,"@tpoterba @cseed @jigold I tested this commend in dataflow, but this does not work.; Script; `hail importbgen -s impv1.hail.sample chr21impv1.bgen \; variantqc \; annotatevariants expr -c 'va.infoMetrc = gs.infoScore()' \; exportvariants -c 'Chrom=v.contig,rsID = va.rsid,info=infoScore,Pos=v.start,Ref=v.ref,Alt=v.alt,MAF=va.qc.AF' -o file:///medpop/afib/schoi/projects/ukbb/Result/QC/variantQC.tsv`. Log; `hail: info: running: importbgen -s impv1.hail.sample chr21impv1.bgen; hail: info: Number of BGEN files parsed: 1; hail: info: Number of samples in BGEN files: 152249; hail: info: Number of variants across all BGEN files: 982854; [Stage 0:======================================================>(155 + 1) / 156]hail: info: Coerced almost-sorted dataset; hail: info: running: variantqc; hail: info: running: annotatevariants expr -c 'va.infoMetrc = gs.infoScore()'; hail: fatal: annotatevariants expr: 'no matching signature for 'infoScore()' on 'Aggregable[Genotype]'; <input>:1:va.infoMetrc = gs.infoScore()`. Thank you",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/685#issuecomment-251701348
https://github.com/hail-is/hail/issues/685#issuecomment-251708654:29,Deployability,update,updated,29,"Can you try it again? @cseed updated the JAR file on data flow. Also, your code is not correct -- should be this:. ```; hail importbgen -s impv1.hail.sample chr21impv1.bgen \ variantqc \ annotatevariants expr -c 'va.infoMetrc = gs.infoScore()' \ exportvariants -c 'Chrom=v.contig,rsID = va.rsid,info= va.infoMetrc.score,Pos=v.start,Ref=v.ref,Alt=v.alt,MAF=va.qc.AF' -o file:///medpop/afib/schoi/projects/ukbb/Result/QC/variantQC.tsv; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/685#issuecomment-251708654
https://github.com/hail-is/hail/pull/686#issuecomment-242558645:163,Safety,safe,safe,163,"I think there should be a warning in the Coalesce command if k > nPartitions, and the VSM.coalesce function should require that k < nPartitions. That seems pretty safe. . Other than that, I am happy with this!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/686#issuecomment-242558645
https://github.com/hail-is/hail/pull/696#issuecomment-244097334:22,Usability,clear,clear,22,@cseed this should be clear for merge now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/696#issuecomment-244097334
https://github.com/hail-is/hail/pull/699#issuecomment-243136925:16,Modifiability,rewrite,rewrite,16,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925
https://github.com/hail-is/hail/pull/699#issuecomment-243136925:360,Testability,test,tested,360,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925
https://github.com/hail-is/hail/pull/699#issuecomment-243136925:52,Usability,feedback,feedback,52,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925
https://github.com/hail-is/hail/pull/699#issuecomment-243136925:349,Usability,usab,usable,349,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925
https://github.com/hail-is/hail/pull/717#issuecomment-244904495:479,Performance,cache,cache,479,"Addressed comments, ready for another look. Change to `Gen.partition` revealed a few other bugs in master, which don't have elegant solutions:; 1. orderedPartitioner either needs an `empty` boolean flag, or a special `EmptyOrderedPartitioner` where `partitioner.nPartitions` is 0. Otherwise, rdd.fullOuterJoin(other) crashes because it looks at partitioner nPartitions instead of rdd nPartitions. This fix involves more logic in VSM.read as well.; 2. OrderedLeftJoinRDD needs to cache partitions and pass them to OrderedRDDIterator. There is no guarantee that `getPartitions` can be called on an executor, and this was crashing in particular for ParallelCollectionRDDs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/717#issuecomment-244904495
https://github.com/hail-is/hail/pull/717#issuecomment-244904495:420,Testability,log,logic,420,"Addressed comments, ready for another look. Change to `Gen.partition` revealed a few other bugs in master, which don't have elegant solutions:; 1. orderedPartitioner either needs an `empty` boolean flag, or a special `EmptyOrderedPartitioner` where `partitioner.nPartitions` is 0. Otherwise, rdd.fullOuterJoin(other) crashes because it looks at partitioner nPartitions instead of rdd nPartitions. This fix involves more logic in VSM.read as well.; 2. OrderedLeftJoinRDD needs to cache partitions and pass them to OrderedRDDIterator. There is no guarantee that `getPartitions` can be called on an executor, and this was crashing in particular for ParallelCollectionRDDs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/717#issuecomment-244904495
https://github.com/hail-is/hail/pull/728#issuecomment-244372653:36,Deployability,patch,patch,36,"Just for the record, I have another patch coming that will ignore the partitioner on `InvalidClassException`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/728#issuecomment-244372653
https://github.com/hail-is/hail/pull/732#issuecomment-244518077:1,Integrability,message,message,1,[message added](https://github.com/hail-is/hail/issues/400#issuecomment-244517801). I think if the outer most `forAll` was a method on the Suite class we'd be fine. It's return type would be constrained to be the type of the real `forAll` monad. All the inner `forAll`s would be nameless.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/732#issuecomment-244518077
https://github.com/hail-is/hail/issues/733#issuecomment-244475645:129,Deployability,configurat,configuration,129,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645
https://github.com/hail-is/hail/issues/733#issuecomment-244475645:200,Deployability,configurat,configuration,200,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645
https://github.com/hail-is/hail/issues/733#issuecomment-244475645:62,Integrability,depend,dependencies,62,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645
https://github.com/hail-is/hail/issues/733#issuecomment-244475645:129,Modifiability,config,configuration,129,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645
https://github.com/hail-is/hail/issues/733#issuecomment-244475645:200,Modifiability,config,configuration,200,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645
https://github.com/hail-is/hail/pull/736#issuecomment-244489292:55,Performance,optimiz,optimized,55,"This looks good. Long-term, we'll have Jon's HCS as an optimized gt-only VDS, yes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/736#issuecomment-244489292
https://github.com/hail-is/hail/pull/738#issuecomment-245359022:124,Testability,test,tests,124,@cseed:; 1. We talked about finding a better way to implement this so that it's faster. I have thus far focused only on the tests and the core algorithm. The `Command` and associated docs are not yet written.; 2. I will extract `assertSameElements` and the fuzzy equality stuff into separate PRs before submitting this for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-245359022
https://github.com/hail-is/hail/pull/738#issuecomment-245359022:229,Testability,assert,assertSameElements,229,@cseed:; 1. We talked about finding a better way to implement this so that it's faster. I have thus far focused only on the tests and the core algorithm. The `Command` and associated docs are not yet written.; 2. I will extract `assertSameElements` and the fuzzy equality stuff into separate PRs before submitting this for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-245359022
https://github.com/hail-is/hail/pull/738#issuecomment-249995538:147,Deployability,install,install,147,"Here's what _local_ looks like now. Note that I've already converted to a `vds` this time. ```; dking@wmb16-359 # rm -rf foo && time ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' ; hail: info: running: read -i profile.vds; [Stage 1:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:=====================================================> (210 + 4) / 214]hail: info: timing:; read: 3.047s; ibd: 4m35.1s; ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' 924.50s user 16.11s system 333% cpu 4:42.04 total; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-249995538
https://github.com/hail-is/hail/pull/738#issuecomment-249995538:725,Deployability,install,install,725,"Here's what _local_ looks like now. Note that I've already converted to a `vds` this time. ```; dking@wmb16-359 # rm -rf foo && time ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' ; hail: info: running: read -i profile.vds; [Stage 1:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:=====================================================> (210 + 4) / 214]hail: info: timing:; read: 3.047s; ibd: 4m35.1s; ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' 924.50s user 16.11s system 333% cpu 4:42.04 total; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-249995538
https://github.com/hail-is/hail/pull/738#issuecomment-249995538:286,Performance,load,load,286,"Here's what _local_ looks like now. Note that I've already converted to a `vds` this time. ```; dking@wmb16-359 # rm -rf foo && time ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' ; hail: info: running: read -i profile.vds; [Stage 1:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:=====================================================> (210 + 4) / 214]hail: info: timing:; read: 3.047s; ibd: 4m35.1s; ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' 924.50s user 16.11s system 333% cpu 4:42.04 total; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-249995538
https://github.com/hail-is/hail/pull/738#issuecomment-249995538:375,Testability,log,logger,375,"Here's what _local_ looks like now. Note that I've already converted to a `vds` this time. ```; dking@wmb16-359 # rm -rf foo && time ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' ; hail: info: running: read -i profile.vds; [Stage 1:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:=====================================================> (210 + 4) / 214]hail: info: timing:; read: 3.047s; ibd: 4m35.1s; ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' 924.50s user 16.11s system 333% cpu 4:42.04 total; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-249995538
https://github.com/hail-is/hail/pull/738#issuecomment-250288185:296,Performance,perform,performance,296,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
https://github.com/hail-is/hail/pull/738#issuecomment-250288185:331,Performance,perform,performance,331,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
https://github.com/hail-is/hail/pull/738#issuecomment-250288185:556,Performance,load,load,556,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
https://github.com/hail-is/hail/pull/738#issuecomment-250288185:57,Testability,test,tests,57,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
https://github.com/hail-is/hail/pull/738#issuecomment-250288185:119,Testability,log,log,119,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
https://github.com/hail-is/hail/pull/738#issuecomment-250288185:645,Testability,log,logger,645,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
https://github.com/hail-is/hail/pull/738#issuecomment-250288185:355,Usability,usab,usability,355,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
https://github.com/hail-is/hail/issues/742#issuecomment-245033280:19,Deployability,Configurat,Configuration,19,"The [Hail CI Build Configuration](https://ci.hail.is/admin/editBuildRunners.html?id=buildType:HailSourceCode_HailCi) (admin login required) now runs `gradle clean compileTestScala` against three spark versions: `1.6.2`, `1.5.2`, and `1.6.0-cdh5.7.2`. If all of those succeed, it runs `gradle clean test createDocs` against the default spark version in the gradle script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245033280
https://github.com/hail-is/hail/issues/742#issuecomment-245033280:19,Modifiability,Config,Configuration,19,"The [Hail CI Build Configuration](https://ci.hail.is/admin/editBuildRunners.html?id=buildType:HailSourceCode_HailCi) (admin login required) now runs `gradle clean compileTestScala` against three spark versions: `1.6.2`, `1.5.2`, and `1.6.0-cdh5.7.2`. If all of those succeed, it runs `gradle clean test createDocs` against the default spark version in the gradle script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245033280
https://github.com/hail-is/hail/issues/742#issuecomment-245033280:124,Testability,log,login,124,"The [Hail CI Build Configuration](https://ci.hail.is/admin/editBuildRunners.html?id=buildType:HailSourceCode_HailCi) (admin login required) now runs `gradle clean compileTestScala` against three spark versions: `1.6.2`, `1.5.2`, and `1.6.0-cdh5.7.2`. If all of those succeed, it runs `gradle clean test createDocs` against the default spark version in the gradle script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245033280
https://github.com/hail-is/hail/issues/742#issuecomment-245033280:298,Testability,test,test,298,"The [Hail CI Build Configuration](https://ci.hail.is/admin/editBuildRunners.html?id=buildType:HailSourceCode_HailCi) (admin login required) now runs `gradle clean compileTestScala` against three spark versions: `1.6.2`, `1.5.2`, and `1.6.0-cdh5.7.2`. If all of those succeed, it runs `gradle clean test createDocs` against the default spark version in the gradle script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245033280
https://github.com/hail-is/hail/issues/742#issuecomment-245038405:41,Deployability,configurat,configuration,41,"[First successful master build with this configuration](https://ci.hail.is/viewLog.html?buildId=493&buildTypeId=HailSourceCode_HailCi&tab=buildLog&consoleStyle=false#_state=116,125&focus=123). PR builds will also trigger the three compiles before the `clean test createDocs`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245038405
https://github.com/hail-is/hail/issues/742#issuecomment-245038405:41,Modifiability,config,configuration,41,"[First successful master build with this configuration](https://ci.hail.is/viewLog.html?buildId=493&buildTypeId=HailSourceCode_HailCi&tab=buildLog&consoleStyle=false#_state=116,125&focus=123). PR builds will also trigger the three compiles before the `clean test createDocs`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245038405
https://github.com/hail-is/hail/issues/742#issuecomment-245038405:258,Testability,test,test,258,"[First successful master build with this configuration](https://ci.hail.is/viewLog.html?buildId=493&buildTypeId=HailSourceCode_HailCi&tab=buildLog&consoleStyle=false#_state=116,125&focus=123). PR builds will also trigger the three compiles before the `clean test createDocs`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245038405
https://github.com/hail-is/hail/issues/743#issuecomment-287896407:123,Testability,test,tests,123,@danking We can delete this right? No more Cray plus Dataflow seems like it's not something we wanna be using. Plus we run tests on cloud.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/743#issuecomment-287896407
https://github.com/hail-is/hail/issues/744#issuecomment-248415882:35,Deployability,deploy,deploy,35,You should be able to use the test/deploy script from internal-resources: https://github.com/hail-is/internal-resources/blob/master/gcp_build.sh.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/744#issuecomment-248415882
https://github.com/hail-is/hail/issues/744#issuecomment-248415882:30,Testability,test,test,30,You should be able to use the test/deploy script from internal-resources: https://github.com/hail-is/internal-resources/blob/master/gcp_build.sh.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/744#issuecomment-248415882
https://github.com/hail-is/hail/issues/747#issuecomment-256325883:147,Availability,echo,echo,147,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883
https://github.com/hail-is/hail/issues/747#issuecomment-256325883:162,Testability,test,test,162,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883
https://github.com/hail-is/hail/issues/747#issuecomment-256325883:196,Testability,test,test,196,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883
https://github.com/hail-is/hail/issues/747#issuecomment-256325883:237,Testability,test,test,237,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883
https://github.com/hail-is/hail/issues/747#issuecomment-256325883:244,Testability,test,test,244,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883
https://github.com/hail-is/hail/issues/747#issuecomment-256325883:278,Testability,test,test,278,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883
https://github.com/hail-is/hail/pull/748#issuecomment-245383248:488,Testability,assert,assert,488,"I'm pretty sure we do need to fix evaluation, too -- the java type of the result needs to be consistent, because the way we box things means it can't be implicitly converted the way numeric types can in java. So if an `Int`/`Long` if statement is promoted to `Long`, the result of evaluation needs to be a long in both cases. I think you can add this case to ExprSuite and it'll show the problem:. ``` scala; def checkTypeConcordance(s: String) {; val (t, result) = evalWithType[Any](s); assert(t.asInstanceOf[Type].typeCheck(result)); }; checkTypeConcordance(""""""if (true) 0 else 0.toLong""""""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/748#issuecomment-245383248
https://github.com/hail-is/hail/pull/748#issuecomment-245384057:9,Usability,simpl,simpler,9,"Another (simpler) check that I think will produce a `ClassCastException` :. ``` scala; eval[Int]("""""" (if (true) 0 else 0.toLong).toInt """""" ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/748#issuecomment-245384057
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:242,Availability,error,errors,242,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:718,Availability,down,download,718,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:913,Availability,down,downloaded,913,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:200,Deployability,patch,patch,200,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:735,Deployability,patch,patch,735,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:828,Deployability,patch,patch,828,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:924,Deployability,patch,patch,924,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-248645143:125,Usability,simpl,simple,125,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
https://github.com/hail-is/hail/pull/753#issuecomment-251409770:97,Deployability,patch,patch,97,Thank you for the clear and easy-to-follow instructions! I've rebased my commit and applied your patch. What do I need to do now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-251409770
https://github.com/hail-is/hail/pull/753#issuecomment-251409770:18,Usability,clear,clear,18,Thank you for the clear and easy-to-follow instructions! I've rebased my commit and applied your patch. What do I need to do now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-251409770
https://github.com/hail-is/hail/pull/753#issuecomment-257354062:10,Availability,ping,ping,10,@tpoterba ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-257354062
https://github.com/hail-is/hail/pull/753#issuecomment-257355964:168,Availability,error,errors,168,"Yeah, sorry about this. I've been working on de novo stuff on the side and am really unhappy about the current trio abstraction (pasting a bunch of code in from Mendel errors). I'll review as it is now, though, so we can get this in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-257355964
https://github.com/hail-is/hail/issues/754#issuecomment-245692697:221,Availability,avail,available,221,"I modified the artifacts to include the two uber jars:. ```; +:build/docs => docs; +:build/libs/hail-all-spark.jar; +:build/libs/hail-all-spark-test.jar; ```. The latest successful master build of `hail-all-spark.jar` is available at:. https://ci.hail.is/httpAuth/app/rest/builds/buildType:(id:HailSourceCode_HailCi),count:1,status:SUCCESS/artifacts/content/hail-all-spark.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/754#issuecomment-245692697
https://github.com/hail-is/hail/issues/754#issuecomment-245692697:144,Testability,test,test,144,"I modified the artifacts to include the two uber jars:. ```; +:build/docs => docs; +:build/libs/hail-all-spark.jar; +:build/libs/hail-all-spark-test.jar; ```. The latest successful master build of `hail-all-spark.jar` is available at:. https://ci.hail.is/httpAuth/app/rest/builds/buildType:(id:HailSourceCode_HailCi),count:1,status:SUCCESS/artifacts/content/hail-all-spark.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/754#issuecomment-245692697
https://github.com/hail-is/hail/pull/758#issuecomment-245462802:161,Testability,test,test,161,"Hmm. I'd think that Int + Float would be promoted to Double, is that not so? Maybe we've got an issue somewhere else. . I think putting that example in the expr test you added would be a great idea!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/758#issuecomment-245462802
https://github.com/hail-is/hail/pull/758#issuecomment-245465288:63,Testability,log,logic,63,"Ah, good catch! Yep, this is a bug in the current master. That logic should be replaced with the conversions, and `makeDouble` and `makeLong` should go away.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/758#issuecomment-245465288
https://github.com/hail-is/hail/pull/765#issuecomment-248466826:235,Testability,test,test,235,"I moved the info score calculation to the stats package, got rid of the combiner class, and used `mapAnnotations` to modify the VDS. Let me know if I should use `mapAnnotationsWithAggregate` instead. I also removed the RDD join in the test suite. The reason for not comparing to QCTOOL is how Cotton and I decided to handle missing values (0,0,0). QCTOOL uses the MAF estimate from the other samples to mean impute the dosage for samples with missing values. We do not include those samples at all in the computation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/765#issuecomment-248466826
https://github.com/hail-is/hail/pull/767#issuecomment-250774694:0,Deployability,Update,Updated,0,Updated docs. Back to you,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/767#issuecomment-250774694
https://github.com/hail-is/hail/issues/780#issuecomment-279763636:249,Integrability,interface,interface,249,"No, I wanted the equivalent of this:. Inbreeding | `--het` | `vds.annotate_samples_expr(""sa.het = gs.inbreeding(g => va.qc.AF)"")`; Sex Check | `--check-sex` | `vds.impute_sex()....`; etc. I think we're getting close to being able to do this (python interface is stable).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/780#issuecomment-279763636
https://github.com/hail-is/hail/issues/786#issuecomment-247090248:152,Integrability,message,message,152,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/786#issuecomment-247090248
https://github.com/hail-is/hail/issues/786#issuecomment-247090248:198,Integrability,message,message,198,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/786#issuecomment-247090248
https://github.com/hail-is/hail/issues/786#issuecomment-247090248:285,Integrability,message,message,285,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/786#issuecomment-247090248
https://github.com/hail-is/hail/issues/786#issuecomment-247090248:452,Testability,assert,assert,452,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/786#issuecomment-247090248
https://github.com/hail-is/hail/issues/797#issuecomment-247439570:60,Safety,unsafe,unsafe,60,"Do this if smaller number of partitions than VDS, otherwise unsafe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/797#issuecomment-247439570
https://github.com/hail-is/hail/pull/799#issuecomment-247485500:64,Testability,test,test,64,"@danking: I addressed a few outstanding issues, and added a new test for coercion of unsorted-within-partitions RDDs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/799#issuecomment-247485500
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:319,Availability,down,down,319,"When I run the current GRM with BlockMatrix locally on profile225.hardcalls.vds (2535 samples, 225k variants), I get:. ```; java.lang.ArrayIndexOutOfBoundsException: 1048578; ```. When I run on profile.hardcalls.vds (only 25k variants), I get:. ```; java.lang.OutOfMemoryError: Java heap space; ```. When I cut profile down to only 10k variants, grm takes about 66s:. ```; read: 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:1588,Availability,Down,Down,1588,"we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObjec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2100,Availability,failure,failure,2100,"pper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2157,Availability,failure,failure,2157,"eAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.coll",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:1398,Deployability,update,updates,1398,": 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoft",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:1675,Deployability,update,updates,1675,"we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObjec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4632,Energy Efficiency,schedul,scheduler,4632,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4703,Energy Efficiency,schedul,scheduler,4703,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:3845,Performance,Cache,CacheManager,3845,.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:3871,Performance,Cache,CacheManager,3871,ollection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4311,Performance,Cache,CacheManager,4311,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4337,Performance,Cache,CacheManager,4337,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4825,Performance,concurren,concurrent,4825,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4909,Performance,concurren,concurrent,4909,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:901,Safety,avoid,avoiding,901,"When I run the current GRM with BlockMatrix locally on profile225.hardcalls.vds (2535 samples, 225k variants), I get:. ```; java.lang.ArrayIndexOutOfBoundsException: 1048578; ```. When I run on profile.hardcalls.vds (only 25k variants), I get:. ```; java.lang.OutOfMemoryError: Java heap space; ```. When I cut profile down to only 10k variants, grm takes about 66s:. ```; read: 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2079,Safety,abort,aborted,2079,"pper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2320,Usability,clear,clear,2320,"```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
https://github.com/hail-is/hail/pull/801#issuecomment-247862866:526,Performance,perform,performant,526,"@cseed I understand that BlockMatrix is useful when the sample by sample matrix is too large to store on each node. But for the current master we are still converting the result .toLocalMatrix on the driver, so each dimension on the square matrix cannot exceed the bound of 2^16 = 65k checked by the computeGrammianMatrix function anyway, as underlying Breeze DenseMatrix is backed by a single Java array. For samples > 65k, we could use BlockMatrix and write out via some sort of streaming over blocks (though it may be more performant to directly index chunks up to 65k samples each and computeGrammianMatrix on each pair of chunks). But if the number of samples is under 65k, it looks like we should still use computeGrammianMatrix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247862866
https://github.com/hail-is/hail/pull/801#issuecomment-249927850:294,Testability,test,testing,294,"Indeed, the Grammian still fails on the Kyle's 20k genomes after filtering to Purcell 5k, whereas BlockMatrix succeeds in 8 minutes on 385 cores. Using 1.6K genomes with 100k variants, timing is 2min with Grammian and 4 minutes with BlockMatrix. I'm going to close this PR until I've done more testing on the tradeoff. We should keep BlockMatrix as master, but there should be an N under which we use Grammian. These experiments are relevant to LMM too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-249927850
https://github.com/hail-is/hail/issues/803#issuecomment-279597422:9,Usability,feedback,feedback,9,@bw2 any feedback on above? Should I close this issue or do you have more inquiries on symbolic variants?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/803#issuecomment-279597422
https://github.com/hail-is/hail/pull/805#issuecomment-248069173:71,Deployability,update,update,71,"@bw2 if you squash locally and force-push to your branch, this PR will update. No need to close/open a new one",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248069173
https://github.com/hail-is/hail/pull/805#issuecomment-248072411:197,Availability,error,error,197,"Hey @tpoterba I tried, but am getting ; `remote: Permission to broadinstitute/hail.git denied to bw2.; fatal: unable to access 'https://github.com/broadinstitute/hail/': The requested URL returned error: 403`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248072411
https://github.com/hail-is/hail/pull/805#issuecomment-248072411:120,Security,access,access,120,"Hey @tpoterba I tried, but am getting ; `remote: Permission to broadinstitute/hail.git denied to bw2.; fatal: unable to access 'https://github.com/broadinstitute/hail/': The requested URL returned error: 403`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248072411
https://github.com/hail-is/hail/pull/805#issuecomment-248074937:85,Availability,error,error,85,That's a bit weird. You're pushing to the branch on your fork and still getting that error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248074937
https://github.com/hail-is/hail/pull/805#issuecomment-248081179:425,Deployability,patch,patch-,425,"@bw2 Could you paste the output of `git remote -v`? I suspect your `origin` is the broad institute origin. There's two problems with that:; - we've moved the hail repository to hail-is/hail, and; - you can only modify branches in your fork. So let's start by squashing your commits. Below I'll refer to your fork of hail as `myfork`. ```; git remote add myfork https://github.com/bw2/hail.git; git fetch myfork; git checkout patch-3; git rebase -i ecd1ed9^; git push myfork patch-3; ```. When you return to this page, you should see just one commit listed. As to your origin pointing to broad institute. If your origin looks like:. ```; origin https://github.com/broadinstitute/hail.git (fetch); origin https://github.com/broadinstitute/hail.git (push); ```. Then let's switch it to the new hail-is repository:. ```; git remote remove origin; git remote add origin https://github.com/hail-is/hail.git; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248081179
https://github.com/hail-is/hail/pull/805#issuecomment-248081179:474,Deployability,patch,patch-,474,"@bw2 Could you paste the output of `git remote -v`? I suspect your `origin` is the broad institute origin. There's two problems with that:; - we've moved the hail repository to hail-is/hail, and; - you can only modify branches in your fork. So let's start by squashing your commits. Below I'll refer to your fork of hail as `myfork`. ```; git remote add myfork https://github.com/bw2/hail.git; git fetch myfork; git checkout patch-3; git rebase -i ecd1ed9^; git push myfork patch-3; ```. When you return to this page, you should see just one commit listed. As to your origin pointing to broad institute. If your origin looks like:. ```; origin https://github.com/broadinstitute/hail.git (fetch); origin https://github.com/broadinstitute/hail.git (push); ```. Then let's switch it to the new hail-is repository:. ```; git remote remove origin; git remote add origin https://github.com/hail-is/hail.git; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248081179
https://github.com/hail-is/hail/pull/805#issuecomment-248360486:329,Deployability,patch,patch-,329,"@bw2 Clicking the edit button on the `hail-is/hail` repo is definitely one of the supported interaction pathways. Each time you click it, you should get a [fresh branch under your hail fork](https://github.com/bw2/hail/branches). So, the particular branch that needs to be squashed is [bw2/hail](https://github.com/bw2/hail/tree/patch-3). Any new commits or force pushes to bw2/hail will automatically update this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248360486
https://github.com/hail-is/hail/pull/805#issuecomment-248360486:402,Deployability,update,update,402,"@bw2 Clicking the edit button on the `hail-is/hail` repo is definitely one of the supported interaction pathways. Each time you click it, you should get a [fresh branch under your hail fork](https://github.com/bw2/hail/branches). So, the particular branch that needs to be squashed is [bw2/hail](https://github.com/bw2/hail/tree/patch-3). Any new commits or force pushes to bw2/hail will automatically update this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248360486
https://github.com/hail-is/hail/pull/805#issuecomment-248415712:4,Deployability,configurat,configuration,4,the configuration changes we made mean that it's perfectly natural to handle this PR!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248415712
https://github.com/hail-is/hail/pull/805#issuecomment-248415712:4,Modifiability,config,configuration,4,the configuration changes we made mean that it's perfectly natural to handle this PR!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248415712
https://github.com/hail-is/hail/pull/807#issuecomment-248978679:72,Testability,log,log,72,@cseed Back to you; addressed & rebased except for comment about binary log and pow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/807#issuecomment-248978679
https://github.com/hail-is/hail/issues/813#issuecomment-251530669:254,Performance,load,loading,254,"@cseed @tpoterba . See [the if section of AST.scala in my issue-813 branch](https://github.com/danking/hail/blob/issue-813/src/main/scala/org/broadinstitute/hail/expr/AST.scala#L1718-L1834). This doesn't work on a cluster because we need to do the class loading on the agents, not on the driver.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/813#issuecomment-251530669
https://github.com/hail-is/hail/issues/821#issuecomment-301784690:14,Performance,optimiz,optimizer,14,part of query optimizer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/821#issuecomment-301784690
https://github.com/hail-is/hail/pull/824#issuecomment-248511352:375,Deployability,update,updated,375,"I think this will fix the problem @xiaolicbs is having on dataflow. Some (somewhat) unrelated remarks on why `-b 16` didn't work for him. It's more subtle than I made out on gitter. `-b` sets the min block size, yes. `sc.defaultMinPartitions` is `sc.defaultParallelism`. On Cray that is 260 or however many cores we're asking for. On dataflow under YARN, that is dynamically updated based on the number of cores we have. If we're still building the DAG and haven't requested any executors yet, it is 2. We can set the default parallelism (although it won't update dynamically anymore) with `--conf spark.default.parallelism=250` for example. We should probably do this on dataflow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248511352
https://github.com/hail-is/hail/pull/824#issuecomment-248511352:557,Deployability,update,update,557,"I think this will fix the problem @xiaolicbs is having on dataflow. Some (somewhat) unrelated remarks on why `-b 16` didn't work for him. It's more subtle than I made out on gitter. `-b` sets the min block size, yes. `sc.defaultMinPartitions` is `sc.defaultParallelism`. On Cray that is 260 or however many cores we're asking for. On dataflow under YARN, that is dynamically updated based on the number of cores we have. If we're still building the DAG and haven't requested any executors yet, it is 2. We can set the default parallelism (although it won't update dynamically anymore) with `--conf spark.default.parallelism=250` for example. We should probably do this on dataflow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248511352
https://github.com/hail-is/hail/pull/824#issuecomment-248515508:10,Availability,down,down,10,I tracked down the failure. sc.textFile with a glob is non-deterministic (unlike the sc.union it replaced). So we need to either go back to the old code or reorder the partitions a la partitioned parquet reader. I'll deal with it tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248515508
https://github.com/hail-is/hail/pull/824#issuecomment-248515508:19,Availability,failure,failure,19,I tracked down the failure. sc.textFile with a glob is non-deterministic (unlike the sc.union it replaced). So we need to either go back to the old code or reorder the partitions a la partitioned parquet reader. I'll deal with it tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248515508
https://github.com/hail-is/hail/pull/824#issuecomment-248592567:102,Availability,error,error,102,"I think this is a good change but the partitioner hint won't fix Xiao's problem. Here's why:. His OOM error comes from the way we do ordered joins. His workflow was basically annotatevariants table x10, so each partition of the left ended up pulling 10 128M (compressed, so really more) chunks into memory, and boom goes the dataflow. Each table was sorted, so the partitioner hint is never applied. . I'm not sure how we can fix this without a query optimizer. It certainly seems like our current model is dangerous. If I were hand-optimizing his workflow, I might want to shuffle small text files against the vds partitioner regardless of sortedness",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248592567
https://github.com/hail-is/hail/pull/824#issuecomment-248592567:451,Performance,optimiz,optimizer,451,"I think this is a good change but the partitioner hint won't fix Xiao's problem. Here's why:. His OOM error comes from the way we do ordered joins. His workflow was basically annotatevariants table x10, so each partition of the left ended up pulling 10 128M (compressed, so really more) chunks into memory, and boom goes the dataflow. Each table was sorted, so the partitioner hint is never applied. . I'm not sure how we can fix this without a query optimizer. It certainly seems like our current model is dangerous. If I were hand-optimizing his workflow, I might want to shuffle small text files against the vds partitioner regardless of sortedness",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248592567
https://github.com/hail-is/hail/pull/824#issuecomment-248592567:533,Performance,optimiz,optimizing,533,"I think this is a good change but the partitioner hint won't fix Xiao's problem. Here's why:. His OOM error comes from the way we do ordered joins. His workflow was basically annotatevariants table x10, so each partition of the left ended up pulling 10 128M (compressed, so really more) chunks into memory, and boom goes the dataflow. Each table was sorted, so the partitioner hint is never applied. . I'm not sure how we can fix this without a query optimizer. It certainly seems like our current model is dangerous. If I were hand-optimizing his workflow, I might want to shuffle small text files against the vds partitioner regardless of sortedness",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248592567
https://github.com/hail-is/hail/pull/824#issuecomment-248632396:472,Performance,Load,LoadVCF,472,"I temporarily added this test to `OrderedRDD.coerce`:. ```; fastKeys match {; case Some(fastKeys) =>; assert(fastKeys.partitions.length == rdd.partitions.length). val A = fastKeys.mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); val B = rdd.map(_._1); .mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); assert(A == B); case None =>; }; ```. It is too expensive to run all the time. It also fails for `LoadVCF`, which doesn't filter the symbolic variants in `justVariants`. This can be fixed by filtering once beforehand.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248632396
https://github.com/hail-is/hail/pull/824#issuecomment-248632396:25,Testability,test,test,25,"I temporarily added this test to `OrderedRDD.coerce`:. ```; fastKeys match {; case Some(fastKeys) =>; assert(fastKeys.partitions.length == rdd.partitions.length). val A = fastKeys.mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); val B = rdd.map(_._1); .mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); assert(A == B); case None =>; }; ```. It is too expensive to run all the time. It also fails for `LoadVCF`, which doesn't filter the symbolic variants in `justVariants`. This can be fixed by filtering once beforehand.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248632396
https://github.com/hail-is/hail/pull/824#issuecomment-248632396:102,Testability,assert,assert,102,"I temporarily added this test to `OrderedRDD.coerce`:. ```; fastKeys match {; case Some(fastKeys) =>; assert(fastKeys.partitions.length == rdd.partitions.length). val A = fastKeys.mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); val B = rdd.map(_._1); .mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); assert(A == B); case None =>; }; ```. It is too expensive to run all the time. It also fails for `LoadVCF`, which doesn't filter the symbolic variants in `justVariants`. This can be fixed by filtering once beforehand.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248632396
https://github.com/hail-is/hail/pull/824#issuecomment-248632396:374,Testability,assert,assert,374,"I temporarily added this test to `OrderedRDD.coerce`:. ```; fastKeys match {; case Some(fastKeys) =>; assert(fastKeys.partitions.length == rdd.partitions.length). val A = fastKeys.mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); val B = rdd.map(_._1); .mapPartitionsWithIndex { case (i, it) =>; Iterator((i, it.toSet)); }.collectAsMap(); assert(A == B); case None =>; }; ```. It is too expensive to run all the time. It also fails for `LoadVCF`, which doesn't filter the symbolic variants in `justVariants`. This can be fixed by filtering once beforehand.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248632396
https://github.com/hail-is/hail/issues/825#issuecomment-248826154:22,Deployability,configurat,configuration,22,"When I check my spark configuration, it appears that:; My spark version is version 1.6.0; Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67). scala> System.getProperty(""java.version""); res0: String = 1.7.0_67. scala> val rdd = sc.parallelize(0 to 1000, 4); scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.7.0_67, 1.7.0_67, 1.7.0_67, 1.7.0_67). It seems I should also update the java relate to the Spark cluster, Thank you !",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-248826154
https://github.com/hail-is/hail/issues/825#issuecomment-248826154:467,Deployability,update,update,467,"When I check my spark configuration, it appears that:; My spark version is version 1.6.0; Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67). scala> System.getProperty(""java.version""); res0: String = 1.7.0_67. scala> val rdd = sc.parallelize(0 to 1000, 4); scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.7.0_67, 1.7.0_67, 1.7.0_67, 1.7.0_67). It seems I should also update the java relate to the Spark cluster, Thank you !",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-248826154
https://github.com/hail-is/hail/issues/825#issuecomment-248826154:22,Modifiability,config,configuration,22,"When I check my spark configuration, it appears that:; My spark version is version 1.6.0; Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67). scala> System.getProperty(""java.version""); res0: String = 1.7.0_67. scala> val rdd = sc.parallelize(0 to 1000, 4); scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.7.0_67, 1.7.0_67, 1.7.0_67, 1.7.0_67). It seems I should also update the java relate to the Spark cluster, Thank you !",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-248826154
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:871,Availability,error,errors,871,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:975,Availability,failure,failure,975,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1032,Availability,failure,failure,1032,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1178,Availability,error,error,1178,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1300,Availability,error,error,1300,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1391,Availability,error,errors,1391,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1425,Availability,error,error,1425,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1431,Integrability,message,message,1431,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:20,Modifiability,config,configured,20,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:954,Safety,abort,aborted,954,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:472,Testability,test,testing,472,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:539,Testability,test,test,539,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:920,Testability,log,log,920,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1351,Testability,test,tested,1351,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:264,Availability,error,errors,264,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:353,Security,access,access,353,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:717,Security,access,access,717,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:220,Testability,log,log,220,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:286,Testability,log,log-file,286,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:539,Testability,log,log-file,539,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:564,Testability,log,log,564,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:744,Testability,log,log,744,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-250746848:82,Usability,guid,guides,82,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:86,Availability,error,error,86,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:344,Availability,error,error,344,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:350,Integrability,message,message,350,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:59,Security,access,access,59,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:153,Security,access,access,153,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:270,Security,access,access,270,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:34,Testability,log,log,34,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:143,Testability,log,log,143,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:236,Testability,log,log,236,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:244,Testability,log,log,244,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252404979:335,Testability,log,log,335,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:445,Availability,ERROR,ERROR,445,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:903,Availability,failure,failure,903,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:960,Availability,failure,failure,960,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:1104,Availability,error,error,1104,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:1317,Availability,error,error,1317,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:803,Deployability,deploy,deploy,803,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:1185,Deployability,deploy,deploy,1185,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:58,Modifiability,config,configured,58,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:762,Safety,abort,aborted,762,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:882,Safety,abort,aborted,882,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:73,Testability,log,log,73,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:294,Testability,log,log-file,294,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/issues/825#issuecomment-252825829:319,Testability,log,log,319,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829
https://github.com/hail-is/hail/pull/826#issuecomment-248641543:142,Deployability,configurat,configuration,142,"Laurent, I was totally wrong about being able to do this per-command -- I'm really sorry. I thought that it would be possible to create a new configuration just for this command and use that, but this is only possible for `HadoopConfiguration`s and not `SparkContext`s. Can you reopen the old PR? That model is our only option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248641543
https://github.com/hail-is/hail/pull/826#issuecomment-248641543:142,Modifiability,config,configuration,142,"Laurent, I was totally wrong about being able to do this per-command -- I'm really sorry. I thought that it would be possible to create a new configuration just for this command and use that, but this is only possible for `HadoopConfiguration`s and not `SparkContext`s. Can you reopen the old PR? That model is our only option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248641543
https://github.com/hail-is/hail/pull/826#issuecomment-248643185:315,Deployability,configurat,configuration,315,"Hi Tim,. What's the problem with this implementation? I've tested it and it works... On Wed, Sep 21, 2016 at 11:07 AM, Tim Poterba notifications@github.com; wrote:. > Laurent, I was totally wrong about being able to do this per-command --; > I'm really sorry. I thought that it would be possible to create a new; > configuration just for this command and use that, but this is only possible; > for HadoopConfigurations and not SparkContexts. Can you reopen the old; > PR? That model is our only option.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248641543, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgcPW4xK16W3DlZfdE5U6RTcVmJthks5qsUhMgaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248643185
https://github.com/hail-is/hail/pull/826#issuecomment-248643185:315,Modifiability,config,configuration,315,"Hi Tim,. What's the problem with this implementation? I've tested it and it works... On Wed, Sep 21, 2016 at 11:07 AM, Tim Poterba notifications@github.com; wrote:. > Laurent, I was totally wrong about being able to do this per-command --; > I'm really sorry. I thought that it would be possible to create a new; > configuration just for this command and use that, but this is only possible; > for HadoopConfigurations and not SparkContexts. Can you reopen the old; > PR? That model is our only option.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248641543, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgcPW4xK16W3DlZfdE5U6RTcVmJthks5qsUhMgaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248643185
https://github.com/hail-is/hail/pull/826#issuecomment-248643185:59,Testability,test,tested,59,"Hi Tim,. What's the problem with this implementation? I've tested it and it works... On Wed, Sep 21, 2016 at 11:07 AM, Tim Poterba notifications@github.com; wrote:. > Laurent, I was totally wrong about being able to do this per-command --; > I'm really sorry. I thought that it would be possible to create a new; > configuration just for this command and use that, but this is only possible; > for HadoopConfigurations and not SparkContexts. Can you reopen the old; > PR? That model is our only option.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248641543, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgcPW4xK16W3DlZfdE5U6RTcVmJthks5qsUhMgaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248643185
https://github.com/hail-is/hail/pull/826#issuecomment-248645129:14,Deployability,configurat,configuration,14,"This sets the configuration permanently -- any following commands will use the overridden codecs. Setting a global option is almost certainly better than getting this kind of leakage, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248645129
https://github.com/hail-is/hail/pull/826#issuecomment-248645129:14,Modifiability,config,configuration,14,"This sets the configuration permanently -- any following commands will use the overridden codecs. Setting a global option is almost certainly better than getting this kind of leakage, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248645129
https://github.com/hail-is/hail/pull/826#issuecomment-248646084:358,Deployability,configurat,configuration,358,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084
https://github.com/hail-is/hail/pull/826#issuecomment-248646084:358,Modifiability,config,configuration,358,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084
https://github.com/hail-is/hail/pull/826#issuecomment-248646084:74,Performance,load,loading,74,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084
https://github.com/hail-is/hail/pull/826#issuecomment-248646084:40,Testability,test,tested,40,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084
https://github.com/hail-is/hail/pull/837#issuecomment-249244766:94,Availability,down,download,94,Two high-level comments:; - Here is the default documentation:. https://ci.hail.is/repository/download/HailSourceCode_HailCi/846:id/docs/index.html#exportaggregate. Some documentation of the output format and maybe and example or two (with and without `--by-matrix`) would be awesome.; - We need at least some testing. I think a simple aggregation on a small file that you verify by hand would be sufficient. I'll look over the code and let you know if I have additional comments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249244766
https://github.com/hail-is/hail/pull/837#issuecomment-249244766:310,Testability,test,testing,310,Two high-level comments:; - Here is the default documentation:. https://ci.hail.is/repository/download/HailSourceCode_HailCi/846:id/docs/index.html#exportaggregate. Some documentation of the output format and maybe and example or two (with and without `--by-matrix`) would be awesome.; - We need at least some testing. I think a simple aggregation on a small file that you verify by hand would be sufficient. I'll look over the code and let you know if I have additional comments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249244766
https://github.com/hail-is/hail/pull/837#issuecomment-249244766:329,Usability,simpl,simple,329,Two high-level comments:; - Here is the default documentation:. https://ci.hail.is/repository/download/HailSourceCode_HailCi/846:id/docs/index.html#exportaggregate. Some documentation of the output format and maybe and example or two (with and without `--by-matrix`) would be awesome.; - We need at least some testing. I think a simple aggregation on a small file that you verify by hand would be sufficient. I'll look over the code and let you know if I have additional comments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249244766
https://github.com/hail-is/hail/pull/837#issuecomment-249248847:59,Testability,test,tests,59,Agreed on both of these. I'll write some doc and add a few tests (maybe one for the long format and one for the matrix one).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249248847
https://github.com/hail-is/hail/issues/846#issuecomment-249638782:62,Availability,error,error,62,"@jigold this jogged a memory, is there, perhaps, a really bad error message for `g.ad.sum()`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249638782
https://github.com/hail-is/hail/issues/846#issuecomment-249638782:68,Integrability,message,message,68,"@jigold this jogged a memory, is there, perhaps, a really bad error message for `g.ad.sum()`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249638782
https://github.com/hail-is/hail/issues/846#issuecomment-249639159:57,Availability,error,error,57,"Yeah, it says no such method found. One of many bad expr error messages",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249639159
https://github.com/hail-is/hail/issues/846#issuecomment-249639159:63,Integrability,message,messages,63,"Yeah, it says no such method found. One of many bad expr error messages",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249639159
https://github.com/hail-is/hail/pull/851#issuecomment-250204932:8,Deployability,update,update,8,Need to update list of annotations in sampleqc doc.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/851#issuecomment-250204932
https://github.com/hail-is/hail/pull/852#issuecomment-250204532:15,Deployability,update,update,15,Also needs doc update?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/852#issuecomment-250204532
https://github.com/hail-is/hail/pull/852#issuecomment-250934102:0,Deployability,Update,Updated,0,Updated significantly. Ready for another look.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/852#issuecomment-250934102
https://github.com/hail-is/hail/issues/859#issuecomment-317578194:181,Availability,error,error,181,"The problem was the eventually consistency of the object store. You could delete a large vds, make a new one at the same path with fewer partitions, and read the new one and see an error because partition 100010 was still there",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/859#issuecomment-317578194
https://github.com/hail-is/hail/issues/862#issuecomment-319509326:91,Integrability,message,message,91,Even better: append a d (or multiple ds to make it unique) to duplicate IDs and generate a message that it happened.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/862#issuecomment-319509326
https://github.com/hail-is/hail/issues/863#issuecomment-250805502:95,Usability,simpl,simple,95,"More directly, this is known as the maximal independent set (MIS) problem, for which there are simple parallel algorithms: https://cstar.iiit.ac.in/~kkishore/MISStudy.pdf. MIS should be implemented in Spark GraphX, but I can't find it!; http://ilpubs.stanford.edu:8090/1085/2/primitives_tr_sig_alternate.pdf",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/863#issuecomment-250805502
https://github.com/hail-is/hail/issues/863#issuecomment-279597753:11,Deployability,update,update,11,@johnc1231 update this one your PR is in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/863#issuecomment-279597753
https://github.com/hail-is/hail/issues/863#issuecomment-279758262:27,Integrability,message,message,27,@johnc1231 If your *commit message* (not the PR) contains `fixes #863` you should be good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/863#issuecomment-279758262
https://github.com/hail-is/hail/issues/869#issuecomment-251128189:348,Modifiability,maintainab,maintainability,348,"I wonder if this occurs with the Hadoop-BAM codec too? I think the issue before that led to it being reverted (https://github.com/hail-is/hail/issues/566) was because the split size was too small, which would be fairly easy to fix by putting a limit in place (so that -b/-n are hints, in other words). I'd be happy to work on such a change. From a maintainability point of view it would be best if we could have a single codec that is shared (GATK is using the Hadoop-BAM codec, for example) so we don't duplicate work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/869#issuecomment-251128189
https://github.com/hail-is/hail/pull/885#issuecomment-251244777:20,Availability,Reboot,Rebooting,20,Addressed comments. Rebooting IntelliJ didn't fix it. I added some extra braces and now it looks correct. Also included @danking's suggestion.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/885#issuecomment-251244777
https://github.com/hail-is/hail/pull/886#issuecomment-251185250:83,Deployability,update,updated,83,Thanks for the comments and the rewording in the docs @danking ! I just pushed the updated scala files.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/886#issuecomment-251185250
https://github.com/hail-is/hail/pull/894#issuecomment-251433966:363,Usability,clear,clearer,363,"Suggestions:; 1) change `va.maf` to `va.panel_maf` in the example, say first something like ""Suppose we have already added a variant annotation `va.panel_maf` with allele frequencies computed from a reference panel.""; 2) add ""if unspecified, MAF will be estimated from the dataset"" to `-m`. How about using `-maf` instead of `-m`, it's still super short and much clearer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/894#issuecomment-251433966
https://github.com/hail-is/hail/issues/902#issuecomment-251721691:298,Deployability,configurat,configuration,298,"This is a little confusing and not especially well documented on our end. The `--tmpdir` option set within Hail defines the scratch space for certain Hail operations, but not Spark ones. The tempdir that's blowing up on you is the `spark.local.dir` setting (see http://spark.apache.org/docs/latest/configuration.html for more info). This directory is used when Spark needs to do a data shuffle, and currently the `splitmulti` command requires this. I assume you're submitting to a cluster with a `spark-submit` invocation. You can set spark settings by passing the `--conf` argument, as here:. ``` text; /usr/bin/spark-submit \ ; --conf spark.shuffle.compress=true \ ; --conf spark.local.dir=/correct/tmp/dir \; --class org.broadinstitute.hail.driver.Main \ ; JAR \; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251721691
https://github.com/hail-is/hail/issues/902#issuecomment-251721691:298,Modifiability,config,configuration,298,"This is a little confusing and not especially well documented on our end. The `--tmpdir` option set within Hail defines the scratch space for certain Hail operations, but not Spark ones. The tempdir that's blowing up on you is the `spark.local.dir` setting (see http://spark.apache.org/docs/latest/configuration.html for more info). This directory is used when Spark needs to do a data shuffle, and currently the `splitmulti` command requires this. I assume you're submitting to a cluster with a `spark-submit` invocation. You can set spark settings by passing the `--conf` argument, as here:. ``` text; /usr/bin/spark-submit \ ; --conf spark.shuffle.compress=true \ ; --conf spark.local.dir=/correct/tmp/dir \; --class org.broadinstitute.hail.driver.Main \ ; JAR \; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251721691
https://github.com/hail-is/hail/issues/902#issuecomment-251732459:22,Deployability,install,installed,22,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459
https://github.com/hail-is/hail/issues/902#issuecomment-251732459:49,Deployability,install,installDist,49,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459
https://github.com/hail-is/hail/issues/902#issuecomment-251732459:183,Deployability,install,installing,183,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459
https://github.com/hail-is/hail/issues/902#issuecomment-251732459:237,Modifiability,config,configure,237,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459
https://github.com/hail-is/hail/issues/902#issuecomment-251732459:135,Performance,load,loaded,135,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459
https://github.com/hail-is/hail/issues/902#issuecomment-251747734:75,Energy Efficiency,schedul,scheduler,75,Can you give us a bit more information about your cluster? Are you using a scheduler like GridEngine or LSF? Are your files stored in NFS?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251747734
https://github.com/hail-is/hail/issues/902#issuecomment-251762194:32,Energy Efficiency,schedul,scheduler,32,"I am using a cluster with a PBS scheduler. Hail and my files are located in my home directory which is on a mounted NFS. The same NFS is mounted, and accessible, on the worker nodes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251762194
https://github.com/hail-is/hail/issues/902#issuecomment-251762194:150,Security,access,accessible,150,"I am using a cluster with a PBS scheduler. Hail and my files are located in my home directory which is on a mounted NFS. The same NFS is mounted, and accessible, on the worker nodes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251762194
https://github.com/hail-is/hail/issues/902#issuecomment-251802552:187,Security,access,access,187,I feel I may have complicated things slightly. It's probably not relevant that its running on a cluster. I'm essentially just using a node on the cluster as a workstation as I can easily access the VCFs from there. At the moment I'm trying out basic functionality of hail. I'm just using a single node and running a single instance of hail. All I have done so far is import a VCF and filter out some samples. Then when trying to annotate variants using a bed file I ran into the above issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251802552
https://github.com/hail-is/hail/issues/902#issuecomment-251923422:66,Modifiability,variab,variable,66,I have found a solution. It is necessary to set the environmental variable SPARK_LOCAL_DIRS to the desired temp location. e.g. ```; export SPARK_LOCAL_DIRS=/local; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251923422
https://github.com/hail-is/hail/issues/902#issuecomment-256958955:26,Usability,feedback,feedback,26,"Great, and thanks for the feedback. I'll close the issue now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-256958955
https://github.com/hail-is/hail/pull/906#issuecomment-251835211:3189,Energy Efficiency,green,green,3189,"svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 75.9 7.194; 3500 15.350 6.516 70.9 7.210; 4000 20.584 9.111 112.6 10.725; 4000 22.085 9.476 110.3 10.594; 4000 21.920 9.461 108.2 11.062; 4500 29.075 13.488 143.8 15.440; 4500 30.305 13.402 140.2 15.338; 4500 31.339 13.562 134.3 15.294; 5000 43.908 17.818 196.0 21.286; 5000 40.874 17.821 197.7 21.231; 5000 41.582 18.088 198.9 21.357; 5500 58.772 24.747 271.1 28.879; 5500 60.6 23.844 269.8 28.130; 5500 60.9 24.197 275.3 28.356; ```. Here's the R code for the plot for reference. I gave up on making a proper legend. ```; library(ggplot2); df <- read.table(""/Users/Jon/Desktop/svdEigen.tsv"",header=TRUE); ggplot(df, aes(dim)) + ; geom_point(aes(y = svd), color=""black"") + ; geom_smooth(aes(y = svd), method=lm, formula = y ~ poly(x, 3), color=""orange"", fullrange=TRUE) +; geom_point(aes(y = symEigD), color=""black"") + ; geom_smooth(aes(y = symEigD), method=lm, formula = y ~ poly(x, 3), color=""blue"", fullrange=TRUE) +; geom_point(aes(y = symEig), color=""black"") + ; geom_smooth(aes(y = symEig), method=lm, formula = y ~ poly(x, 3), color=""red"", fullrange=TRUE) +; geom_point(aes(y = symEigR), color=""black"") + ; geom_smooth(aes(y = symEigR), method=lm, formula = y ~ poly(x, 3), color=""green"", fullrange=TRUE) +; xlim(0, 360) +; ylim(0, 6000) +; ggtitle(""symEig (red), svd (orange), symEigR (green), symEigD (blue)\nstandard Wishart matrix, cubic spline"") +; labs(x=""dimension"", y=""seconds""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211
https://github.com/hail-is/hail/pull/906#issuecomment-251835211:3295,Energy Efficiency,green,green,3295,"svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 75.9 7.194; 3500 15.350 6.516 70.9 7.210; 4000 20.584 9.111 112.6 10.725; 4000 22.085 9.476 110.3 10.594; 4000 21.920 9.461 108.2 11.062; 4500 29.075 13.488 143.8 15.440; 4500 30.305 13.402 140.2 15.338; 4500 31.339 13.562 134.3 15.294; 5000 43.908 17.818 196.0 21.286; 5000 40.874 17.821 197.7 21.231; 5000 41.582 18.088 198.9 21.357; 5500 58.772 24.747 271.1 28.879; 5500 60.6 23.844 269.8 28.130; 5500 60.9 24.197 275.3 28.356; ```. Here's the R code for the plot for reference. I gave up on making a proper legend. ```; library(ggplot2); df <- read.table(""/Users/Jon/Desktop/svdEigen.tsv"",header=TRUE); ggplot(df, aes(dim)) + ; geom_point(aes(y = svd), color=""black"") + ; geom_smooth(aes(y = svd), method=lm, formula = y ~ poly(x, 3), color=""orange"", fullrange=TRUE) +; geom_point(aes(y = symEigD), color=""black"") + ; geom_smooth(aes(y = symEigD), method=lm, formula = y ~ poly(x, 3), color=""blue"", fullrange=TRUE) +; geom_point(aes(y = symEig), color=""black"") + ; geom_smooth(aes(y = symEig), method=lm, formula = y ~ poly(x, 3), color=""red"", fullrange=TRUE) +; geom_point(aes(y = symEigR), color=""black"") + ; geom_smooth(aes(y = symEigR), method=lm, formula = y ~ poly(x, 3), color=""green"", fullrange=TRUE) +; xlim(0, 360) +; ylim(0, 6000) +; ggtitle(""symEig (red), svd (orange), symEigR (green), symEigD (blue)\nstandard Wishart matrix, cubic spline"") +; labs(x=""dimension"", y=""seconds""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211
https://github.com/hail-is/hail/pull/906#issuecomment-251835211:349,Integrability,rout,routines-dsyev-dsyevd-dsyevx-and-d,349,"We'll use these functions for eigen-decomposition of kernel in linear mixed model, and elsewhere. The Breeze method symEig calls to LAPACK dsyev, which performs poorly relative to dsyevd and dsyevr. See:; http://www.netlib.org/lapack/lawnspdf/lawn183.pdf. http://scicomp.stackexchange.com/questions/11827/flop-counts-for-lapack-symmetric-eigenvalue-routines-dsyev-dsyevd-dsyevx-and-d. I added interfaces to [dsyevd](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html) and [dsyevr](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga2ad9f4a91cddbf67fe41b621bd158f5c.html) and used a version of symEigSpeedTest in symEigDSuite to test performance on my mid 2015 MacBook Pro (2.8 GHz Intel Core i7). Here's a plot from dimension 500 to 5500:; [symEig6k.pdf](https://github.com/hail-is/hail/files/512569/symEig6k.pdf). Extrapolating to 10k:; [symEig10k.pdf](https://github.com/hail-is/hail/files/512571/symEig10k.pdf). And to 25k for kicks (though by then we're at 5G of RAM...):; [symEig25k.pdf](https://github.com/hail-is/hail/files/512580/symEig25k.pdf). So at least on a [Wishart matrix](https://en.wikipedia.org/wiki/Wishart_distribution), symEigD (dsyevd) is best, with symEigR (dsyevr) close behind, then svd (dgesdd) about 2.5x worse than symEigD, and then symEig (dsyev) about 10x worse than symEigD. ```; dim svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211
https://github.com/hail-is/hail/pull/906#issuecomment-251835211:393,Integrability,interface,interfaces,393,"We'll use these functions for eigen-decomposition of kernel in linear mixed model, and elsewhere. The Breeze method symEig calls to LAPACK dsyev, which performs poorly relative to dsyevd and dsyevr. See:; http://www.netlib.org/lapack/lawnspdf/lawn183.pdf. http://scicomp.stackexchange.com/questions/11827/flop-counts-for-lapack-symmetric-eigenvalue-routines-dsyev-dsyevd-dsyevx-and-d. I added interfaces to [dsyevd](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html) and [dsyevr](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga2ad9f4a91cddbf67fe41b621bd158f5c.html) and used a version of symEigSpeedTest in symEigDSuite to test performance on my mid 2015 MacBook Pro (2.8 GHz Intel Core i7). Here's a plot from dimension 500 to 5500:; [symEig6k.pdf](https://github.com/hail-is/hail/files/512569/symEig6k.pdf). Extrapolating to 10k:; [symEig10k.pdf](https://github.com/hail-is/hail/files/512571/symEig10k.pdf). And to 25k for kicks (though by then we're at 5G of RAM...):; [symEig25k.pdf](https://github.com/hail-is/hail/files/512580/symEig25k.pdf). So at least on a [Wishart matrix](https://en.wikipedia.org/wiki/Wishart_distribution), symEigD (dsyevd) is best, with symEigR (dsyevr) close behind, then svd (dgesdd) about 2.5x worse than symEigD, and then symEig (dsyev) about 10x worse than symEigD. ```; dim svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211
https://github.com/hail-is/hail/pull/906#issuecomment-251835211:152,Performance,perform,performs,152,"We'll use these functions for eigen-decomposition of kernel in linear mixed model, and elsewhere. The Breeze method symEig calls to LAPACK dsyev, which performs poorly relative to dsyevd and dsyevr. See:; http://www.netlib.org/lapack/lawnspdf/lawn183.pdf. http://scicomp.stackexchange.com/questions/11827/flop-counts-for-lapack-symmetric-eigenvalue-routines-dsyev-dsyevd-dsyevx-and-d. I added interfaces to [dsyevd](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html) and [dsyevr](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga2ad9f4a91cddbf67fe41b621bd158f5c.html) and used a version of symEigSpeedTest in symEigDSuite to test performance on my mid 2015 MacBook Pro (2.8 GHz Intel Core i7). Here's a plot from dimension 500 to 5500:; [symEig6k.pdf](https://github.com/hail-is/hail/files/512569/symEig6k.pdf). Extrapolating to 10k:; [symEig10k.pdf](https://github.com/hail-is/hail/files/512571/symEig10k.pdf). And to 25k for kicks (though by then we're at 5G of RAM...):; [symEig25k.pdf](https://github.com/hail-is/hail/files/512580/symEig25k.pdf). So at least on a [Wishart matrix](https://en.wikipedia.org/wiki/Wishart_distribution), symEigD (dsyevd) is best, with symEigR (dsyevr) close behind, then svd (dgesdd) about 2.5x worse than symEigD, and then symEig (dsyev) about 10x worse than symEigD. ```; dim svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211
https://github.com/hail-is/hail/pull/906#issuecomment-251835211:717,Performance,perform,performance,717,"We'll use these functions for eigen-decomposition of kernel in linear mixed model, and elsewhere. The Breeze method symEig calls to LAPACK dsyev, which performs poorly relative to dsyevd and dsyevr. See:; http://www.netlib.org/lapack/lawnspdf/lawn183.pdf. http://scicomp.stackexchange.com/questions/11827/flop-counts-for-lapack-symmetric-eigenvalue-routines-dsyev-dsyevd-dsyevx-and-d. I added interfaces to [dsyevd](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html) and [dsyevr](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga2ad9f4a91cddbf67fe41b621bd158f5c.html) and used a version of symEigSpeedTest in symEigDSuite to test performance on my mid 2015 MacBook Pro (2.8 GHz Intel Core i7). Here's a plot from dimension 500 to 5500:; [symEig6k.pdf](https://github.com/hail-is/hail/files/512569/symEig6k.pdf). Extrapolating to 10k:; [symEig10k.pdf](https://github.com/hail-is/hail/files/512571/symEig10k.pdf). And to 25k for kicks (though by then we're at 5G of RAM...):; [symEig25k.pdf](https://github.com/hail-is/hail/files/512580/symEig25k.pdf). So at least on a [Wishart matrix](https://en.wikipedia.org/wiki/Wishart_distribution), symEigD (dsyevd) is best, with symEigR (dsyevr) close behind, then svd (dgesdd) about 2.5x worse than symEigD, and then symEig (dsyev) about 10x worse than symEigD. ```; dim svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211
https://github.com/hail-is/hail/pull/906#issuecomment-251835211:712,Testability,test,test,712,"We'll use these functions for eigen-decomposition of kernel in linear mixed model, and elsewhere. The Breeze method symEig calls to LAPACK dsyev, which performs poorly relative to dsyevd and dsyevr. See:; http://www.netlib.org/lapack/lawnspdf/lawn183.pdf. http://scicomp.stackexchange.com/questions/11827/flop-counts-for-lapack-symmetric-eigenvalue-routines-dsyev-dsyevd-dsyevx-and-d. I added interfaces to [dsyevd](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html) and [dsyevr](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga2ad9f4a91cddbf67fe41b621bd158f5c.html) and used a version of symEigSpeedTest in symEigDSuite to test performance on my mid 2015 MacBook Pro (2.8 GHz Intel Core i7). Here's a plot from dimension 500 to 5500:; [symEig6k.pdf](https://github.com/hail-is/hail/files/512569/symEig6k.pdf). Extrapolating to 10k:; [symEig10k.pdf](https://github.com/hail-is/hail/files/512571/symEig10k.pdf). And to 25k for kicks (though by then we're at 5G of RAM...):; [symEig25k.pdf](https://github.com/hail-is/hail/files/512580/symEig25k.pdf). So at least on a [Wishart matrix](https://en.wikipedia.org/wiki/Wishart_distribution), symEigD (dsyevd) is best, with symEigR (dsyevr) close behind, then svd (dgesdd) about 2.5x worse than symEigD, and then symEig (dsyev) about 10x worse than symEigD. ```; dim svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211
https://github.com/hail-is/hail/pull/922#issuecomment-252512944:91,Testability,test,testing,91,These are especially useful for generating random quantitative phenotypes for examples and testing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/922#issuecomment-252512944
https://github.com/hail-is/hail/pull/929#issuecomment-253989005:25,Modifiability,rewrite,rewrite,25,Closed because I want to rewrite it to take an environment VDS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/929#issuecomment-253989005
https://github.com/hail-is/hail/pull/954#issuecomment-253405048:55,Deployability,pipeline,pipeline,55,"Performance note:; to do an aggregation - export sites pipeline, master took 7m, this branch took 14s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/954#issuecomment-253405048
https://github.com/hail-is/hail/pull/954#issuecomment-253405048:0,Performance,Perform,Performance,0,"Performance note:; to do an aggregation - export sites pipeline, master took 7m, this branch took 14s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/954#issuecomment-253405048
https://github.com/hail-is/hail/pull/967#issuecomment-254005264:19,Testability,test,testing,19,This is useful for testing commands that start from a vds by first constructing a small matrix of genotypes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/967#issuecomment-254005264
https://github.com/hail-is/hail/pull/972#issuecomment-254070715:22,Testability,test,test,22,"I originally tried to test against SnpSift (another tool that does the same type of outer join on variants and inner join on samples), but encountered an inconsistent behavior in that tool and so wrote a slightly less rigorous unit test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/972#issuecomment-254070715
https://github.com/hail-is/hail/pull/972#issuecomment-254070715:232,Testability,test,test,232,"I originally tried to test against SnpSift (another tool that does the same type of outer join on variants and inner join on samples), but encountered an inconsistent behavior in that tool and so wrote a slightly less rigorous unit test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/972#issuecomment-254070715
https://github.com/hail-is/hail/pull/979#issuecomment-254556974:61,Testability,test,testing,61,"OK, got these changes in -- thanks @cseed! Still waiting for testing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/979#issuecomment-254556974
https://github.com/hail-is/hail/pull/1000#issuecomment-257306183:20,Usability,clear,clearer,20,"Great, this is much clearer now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1000#issuecomment-257306183
https://github.com/hail-is/hail/issues/1003#issuecomment-256194596:16,Availability,error,error,16,We've seen this error before on other deployments with no easy fix. I'll continue to investigate over the next couple days and get back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-256194596
https://github.com/hail-is/hail/issues/1003#issuecomment-256194596:38,Deployability,deploy,deployments,38,We've seen this error before on other deployments with no easy fix. I'll continue to investigate over the next couple days and get back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-256194596
https://github.com/hail-is/hail/issues/1003#issuecomment-261481148:268,Testability,log,log-file,268,"@tpoterba. I am sorry to replied too late! ; I was going to write to a HDFS file system. In the command:; ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```; /user/hail was the HDFS path. The spark version is :; ```; root ~ $ spark-shell; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; ```; Thank you very much for help!. *************************From Digital China Health***********************",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-261481148
https://github.com/hail-is/hail/issues/1003#issuecomment-261481148:293,Testability,log,log,293,"@tpoterba. I am sorry to replied too late! ; I was going to write to a HDFS file system. In the command:; ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```; /user/hail was the HDFS path. The spark version is :; ```; root ~ $ spark-shell; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; ```; Thank you very much for help!. *************************From Digital China Health***********************",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-261481148
https://github.com/hail-is/hail/issues/1003#issuecomment-319521657:175,Availability,error,error,175,I am having this problem when writing to `HDFS` with `write.parquet()` function.; ```java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN.```. Spark 1.6.2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319521657
https://github.com/hail-is/hail/issues/1003#issuecomment-319526578:214,Availability,reliab,reliable,214,"@zenghz What version of Hail are you using? It must be ancient. We no longer support Spark 1 and haven't for quite some time. We saw this in sporadically in some deployments, but never understood it or developed a reliable workaround. My advice would be to upgrade to Spark 2 and the latest version of Hail if possible. You might search on other forums for ideas/work arounds, for example: https://stackoverflow.com/questions/29960686/parquet-error-when-saving-from-spark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578
https://github.com/hail-is/hail/issues/1003#issuecomment-319526578:443,Availability,error,error-when-saving-from-spark,443,"@zenghz What version of Hail are you using? It must be ancient. We no longer support Spark 1 and haven't for quite some time. We saw this in sporadically in some deployments, but never understood it or developed a reliable workaround. My advice would be to upgrade to Spark 2 and the latest version of Hail if possible. You might search on other forums for ideas/work arounds, for example: https://stackoverflow.com/questions/29960686/parquet-error-when-saving-from-spark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578
https://github.com/hail-is/hail/issues/1003#issuecomment-319526578:162,Deployability,deploy,deployments,162,"@zenghz What version of Hail are you using? It must be ancient. We no longer support Spark 1 and haven't for quite some time. We saw this in sporadically in some deployments, but never understood it or developed a reliable workaround. My advice would be to upgrade to Spark 2 and the latest version of Hail if possible. You might search on other forums for ideas/work arounds, for example: https://stackoverflow.com/questions/29960686/parquet-error-when-saving-from-spark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578
https://github.com/hail-is/hail/issues/1003#issuecomment-319526578:257,Deployability,upgrade,upgrade,257,"@zenghz What version of Hail are you using? It must be ancient. We no longer support Spark 1 and haven't for quite some time. We saw this in sporadically in some deployments, but never understood it or developed a reliable workaround. My advice would be to upgrade to Spark 2 and the latest version of Hail if possible. You might search on other forums for ideas/work arounds, for example: https://stackoverflow.com/questions/29960686/parquet-error-when-saving-from-spark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578
https://github.com/hail-is/hail/issues/1003#issuecomment-333867936:4,Availability,error,error,4,The error mentioned above is cause when container runs out of Memory. Try running with higher Memory parameters.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-333867936
https://github.com/hail-is/hail/pull/1005#issuecomment-256483820:43,Availability,error,error,43,Looks good other than a couple nits on the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820
https://github.com/hail-is/hail/pull/1005#issuecomment-256483820:49,Integrability,message,message,49,Looks good other than a couple nits on the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820
https://github.com/hail-is/hail/pull/1007#issuecomment-257220862:30,Availability,error,errors,30,Why do delete and query throw errors whenever the path is non-empty? Is there some magic recursion happening to prune to move to the end of the path?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257220862
https://github.com/hail-is/hail/pull/1007#issuecomment-257303972:440,Availability,error,error,440,"The query method is present on the generic `Type`, and overridden by `TStruct`. If you query ""info"", ""AC"", ""Test"", then you'll go to the struct implementation first, correctly identify the field ""info"", pass [""AC"", ""Test""] to that field. That field is also a struct, so you go to the struct implementation again, correctly identify ""AC"", and pass [""Test""] to that field. However, now you're querying ""Test"" on a `TArray`: this has to be an error. We catch AnnotationPathException in VSM.query. This fix is only meant to address a persistent compiler bug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972
https://github.com/hail-is/hail/pull/1007#issuecomment-257303972:108,Testability,Test,Test,108,"The query method is present on the generic `Type`, and overridden by `TStruct`. If you query ""info"", ""AC"", ""Test"", then you'll go to the struct implementation first, correctly identify the field ""info"", pass [""AC"", ""Test""] to that field. That field is also a struct, so you go to the struct implementation again, correctly identify ""AC"", and pass [""Test""] to that field. However, now you're querying ""Test"" on a `TArray`: this has to be an error. We catch AnnotationPathException in VSM.query. This fix is only meant to address a persistent compiler bug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972
https://github.com/hail-is/hail/pull/1007#issuecomment-257303972:216,Testability,Test,Test,216,"The query method is present on the generic `Type`, and overridden by `TStruct`. If you query ""info"", ""AC"", ""Test"", then you'll go to the struct implementation first, correctly identify the field ""info"", pass [""AC"", ""Test""] to that field. That field is also a struct, so you go to the struct implementation again, correctly identify ""AC"", and pass [""Test""] to that field. However, now you're querying ""Test"" on a `TArray`: this has to be an error. We catch AnnotationPathException in VSM.query. This fix is only meant to address a persistent compiler bug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972
https://github.com/hail-is/hail/pull/1007#issuecomment-257303972:349,Testability,Test,Test,349,"The query method is present on the generic `Type`, and overridden by `TStruct`. If you query ""info"", ""AC"", ""Test"", then you'll go to the struct implementation first, correctly identify the field ""info"", pass [""AC"", ""Test""] to that field. That field is also a struct, so you go to the struct implementation again, correctly identify ""AC"", and pass [""Test""] to that field. However, now you're querying ""Test"" on a `TArray`: this has to be an error. We catch AnnotationPathException in VSM.query. This fix is only meant to address a persistent compiler bug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972
https://github.com/hail-is/hail/pull/1007#issuecomment-257303972:401,Testability,Test,Test,401,"The query method is present on the generic `Type`, and overridden by `TStruct`. If you query ""info"", ""AC"", ""Test"", then you'll go to the struct implementation first, correctly identify the field ""info"", pass [""AC"", ""Test""] to that field. That field is also a struct, so you go to the struct implementation again, correctly identify ""AC"", and pass [""Test""] to that field. However, now you're querying ""Test"" on a `TArray`: this has to be an error. We catch AnnotationPathException in VSM.query. This fix is only meant to address a persistent compiler bug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972
https://github.com/hail-is/hail/issues/1010#issuecomment-256957523:124,Availability,down,down,124,"We are unlikely to support this in the short term, but are planning to refine our VCF parsing and VDS model to include this down the road.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1010#issuecomment-256957523
https://github.com/hail-is/hail/pull/1011#issuecomment-257051213:16,Modifiability,refactor,refactor,16,Closing while I refactor.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1011#issuecomment-257051213
https://github.com/hail-is/hail/pull/1012#issuecomment-257353710:108,Usability,clear,clear,108,"@jigold Did commit 23c9e2f fix the build or did the CI server break something? I guess either way, it's not clear why we had different results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1012#issuecomment-257353710
https://github.com/hail-is/hail/issues/1017#issuecomment-256799341:89,Performance,perform,performance,89,Good catch! We removed this option a week or two ago because it prevented us from making performance improvements. . There's an example here which shows how to export the variantqc annotations with the expression-based `exportvariants` command:. http://discuss.hail.is/t/export-feature-added-splatting-structs-into-multiple-columns/59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1017#issuecomment-256799341
https://github.com/hail-is/hail/issues/1017#issuecomment-256800696:59,Deployability,install,install,59,"to clarify, here's the new command to run:. ```; $ ./build/install/hail/bin/hail read ~/sample.vds \; splitmulti \; sampleqc -o ~/sampleqc.tsv \; variantqc \; exportvariants -o ~/variantqc.tsv -c 'Variant = v, va.qc.*' \; printschema; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1017#issuecomment-256800696
https://github.com/hail-is/hail/issues/1024#issuecomment-316249540:102,Deployability,update,update,102,We have the HTML and CSS for the navbar duplicated on discuss. I think it's fine that way -- we don't update the code very often.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1024#issuecomment-316249540
https://github.com/hail-is/hail/pull/1028#issuecomment-257649514:199,Testability,test,test,199,"I added the command name, and switched to printing the readable string rather than the `StorageLevel(true, false, false, false, 1)` by adding a RichStorageLevel with toReadableString(). Also added a test in UtilsSuite. @tpoterba back to you",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1028#issuecomment-257649514
https://github.com/hail-is/hail/pull/1028#issuecomment-257695252:53,Deployability,update,updates,53,Will merge when tests pass. Make a post on discourse updates to introduce the new command!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1028#issuecomment-257695252
https://github.com/hail-is/hail/pull/1028#issuecomment-257695252:16,Testability,test,tests,16,Will merge when tests pass. Make a post on discourse updates to introduce the new command!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1028#issuecomment-257695252
https://github.com/hail-is/hail/pull/1029#issuecomment-257211215:316,Testability,test,test,316,"The command is hidden because to generate reasonable memory overhead and subsequent task sizes for even 10k samples and 10k variants, we should parallelize data generation of the RDD, rather than using `sc.parallelize` on a matrix at driver. That will be a subsequent improvement. For reference, running an IntelliJ test that simply generates the integer matrix of genotypes for 10k samples and 10k variants and 4 populations with the rest default takes about 7 seconds (one core). Small examples still work fine using this command, but PCA fails at the following scale unless repartition is used first:. ```; hail \; baldingnichols \; -k 3 \; -n 2000 \; -m 10000 \; -f .02,.03,.1 \; -d .2,.3,.5 \; -s 0 \; repartition -n 8 \; printschema -o ~/data/baldingnichols/schema.json \; pca -k 3 -s 'sa.pc' -e 'global.evals' \; showglobals -o ~/data/baldingnichols/global.tsv \; exportsamples -c 'sample = s, pop = sa.bn.pop, pc = sa.pc.*' -o ~/data/baldingnichols/samples.tsv \; exportvariants -c 'variant = v, freq = va.bn.*' -o ~/data/baldingnichols/variants.tsv; ```. Here is the annotation scheme created by `baldingnichols`:. ```; Global annotation schema:; global: Struct {; bn: Struct {; seed: Int,; nPops: Int,; nSamples: Int,; nVariants: Int,; popDist: Array[Double],; Fst: Array[Double]; }; }. Sample annotation schema:; sa: Struct {; bn: Struct {; pop: Int; }; }. Variant annotation schema:; va: Struct {; bn: Struct {; ancAF: Double,; AF0: Double,; AF1: Double,; AF2: Double; }; }; ```. The following python code shows three tight clusters corresponding to population using PC1 and PC2, and that PC3 is noise:. ```; import numpy as np; import matplotlib.pyplot as plt; import pandas as pd. %matplotlib inline. df = pd.read_table(""samples.tsv""); colors = {0: 'r', 1: 'b', 2: 'g'}. df.plot('pc.PC1', 'pc.PC2', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC1', 'pc.PC3', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC2', 'pc.PC3', 'scatter', c=d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1029#issuecomment-257211215
https://github.com/hail-is/hail/pull/1029#issuecomment-257211215:326,Usability,simpl,simply,326,"The command is hidden because to generate reasonable memory overhead and subsequent task sizes for even 10k samples and 10k variants, we should parallelize data generation of the RDD, rather than using `sc.parallelize` on a matrix at driver. That will be a subsequent improvement. For reference, running an IntelliJ test that simply generates the integer matrix of genotypes for 10k samples and 10k variants and 4 populations with the rest default takes about 7 seconds (one core). Small examples still work fine using this command, but PCA fails at the following scale unless repartition is used first:. ```; hail \; baldingnichols \; -k 3 \; -n 2000 \; -m 10000 \; -f .02,.03,.1 \; -d .2,.3,.5 \; -s 0 \; repartition -n 8 \; printschema -o ~/data/baldingnichols/schema.json \; pca -k 3 -s 'sa.pc' -e 'global.evals' \; showglobals -o ~/data/baldingnichols/global.tsv \; exportsamples -c 'sample = s, pop = sa.bn.pop, pc = sa.pc.*' -o ~/data/baldingnichols/samples.tsv \; exportvariants -c 'variant = v, freq = va.bn.*' -o ~/data/baldingnichols/variants.tsv; ```. Here is the annotation scheme created by `baldingnichols`:. ```; Global annotation schema:; global: Struct {; bn: Struct {; seed: Int,; nPops: Int,; nSamples: Int,; nVariants: Int,; popDist: Array[Double],; Fst: Array[Double]; }; }. Sample annotation schema:; sa: Struct {; bn: Struct {; pop: Int; }; }. Variant annotation schema:; va: Struct {; bn: Struct {; ancAF: Double,; AF0: Double,; AF1: Double,; AF2: Double; }; }; ```. The following python code shows three tight clusters corresponding to population using PC1 and PC2, and that PC3 is noise:. ```; import numpy as np; import matplotlib.pyplot as plt; import pandas as pd. %matplotlib inline. df = pd.read_table(""samples.tsv""); colors = {0: 'r', 1: 'b', 2: 'g'}. df.plot('pc.PC1', 'pc.PC2', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC1', 'pc.PC3', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC2', 'pc.PC3', 'scatter', c=d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1029#issuecomment-257211215
https://github.com/hail-is/hail/pull/1041#issuecomment-257949200:715,Availability,redundant,redundant,715,"Current master on representative dataset:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: running: count; [Stage 2:======================================================>(661 + 2) / 663]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 5.760s; filtervariants intervals: 386.191ms; count: 32.617s; total: 38.763s; ```. and new:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: pruned 0 redundant intervals; hail: info: interval filter loaded 67 of 663 partitions; hail: info: running: count; [Stage 2:=======================================================> (65 + 2) / 67]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 9.911s; filtervariants intervals: 445.193ms; count: 3.356s; total: 13.712s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200
https://github.com/hail-is/hail/pull/1041#issuecomment-257949200:764,Performance,load,loaded,764,"Current master on representative dataset:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: running: count; [Stage 2:======================================================>(661 + 2) / 663]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 5.760s; filtervariants intervals: 386.191ms; count: 32.617s; total: 38.763s; ```. and new:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: pruned 0 redundant intervals; hail: info: interval filter loaded 67 of 663 partitions; hail: info: running: count; [Stage 2:=======================================================> (65 + 2) / 67]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 9.911s; filtervariants intervals: 445.193ms; count: 3.356s; total: 13.712s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200
https://github.com/hail-is/hail/pull/1041#issuecomment-257949200:715,Safety,redund,redundant,715,"Current master on representative dataset:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: running: count; [Stage 2:======================================================>(661 + 2) / 663]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 5.760s; filtervariants intervals: 386.191ms; count: 32.617s; total: 38.763s; ```. and new:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: pruned 0 redundant intervals; hail: info: interval filter loaded 67 of 663 partitions; hail: info: running: count; [Stage 2:=======================================================> (65 + 2) / 67]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 9.911s; filtervariants intervals: 445.193ms; count: 3.356s; total: 13.712s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200
https://github.com/hail-is/hail/pull/1041#issuecomment-257949279:11,Availability,redundant,redundant,11,Also added redundant interval pruning for `filtervariants intervals`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949279
https://github.com/hail-is/hail/pull/1041#issuecomment-257949279:11,Safety,redund,redundant,11,Also added redundant interval pruning for `filtervariants intervals`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949279
https://github.com/hail-is/hail/pull/1046#issuecomment-258220342:16,Testability,test,test,16,Added requested test. Back to you @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1046#issuecomment-258220342
https://github.com/hail-is/hail/pull/1051#issuecomment-259964811:68,Availability,error,errors,68,"By using Double and converting to Long:; 1) there may be off-by-one errors from what is expected (both 0.99999999 and -0.99999999 go to 0); 2) you can only represent positive integers up to 2^53 = 9e15 exactly, rather than 2^63:; https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64. So question is whether to create a Long version of aggregator for both Int and Long, analogous to Double for Float and Double.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-259964811
https://github.com/hail-is/hail/pull/1051#issuecomment-260983554:18,Testability,test,tests,18,"There are already tests that sum integers, and all of them still work!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-260983554
https://github.com/hail-is/hail/pull/1051#issuecomment-262054090:111,Deployability,integrat,integrate,111,This is obviated by my imminent aggregator registry changes. The L suffix change should be a separate PR. I'll integrate the tests into my branch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090
https://github.com/hail-is/hail/pull/1051#issuecomment-262054090:111,Integrability,integrat,integrate,111,This is obviated by my imminent aggregator registry changes. The L suffix change should be a separate PR. I'll integrate the tests into my branch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090
https://github.com/hail-is/hail/pull/1051#issuecomment-262054090:125,Testability,test,tests,125,This is obviated by my imminent aggregator registry changes. The L suffix change should be a separate PR. I'll integrate the tests into my branch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090
https://github.com/hail-is/hail/issues/1055#issuecomment-258299456:450,Availability,down,down,450,"Hi TJ! @tpoterba tells me he told you to make an Issue, but he forgot that we're trying to limit Issues to bug reports. Would you mind reposting this feature request on the forum and we'll follow up there?. http://discuss.hail.is/c/features. Can you also spell out a bit more what information you'd like for each parent-proband trio and how this information can be useful? We think the forum will be an easier place to get community feedback to nail down the best spec for all. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456
https://github.com/hail-is/hail/issues/1055#issuecomment-258299456:433,Usability,feedback,feedback,433,"Hi TJ! @tpoterba tells me he told you to make an Issue, but he forgot that we're trying to limit Issues to bug reports. Would you mind reposting this feature request on the forum and we'll follow up there?. http://discuss.hail.is/c/features. Can you also spell out a bit more what information you'd like for each parent-proband trio and how this information can be useful? We think the forum will be an easier place to get community feedback to nail down the best spec for all. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456
https://github.com/hail-is/hail/issues/1055#issuecomment-258307806:220,Testability,test,tests,220,"Do you the cumulative transmitted and untransmitted counts per trio? If so,; what would you do with said information?. On Nov 3, 2016 5:40 PM, ""Tarjinder Singh"" notifications@github.com wrote:. > The current TDT command tests for transmission disequilibrium for each; > variant across a number of trios. However, it would be helpful to get; > transmission information on each parent-proband trio as well, similar to; > how Mendelian-inconsistent variants are identified on Hail. Would this be; > possible?; > Thanks :); > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/issues/1055, or mute the thread; > https://github.com/notifications/unsubscribe-auth/AFZ8NdTOzvtMjAnFJ5N9Ann-ohY0EM8Iks5q6lTdgaJpZM4Ko8yy; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1055#issuecomment-258307806
https://github.com/hail-is/hail/pull/1056#issuecomment-258482849:269,Integrability,message,messages,269,"@tpoterba Rather than review, I made a few changes, mostly around using a right-biased [`Either`](http://typelevel.org/cats/datatypes/either.html) from the [`cats`](http://typelevel.org/cats/) library. I think there's still a bit more work to do (see the commented out messages). I suspect we might actually want a `LookupError` sealed trait with an alternate for ""ambiguous"" and ""not found"" so that the consumer (i.e. `AST.scala`) can produce custom messages if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1056#issuecomment-258482849
https://github.com/hail-is/hail/pull/1056#issuecomment-258482849:451,Integrability,message,messages,451,"@tpoterba Rather than review, I made a few changes, mostly around using a right-biased [`Either`](http://typelevel.org/cats/datatypes/either.html) from the [`cats`](http://typelevel.org/cats/) library. I think there's still a bit more work to do (see the commented out messages). I suspect we might actually want a `LookupError` sealed trait with an alternate for ""ambiguous"" and ""not found"" so that the consumer (i.e. `AST.scala`) can produce custom messages if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1056#issuecomment-258482849
https://github.com/hail-is/hail/pull/1061#issuecomment-258503049:335,Deployability,install,install,335,I pushed another commit with Sphinx docs for HailContext. They can be generated by running:. ```; /path/to/hail/python/pyhail/docs $ PYTHONPATH=/path/to/spark-1.6.2-bin-hadoop2.6/python:/path/to/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip make html; ```. Then the docs will be found in `_build/html`. I don't yet know how to install pyspark through gradle.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061#issuecomment-258503049
https://github.com/hail-is/hail/pull/1061#issuecomment-258921388:290,Deployability,install,installed,290,I added a makePyHailDocs gradle task that uses spark.home System property to set up the PYTHONPATH and copyPyHailDocs that moves them to build/www/pyhail. You should be able to build the docs with:. ```; hail $ gradle -Dspark.home=/path/to/spark-1.6.2-bin-hadoop2.6 createDocs; ```. I also installed Spark 1.6.2 on ci.hail.is and added spark.home to the relevant build steps.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061#issuecomment-258921388
https://github.com/hail-is/hail/pull/1064#issuecomment-260219957:26,Deployability,update,updates,26,"Docs and a bunch of other updates are done. Ready for review!. Once it's in, I'll refactor linreg, logreg, and lmmreg commands to put the phenotype and covariate extraction logic in one place under stats.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957
https://github.com/hail-is/hail/pull/1064#issuecomment-260219957:82,Modifiability,refactor,refactor,82,"Docs and a bunch of other updates are done. Ready for review!. Once it's in, I'll refactor linreg, logreg, and lmmreg commands to put the phenotype and covariate extraction logic in one place under stats.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957
https://github.com/hail-is/hail/pull/1064#issuecomment-260219957:99,Testability,log,logreg,99,"Docs and a bunch of other updates are done. Ready for review!. Once it's in, I'll refactor linreg, logreg, and lmmreg commands to put the phenotype and covariate extraction logic in one place under stats.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957
https://github.com/hail-is/hail/pull/1064#issuecomment-260219957:173,Testability,log,logic,173,"Docs and a bunch of other updates are done. Ready for review!. Once it's in, I'll refactor linreg, logreg, and lmmreg commands to put the phenotype and covariate extraction logic in one place under stats.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957
https://github.com/hail-is/hail/pull/1064#issuecomment-267979500:110,Modifiability,refactor,refactoring,110,"Modulo the exception/`Either` suggestion in `LinearMixedModel.scala`, I'm happy with this. We could push that refactoring into a follow-up pull request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-267979500
https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:257,Integrability,interface,interface,257,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058
https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:184,Modifiability,refactor,refactored,184,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058
https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:319,Safety,safe,safe,319,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058
https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:218,Testability,test,tests,218,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058
https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:300,Testability,test,testing,300,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058
https://github.com/hail-is/hail/pull/1064#issuecomment-273010901:90,Testability,test,test,90,"example python code. ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2'])). vds_kinship = vds.filter_variants_expr('va.qc.AF > .05'). vds = vds.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']). vds.export_variants('lmmreg.tsv', 'Variant = v, lin_beta = va.linreg.beta, lmm_beta = va.lmmreg.beta, lin_pval = va.linreg.pval, lmm_pval = va.lmmreg.pval, va.lmmreg.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-273010901
https://github.com/hail-is/hail/pull/1064#issuecomment-275420288:24,Testability,test,test,24,Rebased! Going to run a test today to see if results change on DataProc with and without natives.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-275420288
https://github.com/hail-is/hail/issues/1065#issuecomment-258770482:232,Performance,perform,performance,232,I've hacked a 'solution' by adding an explicit check at https://github.com/hail-is/hail/blob/607d2b4aa032c24db033359eb6f92da976a8d9f2/src/main/scala/org/broadinstitute/hail/io/vcf/HtsjdkRecordReader.scala#L59 but am not sure of the performance penalty. All tests still pass at least! :). See pull request https://github.com/hail-is/hail/pull/1066,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1065#issuecomment-258770482
https://github.com/hail-is/hail/issues/1065#issuecomment-258770482:257,Testability,test,tests,257,I've hacked a 'solution' by adding an explicit check at https://github.com/hail-is/hail/blob/607d2b4aa032c24db033359eb6f92da976a8d9f2/src/main/scala/org/broadinstitute/hail/io/vcf/HtsjdkRecordReader.scala#L59 but am not sure of the performance penalty. All tests still pass at least! :). See pull request https://github.com/hail-is/hail/pull/1066,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1065#issuecomment-258770482
https://github.com/hail-is/hail/pull/1066#issuecomment-258820833:100,Deployability,patch,patched,100,"Thanks for the review -- that's a much better approach! I've made the change. Happy to say that the patched version has just ingested a 46 million x 1200 VCF without a hitch and in just over an hour, and I'm very much looking forward to seeing what hail can do with the data tomorrow -- thanks for creating such a powerful system!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833
https://github.com/hail-is/hail/pull/1066#issuecomment-258820833:314,Energy Efficiency,power,powerful,314,"Thanks for the review -- that's a much better approach! I've made the change. Happy to say that the patched version has just ingested a 46 million x 1200 VCF without a hitch and in just over an hour, and I'm very much looking forward to seeing what hail can do with the data tomorrow -- thanks for creating such a powerful system!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833
https://github.com/hail-is/hail/issues/1073#issuecomment-365624131:47,Integrability,interface,interface,47,this is totally possible in the current Python interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1073#issuecomment-365624131
https://github.com/hail-is/hail/pull/1075#issuecomment-259742354:126,Integrability,interface,interface,126,"I tried to use `Try[T]` but it doesn't permit me to specify the subclass of `Throwable`, as such consumers are limited to the interface of `Throwable`. This seems like a better solution. I added a minimal definition of the `Either` right-biased monad, we can grow it if necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1075#issuecomment-259742354
https://github.com/hail-is/hail/issues/1082#issuecomment-260067923:689,Availability,mask,mask,689,"## The DSL. For IBS2, we need this mess:. ``` c; __m256i nxor = _mm256_xor_si256(_mm256_xor_si256(x, y), allones);. // if both bits are one, then the genotype is missing; __m256i xna_tmp = _mm256_xor_si256(_mm256_xor_si256(allNA, x), allones);; __m256i xna = _mm256_and_si256(_mm256_srli_epi64(xna_tmp, 1), xna_tmp);; __m256i yna_tmp = _mm256_xor_si256(_mm256_xor_si256(allNA, y), allones);; __m256i yna = _mm256_and_si256(_mm256_srli_epi64(yna_tmp, 1), yna_tmp);; // if either sample is missing a genotype, we ignore that genotype pair; __m256i na = _mm256_and_si256(_mm256_or_si256(xna, yna), rightAllele);; // 1. shift the left alleles over the right ones; // 2. and the alleles; // 3. mask to the right ones; __m256i ibs2 = _mm256_andnot_si256(na, _mm256_and_si256(_mm256_and_si256(_mm256_srli_epi64(nxor, 1), nxor), rightAllele));; // 4. popcnt; uint64_t ibs2sum = _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 0));; ibs2sum += _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 1));; ibs2sum += _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 2));; ibs2sum += _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 3));; ```. It'd be nice to specify it as:. ``` racket; (define (bit-eq x y) (negate (xor x y))). (let ((nxor (bit-eq x y)); (xna (bit-eq allNA x)); (yna (bit-eq allNA x)); (na (and (or xna yna) rightAllele))); (popcnt (andnot na (and (and (>>64 nxor 1) nxor) rightAllele)))); ```. And then have a little compiler that can generate C code for 128/256/512-wide registers and 2/4/16/256 alleles.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1082#issuecomment-260067923
https://github.com/hail-is/hail/issues/1082#issuecomment-422389869:37,Performance,perform,performance,37,"These kinds of ideas bring our local performance closer to PLINK, but I think there's a lot more low hanging fruit before we get here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1082#issuecomment-422389869
https://github.com/hail-is/hail/issues/1083#issuecomment-260091875:440,Security,access,access,440,I have a little reading list I still haven't gotten to yet about approximate median and the like:; - http://info.prelert.com/blog/q-digest-an-algorithm-for-computing-approximate-quantiles-on-a-collection-of-integers; - https://www.quora.com/Is-there-an-online-algorithm-to-calculate-the-median-of-a-stream-of-numbers-if-stream-elements-can-be-added-or-removed-at-any-point; - http://link.springer.com/chapter/10.1007/978-3-642-40273-9_7?no-access=true,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1083#issuecomment-260091875
https://github.com/hail-is/hail/pull/1084#issuecomment-260354398:150,Integrability,interface,interface,150,"I have hidden options in `lmmreg` PR: https://github.com/hail-is/hail/pull/1064/files#diff-f6dd198b9e7d7ee72f9ff0042b8de368. As we move to the Python interface, we may need another solution. For now I've written ""(advanced)"" at the end of these option descriptions: https://github.com/hail-is/hail/pull/1064/files#diff-b462644505a1aefc479455f4060b415a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1084#issuecomment-260354398
https://github.com/hail-is/hail/pull/1085#issuecomment-260388521:6,Testability,log,logging,6,Added logging support (logs no longer written to stderr),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1085#issuecomment-260388521
https://github.com/hail-is/hail/pull/1085#issuecomment-260388521:23,Testability,log,logs,23,Added logging support (logs no longer written to stderr),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1085#issuecomment-260388521
https://github.com/hail-is/hail/pull/1085#issuecomment-261416007:254,Integrability,wrap,wrap,254,"Can you take another look, Cotton? This PR:; - sets up the conf as we did in Main (fixes the issues Konrad has had in the last days); - sets up logging properly; - handles exceptions correctly. All new methods that call into scala VDS operations need to wrap in try/except",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1085#issuecomment-261416007
https://github.com/hail-is/hail/pull/1085#issuecomment-261416007:144,Testability,log,logging,144,"Can you take another look, Cotton? This PR:; - sets up the conf as we did in Main (fixes the issues Konrad has had in the last days); - sets up logging properly; - handles exceptions correctly. All new methods that call into scala VDS operations need to wrap in try/except",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1085#issuecomment-261416007
https://github.com/hail-is/hail/pull/1086#issuecomment-260404138:16,Testability,test,test,16,Balding-Nichols test is failing. Perhaps the seed is not propagating to all generators now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1086#issuecomment-260404138
https://github.com/hail-is/hail/issues/1091#issuecomment-260490870:5,Testability,test,tests,5,Then tests need to check if pandas is present then.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1091#issuecomment-260490870
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:976,Deployability,install,install,976,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:1671,Deployability,install,install,1671,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:390,Performance,load,loaded,390,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:463,Performance,load,loaded,463,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:1148,Performance,load,load,1148,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:325,Safety,detect,detected,325,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:239,Testability,Log,Logging,239,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:256,Testability,log,log,256,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:1237,Testability,log,logger,1237,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345
https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:72,Deployability,install,installDist,72,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639
https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:102,Deployability,install,install,102,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639
https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:358,Deployability,install,installDist,358,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639
https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:530,Performance,load,load,530,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639
https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:619,Testability,log,logger,619,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639
https://github.com/hail-is/hail/pull/1092#issuecomment-261081642:1840,Performance,bottleneck,bottleneck,1840,"In both cases we had 279 cores, 1:1 core:partition. With 37 million variants. ```; nSamples 2,535; nVariants 37,237,504; ```. ```; dking@cluster-2-m:~/hail$ spark-submit --class org.broadinstitute.hail.driver.Main build/libs/hail-all-spark.jar read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.vds/ ibd -o gs://hail-1kg/ALL.1KG.qc.genome --parallel-write; hail: info: running: read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.vds/; [Stage 1:=================================================> (5 + 1) / 6]hail: info: running: ibd -o gs://hail-1kg/ALL.1KG.qc.genome --parallel-write; [Stage 8:======================================================>(278 + 1) / 279]hail: info: timing:; read: 27.197s; ibd: 16m21.3s; total: 16m48.5s; ```. With 76 thousand variants. ```; nSamples 2,535; nVariants 76,628; ```. ```; dking@cluster-2-m:~/hail$ spark-submit --class org.broadinstitute.hail.driver.Main build/libs/hail-all-spark.jar read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.filtered.AF005.callrate98.p1000.vds/ filtervariants expr --keep -c 'v.start % 200 == 0' ibd -o gs://hail-1kg/ALL.1KG.callrate98.p1000.qc.genome; hail: info: running: read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.filtered.AF005.callrate98.p1000.vds/; [Stage 1:======================================================>(269 + 1) / 270]hail: info: running: filtervariants expr --keep -c 'v.start % 200 == 0'; hail: info: running: ibd -o gs://hail-1kg/ALL.1KG.callrate98.p1000.qc.genome; [Stage 8:======================================================>(278 + 1) / 279]hail: info: while writing:; gs://hail-1kg/ALL.1KG.callrate98.p1000.qc.genome; merge time: 3.590s; hail: info: timing:; read: 40.466s; filtervariants expr: 288.900ms; ibd: 40.869s; total: 1m21.6s; ```. I seem to have forgot the `--parallel-write` flag on the second invocation. That should make it a bit faster if writing is the bottleneck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261081642
https://github.com/hail-is/hail/pull/1092#issuecomment-261979579:186,Availability,down,download,186,"I tried to look at this, but libsimdpp is completely spamming the diff visualizer. I'm back to thinking we shouldn't include this in the repo. We should either assume it is installed or download it during the build process. I'm inclined to do the former for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579
https://github.com/hail-is/hail/pull/1092#issuecomment-261979579:173,Deployability,install,installed,173,"I tried to look at this, but libsimdpp is completely spamming the diff visualizer. I'm back to thinking we shouldn't include this in the repo. We should either assume it is installed or download it during the build process. I'm inclined to do the former for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579
https://github.com/hail-is/hail/pull/1092#issuecomment-261980358:150,Deployability,release,releases,150,👍 We can use the [tar](https://github.com/p12tic/libsimdpp/archive/v2.0-rc2.tar.gz) that he published [on github](https://github.com/p12tic/libsimdpp/releases/tag/v2.0-rc2). I'll get that change in today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261980358
https://github.com/hail-is/hail/pull/1110#issuecomment-262096177:4,Testability,test,tests,4,"The tests should be passing. I have some FIXMEs to clean up and need to rebase, but the code is basically there. You can take a look when you have a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1110#issuecomment-262096177
https://github.com/hail-is/hail/pull/1110#issuecomment-265057616:6,Usability,feedback,feedback,6,"Great feedback, thanks! I think I addressed all the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1110#issuecomment-265057616
https://github.com/hail-is/hail/pull/1114#issuecomment-266934270:7,Deployability,update,update,7,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270
https://github.com/hail-is/hail/pull/1114#issuecomment-266934270:61,Usability,guid,guide,61,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270
https://github.com/hail-is/hail/pull/1115#issuecomment-262351018:18,Availability,error,error,18,@danking off by 1 error in test:; `org.scalatest.exceptions.TestFailedException: Some(-9223372036854775807) did not contain -9223372036854775808`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1115#issuecomment-262351018
https://github.com/hail-is/hail/pull/1115#issuecomment-262351018:27,Testability,test,test,27,@danking off by 1 error in test:; `org.scalatest.exceptions.TestFailedException: Some(-9223372036854775807) did not contain -9223372036854775808`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1115#issuecomment-262351018
https://github.com/hail-is/hail/pull/1115#issuecomment-262351018:60,Testability,Test,TestFailedException,60,@danking off by 1 error in test:; `org.scalatest.exceptions.TestFailedException: Some(-9223372036854775807) did not contain -9223372036854775808`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1115#issuecomment-262351018
https://github.com/hail-is/hail/pull/1116#issuecomment-262081893:67,Integrability,wrap,wrapped,67,high level comment: all the places we call into the JVM need to be wrapped in `_raise_py4j_exception`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1116#issuecomment-262081893
https://github.com/hail-is/hail/pull/1116#issuecomment-262160105:6,Usability,feedback,feedback,6,"Great feedback, @tpoterba . I think I addressed all the comments. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1116#issuecomment-262160105
https://github.com/hail-is/hail/pull/1116#issuecomment-262237543:8,Testability,test,test,8,"Failing test, otherwise looks great! Exciting stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1116#issuecomment-262237543
https://github.com/hail-is/hail/pull/1119#issuecomment-264290702:105,Deployability,install,installing,105,@tpoterba I made it lazily check for nullness so that users can still run the Java & Scala tests without installing spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1119#issuecomment-264290702
https://github.com/hail-is/hail/pull/1119#issuecomment-264290702:91,Testability,test,tests,91,@tpoterba I made it lazily check for nullness so that users can still run the Java & Scala tests without installing spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1119#issuecomment-264290702
https://github.com/hail-is/hail/pull/1124#issuecomment-263318325:171,Deployability,install,install,171,"@cseed, not sure if you know, but you can actually run both Spark 1 and 2 on CDH side-by-side (I assume that's what you mean by the on-prem machines). See here for how to install: https://www.cloudera.com/documentation/betas/spark2/latest/topics/spark2_installing.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1124#issuecomment-263318325
https://github.com/hail-is/hail/pull/1124#issuecomment-263332983:132,Deployability,upgrade,upgrade,132,"Thanks @tomwhite, that's great! Unfortunately, we have a second local cluster with another distribution on Spark 1.5 that won't get upgrade until early 2017 at the earliest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1124#issuecomment-263332983
https://github.com/hail-is/hail/pull/1127#issuecomment-263676448:154,Energy Efficiency,power,power,154,Commit message: ; ```; Fixed bug in TextTableReader caused by unsafe ArrayBuilder use. ; Bug occurred for text tables with a number of columns equal to a power of 2; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448
https://github.com/hail-is/hail/pull/1127#issuecomment-263676448:7,Integrability,message,message,7,Commit message: ; ```; Fixed bug in TextTableReader caused by unsafe ArrayBuilder use. ; Bug occurred for text tables with a number of columns equal to a power of 2; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448
https://github.com/hail-is/hail/pull/1127#issuecomment-263676448:62,Safety,unsafe,unsafe,62,Commit message: ; ```; Fixed bug in TextTableReader caused by unsafe ArrayBuilder use. ; Bug occurred for text tables with a number of columns equal to a power of 2; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448
https://github.com/hail-is/hail/pull/1127#issuecomment-263737204:21,Availability,failure,failure,21,I'm confused by this failure. It looks the CI didn't merge with the latest master. Maybe rebase?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263737204
https://github.com/hail-is/hail/issues/1140#issuecomment-280764177:29,Testability,log,logic,29,- [ ] Nuke lz4 decompression logic.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1140#issuecomment-280764177
https://github.com/hail-is/hail/issues/1140#issuecomment-299362173:18,Performance,optimiz,optimizer,18,"With the relation optimizer, metadata is being split into two pieces. signatures and other static metadata should be stored separately from query data: global annotations, sample IDs and sample annotations (the latter possibly as Parquet).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1140#issuecomment-299362173
https://github.com/hail-is/hail/issues/1140#issuecomment-319507752:56,Testability,log,logic,56,"Nuked GenotypeStream, which includes ""lz4 decompression logic"" (although LZ4Utils is still there in case we want to use it), pending: https://github.com/hail-is/hail/pull/2047. nuked constant vector handling and AC which I think resolves the linreg comment: https://github.com/hail-is/hail/pull/2042",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1140#issuecomment-319507752
https://github.com/hail-is/hail/pull/1141#issuecomment-271011013:42,Testability,benchmark,benchmarking,42,Please reopen when you finish cleanup and benchmarking and it is ready for another serious look.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1141#issuecomment-271011013
https://github.com/hail-is/hail/pull/1144#issuecomment-266934292:7,Deployability,update,update,7,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1144#issuecomment-266934292
https://github.com/hail-is/hail/pull/1144#issuecomment-266934292:61,Usability,guid,guide,61,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1144#issuecomment-266934292
https://github.com/hail-is/hail/pull/1145#issuecomment-266934335:7,Deployability,update,update,7,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1145#issuecomment-266934335
https://github.com/hail-is/hail/pull/1145#issuecomment-266934335:61,Usability,guid,guide,61,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1145#issuecomment-266934335
https://github.com/hail-is/hail/pull/1147#issuecomment-265653872:126,Testability,test,test,126,"Extremely nice @lfrancioli, very elegantly done. Rebase and address the minor comments, and it should be good to go. A simple test would be nice, too, but I'll put that on our todo list if you don't get to it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1147#issuecomment-265653872
https://github.com/hail-is/hail/pull/1147#issuecomment-265653872:119,Usability,simpl,simple,119,"Extremely nice @lfrancioli, very elegantly done. Rebase and address the minor comments, and it should be good to go. A simple test would be nice, too, but I'll put that on our todo list if you don't get to it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1147#issuecomment-265653872
https://github.com/hail-is/hail/pull/1147#issuecomment-265872788:97,Testability,test,test,97,Thanks a lot for looking at the PR @cseed! I've made the changes you suggested and added a short test suite. Hope it can go in :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1147#issuecomment-265872788
https://github.com/hail-is/hail/issues/1156#issuecomment-422471680:90,Availability,down,down,90,Closing. The latter issue will be addressed with better handing of categorical covariates down the line.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1156#issuecomment-422471680
https://github.com/hail-is/hail/pull/1157#issuecomment-266346469:248,Testability,test,test,248,"I also added VariantDataset.make_keytable. With this, one can create keytables with genotype-level values, for example:. ```; vds = hc.import_vcf('/home/cotton/sample.vcf'); (vds.make_keytable('v = v, info = va.info', 'gt = g.gt', ['v']); .export('test.txt')); ```. test.txt will have a `s.gt` column, one for each sample `s`. This functionality was already in the Solr and Cassandra export modules, which should now be moved to KeyTable. The user needs more control over how the KeyTable column names are formed in flatten and make_keytable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1157#issuecomment-266346469
https://github.com/hail-is/hail/pull/1157#issuecomment-266346469:266,Testability,test,test,266,"I also added VariantDataset.make_keytable. With this, one can create keytables with genotype-level values, for example:. ```; vds = hc.import_vcf('/home/cotton/sample.vcf'); (vds.make_keytable('v = v, info = va.info', 'gt = g.gt', ['v']); .export('test.txt')); ```. test.txt will have a `s.gt` column, one for each sample `s`. This functionality was already in the Solr and Cassandra export modules, which should now be moved to KeyTable. The user needs more control over how the KeyTable column names are formed in flatten and make_keytable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1157#issuecomment-266346469
https://github.com/hail-is/hail/pull/1157#issuecomment-266540995:25,Testability,test,tests,25,"Also, I see there are no tests for the new features.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1157#issuecomment-266540995
https://github.com/hail-is/hail/issues/1158#issuecomment-266392470:33,Performance,optimiz,optimized,33,"we need to make sure we keep the optimized joins, though. I'm not sure exactly how this'll look (special case for variant or locus keyed keytable)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1158#issuecomment-266392470
https://github.com/hail-is/hail/issues/1158#issuecomment-301549814:66,Integrability,contract,contract,66,We have almost everything here. Will make two separate issues for contract types and partitioning,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1158#issuecomment-301549814
https://github.com/hail-is/hail/issues/1160#issuecomment-422370912:3,Testability,test,test,3,we test split multi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1160#issuecomment-422370912
https://github.com/hail-is/hail/pull/1170#issuecomment-266622157:85,Deployability,configurat,configuration,85,"I verified this works on 1.5.2, 1.6.3 and 2.0.2, but fails if we remove the relevant configuration settings (maxPartitionBytes, parquet.blocks.size).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157
https://github.com/hail-is/hail/pull/1170#issuecomment-266622157:85,Modifiability,config,configuration,85,"I verified this works on 1.5.2, 1.6.3 and 2.0.2, but fails if we remove the relevant configuration settings (maxPartitionBytes, parquet.blocks.size).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157
https://github.com/hail-is/hail/issues/1185#issuecomment-306901327:248,Safety,safe,safe,248,"From googling, this seems like something that was probably relatively out of our control. Like cluster ran out of disk space, communication failed, etc. Considering no one else has complained about this (to my knowledge anyway), this is probably a safe issue to close, assuming you haven't had this problem since. @tpoterba , does this seem reasonable?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185#issuecomment-306901327
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:269,Availability,failure,failure,269,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:304,Availability,ERROR,ERROR,304,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2516,Availability,echo,echo,2516,"gWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://ww",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2604,Availability,echo,echo,2604,"dTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2629,Availability,echo,echo,2629,"dTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6283,Availability,error,error,6283,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6327,Availability,failure,failure,6327,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1509,Energy Efficiency,schedul,scheduler,1509,ternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1591,Energy Efficiency,schedul,scheduler,1591,AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; scr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1680,Energy Efficiency,schedul,scheduler,1680, at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1775,Energy Efficiency,schedul,scheduler,1775,9); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1887,Energy Efficiency,schedul,scheduler,1887,nd(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1992,Energy Efficiency,schedul,scheduler,1992,"pache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2170,Energy Efficiency,schedul,scheduler,2170,".apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:4808,Energy Efficiency,reduce,reduce,4808,",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:4859,Energy Efficiency,reduce,reduce,4859," ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5037,Energy Efficiency,reduce,reduce,5037,"aproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5081,Energy Efficiency,reduce,reduce,5081,""": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5132,Energy Efficiency,reduce,reduce,5132,"broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5695,Energy Efficiency,schedul,scheduler,5695,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5750,Energy Efficiency,schedul,scheduler,5750,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6501,Energy Efficiency,schedul,scheduler,6501,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:3156,Modifiability,config,config,3156,"rk.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:3169,Modifiability,config,configBucket,3169,"rk.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2251,Performance,concurren,concurrent,2251,"gory.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2335,Performance,concurren,concurrent,2335,"6); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-ce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2652,Security,HASH,HASH,2652,"spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.user",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2693,Security,hash,hash,2693,"spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.user",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2749,Security,HASH,HASH,2749,"21); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_contr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2842,Security,HASH,HASH,2842,"$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""http",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:168,Testability,log,log,168,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:183,Testability,log,log,183,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1320,Testability,log,log,1320,ght exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at ja,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1445,Testability,Log,Logging,1445,ava:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```;,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1459,Testability,log,logWarning,1459,va.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1470,Testability,Log,Logging,1470,va.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1534,Testability,log,logWarning,1534,:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2109,Testability,log,logUncaughtExceptions,2109,"pOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:3847,Testability,log,logging,3847,".zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6294,Testability,log,log,6294,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6357,Testability,log,log,6357,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6377,Testability,log,log,6377,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6250,Usability,progress bar,progress bar,6250,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
https://github.com/hail-is/hail/issues/1186#issuecomment-267428726:544,Performance,perform,performing,544,I looked at the Spark worker logs by setting up the SSH tunnel (according to laurent's cloud post) and going to the Executor tab of the spark history server page. It seems we're running into this issue: https://issues.apache.org/jira/browse/SPARK-16845. This is definitely because Andrea's VDS has lots of annotations. Here's a quote from that Spark issue:. > I've been struggling to duplicate this and finally came up with a strategy that duplicates it in a spark-shell. It's a combination of a wide dataset with nested (array) structures and performing a union that seem to trigger it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267428726
https://github.com/hail-is/hail/issues/1186#issuecomment-267428726:29,Testability,log,logs,29,I looked at the Spark worker logs by setting up the SSH tunnel (according to laurent's cloud post) and going to the Executor tab of the spark history server page. It seems we're running into this issue: https://issues.apache.org/jira/browse/SPARK-16845. This is definitely because Andrea's VDS has lots of annotations. Here's a quote from that Spark issue:. > I've been struggling to duplicate this and finally came up with a strategy that duplicates it in a spark-shell. It's a combination of a wide dataset with nested (array) structures and performing a union that seem to trigger it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267428726
https://github.com/hail-is/hail/issues/1186#issuecomment-290859153:81,Deployability,release,release,81,"We have characterized this issue well, and it will be fixed in an imminent spark release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-290859153
https://github.com/hail-is/hail/pull/1194#issuecomment-268636904:59,Availability,down,download,59,This seems like an odd bug?; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_HailCiSpark1/2980:id/build/reports/tests/classes/org.broadinstitute.hail.io.ExportVcfSuite.html#testReadWrite,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1194#issuecomment-268636904
https://github.com/hail-is/hail/pull/1194#issuecomment-268636904:131,Testability,test,tests,131,This seems like an odd bug?; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_HailCiSpark1/2980:id/build/reports/tests/classes/org.broadinstitute.hail.io.ExportVcfSuite.html#testReadWrite,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1194#issuecomment-268636904
https://github.com/hail-is/hail/pull/1194#issuecomment-268636904:192,Testability,test,testReadWrite,192,This seems like an odd bug?; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_HailCiSpark1/2980:id/build/reports/tests/classes/org.broadinstitute.hail.io.ExportVcfSuite.html#testReadWrite,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1194#issuecomment-268636904
https://github.com/hail-is/hail/pull/1196#issuecomment-267986559:60,Modifiability,polymorphi,polymorphic,60,"Also, yes, the `registerNumeric` would require some sort of polymorphic implementation, which this is not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1196#issuecomment-267986559
https://github.com/hail-is/hail/issues/1197#issuecomment-290858742:16,Modifiability,config,config,16,Kill text table config.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1197#issuecomment-290858742
https://github.com/hail-is/hail/issues/1202#issuecomment-317747531:15,Availability,error,error,15,we have better error messages now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531
https://github.com/hail-is/hail/issues/1202#issuecomment-317747531:21,Integrability,message,messages,21,we have better error messages now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531
https://github.com/hail-is/hail/pull/1203#issuecomment-268302819:31,Availability,error,errors,31,"This behavior can produce type errors, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268302819
https://github.com/hail-is/hail/pull/1203#issuecomment-268331577:18,Performance,perform,performance,18,"It was really for performance: we want to remove the filtered allele(s) from R, A and G-based annotations. Now for the gnomAD VDS there are ~ 40 of these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268331577
https://github.com/hail-is/hail/pull/1203#issuecomment-268333484:99,Availability,down,downcode,99,I'm not sure this will make such a performance difference in the common case -- the genotype-level downcode/subset operation will dominate runtime,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484
https://github.com/hail-is/hail/pull/1203#issuecomment-268333484:35,Performance,perform,performance,35,I'm not sure this will make such a performance difference in the common case -- the genotype-level downcode/subset operation will dominate runtime,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484
https://github.com/hail-is/hail/pull/1207#issuecomment-268570534:26,Testability,test,test,26,Can you include a failing test?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1207#issuecomment-268570534
https://github.com/hail-is/hail/pull/1208#issuecomment-271012002:191,Performance,load,load,191,"Yeah, I'm not sure I like this change, for exactly the reason @tpoterba mentions: should we join with `&&` or `||`? This is more natural with files since the only thing a list can mean is to load them all (which is already supported in the syntax). What was the particular use case driving this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1208#issuecomment-271012002
https://github.com/hail-is/hail/pull/1209#issuecomment-268822041:383,Availability,down,downside,383,"I don't think this is the correct solution. As you say, we're incorrectly conflating null to mean two things: the value is legitimately null and it has been filtered out. I see two ways to fix this correctly:. - Values being aggregated indicate having been filtered by making them Option[Any] or storing an extra boolean somewhere to indicate they've been filtered out. This has the downside of more allocations.; - Make value being aggregated an Iterator. This makes aggregables pull-based rather than push-based. It allows aggregators to terminate early (e.g. take(5)). On the other hand, it means the genotype stream will be decoded multiple times, once for each aggregator. That's slow. I think the solution is to unpack the genotype stream into an array before running the aggregators. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1209#issuecomment-268822041
https://github.com/hail-is/hail/pull/1209#issuecomment-268823332:21,Integrability,interface,interface,21,"I think the iterator interface would be much nicer, but I don't see how it could work with the per-sample aggregations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1209#issuecomment-268823332
https://github.com/hail-is/hail/pull/1216#issuecomment-270982982:258,Availability,down,download,258,"@cseed @jbloom22 I don't really like how the examples sections are looking. I'd rather read a short one-line description of what is being done followed by the example. Right now, it's example code with detailed explanation. See https://ci.hail.is/repository/download/HailSourceCode_HailCi/3257:id/www/pyhail/index.html#pyhail.VariantDataset.annotate_variants_bed for an example. Cotton -- I'll defer to you on the variantqc table. Did we come to a consensus what the return type should be for export commands?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1216#issuecomment-270982982
https://github.com/hail-is/hail/pull/1216#issuecomment-270983676:186,Availability,down,download,186,"> I'd rather read a short one-line description of what is being done followed by the example. I agree with this. See, for example, filter_alleles in my PR: https://ci.hail.is/repository/download/HailSourceCode_HailCi/3269:id/www/pyhail/index.html. I'm allowing from some short clarifying remarks after the example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1216#issuecomment-270983676
https://github.com/hail-is/hail/issues/1218#issuecomment-270455160:292,Deployability,release,release,292,"The getting started, tutorial, and command documentation are all coming to python in the next week or two. We can certainly look into registering Hail on PyPI, but I'm not sure how versioning works there -- with the current pace of development, we may want to hold off on that until a stable release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-270455160
https://github.com/hail-is/hail/issues/1218#issuecomment-270732339:26,Testability,test,testing,26,Great! Looking forward to testing out PyHail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-270732339
https://github.com/hail-is/hail/issues/1218#issuecomment-272494749:426,Integrability,depend,depend,426,"Great! I will test it out on our cluster. First, I have question on the; Spark version that is recommended. At the very top of the *Getting Started; with Python API*, the document indicates the latest version of Spark 2; should be used. But later on under the *Running on a Spark cluster and in; the cloud section,* it indicates only Spark 1.5 and 1.6 are supported.; Which version would be the best to use? Or does it really depend on whether; it is run locally or on a cluster?. On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1218#issuecomment-272357689>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AB3rDZjYATb82CmTNeP61RpKxMCFMhInks5rRvvYgaJpZM4La8Pf>; > .; >. -- ; John Farrell, Ph.D.; Biomedical Genetics-Evans 218; Boston University Medical School; 72 East Concord Street; Boston, MA. ph: 617-638-5491",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272494749
https://github.com/hail-is/hail/issues/1218#issuecomment-272494749:14,Testability,test,test,14,"Great! I will test it out on our cluster. First, I have question on the; Spark version that is recommended. At the very top of the *Getting Started; with Python API*, the document indicates the latest version of Spark 2; should be used. But later on under the *Running on a Spark cluster and in; the cloud section,* it indicates only Spark 1.5 and 1.6 are supported.; Which version would be the best to use? Or does it really depend on whether; it is run locally or on a cluster?. On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1218#issuecomment-272357689>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AB3rDZjYATb82CmTNeP61RpKxMCFMhInks5rRvvYgaJpZM4La8Pf>; > .; >. -- ; John Farrell, Ph.D.; Biomedical Genetics-Evans 218; Boston University Medical School; 72 East Concord Street; Boston, MA. ph: 617-638-5491",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272494749
https://github.com/hail-is/hail/issues/1218#issuecomment-272507277:115,Performance,perform,performance,115,"Just to clarify, Spark 2 is preferred. We'll be dropping Spark 1 support in a few of weeks. Spark 2 has a bunch of performance improvements and features we want to take advantage of in the coming months.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272507277
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:772,Availability,error,error,772,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:270,Deployability,install,install,270,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:334,Deployability,install,install,334,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:398,Deployability,install,install,398,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:462,Deployability,install,install,462,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:526,Deployability,install,install,526,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:586,Deployability,install,install,586,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:646,Deployability,install,install,646,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:709,Deployability,install,install,709,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1288,Deployability,install,install,1288,"rc.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1419,Deployability,install,install,1419,".zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, v",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1488,Integrability,protocol,protocol,1488,"k/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1218#issuecomment-2723576",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1549,Integrability,protocol,protocol,1549,"-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1218#issuecomment-272357689>, or mute; > the thread; > <http",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799
https://github.com/hail-is/hail/pull/1220#issuecomment-270740862:23,Testability,test,tests,23,Requires #1222 for the tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1220#issuecomment-270740862
https://github.com/hail-is/hail/pull/1225#issuecomment-271339637:37,Performance,load,loading,37,"A quick look at the source, it isn't loading mathjax.js. No idea why yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271339637
https://github.com/hail-is/hail/pull/1225#issuecomment-271382041:88,Modifiability,variab,variable,88,"Great feedback. Addressed comments, back to you. In the scope lists, I use the format: `variable (*Type*): description`, where the type is in italics but not a hyperlink, but I put a hyperlink in the description when it seemed appropriate. ```; *:ref:`foo`; ```. didn't format the hyperlink. Also, I don't think we can put hyperlinks in double-back-quote literal/code blocks. I didn't address the math stuff. I think we can merge this (and other doc migrations) when it is ready and fix that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041
https://github.com/hail-is/hail/pull/1225#issuecomment-271382041:6,Usability,feedback,feedback,6,"Great feedback. Addressed comments, back to you. In the scope lists, I use the format: `variable (*Type*): description`, where the type is in italics but not a hyperlink, but I put a hyperlink in the description when it seemed appropriate. ```; *:ref:`foo`; ```. didn't format the hyperlink. Also, I don't think we can put hyperlinks in double-back-quote literal/code blocks. I didn't address the math stuff. I think we can merge this (and other doc migrations) when it is ready and fix that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041
https://github.com/hail-is/hail/pull/1232#issuecomment-271977973:35,Availability,failure,failure,35,@jigold This should fix your build failure :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1232#issuecomment-271977973
https://github.com/hail-is/hail/issues/1237#issuecomment-275417860:18,Deployability,update,update,18,Fixed in tutorial update. PR 1298,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1237#issuecomment-275417860
https://github.com/hail-is/hail/pull/1254#issuecomment-274951875:7,Deployability,pipeline,pipeline,7,A Hail pipeline:; ```; read profile225-with-SEX_FROM_STAT.vds \; filtergenotypes -c ' g.dp > 400 ||; (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) ||; (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) ||; (g.isHet && ( (g.ad[0] + g.ad[1]) / g.dp < 0.9 || g.ad[1] / g.dp < 0.20 || g.pl[0] < 20 ))' --keep count -g; ```. `master`/`c5bdcfd`:; ```; hail: info: timing:; read: 4.795s; filtergenotypes: 254.277ms; count: 1m40.8s; total: 1m45.8s; ```; this branch:; ```; hail: info: timing:; read: 5.822s; filtergenotypes: 617.229ms; count: 53.545s; total: 59.984s; ```. Ignoring read time: the compiled version takes 53% of the time. ![The Flash](https://media.giphy.com/media/lRnUWhmllPI9a/giphy.gif),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1254#issuecomment-274951875
https://github.com/hail-is/hail/pull/1254#issuecomment-284071915:0,Testability,Test,Test,0,Test script:. ```; from hail import *; hc = HailContext(); (hc.read('/Users/dking/projects/hail-data/profile225-with-SEX_FROM_STAT.vds'); .filter_genotypes(' g.dp > 400 || \; (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) || \; (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) || \; (g.isHet && ( (g.ad[0] + g.ad[1]) / g.dp < 0.9 || g.ad[1] / g.dp < 0.20 || \; g.pl[0] < 20 ))'); .count(genotypes=True)); ```. Mean of thee runs:. - `master` / `593a469`: 96.66666s; - this branch: 74.33333s. Which is a 23% reduction in time. The gap has closed from both sides. While rebasing I threw away a bunch of FunctionRegistry entries that had been converted to `Code`. I'll add them back in a separate PR and see how that changes things again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1254#issuecomment-284071915
https://github.com/hail-is/hail/pull/1255#issuecomment-272634543:41,Testability,test,tests,41,"These things need to be done:. - [ ] Add tests; - [x] Add support for java types like Genotype, Variant, AltAllele, etc. I think it may be acceptable to convert genotype and variant and such to string representations for now, until we have full support in python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1255#issuecomment-272634543
https://github.com/hail-is/hail/pull/1255#issuecomment-274365803:81,Integrability,wrap,wrapper,81,"This is awesome. Two thoughts:. Objects corresponding to builtin types have py4j wrapper types, e.g.:. ```; >>> samples = vds.query_samples('samples.map(s => s.id).collect()'); >>> samples; [u'C1046::HG02024', ...]; >>> type(samples); <class 'py4j.java_collections.JavaList'>; ```. I think this is in the spirit of python (JavaList is list-like), but code (like ours!) that does things like `isinstance(x, list)` or `isinstance(x, str)` is going to fail. Second, this will work for python collections as described here:. https://www.py4j.org/advanced_topics.html. but won't work for user-defined objects like Variant, etc. that we'll want to introduce. For example, what d you want to return for `vds.query_variants('variants.filter(v => ...).collect()`. Shouldn't you get a python list of python Variant instances? I think to do this we have to do the translation no the python side. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1255#issuecomment-274365803
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:263,Modifiability,config,config,263,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:413,Modifiability,config,config,413,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:128,Testability,test,test,128,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:198,Testability,test,test,198,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:337,Testability,test,test,337,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:484,Testability,log,logreg,484,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:662,Testability,test,test,662,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:1173,Testability,log,logreg,1173,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:1237,Testability,log,logreg,1237,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:1299,Testability,log,logreg,1299,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219
https://github.com/hail-is/hail/pull/1259#issuecomment-272994037:159,Modifiability,refactor,refactoring,159,"This should go in before the `jb_lmm_getthisin` branch, which is rebased off this one to continue the unification of lin, log, and mixed regression, with more refactoring. (alternatively, the `jb_lmm_getthisin` branch supercedes this one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272994037
https://github.com/hail-is/hail/pull/1259#issuecomment-272994037:122,Testability,log,log,122,"This should go in before the `jb_lmm_getthisin` branch, which is rebased off this one to continue the unification of lin, log, and mixed regression, with more refactoring. (alternatively, the `jb_lmm_getthisin` branch supercedes this one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272994037
https://github.com/hail-is/hail/pull/1259#issuecomment-275096710:77,Testability,test,testing,77,"I made the requested changes, improved the docs a bit more, removed the rank testing for now (may bring back in later PR), and am keeping the covariate field as list of expressions, even for one expression. I feel this is more natural in the regression setting, where one can even have no covariates (default is []), and I prefer to pass an array of string into RegressionUtils, though we should make another pass on parsing later (right now, I do still concatenate with ',' just before sending into the parser but would to not merge only to split).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-275096710
https://github.com/hail-is/hail/pull/1263#issuecomment-273224085:25,Testability,test,tests,25,"I'll merge this when the tests are done and announce on slack, gitter, and discourse.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1263#issuecomment-273224085
https://github.com/hail-is/hail/pull/1264#issuecomment-273365626:89,Deployability,update,updates,89,@johnc1231 I've made a PR against your branch that rebases on is.hail and suggests other updates.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1264#issuecomment-273365626
https://github.com/hail-is/hail/pull/1272#issuecomment-273972707:162,Availability,failure,failures,162,@jigold rightly pointed out I didn't add a regression test for this. @jigold next time request changes! @jbloom22 next time request tests! @cseed write tests for failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1272#issuecomment-273972707
https://github.com/hail-is/hail/pull/1272#issuecomment-273972707:54,Testability,test,test,54,@jigold rightly pointed out I didn't add a regression test for this. @jigold next time request changes! @jbloom22 next time request tests! @cseed write tests for failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1272#issuecomment-273972707
https://github.com/hail-is/hail/pull/1272#issuecomment-273972707:132,Testability,test,tests,132,@jigold rightly pointed out I didn't add a regression test for this. @jigold next time request changes! @jbloom22 next time request tests! @cseed write tests for failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1272#issuecomment-273972707
https://github.com/hail-is/hail/pull/1272#issuecomment-273972707:152,Testability,test,tests,152,@jigold rightly pointed out I didn't add a regression test for this. @jigold next time request changes! @jbloom22 next time request tests! @cseed write tests for failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1272#issuecomment-273972707
https://github.com/hail-is/hail/issues/1274#issuecomment-274164170:21,Safety,detect,detecting,21,Looks like we're not detecting the vector extensions supported by your CPU correctly. Can you post the output of `c++ -v` and `sysctl -a | grep machdep.cpu`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274164170
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:2449,Energy Efficiency,sensor,sensor,2449,; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag: 4; machdep.cpu.microcode_version: 21; machdep.cpu.cores_per_package: 8; machdep.cpu.logical_per_package: 16; machdep.cpu.extfeatures: SYSCALL XD EM64T LAHF RDTSCP TSCI; machdep.cpu.leaf7_features: SMEP ERMS RDWRFSGS; machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC POPCNT AES PCID XSAVE OSXSAVE TSCTMR AVX1.0 RDRAND F16C; machdep.cpu.brand: 0; machdep.cpu.signature: 198313; machdep.cpu.extfeature_bits: 4967106816; machdep.cpu.leaf7_feature_bits: 641; machdep.cpu.feature_bits: 9203919201183202303; machdep.cpu.stepping: 9; machdep.cpu.extfamily: 0; machdep.cpu.extmodel: 3; machdep.cpu.model: 58; machdep.cpu.family: 6; machdep.cpu.brand_string: In,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:127,Integrability,wrap,wrapper,127,"**c++ -v**; Using built-in specs.; COLLECT_GCC=c++; COLLECT_LTO_WRAPPER=/opt/local/libexec/gcc/x86_64-apple-darwin15/4.9.3/lto-wrapper; Target: x86_64-apple-darwin15; Configured with: /opt/local/var/macports/build/_opt_mports_dports_lang_gcc49/gcc49/work/gcc-4.9.3/configure --prefix=/opt/local --build=x86_64-apple-darwin15 --enable-languages=c,c++,objc,obj-c++,lto,fortran,java --libdir=/opt/local/lib/gcc49 --includedir=/opt/local/include/gcc49 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-4.9 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:167,Modifiability,Config,Configured,167,"**c++ -v**; Using built-in specs.; COLLECT_GCC=c++; COLLECT_LTO_WRAPPER=/opt/local/libexec/gcc/x86_64-apple-darwin15/4.9.3/lto-wrapper; Target: x86_64-apple-darwin15; Configured with: /opt/local/var/macports/build/_opt_mports_dports_lang_gcc49/gcc49/work/gcc-4.9.3/configure --prefix=/opt/local --build=x86_64-apple-darwin15 --enable-languages=c,c++,objc,obj-c++,lto,fortran,java --libdir=/opt/local/lib/gcc49 --includedir=/opt/local/include/gcc49 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-4.9 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:265,Modifiability,config,configure,265,"**c++ -v**; Using built-in specs.; COLLECT_GCC=c++; COLLECT_LTO_WRAPPER=/opt/local/libexec/gcc/x86_64-apple-darwin15/4.9.3/lto-wrapper; Target: x86_64-apple-darwin15; Configured with: /opt/local/var/macports/build/_opt_mports_dports_lang_gcc49/gcc49/work/gcc-4.9.3/configure --prefix=/opt/local --build=x86_64-apple-darwin15 --enable-languages=c,c++,objc,obj-c++,lto,fortran,java --libdir=/opt/local/lib/gcc49 --includedir=/opt/local/include/gcc49 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-4.9 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1133,Modifiability,config,config,1133,"ple-darwin15; Configured with: /opt/local/var/macports/build/_opt_mports_dports_lang_gcc49/gcc49/work/gcc-4.9.3/configure --prefix=/opt/local --build=x86_64-apple-darwin15 --enable-languages=c,c++,objc,obj-c++,lto,fortran,java --libdir=/opt/local/lib/gcc49 --includedir=/opt/local/include/gcc49 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-4.9 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.ther",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1628,Performance,cache,cache,1628,mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1657,Performance,cache,cache,1657,=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag: 4; machdep.cpu.microcode_ver,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1696,Performance,cache,cache,1696,mp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag: 4; machdep.cpu.microcode_version: 21; machdep.cpu.cores_per_package,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543
https://github.com/hail-is/hail/issues/1274#issuecomment-274296144:163,Deployability,Install,InstalledDir,163,Thanks! Moving over to the Xcode cc worked. **cc --version**; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin16.3.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274296144
https://github.com/hail-is/hail/issues/1274#issuecomment-295648585:21,Availability,error,error,21,I am having the same error on Mac OS 10.12.4 with gcc 4.9.3. How do I move over to Xcode cc?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-295648585
https://github.com/hail-is/hail/issues/1283#issuecomment-274907004:44,Availability,down,downcoding,44,Proposal: subset behavior should default to downcoding behavior if PLs are missing. Downcode should be fixed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-274907004
https://github.com/hail-is/hail/issues/1283#issuecomment-274907004:84,Availability,Down,Downcode,84,Proposal: subset behavior should default to downcoding behavior if PLs are missing. Downcode should be fixed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-274907004
https://github.com/hail-is/hail/issues/1283#issuecomment-318495458:92,Availability,error,error,92,"I don't think this is going to get fixed especially soon. There's not an easy way to warn / error given our current infrastructure. If you can convince Laurent to PR a fix for downcode to work when PLs are missing (which is reasonable, I think) that's probably the best short-term bet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-318495458
https://github.com/hail-is/hail/issues/1283#issuecomment-318495458:176,Availability,down,downcode,176,"I don't think this is going to get fixed especially soon. There's not an easy way to warn / error given our current infrastructure. If you can convince Laurent to PR a fix for downcode to work when PLs are missing (which is reasonable, I think) that's probably the best short-term bet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-318495458
https://github.com/hail-is/hail/issues/1284#issuecomment-274678541:55,Availability,error,error,55,"Notice that this script is working with spark 1.6, the error appears with spark 2. ```; exac_vds_split = hc.read(root + 'andrea_subset_splitmulti_hard.vds'); dbNSFP_vds = hc.read(root + 'dbNSFP_3.2a_variant.filtered.allhg19_nodup.vds'); discovEHR_vds = hc.read(root + 'discoverEHR.vds'); exacV2_vds = hc.read(root + 'exacV2_split_variants_non_in_andrea_subset.vds'); fin_vds = hc.read(root + 'finnish_noexac_subset.vds'). (exac_vds_split; # .filter_variants_expr('v.contig==""7"" && v.start > 75013221 && v.start < 76253221', keep=True); # .filter_variants_expr('v.contig==""20""', keep=True); .annotate_variants_expr('va = drop(va, vep)'); .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); .write(stroot + '/andrea_subset_splitmulti_hard_vep.vds', overwrite=True)). exac_vds_split_vep = hc.read(stroot + 'andrea_subset_splitmulti_hard_vep.vds'). (exac_vds_split_vep; .annotate_global_list(root + 'all_scores_reduced.scores', root='global.allgenes'); .annotate_global_expr_by_sample('global.allgenes = global.allgenes.map(x => x.split(""\\t""))'); .variant_qc(); .annotate_variants_vds(discovEHR_vds,root='va.EHR'); .annotate_variants_vds(exacV2_vds,root='va.EXACV2'); .annotate_variants_vds(fin_vds,root='va.FIN'); .annotate_variants_table(root + 'clinvar_clean.txt','Variant(Variant)', root='va.clinvar'); .annotate_variants_table(root + 'lethal_clean.txt','Variant(Variant)', root='va.lethal'); .annotate_variants_expr(; 	""""""; 	va.clinvar.yes=if(isMissing(va.clinvar.Variant)) 0 else 1,; 	va.lethal.yes=if(isMissing(va.lethal.Variant)) 0 else 1,; 	va.nNonRef = gs.filter(g => g.isCalledNonRef).count(); 	""""""); .annotate_variants_expr(; 	""""""; 	va.freq.AF01 = (va.qc.AF < 0.01),; 	va.freq.AF001 = (va.qc.AF < 0.001),; 	va.freq.DOUBLE = (va.qc.AC == 2),; 	va.freq.SING = (va.nNonRef == 1),; 	va.freq.URVEXACV2 = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC)),; 	va.freq.URVEXACV2EHR = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC) && isMissing(va.EHR.info.AF) && isMissing(va.FIN.qc.AC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541
https://github.com/hail-is/hail/issues/1284#issuecomment-274678541:642,Modifiability,config,config,642,"Notice that this script is working with spark 1.6, the error appears with spark 2. ```; exac_vds_split = hc.read(root + 'andrea_subset_splitmulti_hard.vds'); dbNSFP_vds = hc.read(root + 'dbNSFP_3.2a_variant.filtered.allhg19_nodup.vds'); discovEHR_vds = hc.read(root + 'discoverEHR.vds'); exacV2_vds = hc.read(root + 'exacV2_split_variants_non_in_andrea_subset.vds'); fin_vds = hc.read(root + 'finnish_noexac_subset.vds'). (exac_vds_split; # .filter_variants_expr('v.contig==""7"" && v.start > 75013221 && v.start < 76253221', keep=True); # .filter_variants_expr('v.contig==""20""', keep=True); .annotate_variants_expr('va = drop(va, vep)'); .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); .write(stroot + '/andrea_subset_splitmulti_hard_vep.vds', overwrite=True)). exac_vds_split_vep = hc.read(stroot + 'andrea_subset_splitmulti_hard_vep.vds'). (exac_vds_split_vep; .annotate_global_list(root + 'all_scores_reduced.scores', root='global.allgenes'); .annotate_global_expr_by_sample('global.allgenes = global.allgenes.map(x => x.split(""\\t""))'); .variant_qc(); .annotate_variants_vds(discovEHR_vds,root='va.EHR'); .annotate_variants_vds(exacV2_vds,root='va.EXACV2'); .annotate_variants_vds(fin_vds,root='va.FIN'); .annotate_variants_table(root + 'clinvar_clean.txt','Variant(Variant)', root='va.clinvar'); .annotate_variants_table(root + 'lethal_clean.txt','Variant(Variant)', root='va.lethal'); .annotate_variants_expr(; 	""""""; 	va.clinvar.yes=if(isMissing(va.clinvar.Variant)) 0 else 1,; 	va.lethal.yes=if(isMissing(va.lethal.Variant)) 0 else 1,; 	va.nNonRef = gs.filter(g => g.isCalledNonRef).count(); 	""""""); .annotate_variants_expr(; 	""""""; 	va.freq.AF01 = (va.qc.AF < 0.01),; 	va.freq.AF001 = (va.qc.AF < 0.001),; 	va.freq.DOUBLE = (va.qc.AC == 2),; 	va.freq.SING = (va.nNonRef == 1),; 	va.freq.URVEXACV2 = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC)),; 	va.freq.URVEXACV2EHR = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC) && isMissing(va.EHR.info.AF) && isMissing(va.FIN.qc.AC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541
https://github.com/hail-is/hail/issues/1284#issuecomment-301786782:5,Testability,log,log,5,This log doesn't have the mentioned exception. Closing because it's stale.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284#issuecomment-301786782
https://github.com/hail-is/hail/pull/1288#issuecomment-274950856:186,Availability,recover,recovery,186,"Your diff includes the Genotype class (which changed in master yesterday). Try fetching the current master, rebasing, and force pushing (you might want to copy your branch first to ease recovery in case you have trouble with rebasing)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1288#issuecomment-274950856
https://github.com/hail-is/hail/pull/1288#issuecomment-274950856:186,Safety,recover,recovery,186,"Your diff includes the Genotype class (which changed in master yesterday). Try fetching the current master, rebasing, and force pushing (you might want to copy your branch first to ease recovery in case you have trouble with rebasing)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1288#issuecomment-274950856
https://github.com/hail-is/hail/pull/1292#issuecomment-275122784:25,Testability,benchmark,benchmark,25,what if you do a simpler benchmark: `filter_genotypes('g.gq > 20')`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1292#issuecomment-275122784
https://github.com/hail-is/hail/pull/1292#issuecomment-275122784:17,Usability,simpl,simpler,17,what if you do a simpler benchmark: `filter_genotypes('g.gq > 20')`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1292#issuecomment-275122784
https://github.com/hail-is/hail/pull/1296#issuecomment-275287341:129,Performance,queue,queue,129,@cseed The current code only does a repartition if the number of variants per partition in the input dataset is greater than the queue size - should I always force a repartition to occur to rebalance after the first prune step?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1296#issuecomment-275287341
https://github.com/hail-is/hail/pull/1312#issuecomment-275855290:74,Modifiability,extend,extend,74,"Thanks, I added the IntIterator to Utils and switched hardcallIterator to extend it. Back to you. I definitely switched branches and used shadowJar (I went back and forth twice because I didn't believe the results).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1312#issuecomment-275855290
https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:655,Energy Efficiency,allocate,allocates,655,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:792,Energy Efficiency,allocate,allocates,792,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:92,Integrability,rout,route,92,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:176,Modifiability,extend,extend,176,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:1002,Safety,avoid,avoids,1002,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:160,Usability,simpl,simplest,160,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
https://github.com/hail-is/hail/pull/1318#issuecomment-276210949:39,Availability,avail,available,39,Fix the `(`. Mention in docs the three available distributions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1318#issuecomment-276210949
https://github.com/hail-is/hail/issues/1319#issuecomment-276452831:141,Testability,test,test,141,"This can be done with `aggregate_by_key`:. ```; vds.aggregate_by_key('Sample = s, gene = va.gene', 'gtSum = g.map(g => g.gt).sum()').export(""test.tsv""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1319#issuecomment-276452831
https://github.com/hail-is/hail/issues/1319#issuecomment-276835936:882,Testability,test,test,882,"This is great and it works.; I get an output like this:. Sample 			gene 	gtSum; SRR1725120 	NFKB1 	4; SRR1642840 	C15orf62 	1; SRR1722200 	MPLKIP 	0; SRS1055987 	CHST10 	1. Is there a way to get samples names as columns and genes as rows such as?. gene	SRR1725120	SRR1642840 SRR1722200 SRS1055987; NFKB1		4			0			0			0; C15orf62		0			1			0			0; MPLKIP		0			0			0			0					; CHST10		0			0			0			1. One option might be (but I think is slow according to what you explained me). newly = vds.make_keytable('gene=va.geneann.gene','`` = g.gt',['gene’]); allexp = [s + "" = `"" + s + ""`.sum"" for s in newly.column_names]; newly.aggregate_by_key('gene=gene',allexp). thanks. > On Jan 31, 2017, at 1:43 PM, jigold <notifications@github.com> wrote:; > ; > This can be done with aggregate_by_key:; > ; > vds.aggregate_by_key('Sample = s, gene = va.gene', 'gtSum = g.map(g => g.gt).sum()').export(""test.tsv""); > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub <https://github.com/hail-is/hail/issues/1319#issuecomment-276452831>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADIkAk61N4PGr9xoLXW8tTBF9qCb3Djiks5rX4C4gaJpZM4LxvDi>.; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1319#issuecomment-276835936
https://github.com/hail-is/hail/pull/1322#issuecomment-276272639:38,Testability,Test,Test,38,Looks good! Next steps:. - Use it!; - Test it! I'm not sure how much will break.; - Time it! Do something simple filter genotypes gq >= 20 and a sampleqc or something. Does it help? How much?; - Figure out how you're going to deal with annotations like `va.rare_genos = gs.filter(g => ... some rare condition ...).collect()`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1322#issuecomment-276272639
https://github.com/hail-is/hail/pull/1322#issuecomment-276272639:106,Usability,simpl,simple,106,Looks good! Next steps:. - Use it!; - Test it! I'm not sure how much will break.; - Time it! Do something simple filter genotypes gq >= 20 and a sampleqc or something. Does it help? How much?; - Figure out how you're going to deal with annotations like `va.rare_genos = gs.filter(g => ... some rare condition ...).collect()`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1322#issuecomment-276272639
https://github.com/hail-is/hail/pull/1323#issuecomment-277957797:69,Deployability,pipeline,pipelines,69,"Also, back-compatible with VDS on-disk representation, not necessary pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1323#issuecomment-277957797
https://github.com/hail-is/hail/pull/1324#issuecomment-276567536:41,Modifiability,Refactor,Refactored,41,Thanks a lot for the great comments Tim! Refactored and so much cleaner now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276567536
https://github.com/hail-is/hail/pull/1324#issuecomment-276663883:1077,Availability,error,errors,1077,"Done!. Thanks Tim!. On Wed, Feb 1, 2017 at 8:24 AM, Tim Poterba <notifications@github.com>; wrote:. > *@tpoterba* commented on this pull request.; >; > Need just one tiny change to the py/j connector. Looks great!; > ------------------------------; >; > In python/hail/dataset.py; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>:; >; > > @@ -2336,6 +2336,22 @@ def mendel_errors(self, output, fam):; > pargs = ['mendelerrors', '-o', output, '-f', fam]; > self.hc._run_command(self, pargs); >; > + def min_rep(self):; > + """"""; > + Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; > +; > + ** Examples **; > + 1) Simple trimming of a multi-allelic site, no change in variant position; > + `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`; > +; > + 2) Trimming of a bi-allelic site leading to a change in position; > + `1:10000:AATAA,AAGAA` => `1:10002:T:G`; > +; > + """"""; > + jvds = self._jvds.minrep(); >; > add in the try: / except: here, following the other methods in dataset.py.; >; > The default py4j errors look horrible, so calling our wrapper method helps; > a lot.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgaQoXMxYMPE_V-RMRgYp5mvNSf-Pks5rYIePgaJpZM4LzbBv>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883
https://github.com/hail-is/hail/pull/1324#issuecomment-276663883:1114,Integrability,wrap,wrapper,1114,"Done!. Thanks Tim!. On Wed, Feb 1, 2017 at 8:24 AM, Tim Poterba <notifications@github.com>; wrote:. > *@tpoterba* commented on this pull request.; >; > Need just one tiny change to the py/j connector. Looks great!; > ------------------------------; >; > In python/hail/dataset.py; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>:; >; > > @@ -2336,6 +2336,22 @@ def mendel_errors(self, output, fam):; > pargs = ['mendelerrors', '-o', output, '-f', fam]; > self.hc._run_command(self, pargs); >; > + def min_rep(self):; > + """"""; > + Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; > +; > + ** Examples **; > + 1) Simple trimming of a multi-allelic site, no change in variant position; > + `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`; > +; > + 2) Trimming of a bi-allelic site leading to a change in position; > + `1:10000:AATAA,AAGAA` => `1:10002:T:G`; > +; > + """"""; > + jvds = self._jvds.minrep(); >; > add in the try: / except: here, following the other methods in dataset.py.; >; > The default py4j errors look horrible, so calling our wrapper method helps; > a lot.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgaQoXMxYMPE_V-RMRgYp5mvNSf-Pks5rYIePgaJpZM4LzbBv>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883
https://github.com/hail-is/hail/pull/1324#issuecomment-276663883:691,Usability,Simpl,Simple,691,"Done!. Thanks Tim!. On Wed, Feb 1, 2017 at 8:24 AM, Tim Poterba <notifications@github.com>; wrote:. > *@tpoterba* commented on this pull request.; >; > Need just one tiny change to the py/j connector. Looks great!; > ------------------------------; >; > In python/hail/dataset.py; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>:; >; > > @@ -2336,6 +2336,22 @@ def mendel_errors(self, output, fam):; > pargs = ['mendelerrors', '-o', output, '-f', fam]; > self.hc._run_command(self, pargs); >; > + def min_rep(self):; > + """"""; > + Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; > +; > + ** Examples **; > + 1) Simple trimming of a multi-allelic site, no change in variant position; > + `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`; > +; > + 2) Trimming of a bi-allelic site leading to a change in position; > + `1:10000:AATAA,AAGAA` => `1:10002:T:G`; > +; > + """"""; > + jvds = self._jvds.minrep(); >; > add in the try: / except: here, following the other methods in dataset.py.; >; > The default py4j errors look horrible, so calling our wrapper method helps; > a lot.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgaQoXMxYMPE_V-RMRgYp5mvNSf-Pks5rYIePgaJpZM4LzbBv>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883
https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:410,Availability,error,error,410,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635
https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:503,Availability,Error,Error,503,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635
https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:531,Availability,FAILURE,FAILURE,531,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635
https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:90,Modifiability,Config,Configuring,90,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635
https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:35,Performance,Perform,Performing,35,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635
https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:46,Testability,Test,Test,46,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635
https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:804,Testability,log,log,804,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635
https://github.com/hail-is/hail/issues/1327#issuecomment-276986028:458,Availability,error,error,458,"Hi @tushu1232, can you post the output of:. ```; gcc --version ; g++ --version; ```. We strongly suggest using GCC 4.7 to compile hail because it has [relatively complete feature support](https://gcc.gnu.org/gcc-4.7/cxx0x_status.html) for C++11. If you must use an older version of C++ you can try changing that argument to `-std=c++0x`. If you have a version of GCC >4.7 and are still seeing this issue, I am happy to help determine why you're getting this error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276986028
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:52,Availability,error,error,52,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:2407,Availability,error,error,2407,"ve(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. *",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:3027,Availability,Error,Error,3027,"rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 42.509 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:3174,Availability,error,errors,3174,"rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 42.509 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:3210,Availability,FAILURE,FAILURE,3210,"rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 42.509 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:408,Integrability,depend,dependency,408,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:559,Integrability,depend,dependencies,559,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:642,Integrability,depend,dependencies,642,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1703,Integrability,depend,dependency,1703,"path.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1859,Integrability,depend,dependencies,1859,"haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1942,Integrability,depend,dependencies,1942," overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:2,Performance,load,loaded,2,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:434,Performance,load,loading,434,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1729,Performance,load,loading,1729,"path.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:419,Safety,detect,detected,419,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1714,Safety,detect,detected,1714,"path.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:481,Security,access,access,481,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1507,Security,access,accessed,1507,"type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Ann",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1776,Security,access,access,1776,"n incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:3444,Testability,log,log,3444,"rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 42.509 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831
https://github.com/hail-is/hail/issues/1327#issuecomment-277495418:102,Deployability,patch,patch,102,Also finally I was able to compile it easily with 2.1.0.; Does compiling with 1.6.2 need some special patch as I couldn't find in in the code source tree,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277495418
https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:84,Availability,error,error,84,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404
https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:187,Availability,error,error,187,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404
https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:1422,Availability,error,errors,1422,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404
https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:1458,Availability,FAILURE,FAILURE,1458,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404
https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:407,Modifiability,Config,Configuring,407,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404
https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:1692,Testability,log,log,1692,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404
https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:270,Availability,down,downloaded,270,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246
https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:383,Availability,error,error,383,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246
https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:243,Deployability,install,install,243,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246
https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:330,Deployability,UPDATE,UPDATE,330,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246
https://github.com/hail-is/hail/issues/1327#issuecomment-302838096:11,Availability,error,error,11,Great that error's resolved! but you'll need to compile Hail against the version of Spark you downloaded:. ```; ./gradlew -Dspark.version=2.1.1 shadowJar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302838096
https://github.com/hail-is/hail/issues/1327#issuecomment-302838096:94,Availability,down,downloaded,94,Great that error's resolved! but you'll need to compile Hail against the version of Spark you downloaded:. ```; ./gradlew -Dspark.version=2.1.1 shadowJar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302838096
https://github.com/hail-is/hail/issues/1327#issuecomment-302838927:47,Testability,test,test,47,"Got it, I just realized it hard way during the test run. Had no clue about that -Dspark.version option. Thanks for the prompt responses. 👍",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302838927
https://github.com/hail-is/hail/issues/1332#issuecomment-276719598:108,Deployability,update,update,108,"I'm convinced. @johnc1231, also change ""TruncatedBeta"" to ""TruncatedBetaDist"" global annotation. Be sure to update the documentation as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1332#issuecomment-276719598
https://github.com/hail-is/hail/pull/1338#issuecomment-278004376:48,Testability,test,tests,48,"Killed errant code that snuck into this branch, tests passing again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1338#issuecomment-278004376
https://github.com/hail-is/hail/pull/1342#issuecomment-277010916:103,Availability,failure,failures,103,I'm preparing a change to a `-Wall` and `-Werror` to the `Makefile` as well so we catch these as build failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1342#issuecomment-277010916
https://github.com/hail-is/hail/issues/1349#issuecomment-284852670:204,Usability,clear,clear,204,"I don't think the whitespace stuff belongs in a method, since that's expr language doc. I think if anything, the nMales/nFemales/nSamples stuff should go in query_samples, but I think the docs are pretty clear now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1349#issuecomment-284852670
https://github.com/hail-is/hail/issues/1360#issuecomment-278532107:3,Deployability,update,update,3,"An update: this seems to happen when running from an interactive shell as I'm setting up my own jar and python directory, so maybe I'm doing something wrong? Happens on the cloud but not locally...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1360#issuecomment-278532107
https://github.com/hail-is/hail/pull/1369#issuecomment-278473040:66,Availability,error,error,66,"Fixes #1368 . Yes, TArray is IndexSeq, I've verified it fixes the error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1369#issuecomment-278473040
https://github.com/hail-is/hail/issues/1371#issuecomment-422369573:44,Availability,toler,tolerant,44,"we've moved in the opposite direction -- be tolerant as possible on input, let people minrep if they want",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1371#issuecomment-422369573
https://github.com/hail-is/hail/pull/1373#issuecomment-279190312:4,Testability,test,test,4,Add test please.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1373#issuecomment-279190312
https://github.com/hail-is/hail/pull/1373#issuecomment-279429110:11,Testability,test,tests,11,"Just added tests, @jbloom22 !",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1373#issuecomment-279429110
https://github.com/hail-is/hail/pull/1373#issuecomment-279790111:123,Usability,clear,clear,123,"There are some tiny formatting issues and typos, but the main thing is to add an short examples section in docs that makes clear what this is useful for. I'd put one example for INFO field and one for FORMAT field. For other commands for format, like:; https://hail.is/hail/hail.VariantDataset.html#hail.VariantDataset.pca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1373#issuecomment-279790111
https://github.com/hail-is/hail/pull/1374#issuecomment-279443114:121,Deployability,update,updated,121,"In anticipation of changes to the CI system, I've moved the local tutorial files to `/usr/local/hail-tutorial-files` and updated the CI configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114
https://github.com/hail-is/hail/pull/1374#issuecomment-279443114:136,Deployability,configurat,configuration,136,"In anticipation of changes to the CI system, I've moved the local tutorial files to `/usr/local/hail-tutorial-files` and updated the CI configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114
https://github.com/hail-is/hail/pull/1374#issuecomment-279443114:136,Modifiability,config,configuration,136,"In anticipation of changes to the CI system, I've moved the local tutorial files to `/usr/local/hail-tutorial-files` and updated the CI configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114
https://github.com/hail-is/hail/pull/1374#issuecomment-280135164:43,Availability,error,error,43,"@jbloom22 : Back to you. To remove the red error box on the plot output, I ended up adding CSS to hide the stderr div elements in the HTML.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-280135164
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:491,Testability,test,tests,491,"I created a modified version of `profile225.vds`:. ```; from hail import *; hc = HailContext(); (hc.read('/Users/jbloom/data/profile225.vds'); .filter_multi(); .variant_qc(); .hardcalls(); .annotate_samples_expr('sa.pheno = pcoin(0.5), sa.cov1 = rnorm(0,1), sa.cov2 = rnorm(0,1), sa.cov3 = rnorm(0,1), sa.cov4 = rnorm(0,1), sa.cov5 = rnorm(0,1), sa.cov6 = rnorm(0,1), sa.cov7 = rnorm(0,1), sa.cov8 = rnorm(0,1)'); .write('/Users/jbloom/data/profile225.prelogreg.vds')); ```. And ran all the tests with two samples covariates (note these are all mapVariants so just stacking tranformations):; ```; (hc.read('/Users/jbloom/data/profile225.prelogreg.vds'); 	.linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:702,Testability,log,logreg,702,"I created a modified version of `profile225.vds`:. ```; from hail import *; hc = HailContext(); (hc.read('/Users/jbloom/data/profile225.vds'); .filter_multi(); .variant_qc(); .hardcalls(); .annotate_samples_expr('sa.pheno = pcoin(0.5), sa.cov1 = rnorm(0,1), sa.cov2 = rnorm(0,1), sa.cov3 = rnorm(0,1), sa.cov4 = rnorm(0,1), sa.cov5 = rnorm(0,1), sa.cov6 = rnorm(0,1), sa.cov7 = rnorm(0,1), sa.cov8 = rnorm(0,1)'); .write('/Users/jbloom/data/profile225.prelogreg.vds')); ```. And ran all the tests with two samples covariates (note these are all mapVariants so just stacking tranformations):; ```; (hc.read('/Users/jbloom/data/profile225.prelogreg.vds'); 	.linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:772,Testability,log,logreg,772,"I created a modified version of `profile225.vds`:. ```; from hail import *; hc = HailContext(); (hc.read('/Users/jbloom/data/profile225.vds'); .filter_multi(); .variant_qc(); .hardcalls(); .annotate_samples_expr('sa.pheno = pcoin(0.5), sa.cov1 = rnorm(0,1), sa.cov2 = rnorm(0,1), sa.cov3 = rnorm(0,1), sa.cov4 = rnorm(0,1), sa.cov5 = rnorm(0,1), sa.cov6 = rnorm(0,1), sa.cov7 = rnorm(0,1), sa.cov8 = rnorm(0,1)'); .write('/Users/jbloom/data/profile225.prelogreg.vds')); ```. And ran all the tests with two samples covariates (note these are all mapVariants so just stacking tranformations):; ```; (hc.read('/Users/jbloom/data/profile225.prelogreg.vds'); 	.linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:840,Testability,log,logreg,840,"I created a modified version of `profile225.vds`:. ```; from hail import *; hc = HailContext(); (hc.read('/Users/jbloom/data/profile225.vds'); .filter_multi(); .variant_qc(); .hardcalls(); .annotate_samples_expr('sa.pheno = pcoin(0.5), sa.cov1 = rnorm(0,1), sa.cov2 = rnorm(0,1), sa.cov3 = rnorm(0,1), sa.cov4 = rnorm(0,1), sa.cov5 = rnorm(0,1), sa.cov6 = rnorm(0,1), sa.cov7 = rnorm(0,1), sa.cov8 = rnorm(0,1)'); .write('/Users/jbloom/data/profile225.prelogreg.vds')); ```. And ran all the tests with two samples covariates (note these are all mapVariants so just stacking tranformations):; ```; (hc.read('/Users/jbloom/data/profile225.prelogreg.vds'); 	.linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:912,Testability,log,logreg,912,"I created a modified version of `profile225.vds`:. ```; from hail import *; hc = HailContext(); (hc.read('/Users/jbloom/data/profile225.vds'); .filter_multi(); .variant_qc(); .hardcalls(); .annotate_samples_expr('sa.pheno = pcoin(0.5), sa.cov1 = rnorm(0,1), sa.cov2 = rnorm(0,1), sa.cov3 = rnorm(0,1), sa.cov4 = rnorm(0,1), sa.cov5 = rnorm(0,1), sa.cov6 = rnorm(0,1), sa.cov7 = rnorm(0,1), sa.cov8 = rnorm(0,1)'); .write('/Users/jbloom/data/profile225.prelogreg.vds')); ```. And ran all the tests with two samples covariates (note these are all mapVariants so just stacking tranformations):; ```; (hc.read('/Users/jbloom/data/profile225.prelogreg.vds'); 	.linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:1020,Testability,log,logreg,1020,"ated a modified version of `profile225.vds`:. ```; from hail import *; hc = HailContext(); (hc.read('/Users/jbloom/data/profile225.vds'); .filter_multi(); .variant_qc(); .hardcalls(); .annotate_samples_expr('sa.pheno = pcoin(0.5), sa.cov1 = rnorm(0,1), sa.cov2 = rnorm(0,1), sa.cov3 = rnorm(0,1), sa.cov4 = rnorm(0,1), sa.cov5 = rnorm(0,1), sa.cov6 = rnorm(0,1), sa.cov7 = rnorm(0,1), sa.cov8 = rnorm(0,1)'); .write('/Users/jbloom/data/profile225.prelogreg.vds')); ```. And ran all the tests with two samples covariates (note these are all mapVariants so just stacking tranformations):; ```; (hc.read('/Users/jbloom/data/profile225.prelogreg.vds'); 	.linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:1525,Testability,log,logregbetalrtfirth,1525,"he tests with two samples covariates (note these are all mapVariants so just stacking tranformations):; ```; (hc.read('/Users/jbloom/data/profile225.prelogreg.vds'); 	.linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-94",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:1713,Testability,log,logregpvallrtfirth,1713,"); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://clo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:2396,Testability,log,logregiter,2396,"nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:2530,Testability,log,logreglrtiter,2530,"alrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.git",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:2669,Testability,log,logregfirthiter,2669,"ote that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpva",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:2849,Testability,log,logregpvalwaldlrt,2849," Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://clo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3011,Testability,log,logregpvalscorelrt,3011,"188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3176,Testability,log,logregpvalscorefirth,3176,"s://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](https://cloud.githubusercontent.com/assets/3201642/23096496/ed0cf5b4-f5eb-11e6-8283-d4c02dfb9f76.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3345,Testability,log,logregbetafirthlin,3345,"s://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](https://cloud.githubusercontent.com/assets/3201642/23096496/ed0cf5b4-f5eb-11e6-8283-d4c02dfb9f76.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3498,Testability,log,logregpvalfirthlin,3498,"s://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](https://cloud.githubusercontent.com/assets/3201642/23096496/ed0cf5b4-f5eb-11e6-8283-d4c02dfb9f76.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3656,Testability,test,test,3656,"s://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](https://cloud.githubusercontent.com/assets/3201642/23096496/ed0cf5b4-f5eb-11e6-8283-d4c02dfb9f76.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3665,Testability,log,logregpvalscorelin,3665,"s://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](https://cloud.githubusercontent.com/assets/3201642/23096496/ed0cf5b4-f5eb-11e6-8283-d4c02dfb9f76.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3809,Testability,log,logreg,3809,"s://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](https://cloud.githubusercontent.com/assets/3201642/23096496/ed0cf5b4-f5eb-11e6-8283-d4c02dfb9f76.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3986,Testability,log,logreg,3986,"s://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](https://cloud.githubusercontent.com/assets/3201642/23096496/ed0cf5b4-f5eb-11e6-8283-d4c02dfb9f76.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409
https://github.com/hail-is/hail/pull/1375#issuecomment-279199507:1062,Performance,perform,performance,1062,"Timing on laptop of read, method, count:. count(genotypes=True): 4.17 s; count: 1.84s; linreg: 7.1 s; score: 51 s; lrt: 92 s; wald: 93 s; firth: 300 s. ```; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True); %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True) # 4.17 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count() # 1.84 s. %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 7.1 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 51 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 92 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 93 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 300 s; ```. ![logreg performance](https://cloud.githubusercontent.com/assets/3201642/23095382/18e310de-f5d7-11e6-9b0b-909b6107286b.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507
https://github.com/hail-is/hail/pull/1375#issuecomment-279199507:595,Testability,log,logreg,595,"Timing on laptop of read, method, count:. count(genotypes=True): 4.17 s; count: 1.84s; linreg: 7.1 s; score: 51 s; lrt: 92 s; wald: 93 s; firth: 300 s. ```; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True); %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True) # 4.17 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count() # 1.84 s. %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 7.1 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 51 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 92 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 93 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 300 s; ```. ![logreg performance](https://cloud.githubusercontent.com/assets/3201642/23095382/18e310de-f5d7-11e6-9b0b-909b6107286b.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507
https://github.com/hail-is/hail/pull/1375#issuecomment-279199507:724,Testability,log,logreg,724,"Timing on laptop of read, method, count:. count(genotypes=True): 4.17 s; count: 1.84s; linreg: 7.1 s; score: 51 s; lrt: 92 s; wald: 93 s; firth: 300 s. ```; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True); %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True) # 4.17 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count() # 1.84 s. %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 7.1 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 51 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 92 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 93 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 300 s; ```. ![logreg performance](https://cloud.githubusercontent.com/assets/3201642/23095382/18e310de-f5d7-11e6-9b0b-909b6107286b.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507
https://github.com/hail-is/hail/pull/1375#issuecomment-279199507:851,Testability,log,logreg,851,"Timing on laptop of read, method, count:. count(genotypes=True): 4.17 s; count: 1.84s; linreg: 7.1 s; score: 51 s; lrt: 92 s; wald: 93 s; firth: 300 s. ```; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True); %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True) # 4.17 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count() # 1.84 s. %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 7.1 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 51 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 92 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 93 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 300 s; ```. ![logreg performance](https://cloud.githubusercontent.com/assets/3201642/23095382/18e310de-f5d7-11e6-9b0b-909b6107286b.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507
https://github.com/hail-is/hail/pull/1375#issuecomment-279199507:979,Testability,log,logreg,979,"Timing on laptop of read, method, count:. count(genotypes=True): 4.17 s; count: 1.84s; linreg: 7.1 s; score: 51 s; lrt: 92 s; wald: 93 s; firth: 300 s. ```; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True); %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True) # 4.17 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count() # 1.84 s. %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 7.1 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 51 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 92 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 93 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 300 s; ```. ![logreg performance](https://cloud.githubusercontent.com/assets/3201642/23095382/18e310de-f5d7-11e6-9b0b-909b6107286b.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507
https://github.com/hail-is/hail/pull/1375#issuecomment-279199507:1055,Testability,log,logreg,1055,"Timing on laptop of read, method, count:. count(genotypes=True): 4.17 s; count: 1.84s; linreg: 7.1 s; score: 51 s; lrt: 92 s; wald: 93 s; firth: 300 s. ```; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True); %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count(genotypes=True) # 4.17 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').count() # 1.84 s. %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 7.1 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 51 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 92 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 93 s; %time hc.read('/Users/jbloom/data/profile225.prelogreg.vds').logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2']).count() # 300 s; ```. ![logreg performance](https://cloud.githubusercontent.com/assets/3201642/23095382/18e310de-f5d7-11e6-9b0b-909b6107286b.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279199507
https://github.com/hail-is/hail/pull/1375#issuecomment-279406347:18,Testability,test,testing,18,"don't review yet, testing another branch for speed improvements now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279406347
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1239,Energy Efficiency,reduce,reduce,1239,"h vectors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1420,Energy Efficiency,reduce,reduced,1420,"tors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:76,Performance,perform,performed,76,"More timing info. I tried a QR / TriSolve approach for `fit` as well and it performed worse so removed it. I believe this because solving tiny systems (dimension number of covariates) is dwarfed by time spent working with vectors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1720,Safety,avoid,avoid,1720,"tors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1135,Testability,log,logic,1135,"olving tiny systems (dimension number of covariates) is dwarfed by time spent working with vectors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Doub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1200,Testability,Log,LogisticRegressionFit,1200,"h vectors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1872,Testability,log,log,1872,"tors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1977,Testability,Log,LogisticRegressionFit,1977,"tors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833
https://github.com/hail-is/hail/pull/1375#issuecomment-280150750:43,Performance,perform,performance,43,@cseed rebased! I'll probably put a bit on performance characteristics and graphs in the discuss post to go with it once it's in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-280150750
https://github.com/hail-is/hail/issues/1377#issuecomment-279429609:34,Testability,test,test,34,"Looks like a typo: you wrote ""scr/test/resources/sample.vcf"" instead of ""src/test/resources/sample.vcf""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377#issuecomment-279429609
https://github.com/hail-is/hail/issues/1377#issuecomment-279429609:77,Testability,test,test,77,"Looks like a typo: you wrote ""scr/test/resources/sample.vcf"" instead of ""src/test/resources/sample.vcf""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377#issuecomment-279429609
https://github.com/hail-is/hail/issues/1384#issuecomment-349720138:28,Deployability,release,releases,28,"This is a quadratic task in releases, so it's not really feasible. We just need to be better about writing which interface is used in a post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138
https://github.com/hail-is/hail/issues/1384#issuecomment-349720138:113,Integrability,interface,interface,113,"This is a quadratic task in releases, so it's not really feasible. We just need to be better about writing which interface is used in a post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138
https://github.com/hail-is/hail/issues/1386#issuecomment-279695804:13,Modifiability,extend,extends,13,- [ ] strip `extends Serializable` from KeyTable,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1386#issuecomment-279695804
https://github.com/hail-is/hail/issues/1386#issuecomment-305807317:95,Modifiability,extend,extends,95,"Unchecking box above, as KeyTable is a case class it is serializable by default, regardless of extends Serializable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1386#issuecomment-305807317
https://github.com/hail-is/hail/issues/1391#issuecomment-312126181:93,Integrability,interface,interface,93,"I think we should do this by default, and remove the ability to annotate in this method. The interface is a bit too complicated",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1391#issuecomment-312126181
https://github.com/hail-is/hail/issues/1391#issuecomment-312283421:202,Integrability,interface,interface,202,"I like this proposal!. On Thu, Jun 29, 2017 at 6:30 PM, Tim Poterba <notifications@github.com>; wrote:. > I think we should do this by default, and remove the ability to annotate; > in this method. The interface is a bit too complicated; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1391#issuecomment-312126181>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgbSEIxHUor98MljwApai7csY9q2Zks5sJCWHgaJpZM4MB6sU>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1391#issuecomment-312283421
https://github.com/hail-is/hail/pull/1393#issuecomment-280140767:137,Deployability,update,update,137,Great change. Can you:; - delete chi1; - replace uses of chi1 with your more general version in the few places it appears in the code; - update the docs in HailExpressionLanguage.md to reflect only your version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1393#issuecomment-280140767
https://github.com/hail-is/hail/issues/1396#issuecomment-301546353:17,Deployability,release,release,17,Closing with 0.1 release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1396#issuecomment-301546353
https://github.com/hail-is/hail/pull/1397#issuecomment-280419318:190,Deployability,install,installDist,190,"Python interface changes:; - filter_variants_all -> drop_variants; - filter_samples_all -> drop_samples; - renamed ""condition"" to ""expr"" in parameter names where appropriate. Removed gradle installDist",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318
https://github.com/hail-is/hail/pull/1397#issuecomment-280419318:7,Integrability,interface,interface,7,"Python interface changes:; - filter_variants_all -> drop_variants; - filter_samples_all -> drop_samples; - renamed ""condition"" to ""expr"" in parameter names where appropriate. Removed gradle installDist",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318
https://github.com/hail-is/hail/pull/1406#issuecomment-280786286:26,Availability,error,error,26,"It's possible to get this error:; ```; ----> 5 hc.import_vcf('src/test/resources/sample.vcf').write('sample.vds'). /hail/python/hail/java.py in function_wrapper(*args, **kwargs); 92 except Py4JJavaError as e:; 93 msg = env.jutils.getMinimalMessage(e.java_exception); ---> 94 raise FatalError(msg); 95 except Py4JError as e:; 96 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: UnsupportedClassVersionError: htsjdk/tribble/TribbleException : Unsupported major.minor version 52.0; ```. I figure this change might be nicer, but am happy to hear input",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286
https://github.com/hail-is/hail/pull/1406#issuecomment-280786286:345,Availability,error,error,345,"It's possible to get this error:; ```; ----> 5 hc.import_vcf('src/test/resources/sample.vcf').write('sample.vds'). /hail/python/hail/java.py in function_wrapper(*args, **kwargs); 92 except Py4JJavaError as e:; 93 msg = env.jutils.getMinimalMessage(e.java_exception); ---> 94 raise FatalError(msg); 95 except Py4JError as e:; 96 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: UnsupportedClassVersionError: htsjdk/tribble/TribbleException : Unsupported major.minor version 52.0; ```. I figure this change might be nicer, but am happy to hear input",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286
https://github.com/hail-is/hail/pull/1406#issuecomment-280786286:66,Testability,test,test,66,"It's possible to get this error:; ```; ----> 5 hc.import_vcf('src/test/resources/sample.vcf').write('sample.vds'). /hail/python/hail/java.py in function_wrapper(*args, **kwargs); 92 except Py4JJavaError as e:; 93 msg = env.jutils.getMinimalMessage(e.java_exception); ---> 94 raise FatalError(msg); 95 except Py4JError as e:; 96 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: UnsupportedClassVersionError: htsjdk/tribble/TribbleException : Unsupported major.minor version 52.0; ```. I figure this change might be nicer, but am happy to hear input",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286
https://github.com/hail-is/hail/pull/1406#issuecomment-280786286:339,Testability,log,log,339,"It's possible to get this error:; ```; ----> 5 hc.import_vcf('src/test/resources/sample.vcf').write('sample.vds'). /hail/python/hail/java.py in function_wrapper(*args, **kwargs); 92 except Py4JJavaError as e:; 93 msg = env.jutils.getMinimalMessage(e.java_exception); ---> 94 raise FatalError(msg); 95 except Py4JError as e:; 96 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: UnsupportedClassVersionError: htsjdk/tribble/TribbleException : Unsupported major.minor version 52.0; ```. I figure this change might be nicer, but am happy to hear input",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286
https://github.com/hail-is/hail/pull/1406#issuecomment-280845674:23,Availability,error,error,23,@tpoterba I think your error message is much better.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280845674
https://github.com/hail-is/hail/pull/1406#issuecomment-280845674:29,Integrability,message,message,29,@tpoterba I think your error message is much better.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280845674
https://github.com/hail-is/hail/issues/1410#issuecomment-282414199:132,Availability,error,error,132,"To clarify, you could could say this:. `kt.aggregate('rows.filter(r => r.col1 < r.col2).count()')`. but this would produce a symref error:. `kt.aggregate('col1.filter(c => c < col2).count()')`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1410#issuecomment-282414199
https://github.com/hail-is/hail/issues/1410#issuecomment-282784540:295,Integrability,interface,interface,295,"I agree with your criticism, although my feeling is that rows and r in your proposal are noisy and unnecessary. Two thoughts:. I think this is best resolved in the context of embedding the expression language in Python. I think understanding pandas and what's involved in building a pandas-like interface for VariantDatasets is a good way to start. If we do address it in the current setup, what do we want to write? How about `kt.aggregate('filter(col1 < col2).count()` or, assuming we're doing a summing col1, `filter(col1 < col2).sum(col1)`. Then all the lambdas go away. We clearly need the scope in aggregators. Why not make that explicit, and throw out the single implicit value? Then `filter(col1 < col2).with(col3 = col1 * col2).mean(col3)`. I'm not sure about flatMap. `flatWith(col3 = <array expr>)`? I guess that's the same as `with(col3 = <array expr>).explode(col3)`. Then Aggregables look like Structs:. ```; Aggregable {; col1: Int,; col2: Int, ...; }; ```. Then there's nothing funny going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1410#issuecomment-282784540
https://github.com/hail-is/hail/issues/1410#issuecomment-282784540:578,Usability,clear,clearly,578,"I agree with your criticism, although my feeling is that rows and r in your proposal are noisy and unnecessary. Two thoughts:. I think this is best resolved in the context of embedding the expression language in Python. I think understanding pandas and what's involved in building a pandas-like interface for VariantDatasets is a good way to start. If we do address it in the current setup, what do we want to write? How about `kt.aggregate('filter(col1 < col2).count()` or, assuming we're doing a summing col1, `filter(col1 < col2).sum(col1)`. Then all the lambdas go away. We clearly need the scope in aggregators. Why not make that explicit, and throw out the single implicit value? Then `filter(col1 < col2).with(col3 = col1 * col2).mean(col3)`. I'm not sure about flatMap. `flatWith(col3 = <array expr>)`? I guess that's the same as `with(col3 = <array expr>).explode(col3)`. Then Aggregables look like Structs:. ```; Aggregable {; col1: Int,; col2: Int, ...; }; ```. Then there's nothing funny going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1410#issuecomment-282784540
https://github.com/hail-is/hail/pull/1411#issuecomment-283861433:143,Deployability,release,release,143,"This should go in. For 0-argument functions, you should support both with and without parens for now. We can make it more strict after the 0.1 release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1411#issuecomment-283861433
https://github.com/hail-is/hail/pull/1415#issuecomment-282545923:79,Availability,error,error,79,"Hi guys,; I've cloned the master and rebuilt, but unfortunately I get the same error when annotating the VCF just read.; ```; [Stage 0:====================================================>(2111 + 1) / 2112]hail: info: Coerced sorted dataset. ------------Annotate the VCF file-------------; [Stage 1:> (0 + 160) / 2112]; [Stage 1:> (0 + 160) / 2112]found fatal is.hail.utils.package$FatalExcepti; on: swe.vcf.bgz: invalid AD field `24,0,0': expected 2 values, but got 3.; offending line: 1	65684548	.	G	A	3524.14	VQSRTrancheSNP99.60to99.80	AC=1;AF=...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1415#issuecomment-282545923
https://github.com/hail-is/hail/pull/1415#issuecomment-282566840:32,Availability,error,error,32,"This doesn't look like the same error. This error is found in a genotype call in a biallelic variant with 3 AD values -- a violation of the VCF spec (AD is ""R""-numbered). . We've seen this before on VCFs that were split by certain tools, and since there were enough of them, we added an option `skip_bad_ad` on import_vcf: https://hail.is/hail/hail.HailContext.html#hail.HailContext.import_vcf. You'll want to run with that option set to true.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1415#issuecomment-282566840
https://github.com/hail-is/hail/pull/1415#issuecomment-282566840:44,Availability,error,error,44,"This doesn't look like the same error. This error is found in a genotype call in a biallelic variant with 3 AD values -- a violation of the VCF spec (AD is ""R""-numbered). . We've seen this before on VCFs that were split by certain tools, and since there were enough of them, we added an option `skip_bad_ad` on import_vcf: https://hail.is/hail/hail.HailContext.html#hail.HailContext.import_vcf. You'll want to run with that option set to true.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1415#issuecomment-282566840
https://github.com/hail-is/hail/issues/1416#issuecomment-284279573:21,Testability,test,test,21,"I'm yanking out this test in PR #1475 in favor of a direct comparison of h2 with FaST-LMM. That said, we should still understand what was causing different runs to give different values.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416#issuecomment-284279573
https://github.com/hail-is/hail/issues/1419#issuecomment-281713988:113,Testability,test,testing,113,"Hi thanks for the bug report, I'm investigating now. Unfortunately, Spark 2.1.0 is not included in our automated testing system. I will also set up automated testing of Spark 2.1.0 to prevent future regressions like this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281713988
https://github.com/hail-is/hail/issues/1419#issuecomment-281713988:158,Testability,test,testing,158,"Hi thanks for the bug report, I'm investigating now. Unfortunately, Spark 2.1.0 is not included in our automated testing system. I will also set up automated testing of Spark 2.1.0 to prevent future regressions like this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281713988
https://github.com/hail-is/hail/issues/1419#issuecomment-281757910:123,Deployability,upgrade,upgraded,123,"The breeze version packaged with Spark [was changed to 0.12](https://issues.apache.org/jira/browse/SPARK-16494) when Spark upgraded from 2.0.2 to 2.1.0. . According to [the PR](https://github.com/apache/spark/pull/14150/files#diff-06b6ad3483185a20d3095743faa5e4f0L15) linked from that JIRA issue, the breeze packaged with Spark 2.0.2 was version 0.11.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281757910
https://github.com/hail-is/hail/issues/1419#issuecomment-281760653:98,Testability,log,log,98,"@natestockham, I cannot reproduce your issue with this master commit:. ```; dking@wmb16-359 # git log | head -n 3; commit ee646dc7fce6131dffbe86e6ebafc6e9fd224659; Author: John Compitello <johnc@broadinstitute.org>; Date: Tue Feb 21 17:15:13 2017 -0500; ```; and this invocation of gradle (the `SPARK_HOME` path is, of course, peculiar to my machine):; ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test -Dspark.version=2.1.0; ```. However, the above innovation fails in the `LinearMixedRegressionSuite`:. ```; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED; org.scalatest.exceptions.TestFailedException at LinearMixedRegressionSuite.scala:370. 1 test completed, 1 failed; ```. The test verifies that the fit beta is equivalent, within a certain precision, to `0.8410147169942509`. The fit beta produced with Spark 2.0.2 passes this test. The fit beta produced with Spark 2.1.0 is `1.3081684472318504`. I'm on Mac OS version 10.11.6. Processor is Intel Core i7. I have not yet tried on cloud machines or a GNU/Linux VM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281760653
https://github.com/hail-is/hail/issues/1419#issuecomment-281760653:420,Testability,test,test,420,"@natestockham, I cannot reproduce your issue with this master commit:. ```; dking@wmb16-359 # git log | head -n 3; commit ee646dc7fce6131dffbe86e6ebafc6e9fd224659; Author: John Compitello <johnc@broadinstitute.org>; Date: Tue Feb 21 17:15:13 2017 -0500; ```; and this invocation of gradle (the `SPARK_HOME` path is, of course, peculiar to my machine):; ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test -Dspark.version=2.1.0; ```. However, the above innovation fails in the `LinearMixedRegressionSuite`:. ```; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED; org.scalatest.exceptions.TestFailedException at LinearMixedRegressionSuite.scala:370. 1 test completed, 1 failed; ```. The test verifies that the fit beta is equivalent, within a certain precision, to `0.8410147169942509`. The fit beta produced with Spark 2.0.2 passes this test. The fit beta produced with Spark 2.1.0 is `1.3081684472318504`. I'm on Mac OS version 10.11.6. Processor is Intel Core i7. I have not yet tried on cloud machines or a GNU/Linux VM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281760653
https://github.com/hail-is/hail/issues/1419#issuecomment-281760653:554,Testability,test,test,554,"@natestockham, I cannot reproduce your issue with this master commit:. ```; dking@wmb16-359 # git log | head -n 3; commit ee646dc7fce6131dffbe86e6ebafc6e9fd224659; Author: John Compitello <johnc@broadinstitute.org>; Date: Tue Feb 21 17:15:13 2017 -0500; ```; and this invocation of gradle (the `SPARK_HOME` path is, of course, peculiar to my machine):; ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test -Dspark.version=2.1.0; ```. However, the above innovation fails in the `LinearMixedRegressionSuite`:. ```; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED; org.scalatest.exceptions.TestFailedException at LinearMixedRegressionSuite.scala:370. 1 test completed, 1 failed; ```. The test verifies that the fit beta is equivalent, within a certain precision, to `0.8410147169942509`. The fit beta produced with Spark 2.0.2 passes this test. The fit beta produced with Spark 2.1.0 is `1.3081684472318504`. I'm on Mac OS version 10.11.6. Processor is Intel Core i7. I have not yet tried on cloud machines or a GNU/Linux VM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281760653
https://github.com/hail-is/hail/issues/1419#issuecomment-281760653:650,Testability,Test,TestFailedException,650,"@natestockham, I cannot reproduce your issue with this master commit:. ```; dking@wmb16-359 # git log | head -n 3; commit ee646dc7fce6131dffbe86e6ebafc6e9fd224659; Author: John Compitello <johnc@broadinstitute.org>; Date: Tue Feb 21 17:15:13 2017 -0500; ```; and this invocation of gradle (the `SPARK_HOME` path is, of course, peculiar to my machine):; ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test -Dspark.version=2.1.0; ```. However, the above innovation fails in the `LinearMixedRegressionSuite`:. ```; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED; org.scalatest.exceptions.TestFailedException at LinearMixedRegressionSuite.scala:370. 1 test completed, 1 failed; ```. The test verifies that the fit beta is equivalent, within a certain precision, to `0.8410147169942509`. The fit beta produced with Spark 2.0.2 passes this test. The fit beta produced with Spark 2.1.0 is `1.3081684472318504`. I'm on Mac OS version 10.11.6. Processor is Intel Core i7. I have not yet tried on cloud machines or a GNU/Linux VM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281760653
https://github.com/hail-is/hail/issues/1419#issuecomment-281760653:713,Testability,test,test,713,"@natestockham, I cannot reproduce your issue with this master commit:. ```; dking@wmb16-359 # git log | head -n 3; commit ee646dc7fce6131dffbe86e6ebafc6e9fd224659; Author: John Compitello <johnc@broadinstitute.org>; Date: Tue Feb 21 17:15:13 2017 -0500; ```; and this invocation of gradle (the `SPARK_HOME` path is, of course, peculiar to my machine):; ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test -Dspark.version=2.1.0; ```. However, the above innovation fails in the `LinearMixedRegressionSuite`:. ```; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED; org.scalatest.exceptions.TestFailedException at LinearMixedRegressionSuite.scala:370. 1 test completed, 1 failed; ```. The test verifies that the fit beta is equivalent, within a certain precision, to `0.8410147169942509`. The fit beta produced with Spark 2.0.2 passes this test. The fit beta produced with Spark 2.1.0 is `1.3081684472318504`. I'm on Mac OS version 10.11.6. Processor is Intel Core i7. I have not yet tried on cloud machines or a GNU/Linux VM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281760653
https://github.com/hail-is/hail/issues/1419#issuecomment-281760653:748,Testability,test,test,748,"@natestockham, I cannot reproduce your issue with this master commit:. ```; dking@wmb16-359 # git log | head -n 3; commit ee646dc7fce6131dffbe86e6ebafc6e9fd224659; Author: John Compitello <johnc@broadinstitute.org>; Date: Tue Feb 21 17:15:13 2017 -0500; ```; and this invocation of gradle (the `SPARK_HOME` path is, of course, peculiar to my machine):; ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test -Dspark.version=2.1.0; ```. However, the above innovation fails in the `LinearMixedRegressionSuite`:. ```; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED; org.scalatest.exceptions.TestFailedException at LinearMixedRegressionSuite.scala:370. 1 test completed, 1 failed; ```. The test verifies that the fit beta is equivalent, within a certain precision, to `0.8410147169942509`. The fit beta produced with Spark 2.0.2 passes this test. The fit beta produced with Spark 2.1.0 is `1.3081684472318504`. I'm on Mac OS version 10.11.6. Processor is Intel Core i7. I have not yet tried on cloud machines or a GNU/Linux VM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281760653
https://github.com/hail-is/hail/issues/1419#issuecomment-281760653:899,Testability,test,test,899,"@natestockham, I cannot reproduce your issue with this master commit:. ```; dking@wmb16-359 # git log | head -n 3; commit ee646dc7fce6131dffbe86e6ebafc6e9fd224659; Author: John Compitello <johnc@broadinstitute.org>; Date: Tue Feb 21 17:15:13 2017 -0500; ```; and this invocation of gradle (the `SPARK_HOME` path is, of course, peculiar to my machine):; ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test -Dspark.version=2.1.0; ```. However, the above innovation fails in the `LinearMixedRegressionSuite`:. ```; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED; org.scalatest.exceptions.TestFailedException at LinearMixedRegressionSuite.scala:370. 1 test completed, 1 failed; ```. The test verifies that the fit beta is equivalent, within a certain precision, to `0.8410147169942509`. The fit beta produced with Spark 2.0.2 passes this test. The fit beta produced with Spark 2.1.0 is `1.3081684472318504`. I'm on Mac OS version 10.11.6. Processor is Intel Core i7. I have not yet tried on cloud machines or a GNU/Linux VM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281760653
https://github.com/hail-is/hail/issues/1419#issuecomment-281763304:135,Testability,test,test,135,"Moreover, this issue appears to not be due to Breeze 0.12 natives:. ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test --tests 'is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM' -Dspark.version=2.1.0 -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS -Dcom.github.fommil.netlib.LAPACK=com.github.fommil.netlib.F2jLAPACK -Dcom.github.fommil.netlib.ARPACK=com.github.fommil.netlib.F2jARPACK; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281763304
https://github.com/hail-is/hail/issues/1419#issuecomment-281763304:142,Testability,test,tests,142,"Moreover, this issue appears to not be due to Breeze 0.12 natives:. ```; SPARK_HOME=/Users/dking/borg/spark-2.1.0-bin-hadoop2.7 gradle test --tests 'is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM' -Dspark.version=2.1.0 -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS -Dcom.github.fommil.netlib.LAPACK=com.github.fommil.netlib.F2jLAPACK -Dcom.github.fommil.netlib.ARPACK=com.github.fommil.netlib.F2jARPACK; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281763304
https://github.com/hail-is/hail/issues/1419#issuecomment-281827437:93,Availability,error,error,93,I think the primary question is:. > Is the difference between `0.84` and `1.31` a sign of an error or is this a reasonable value for the search to find for `delta`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281827437
https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:91,Availability,echo,echo,91,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119
https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:109,Availability,echo,echo,109,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119
https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:126,Availability,echo,echo,126,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119
https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:216,Availability,failure,failure,216,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119
https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:393,Deployability,install,installation,393,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119
https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:211,Testability,test,test,211,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119
https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:308,Testability,test,test,308,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:2319,Availability,echo,echo,2319," that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvironments/hail/python`. `echo $HAIL_HOME; /scratch/PI/dpwall/computeEnvironments/hail`. Thank you, and if you have any ideas why the above tests are failing I would love to hear it. Thanks again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:2392,Availability,echo,echo,2392," that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvironments/hail/python`. `echo $HAIL_HOME; /scratch/PI/dpwall/computeEnvironments/hail`. Thank you, and if you have any ideas why the above tests are failing I would love to hear it. Thanks again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:2648,Availability,echo,echo,2648," that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvironments/hail/python`. `echo $HAIL_HOME; /scratch/PI/dpwall/computeEnvironments/hail`. Thank you, and if you have any ideas why the above tests are failing I would love to hear it. Thanks again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:919,Deployability,install,installation,919,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:1571,Deployability,install,install,1571,"FitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:129,Testability,test,tests,129,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:217,Testability,test,test,217,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:301,Testability,test,test,301,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:333,Testability,test,testIBDPlink,333,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:376,Testability,test,test,376,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:414,Testability,test,testImputeSexPlinkVersion,414,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:470,Testability,test,test,470,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:518,Testability,test,testIbcPlinkVersion,518,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:568,Testability,test,test,568,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:662,Testability,test,tests,662,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:710,Testability,test,test,710,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:725,Testability,test,tests,725,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:781,Testability,test,tests,781,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:1037,Testability,test,tests,1037," on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_H",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:1530,Testability,test,testing,1530,"efficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:1645,Testability,test,tests,1645,"FitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:1754,Testability,test,test,1754,"les/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvironments/hail/python`. `echo $HAIL_HOME; /scratch/PI/dpwall/computeEnvironments/hail`. Thank you, and if you have any ideas why the above tests ar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:2762,Testability,test,tests,2762," that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvironments/hail/python`. `echo $HAIL_HOME; /scratch/PI/dpwall/computeEnvironments/hail`. Thank you, and if you have any ideas why the above tests are failing I would love to hear it. Thanks again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:621,Availability,failure,failure,621,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:655,Availability,failure,failure,655,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:227,Testability,test,tests,227,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:279,Testability,test,tests,279,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:401,Testability,test,testing,401,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:470,Testability,test,tests,470,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:579,Testability,test,tests,579,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:885,Testability,test,test,885,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:948,Testability,test,test,948,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:1009,Testability,test,test,1009,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:1112,Testability,test,test,1112,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:1200,Testability,test,tests,1200,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:1375,Testability,test,test,1375,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:1177,Usability,usab,usable,1177,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:334,Availability,error,errors,334,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423
https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:390,Availability,error,error,390,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423
https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:467,Availability,failure,failure,467,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423
https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:59,Modifiability,variab,variable,59,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423
https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:269,Testability,test,test,269,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423
https://github.com/hail-is/hail/issues/1419#issuecomment-282453922:229,Availability,failure,failure,229,"@natestockham there's been some data science-y investigations on our end, final results not yet ready, but it looks like that test is way over constrained. Our confidence in the LMM method has not yet changed as a result of this failure. I'll be pushing a commit to remove the test failure by Monday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282453922
https://github.com/hail-is/hail/issues/1419#issuecomment-282453922:282,Availability,failure,failure,282,"@natestockham there's been some data science-y investigations on our end, final results not yet ready, but it looks like that test is way over constrained. Our confidence in the LMM method has not yet changed as a result of this failure. I'll be pushing a commit to remove the test failure by Monday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282453922
https://github.com/hail-is/hail/issues/1419#issuecomment-282453922:126,Testability,test,test,126,"@natestockham there's been some data science-y investigations on our end, final results not yet ready, but it looks like that test is way over constrained. Our confidence in the LMM method has not yet changed as a result of this failure. I'll be pushing a commit to remove the test failure by Monday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282453922
https://github.com/hail-is/hail/issues/1419#issuecomment-282453922:277,Testability,test,test,277,"@natestockham there's been some data science-y investigations on our end, final results not yet ready, but it looks like that test is way over constrained. Our confidence in the LMM method has not yet changed as a result of this failure. I'll be pushing a commit to remove the test failure by Monday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282453922
https://github.com/hail-is/hail/issues/1419#issuecomment-282776933:80,Availability,failure,failure,80,PR #1437 resolves the `is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM` failure under Spark 2.1.0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282776933
https://github.com/hail-is/hail/pull/1421#issuecomment-281760846:88,Performance,load,load-loses-nullable-status-of-schema,88,Old but relevant: http://stackoverflow.com/questions/30004295/dataframe-save-sqlcontext-load-loses-nullable-status-of-schema,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-281760846
https://github.com/hail-is/hail/pull/1421#issuecomment-281967861:179,Deployability,update,updated,179,"It looks like the Parquet schema in the file stores the `nullable = false` (as a `required` field), but it's ignored when being read back. I ran the following on a Spark 2 shell (updated from the Stack Overflow question, note that the Parquet handling has been rewritten since then, see https://issues.apache.org/jira/browse/SPARK-9095):. ```scala; scala> import org.apache.spark.sql._; import org.apache.spark.sql._. scala> import org.apache.spark.sql.types._; import org.apache.spark.sql.types._. scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc); warning: there was one deprecation warning; re-run with -deprecation for details; sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@625f5712. scala> val schema = StructType(Seq(StructField(""foo"", IntegerType, false))); schema: org.apache.spark.sql.types.StructType = StructType(StructField(foo,IntegerType,false)). scala> val df1 = sqlContext.createDataFrame(sc.parallelize(Array(Row(1))), schema); df1: org.apache.spark.sql.DataFrame = [foo: int]. scala> df1.printSchema; root; |-- foo: integer (nullable = false). scala> df1.write.parquet(""temp.df1""); ; scala> val df2 = sqlContext.read.parquet(""temp.df1""); df2: org.apache.spark.sql.DataFrame = [foo: int]. scala> df2.printSchema; root; |-- foo: integer (nullable = true); ```. Then. ```bash; parquet-tools schema part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; message spark_schema {; required int32 foo;; }; ```. Which shows that the field is `required`, not `optional`. Also. ```bash; parquet-tools meta part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; creator: parquet-mr version 1.5.0-cdh5.7.0 (build ${buildNumber}) ; extra: org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields"":[{""name"":""foo"",""type"":""integer"",""nullable"":false,""metadata"":{}}]} . file schema: spark_schema ; ----------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861
https://github.com/hail-is/hail/pull/1421#issuecomment-281967861:1425,Integrability,message,message,1425,"estion, note that the Parquet handling has been rewritten since then, see https://issues.apache.org/jira/browse/SPARK-9095):. ```scala; scala> import org.apache.spark.sql._; import org.apache.spark.sql._. scala> import org.apache.spark.sql.types._; import org.apache.spark.sql.types._. scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc); warning: there was one deprecation warning; re-run with -deprecation for details; sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@625f5712. scala> val schema = StructType(Seq(StructField(""foo"", IntegerType, false))); schema: org.apache.spark.sql.types.StructType = StructType(StructField(foo,IntegerType,false)). scala> val df1 = sqlContext.createDataFrame(sc.parallelize(Array(Row(1))), schema); df1: org.apache.spark.sql.DataFrame = [foo: int]. scala> df1.printSchema; root; |-- foo: integer (nullable = false). scala> df1.write.parquet(""temp.df1""); ; scala> val df2 = sqlContext.read.parquet(""temp.df1""); df2: org.apache.spark.sql.DataFrame = [foo: int]. scala> df2.printSchema; root; |-- foo: integer (nullable = true); ```. Then. ```bash; parquet-tools schema part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; message spark_schema {; required int32 foo;; }; ```. Which shows that the field is `required`, not `optional`. Also. ```bash; parquet-tools meta part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; creator: parquet-mr version 1.5.0-cdh5.7.0 (build ${buildNumber}) ; extra: org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields"":[{""name"":""foo"",""type"":""integer"",""nullable"":false,""metadata"":{}}]} . file schema: spark_schema ; -------------------------------------------------------------------------------------------------------------------------------------------------------------------; foo: REQUIRED INT32 R:0 D:0; ```. Which shows that the Spark SQL schema is stored in the Parquet metadata. So, looks like a bug. Is this causing correctness/perf problems?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861
https://github.com/hail-is/hail/pull/1421#issuecomment-282325862:107,Performance,Perform,Performance,107,"> So, looks like a bug. Is this causing correctness/perf problems?. We aren't seeing correctness problems. Performance, I dunno. You'd hope Spark would take advantage of non-missingness, but this makes me think it isn't. I doubt the only barrier is tracking the information at the type level.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-282325862
https://github.com/hail-is/hail/pull/1423#issuecomment-281771114:157,Usability,clear,clear,157,"Ok. In ""Setting limit to negative disables limiting the number of split and trailing empty strings are preserved"", you're missing ""s"" on split, and it's not clear if negative is necessary for trailing empty strings to be preserved. How about:. Setting limit to negative disables limiting the number of splits. Trailing empty strings are preserved, so ""a,b,,"".split("","", -1) gives [""a"", ""b"", """"]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281771114
https://github.com/hail-is/hail/pull/1423#issuecomment-281794307:9,Usability,clear,clearer,9,yes much clearer. Corrected!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281794307
https://github.com/hail-is/hail/pull/1423#issuecomment-281819089:9,Usability,clear,clear,9,"It's not clear what happens to empty strings at the beginning. I think if you replace with this, we'll be all set:. Setting `limit` to negative disables limiting the number of splits. Trailing empty strings are preserved, so "",a,b,,"".split("","", -1) gives ["""", ""a"", ""b"", """", """"] whereas "",a,b,,"".split("","") gives ["""", ""a"", ""b""].",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281819089
https://github.com/hail-is/hail/pull/1423#issuecomment-281823930:34,Security,access,access,34,Great. I don't think I have write access to merge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281823930
https://github.com/hail-is/hail/pull/1425#issuecomment-281864240:26,Modifiability,extend,extends,26,"Ok, IntIterator no longer extends Iterator[Int], and I added the necessary methods for all the regression methods. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-281864240
https://github.com/hail-is/hail/pull/1425#issuecomment-285771238:90,Testability,log,logregperformancecomp,90,"[(1.9, 1.8), (4.71, 2.2), (7.1, 4.06), (51, 13.8), (92, 55.7), (93, 54.6), (300, 224)]. ![logregperformancecomp](https://cloud.githubusercontent.com/assets/3201642/23811222/07899f02-05a3-11e7-8799-40e5373d4457.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-285771238
https://github.com/hail-is/hail/pull/1425#issuecomment-285811906:274,Deployability,update,updates,274,"With regard to #1314, this PR substitutes use of IntIterator in count(genotypes=True), linreg, logreg, lmmreg, computing the normalizations that go into RRM and GRM, and ExportPlink (got rid of ByteArrayBuilder there as well). I'm leaving IBD alone until @johnc1231 has his updates in and we can discuss. And hardcalls has the fakeRef issue to be dealt with later. ToStandardizedIndexRowMatrix is now ToHWENormalizedIndexedRowMatrix, alongside ToNormalizedIndexedRowMatrix and ToNormalizedRowMatrix. These all in turn map over genotype-array generating functions in RegressionUtils. I reimplemented that function for ToHWENormalizedIndexedRowMatrix as RegressionUtils.toHWENormalizedGtArray. The only methods using ToHWENormalizedIndexedRowMatrix are PCA and GRM, which both require split vds. So I've required the same on ToHWENormalizedIndexedRowMatrix for consistency and to take advantage of IntIterator in it's current state. We can circle back to the right more general approach later. I've also changed logreg to mapPartitions and wrote RegressionUtils.mutateLastColumn so logreg no longer constructs a new covariate matrix per variants, but rather mutates the last (gt) column of a shared matrix. This helped drive, for example, the 4x speedup on the score test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-285811906
https://github.com/hail-is/hail/pull/1425#issuecomment-285811906:95,Testability,log,logreg,95,"With regard to #1314, this PR substitutes use of IntIterator in count(genotypes=True), linreg, logreg, lmmreg, computing the normalizations that go into RRM and GRM, and ExportPlink (got rid of ByteArrayBuilder there as well). I'm leaving IBD alone until @johnc1231 has his updates in and we can discuss. And hardcalls has the fakeRef issue to be dealt with later. ToStandardizedIndexRowMatrix is now ToHWENormalizedIndexedRowMatrix, alongside ToNormalizedIndexedRowMatrix and ToNormalizedRowMatrix. These all in turn map over genotype-array generating functions in RegressionUtils. I reimplemented that function for ToHWENormalizedIndexedRowMatrix as RegressionUtils.toHWENormalizedGtArray. The only methods using ToHWENormalizedIndexedRowMatrix are PCA and GRM, which both require split vds. So I've required the same on ToHWENormalizedIndexedRowMatrix for consistency and to take advantage of IntIterator in it's current state. We can circle back to the right more general approach later. I've also changed logreg to mapPartitions and wrote RegressionUtils.mutateLastColumn so logreg no longer constructs a new covariate matrix per variants, but rather mutates the last (gt) column of a shared matrix. This helped drive, for example, the 4x speedup on the score test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-285811906
https://github.com/hail-is/hail/pull/1425#issuecomment-285811906:1010,Testability,log,logreg,1010,"With regard to #1314, this PR substitutes use of IntIterator in count(genotypes=True), linreg, logreg, lmmreg, computing the normalizations that go into RRM and GRM, and ExportPlink (got rid of ByteArrayBuilder there as well). I'm leaving IBD alone until @johnc1231 has his updates in and we can discuss. And hardcalls has the fakeRef issue to be dealt with later. ToStandardizedIndexRowMatrix is now ToHWENormalizedIndexedRowMatrix, alongside ToNormalizedIndexedRowMatrix and ToNormalizedRowMatrix. These all in turn map over genotype-array generating functions in RegressionUtils. I reimplemented that function for ToHWENormalizedIndexedRowMatrix as RegressionUtils.toHWENormalizedGtArray. The only methods using ToHWENormalizedIndexedRowMatrix are PCA and GRM, which both require split vds. So I've required the same on ToHWENormalizedIndexedRowMatrix for consistency and to take advantage of IntIterator in it's current state. We can circle back to the right more general approach later. I've also changed logreg to mapPartitions and wrote RegressionUtils.mutateLastColumn so logreg no longer constructs a new covariate matrix per variants, but rather mutates the last (gt) column of a shared matrix. This helped drive, for example, the 4x speedup on the score test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-285811906
https://github.com/hail-is/hail/pull/1425#issuecomment-285811906:1080,Testability,log,logreg,1080,"With regard to #1314, this PR substitutes use of IntIterator in count(genotypes=True), linreg, logreg, lmmreg, computing the normalizations that go into RRM and GRM, and ExportPlink (got rid of ByteArrayBuilder there as well). I'm leaving IBD alone until @johnc1231 has his updates in and we can discuss. And hardcalls has the fakeRef issue to be dealt with later. ToStandardizedIndexRowMatrix is now ToHWENormalizedIndexedRowMatrix, alongside ToNormalizedIndexedRowMatrix and ToNormalizedRowMatrix. These all in turn map over genotype-array generating functions in RegressionUtils. I reimplemented that function for ToHWENormalizedIndexedRowMatrix as RegressionUtils.toHWENormalizedGtArray. The only methods using ToHWENormalizedIndexedRowMatrix are PCA and GRM, which both require split vds. So I've required the same on ToHWENormalizedIndexedRowMatrix for consistency and to take advantage of IntIterator in it's current state. We can circle back to the right more general approach later. I've also changed logreg to mapPartitions and wrote RegressionUtils.mutateLastColumn so logreg no longer constructs a new covariate matrix per variants, but rather mutates the last (gt) column of a shared matrix. This helped drive, for example, the 4x speedup on the score test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-285811906
https://github.com/hail-is/hail/pull/1425#issuecomment-285811906:1265,Testability,test,test,1265,"With regard to #1314, this PR substitutes use of IntIterator in count(genotypes=True), linreg, logreg, lmmreg, computing the normalizations that go into RRM and GRM, and ExportPlink (got rid of ByteArrayBuilder there as well). I'm leaving IBD alone until @johnc1231 has his updates in and we can discuss. And hardcalls has the fakeRef issue to be dealt with later. ToStandardizedIndexRowMatrix is now ToHWENormalizedIndexedRowMatrix, alongside ToNormalizedIndexedRowMatrix and ToNormalizedRowMatrix. These all in turn map over genotype-array generating functions in RegressionUtils. I reimplemented that function for ToHWENormalizedIndexedRowMatrix as RegressionUtils.toHWENormalizedGtArray. The only methods using ToHWENormalizedIndexedRowMatrix are PCA and GRM, which both require split vds. So I've required the same on ToHWENormalizedIndexedRowMatrix for consistency and to take advantage of IntIterator in it's current state. We can circle back to the right more general approach later. I've also changed logreg to mapPartitions and wrote RegressionUtils.mutateLastColumn so logreg no longer constructs a new covariate matrix per variants, but rather mutates the last (gt) column of a shared matrix. This helped drive, for example, the 4x speedup on the score test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-285811906
https://github.com/hail-is/hail/pull/1425#issuecomment-286551014:69,Testability,Log,LogisticRegression,69,"Back to you. I've added `require(vds.wasSplit)` to LinearRegression, LogisticRegression, and LinearMixedRegression, ToNormalizedRowMatrix, ToNormalizedIndexedRowMatrix, ToHWENormalizedIndexedRowMatrix. I think this is more reasonable than a fatal since the existing fatals would be tripped through the Python or Scala APIs. I've also noted `// requires bi-allelic` above the corresponding functions in RegressionUtils.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-286551014
https://github.com/hail-is/hail/pull/1427#issuecomment-281827094:221,Testability,Test,Test,221,"Yes, I shouldn't have had both. However, there's still no guarantee that even this expected equivalence will hold with a different seed. It's just ""likely"" so long as the code is correct. Originally I didn't include the @Test annotation at all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1427#issuecomment-281827094
https://github.com/hail-is/hail/pull/1430#issuecomment-282137009:57,Availability,error,error,57,@tpoterba How do I get the java stack trace to debug the error in the docs build (failed in the Tutorial testing)?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1430#issuecomment-282137009
https://github.com/hail-is/hail/pull/1430#issuecomment-282137009:105,Testability,test,testing,105,@tpoterba How do I get the java stack trace to debug the error in the docs build (failed in the Tutorial testing)?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1430#issuecomment-282137009
https://github.com/hail-is/hail/pull/1439#issuecomment-283120768:3,Usability,intuit,intuition,3,My intuition is that the reference should be directed the other way. PCA should refer to the GRM docs when discussing the GRM. The GRM docs should actually contain the text explaining the GRM.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1439#issuecomment-283120768
https://github.com/hail-is/hail/pull/1446#issuecomment-283500834:64,Testability,test,tests,64,@jigold Changes look good (approved) but something broke in the tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1446#issuecomment-283500834
https://github.com/hail-is/hail/pull/1452#issuecomment-290546429:561,Deployability,integrat,integration,561,"Hey @tomwhite, sorry for the massive delay. There was some concern about not having instructions generic to any cluster in the docs, so I've restructured your PR a bit more to capture the generic Spark cluster instructions and then have a separate section on getting started with a Cloudera cluster. I also opted for ""Cloudera"" instead of ""CDH"" because I don't think our users will recognize the acronym. Does that seem OK to you?. I made my changes as [a PR into your branch](https://github.com/tomwhite/hail/pull/1/files). Also, don't worry about the failing integration test, that's a CI issue on our end. It should resolve it self after the next new commit to your branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429
https://github.com/hail-is/hail/pull/1452#issuecomment-290546429:561,Integrability,integrat,integration,561,"Hey @tomwhite, sorry for the massive delay. There was some concern about not having instructions generic to any cluster in the docs, so I've restructured your PR a bit more to capture the generic Spark cluster instructions and then have a separate section on getting started with a Cloudera cluster. I also opted for ""Cloudera"" instead of ""CDH"" because I don't think our users will recognize the acronym. Does that seem OK to you?. I made my changes as [a PR into your branch](https://github.com/tomwhite/hail/pull/1/files). Also, don't worry about the failing integration test, that's a CI issue on our end. It should resolve it self after the next new commit to your branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429
https://github.com/hail-is/hail/pull/1452#issuecomment-290546429:573,Testability,test,test,573,"Hey @tomwhite, sorry for the massive delay. There was some concern about not having instructions generic to any cluster in the docs, so I've restructured your PR a bit more to capture the generic Spark cluster instructions and then have a separate section on getting started with a Cloudera cluster. I also opted for ""Cloudera"" instead of ""CDH"" because I don't think our users will recognize the acronym. Does that seem OK to you?. I made my changes as [a PR into your branch](https://github.com/tomwhite/hail/pull/1/files). Also, don't worry about the failing integration test, that's a CI issue on our end. It should resolve it self after the next new commit to your branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429
https://github.com/hail-is/hail/pull/1452#issuecomment-290754177:24,Deployability,update,update,24,@danking thanks for the update. Looks good to me!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290754177
https://github.com/hail-is/hail/pull/1457#issuecomment-286238444:36,Integrability,rout,routine,36,@johnc1231 Could you also time this routine on the european subset of 1000KG? with PI_HAT=0.2? I'd like to have that information in the PR comments in case we need to know that in the future.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1457#issuecomment-286238444
https://github.com/hail-is/hail/pull/1457#issuecomment-286524152:36,Testability,benchmark,benchmark,36,"I fixed that last thing, I will add benchmark comment when I have it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1457#issuecomment-286524152
https://github.com/hail-is/hail/pull/1458#issuecomment-283766450:54,Modifiability,variab,variable,54,@cseed Back to you. Can you please verify the `codec` variable is being used correctly?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1458#issuecomment-283766450
https://github.com/hail-is/hail/issues/1459#issuecomment-283822337:68,Availability,error,error-importerror-no-module-named-decorator,68,Here's one of the easier solutions: http://discuss.hail.is/t/python-error-importerror-no-module-named-decorator/131/4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1459#issuecomment-283822337
https://github.com/hail-is/hail/pull/1463#issuecomment-283826241:209,Modifiability,config,config,209,"I'll make a quick Discuss post when this goes in. Here is a quick example of renaming as before with a mapping file:. ```; m2 = {r._0: r._1 for r in hc.import_keytable(test_resources + '/sample2_rename.tsv',; config=TextTableConfig(noheader=True)); .collect()}; self.assertEqual(sample2.join(sample2.rename_samples(m2)); .count()['nSamples'], 200); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1463#issuecomment-283826241
https://github.com/hail-is/hail/pull/1463#issuecomment-283826241:267,Testability,assert,assertEqual,267,"I'll make a quick Discuss post when this goes in. Here is a quick example of renaming as before with a mapping file:. ```; m2 = {r._0: r._1 for r in hc.import_keytable(test_resources + '/sample2_rename.tsv',; config=TextTableConfig(noheader=True)); .collect()}; self.assertEqual(sample2.join(sample2.rename_samples(m2)); .count()['nSamples'], 200); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1463#issuecomment-283826241
https://github.com/hail-is/hail/pull/1463#issuecomment-283829662:74,Performance,perform,perform,74,@danking Any idea why this is failing? On the CI I'm seeing:. > Failed to perform checkout on agent: Cannot find commit 8955cc391e0e811ecb3a5325a82dff96eb3a0824,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1463#issuecomment-283829662
https://github.com/hail-is/hail/pull/1463#issuecomment-283841404:89,Energy Efficiency,green,green,89,I think that's exactly what happened. It rebuilt on its own accord and eventually turned green. Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1463#issuecomment-283841404
https://github.com/hail-is/hail/issues/1472#issuecomment-284199231:28,Testability,test,tested,28,"I agree! The docs are being tested in 2 places. The first is easy to fix; make a new build task that is identical to `makeHailDocs` with a command line of `make clean html` instead. The second place is the tutorial iPython notebook. There is an option `nbsphinx_execute = 'never'` that could be added to `conf.py`, but I haven't figured out what the best way to pass a custom parameter argument to Sphinx is as we are not running `conf.py` directly when we call Sphinx.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1472#issuecomment-284199231
https://github.com/hail-is/hail/pull/1473#issuecomment-284198386:8,Integrability,depend,depends,8,This PR depends on #1466.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1473#issuecomment-284198386
https://github.com/hail-is/hail/pull/1474#issuecomment-284267067:34,Testability,test,test,34,I'll make a PR now that axes this test in favor of a test that compares delta results with FastLMM,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1474#issuecomment-284267067
https://github.com/hail-is/hail/pull/1474#issuecomment-284267067:53,Testability,test,test,53,I'll make a PR now that axes this test in favor of a test that compares delta results with FastLMM,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1474#issuecomment-284267067
https://github.com/hail-is/hail/pull/1474#issuecomment-284282339:22,Testability,test,tests,22,"Once #1475 is merged, tests will pass here too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1474#issuecomment-284282339
https://github.com/hail-is/hail/pull/1475#issuecomment-284275408:44,Testability,test,test,44,And I removed the genAndFitTest as this new test is an exact comparison of fit h2 with another tool rather than a statistical check. We will keep the issue of why the old test was flaky open here #1416,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1475#issuecomment-284275408
https://github.com/hail-is/hail/pull/1475#issuecomment-284275408:171,Testability,test,test,171,And I removed the genAndFitTest as this new test is an exact comparison of fit h2 with another tool rather than a statistical check. We will keep the issue of why the old test was flaky open here #1416,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1475#issuecomment-284275408
https://github.com/hail-is/hail/pull/1478#issuecomment-284475675:21,Testability,test,test,21,"Woo, awesome! 😄 Will test now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1478#issuecomment-284475675
https://github.com/hail-is/hail/pull/1491#issuecomment-284720278:357,Modifiability,extend,extend,357,"Recommended changes to doc strings (I had an issue making PR against yours in browser, so you can just cut and paste into your code if you agree with changes):. ```; registerMethod(""append"", (a: IndexedSeq[Any], b: Any) => a :+ b, ""Returns the result of adding the element `b` to the end of Array `a`."")(arrayHr(TTHr), TTHr, arrayHr(TTHr)); registerMethod(""extend"", (a: IndexedSeq[Any], b: IndexedSeq[Any]) => a ++ b, ""Returns the concatenation of Array `a` and Array `b`."")(arrayHr(TTHr), arrayHr(TTHr), arrayHr(TTHr)). registerMethod(""add"", (a: Set[Any], b: Any) => a + b, ""Returns the result of adding the element `b` to Set `a`."")(setHr(TTHr), TTHr, setHr(TTHr)); registerMethod(""union"", (a: Set[Any], b: Set[Any]) => a ++ b, ""Returns the union of Sets `a` and `b`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""intersection"", (a: Set[Any], b: Set[Any]) => a & b, ""Returns the intersection of Sets `a` and `b`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""difference"", (a: Set[Any], b: Set[Any]) => a &~ b, ""Returns the elements of Set `a` that are not in Set `b`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""issubset"", (a: Set[Any], b: Set[Any]) => a.subsetOf(b), ""Returns true if Set `a` is a subset of Set `b`."")(setHr(TTHr), setHr(TTHr), boolHr); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284720278
https://github.com/hail-is/hail/pull/1491#issuecomment-284746931:47,Usability,clear,clear,47,"Things aren't really consistent, so there's no clear pattern to follow. I'm happy to mimic python set/list functionality",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284746931
https://github.com/hail-is/hail/pull/1491#issuecomment-284773672:102,Availability,down,download,102,"@lfrancioli look at the built docs (under TeamCity, artifacts, index):; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/9716:id/www/hail/types.html#set-t. There is an issue of variable naming: your a is implicit (not named), and your b is our a. So for example:; ```; add(a: T): Set[T] – Returns the result of adding the element b to Set a.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672
https://github.com/hail-is/hail/pull/1491#issuecomment-284773672:205,Modifiability,variab,variable,205,"@lfrancioli look at the built docs (under TeamCity, artifacts, index):; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/9716:id/www/hail/types.html#set-t. There is an issue of variable naming: your a is implicit (not named), and your b is our a. So for example:; ```; add(a: T): Set[T] – Returns the result of adding the element b to Set a.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672
https://github.com/hail-is/hail/pull/1491#issuecomment-284774704:225,Modifiability,extend,extend,225,"Look over this correction:; ```; registerMethod(""append"", (a: IndexedSeq[Any], b: Any) => a :+ b, ""Returns the result of adding the element `a` to the end of this Array."")(arrayHr(TTHr), TTHr, arrayHr(TTHr)); registerMethod(""extend"", (a: IndexedSeq[Any], b: IndexedSeq[Any]) => a ++ b, ""Returns the concatenation of this Array followed by Array `a`."")(arrayHr(TTHr), arrayHr(TTHr), arrayHr(TTHr)). registerMethod(""add"", (a: Set[Any], b: Any) => a + b, ""Returns the result of adding the element `a` to this Set."")(setHr(TTHr), TTHr, setHr(TTHr)); registerMethod(""union"", (a: Set[Any], b: Set[Any]) => a ++ b, ""Returns the union of this Set and Set `a`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""intersection"", (a: Set[Any], b: Set[Any]) => a & b, ""Returns the intersection of this Set and Set `a`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""difference"", (a: Set[Any], b: Set[Any]) => a &~ b, ""Returns the elements of this Set that are not in Set `a`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""issubset"", (a: Set[Any], b: Set[Any]) => a.subsetOf(b), ""Returns true if this Set is a subset of Set `a`."")(setHr(TTHr), setHr(TTHr), boolHr); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284774704
https://github.com/hail-is/hail/pull/1494#issuecomment-284814488:57,Usability,guid,guide,57,"I just noticed that the spacing doesn't follow the style guide. Not critical, but try to autoformat before PRing",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1494#issuecomment-284814488
https://github.com/hail-is/hail/pull/1494#issuecomment-284817572:163,Usability,guid,guide,163,"Sorry about this! Will do!. On Tue, Mar 7, 2017 at 13:36 Tim Poterba <notifications@github.com> wrote:. > I just noticed that the spacing doesn't follow the style guide. Not; > critical, but try to autoformat before PRing; >; >; >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1494#issuecomment-284814488>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgc012iIeaevvZPZfYZuXXj1nuiE2ks5rjaOQgaJpZM4MVwyK>; > .; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1494#issuecomment-284817572
https://github.com/hail-is/hail/pull/1499#issuecomment-284842421:10,Modifiability,rewrite,rewrite,10,"Suggested rewrite:. While this method parallelizes over a list of BGEN files, each file is indexed serially by one core. So indexing several BGEN files on a large cluster is a waste of resources, and indexing should generally be done separately from large analyses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1499#issuecomment-284842421
https://github.com/hail-is/hail/pull/1501#issuecomment-284859085:169,Availability,down,downstream,169,"Put **Notes** after example. Suggested rewrite:. ""This method registers new global annotations in the VDS. These annotations can then be accessed through expressions in downstream operations. The Hail data type must be provided and match the type of the Python object.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085
https://github.com/hail-is/hail/pull/1501#issuecomment-284859085:39,Modifiability,rewrite,rewrite,39,"Put **Notes** after example. Suggested rewrite:. ""This method registers new global annotations in the VDS. These annotations can then be accessed through expressions in downstream operations. The Hail data type must be provided and match the type of the Python object.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085
https://github.com/hail-is/hail/pull/1501#issuecomment-284859085:137,Security,access,accessed,137,"Put **Notes** after example. Suggested rewrite:. ""This method registers new global annotations in the VDS. These annotations can then be accessed through expressions in downstream operations. The Hail data type must be provided and match the type of the Python object.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085
https://github.com/hail-is/hail/pull/1502#issuecomment-284850456:25,Availability,error,error,25,"Calling that the ""right"" error is a little bit bold, but you certainly fixed the bug!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502#issuecomment-284850456
https://github.com/hail-is/hail/pull/1503#issuecomment-284853192:206,Integrability,interface,interface,206,"yes, broke up first line because it was too long. I think this method should just be removed, really. We have num_samples, count_variants, and query_genotypes that make it easy to do all of this stuff. The interface is also weird",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1503#issuecomment-284853192
https://github.com/hail-is/hail/pull/1503#issuecomment-284854399:122,Performance,perform,performance,122,"So you want folks to do this for callRate?; `call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)')`. Is there a performance difference?. Also, this is grammatically off:; ""Perform aggregation queries over genotypes, and returns python objects."". https://hail.is/hail/hail.VariantDataset.html?highlight=query_genotypes#hail.VariantDataset.query_genotypes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1503#issuecomment-284854399
https://github.com/hail-is/hail/pull/1503#issuecomment-284854399:182,Performance,Perform,Perform,182,"So you want folks to do this for callRate?; `call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)')`. Is there a performance difference?. Also, this is grammatically off:; ""Perform aggregation queries over genotypes, and returns python objects."". https://hail.is/hail/hail.VariantDataset.html?highlight=query_genotypes#hail.VariantDataset.query_genotypes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1503#issuecomment-284854399
https://github.com/hail-is/hail/issues/1505#issuecomment-284860282:93,Energy Efficiency,efficient,efficient,93,"I prefer reworking count, killing the genotypes parameter, so that it's always just a simple/efficient way to get (nSamples, nVariants, nGenotypes, nCalled, callRate). I don't see why a tuple is better than a dict.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505#issuecomment-284860282
https://github.com/hail-is/hail/issues/1505#issuecomment-284860282:86,Usability,simpl,simple,86,"I prefer reworking count, killing the genotypes parameter, so that it's always just a simple/efficient way to get (nSamples, nVariants, nGenotypes, nCalled, callRate). I don't see why a tuple is better than a dict.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505#issuecomment-284860282
https://github.com/hail-is/hail/pull/1506#issuecomment-284930607:84,Testability,test,test,84,@jigold I think the merging of orMissing to function registry in master now makes a test fail here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1506#issuecomment-284930607
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:58,Availability,error,error,58,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:811,Availability,error,error,811,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:1167,Availability,Error,ErrorHandling,1167,"ing too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:1193,Availability,Error,ErrorHandling,1193,"ing too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:925,Deployability,configurat,configuration,925,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:734,Integrability,protocol,protocol,734,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:784,Integrability,protocol,protocol,784,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:925,Modifiability,config,configuration,925,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:949,Modifiability,config,config,949,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:1052,Modifiability,config,config,1052,"d I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978
https://github.com/hail-is/hail/issues/1509#issuecomment-349717645:13,Integrability,interface,interface,13,Hail2 python interface makes this a lot more consistent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1509#issuecomment-349717645
https://github.com/hail-is/hail/pull/1517#issuecomment-285446701:23,Testability,test,tests,23,I'm fixing the failing tests now..,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1517#issuecomment-285446701
https://github.com/hail-is/hail/issues/1518#issuecomment-287838501:15,Availability,down,down,15,"We narrowed it down to star alleles. I think @lfrancioli has a PR for it, but not sure if it's through yet?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518#issuecomment-287838501
https://github.com/hail-is/hail/issues/1520#issuecomment-285803343:86,Availability,error,error,86,Can you post the output of:; ```; cat /proc/cpuinfo | grep flags; ```. I suspect this error might arise if your processor doesn't support the pop count intrinsic. If this is the case it's my mistake for not including a back up implementation in terms of bit shifting. I can fix this tonight if that's the issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520#issuecomment-285803343
https://github.com/hail-is/hail/issues/1520#issuecomment-285804122:220,Energy Efficiency,monitor,monitor,220,"I get. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm ida dtherm tpr_shadow vnmi flexpriority; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm ida dtherm tpr_shadow vnmi flexpriority. Thanks,. Rob K.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520#issuecomment-285804122
https://github.com/hail-is/hail/issues/1520#issuecomment-285804122:547,Energy Efficiency,monitor,monitor,547,"I get. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm ida dtherm tpr_shadow vnmi flexpriority; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm ida dtherm tpr_shadow vnmi flexpriority. Thanks,. Rob K.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520#issuecomment-285804122
https://github.com/hail-is/hail/pull/1523#issuecomment-285818370:111,Integrability,message,messages,111,"Remove space in ""Hail Context has already been created"", or make context lowercase to be consistent with other messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1523#issuecomment-285818370
https://github.com/hail-is/hail/pull/1531#issuecomment-286852400:260,Deployability,pipeline,pipeline,260,"Had a quick chat with Konrad and since the other VEP bug is unrelated to these changes, it would be great if we could get this into master (if you're happy with these changes obviously) as at the moment we have to use different jars for different parts of the pipeline.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1531#issuecomment-286852400
https://github.com/hail-is/hail/pull/1532#issuecomment-286491775:15,Testability,test,tests,15,"Looks good but tests are failing, maybe related to https://github.com/hail-is/hail/pull/1538?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1532#issuecomment-286491775
https://github.com/hail-is/hail/pull/1548#issuecomment-286622901:29,Availability,failure,failure,29,Closing while I address test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1548#issuecomment-286622901
https://github.com/hail-is/hail/pull/1548#issuecomment-286622901:24,Testability,test,test,24,Closing while I address test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1548#issuecomment-286622901
https://github.com/hail-is/hail/pull/1549#issuecomment-287082175:384,Modifiability,extend,extend,384,"This is good. We should do the same thing with IntIterator, etc. (Maybe call it `SIterator` for specialized iterator?). A few remarks:. - Looking over the bytecode, making the members private[this] makes the bytecode much tighter since it doesn't generate accessor methods for b and size_. I'll make a quick PR for this. - Even tho ArrayBuilder is invariant, the specialized versions extend ArrayBuilder[Object] and implement all the generic, unspecialized methods. That worries me, but I don't know why it would ever get called. Maybe for backward compatibility to code compiled without specialization?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1549#issuecomment-287082175
https://github.com/hail-is/hail/pull/1549#issuecomment-287082175:256,Security,access,accessor,256,"This is good. We should do the same thing with IntIterator, etc. (Maybe call it `SIterator` for specialized iterator?). A few remarks:. - Looking over the bytecode, making the members private[this] makes the bytecode much tighter since it doesn't generate accessor methods for b and size_. I'll make a quick PR for this. - Even tho ArrayBuilder is invariant, the specialized versions extend ArrayBuilder[Object] and implement all the generic, unspecialized methods. That worries me, but I don't know why it would ever get called. Maybe for backward compatibility to code compiled without specialization?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1549#issuecomment-287082175
https://github.com/hail-is/hail/pull/1549#issuecomment-287083666:63,Modifiability,extend,extend,63,"> Even tho ArrayBuilder is invariant, the specialized versions extend ArrayBuilder[Object] and implement all the generic, unspecialized methods. That worries me, but I don't know why it would ever get called. Maybe for backward compatibility to code compiled without specialization?. I noticed this as well and figured it was for compatibility with Java code that doesn't have the source-level knowledge of `@specialized`. AFAICT, it shouldn't matter unless we're writing Java code or want super tiny byte code sizes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1549#issuecomment-287083666
https://github.com/hail-is/hail/pull/1551#issuecomment-287086381:64,Testability,test,tests,64,"Yes. 1. I'm not aware of anyone using GEN right now (except for tests), so it is much less important, and 2. my PR was already up to +100 lines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1551#issuecomment-287086381
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:95,Availability,error,errors,95,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:129,Availability,error,errors,129,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:178,Availability,error,errors,178,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:690,Availability,error,error,690,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:144,Safety,abort,abort,144,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:57,Testability,Assert,Assert,57,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:165,Testability,assert,assert,165,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:412,Testability,log,log,412,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:638,Usability,usab,usability,638,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:182,Availability,error,error,182,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:424,Availability,error,errors,424,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:188,Integrability,message,messages,188,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:510,Integrability,interface,interface,510,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:151,Usability,clear,clear,151,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:207,Usability,clear,clear,207,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:485,Usability,simpl,simple,485,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
https://github.com/hail-is/hail/pull/1552#issuecomment-287190590:17,Availability,error,error,17,"Reopening, every error will produce a stack trace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190590
https://github.com/hail-is/hail/pull/1552#issuecomment-287190661:52,Integrability,depend,dependence,52,"Got rid of @handle_py4j, as well, which removes our dependence on decorator 👍",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190661
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:22,Availability,error,error,22,"here are some example error msgs:; ```; In [2]: vds.linreg([]); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-2-b8bbc41a5ebd> in <module>(); ----> 1 vds.linreg([]). /Users/tpoterba/hail/python/hail/dataset.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1169,Availability,error,error,1169,"erba/hail/python/hail/dataset.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1216,Availability,error,error,1216,"erba/hail/python/hail/dataset.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1436,Availability,error,error,1436,"et.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to invalid parameter types```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1538,Availability,error,error,1538,"et.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to invalid parameter types```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:2196,Availability,ERROR,ERROR,2196,"et.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to invalid parameter types```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:2214,Availability,error,error,2214,"et.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to invalid parameter types```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1011,Integrability,protocol,protocol,1011,"xample error msgs:; ```; In [2]: vds.linreg([]); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-2-b8bbc41a5ebd> in <module>(); ----> 1 vds.linreg([]). /Users/tpoterba/hail/python/hail/dataset.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1383,Integrability,message,message,1383,"et.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to invalid parameter types```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1440,Availability,error,error,1440,"ct(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1819,Availability,failure,failure,1819,"4; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1876,Availability,failure,failure,1876,".0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.has",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2222,Availability,Error,ErrorHandling,2222,"alMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2248,Availability,Error,ErrorHandling,2248,"eption); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6700,Availability,Error,ErrorHandling,6700,eflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6726,Availability,Error,ErrorHandling,6726,ccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:8236,Availability,ERROR,ERROR,8236,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3363,Energy Efficiency,schedul,scheduler,3363,(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3435,Energy Efficiency,schedul,scheduler,3435, 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3799,Energy Efficiency,schedul,scheduler,3799,38); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3839,Energy Efficiency,schedul,scheduler,3839,non$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3938,Energy Efficiency,schedul,scheduler,3938,ext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4036,Energy Efficiency,schedul,scheduler,4036,rg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4290,Energy Efficiency,schedul,scheduler,4290,onfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4371,Energy Efficiency,schedul,scheduler,4371,ultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4477,Energy Efficiency,schedul,scheduler,4477,he.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4627,Energy Efficiency,schedul,scheduler,4627,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4716,Energy Efficiency,schedul,scheduler,4716,va.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.Nati,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4814,Energy Efficiency,schedul,scheduler,4814,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Nativ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4910,Energy Efficiency,schedul,scheduler,4910,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMeth,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:5075,Energy Efficiency,schedul,scheduler,5075,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7841,Energy Efficiency,schedul,scheduler,7841,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7913,Energy Efficiency,schedul,scheduler,7913,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1014,Integrability,protocol,protocol,1014,"port_vcf('src/test/resources/malformed.vcf').count(); hail: info: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-955ea6a16c80> in <module>(); ----> 1 hc.import_vcf('src/test/resources/malformed.vcf').count(). /Users/tpoterba/hail/python/hail/dataset.py in count(self, genotypes); 1127 """"""; 1128; -> 1129 return dict(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$Inte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1382,Integrability,protocol,protocol,1382,"y in count(self, genotypes); 1127 """"""; 1128; -> 1129 return dict(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2355,Integrability,wrap,wrapException,2355,"112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.sch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6833,Integrability,wrap,wrapException,6833,62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.sch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2460,Performance,Load,LoadVCF,2460,"Error: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2503,Performance,Load,LoadVCF,2503,"ormed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Exec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2542,Performance,Load,LoadVCF,2542,"alCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2585,Performance,Load,LoadVCF,2585,"le with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3559,Performance,concurren,concurrent,3559,nonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3644,Performance,concurren,concurrent,3644,ur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6938,Performance,Load,LoadVCF,6938,flect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6981,Performance,Load,LoadVCF,6981,.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Exec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7020,Performance,Load,LoadVCF,7020,hodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7063,Performance,Load,LoadVCF,7063,py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:8037,Performance,concurren,concurrent,8037,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:8122,Performance,concurren,concurrent,8122,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1798,Safety,abort,aborted,1798,"4; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3970,Safety,abort,abortStage,3970,); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4068,Safety,abort,abortStage,4068,D$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4313,Safety,abort,abortStage,4313,ext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:32,Testability,test,test,32,"```; In [4]: hc.import_vcf('src/test/resources/malformed.vcf').count(); hail: info: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-955ea6a16c80> in <module>(); ----> 1 hc.import_vcf('src/test/resources/malformed.vcf').count(). /Users/tpoterba/hail/python/hail/dataset.py in count(self, genotypes); 1127 """"""; 1128; -> 1129 return dict(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.Tri",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:304,Testability,test,test,304,"```; In [4]: hc.import_vcf('src/test/resources/malformed.vcf').count(); hail: info: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-955ea6a16c80> in <module>(); ----> 1 hc.import_vcf('src/test/resources/malformed.vcf').count(). /Users/tpoterba/hail/python/hail/dataset.py in count(self, genotypes); 1127 """"""; 1128; -> 1129 return dict(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.Tri",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882
https://github.com/hail-is/hail/pull/1552#issuecomment-287453582:25,Testability,test,tests,25,"Okay, this isn't passing tests because Py4J actually catches Py4JErrors and uses them to stop iteration on java collections. Very frustrating.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287453582
https://github.com/hail-is/hail/pull/1558#issuecomment-287173180:10,Testability,test,tested,10,I haven't tested at all yet. Might have a catastrophic problem.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1558#issuecomment-287173180
https://github.com/hail-is/hail/pull/1558#issuecomment-287174326:18,Testability,test,test,18,I'll run the evil test case 😈,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1558#issuecomment-287174326
https://github.com/hail-is/hail/pull/1558#issuecomment-287177936:18,Usability,simpl,simple,18,"It's working on a simple example on the Cray:. ```; >>> (hc; ... .import_vcf('file:///mnt/lustre/cseed/sample.vcf'); ... .vep('/mnt/lustre/cseed/vep.properties'); ... .write('file:///mnt/lustre/cseed/sample.vds', overwrite=True)); hail: info: Coerced sorted dataset; hail: info: vep: annotated 346 variants; >>> vds = hc.read('file:///mnt/lustre/cseed/sample.vds'); >>> vds.count(); {u'nVariants': 346L, u'nSamples': 100, u'nGenotypes': 34600L}; >>> vds.filter_variants_expr('isDefined(va.vep)').count(); {u'nVariants': 346L, u'nSamples': 100, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1558#issuecomment-287177936
https://github.com/hail-is/hail/pull/1559#issuecomment-287180359:51,Deployability,integrat,integrate,51,"Sorry @tpoterba , there's still more work to do to integrate this throughout Hail without tests failing. That's the next step!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359
https://github.com/hail-is/hail/pull/1559#issuecomment-287180359:51,Integrability,integrat,integrate,51,"Sorry @tpoterba , there's still more work to do to integrate this throughout Hail without tests failing. That's the next step!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359
https://github.com/hail-is/hail/pull/1559#issuecomment-287180359:90,Testability,test,tests,90,"Sorry @tpoterba , there's still more work to do to integrate this throughout Hail without tests failing. That's the next step!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359
https://github.com/hail-is/hail/pull/1559#issuecomment-287181344:130,Performance,perform,performance,130,"@cseed this code in GenericGenotype is not in MutableGenotype. Do you think it belongs in read? Seems like it will hit (relative) performance more significantly in the mutable case. ```; require(unboxedGT >= -1, s""invalid _gt value: $unboxedGT""); require(unboxedDP >= -1, s""invalid _dp value: $unboxedDP""). if (isDosage) {; require(unboxedGQ == -1); if (unboxedPX == null); require(unboxedGT == -1); else {; require(unboxedPX.sum == 32768); require(unboxedGT == Genotype.gtFromLinear(unboxedPX).getOrElse(-1)); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287181344
https://github.com/hail-is/hail/pull/1566#issuecomment-287546850:22,Performance,perform,performance,22,Is there a noticeable performance improvement?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1566#issuecomment-287546850
https://github.com/hail-is/hail/pull/1566#issuecomment-287549194:83,Safety,unsafe,unsafeValueAt,83,@danking it seems to me that this is exactly the situation for which they exposed `unsafeValueAt`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1566#issuecomment-287549194
https://github.com/hail-is/hail/pull/1566#issuecomment-287549194:74,Security,expose,exposed,74,@danking it seems to me that this is exactly the situation for which they exposed `unsafeValueAt`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1566#issuecomment-287549194
https://github.com/hail-is/hail/pull/1572#issuecomment-287900602:4,Deployability,integrat,integration,4,"The integration test is failing. Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602
https://github.com/hail-is/hail/pull/1572#issuecomment-287900602:4,Integrability,integrat,integration,4,"The integration test is failing. Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602
https://github.com/hail-is/hail/pull/1572#issuecomment-287900602:16,Testability,test,test,16,"The integration test is failing. Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602
https://github.com/hail-is/hail/pull/1572#issuecomment-287901000:27,Availability,failure,failure,27,"Sorry, was actually a docs failure. I'll see if I can figure out what it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287901000
https://github.com/hail-is/hail/pull/1572#issuecomment-287901053:8,Deployability,update,update,8,"I can't update that until this goes in, though. Otherwise everything else will fail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287901053
https://github.com/hail-is/hail/pull/1573#issuecomment-287743256:11,Availability,echo,echo,11,Change to `echo $SPARK_HOME/python/lib/py4j*-src.zip`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287743256
https://github.com/hail-is/hail/pull/1573#issuecomment-287745455:30,Availability,echo,echo,30,I'm having second thoughts on echo. let's let @danking tie break.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287745455
https://github.com/hail-is/hail/pull/1573#issuecomment-287766096:55,Availability,echo,echo,55,"👍. Either way seems fine to me. Slight preference for `echo` because it's a shell built-in, so no external executables are invoked. AFAICT, [a POSIX-compliant shell](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html) will do what we want. If there is more than one py4j in that directory things will definitely break. AFAIK, more than one py4j in that directory is an error, correct? If not, we could do something like `$(ls ... | tr '\n' ':')` to list all of them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287766096
https://github.com/hail-is/hail/pull/1573#issuecomment-287766096:391,Availability,error,error,391,"👍. Either way seems fine to me. Slight preference for `echo` because it's a shell built-in, so no external executables are invoked. AFAICT, [a POSIX-compliant shell](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html) will do what we want. If there is more than one py4j in that directory things will definitely break. AFAIK, more than one py4j in that directory is an error, correct? If not, we could do something like `$(ls ... | tr '\n' ':')` to list all of them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287766096
https://github.com/hail-is/hail/issues/1587#issuecomment-288400774:44,Availability,error,error,44,I think the best thing to do is to throw an error here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1587#issuecomment-288400774
https://github.com/hail-is/hail/pull/1590#issuecomment-288752989:52,Testability,test,test,52,Alright cool. I'll hold off on handling the failing test until you do that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1590#issuecomment-288752989
https://github.com/hail-is/hail/pull/1590#issuecomment-288834232:10,Testability,test,test,10,Fixed the test. Feel free to look over whenever,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1590#issuecomment-288834232
https://github.com/hail-is/hail/pull/1595#issuecomment-289141495:0,Testability,Benchmark,Benchmarking,0,"Benchmarking:. Generated 100k by 10k IndexedRowMatrix of random doubles between 0 and 1, converted to BlockMatrix and multiplied it by its transpose. This was on 128 cores. . The multiplication took 35 seconds using my new method. It took 252 seconds without changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1595#issuecomment-289141495
https://github.com/hail-is/hail/pull/1598#issuecomment-290948269:62,Availability,failure,failure,62,"@danking once this builds again, can you see whether the docs failure is related to your change? it could have also been disk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1598#issuecomment-290948269
https://github.com/hail-is/hail/pull/1598#issuecomment-290987968:19,Availability,fault,fault,19,@jbloom this is my fault. I changed the expression to code without ensuring the VDS actually had the relevant fields. The doc test correctly points this out. I'll fix Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1598#issuecomment-290987968
https://github.com/hail-is/hail/pull/1598#issuecomment-290987968:126,Testability,test,test,126,@jbloom this is my fault. I changed the expression to code without ensuring the VDS actually had the relevant fields. The doc test correctly points this out. I'll fix Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1598#issuecomment-290987968
https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:332,Availability,error,errors,332,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582
https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:90,Modifiability,config,configure,90,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582
https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:367,Modifiability,config,configure,367,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582
https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:386,Modifiability,config,configure,386,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582
https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:608,Modifiability,variab,variable,608,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582
https://github.com/hail-is/hail/issues/1610#issuecomment-349721354:39,Integrability,interface,interface,39,This doesn't actually affect the hail2 interface,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1610#issuecomment-349721354
https://github.com/hail-is/hail/pull/1613#issuecomment-290197728:171,Deployability,configurat,configuration,171,"I'm rather inclined to wait for @cseed's commentary on this before merging it. I'm not sure if this is the best solution, but it seems like a viable way to hold necessary configuration parameters.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728
https://github.com/hail-is/hail/pull/1613#issuecomment-290197728:171,Modifiability,config,configuration,171,"I'm rather inclined to wait for @cseed's commentary on this before merging it. I'm not sure if this is the best solution, but it seems like a viable way to hold necessary configuration parameters.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:126,Availability,FAILURE,FAILURE,126,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:710,Availability,FAILURE,FAILURE,710,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:1080,Deployability,install,installed,1080,"action for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:1577,Energy Efficiency,schedul,scheduled,1577,"action for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:377,Modifiability,config,configure,377,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:567,Modifiability,config,configure,567,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:1818,Modifiability,Config,Configuring,1818,"action for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:495,Testability,log,log,495,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:1426,Testability,log,log,1426,"action for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020
https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:102,Availability,FAILURE,FAILURE,102,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637
https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:751,Energy Efficiency,schedul,scheduled,751,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637
https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:353,Modifiability,config,configure,353,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637
https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:543,Modifiability,config,configure,543,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637
https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:992,Modifiability,Config,Configuring,992,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637
https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:471,Testability,log,log,471,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637
https://github.com/hail-is/hail/pull/1613#issuecomment-290201772:16,Deployability,update,update,16,We will need to update the build server to create a `gradle.properties` file before execution.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201772
https://github.com/hail-is/hail/issues/1616#issuecomment-294024200:74,Integrability,message,message,74,Maybe we could keep a list here of things that we notice don't have a log message as we notice them?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1616#issuecomment-294024200
https://github.com/hail-is/hail/issues/1616#issuecomment-294024200:70,Testability,log,log,70,Maybe we could keep a list here of things that we notice don't have a log message as we notice them?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1616#issuecomment-294024200
https://github.com/hail-is/hail/issues/1616#issuecomment-422389477:105,Modifiability,enhance,enhancements,105,"I think this is a bad idea in the current model. However, we should explore IR visualization / execution enhancements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1616#issuecomment-422389477
https://github.com/hail-is/hail/issues/1623#issuecomment-290762181:11,Availability,error,error,11,"also, this error message is WAY better and fully debuggable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1623#issuecomment-290762181
https://github.com/hail-is/hail/issues/1623#issuecomment-290762181:17,Integrability,message,message,17,"also, this error message is WAY better and fully debuggable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1623#issuecomment-290762181
https://github.com/hail-is/hail/issues/1629#issuecomment-290846410:37,Deployability,Update,UpdatedRow,37,"Suggested fix:. ```scala; case class UpdatedRow(orig: Row, i: Int, update: Any) extends Row {; ...; }. ```; This gets you the update with one allocation and no copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410
https://github.com/hail-is/hail/issues/1629#issuecomment-290846410:67,Deployability,update,update,67,"Suggested fix:. ```scala; case class UpdatedRow(orig: Row, i: Int, update: Any) extends Row {; ...; }. ```; This gets you the update with one allocation and no copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410
https://github.com/hail-is/hail/issues/1629#issuecomment-290846410:126,Deployability,update,update,126,"Suggested fix:. ```scala; case class UpdatedRow(orig: Row, i: Int, update: Any) extends Row {; ...; }. ```; This gets you the update with one allocation and no copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410
https://github.com/hail-is/hail/issues/1629#issuecomment-290846410:80,Modifiability,extend,extends,80,"Suggested fix:. ```scala; case class UpdatedRow(orig: Row, i: Int, update: Any) extends Row {; ...; }. ```; This gets you the update with one allocation and no copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410
https://github.com/hail-is/hail/pull/1630#issuecomment-291159078:85,Modifiability,variab,variable,85,"@tpoterba I was expecting we'd change the function registry to require the same type variable for left and right, i.e.:. ```scala; register(""=="", (a: Any, b: Any) => a == b, null)(TTHr, TTHr, boolHr); register(""!="", (a: Any, b: Any) => a != b, null)(TTHr, TTHr, boolHr); ```. Did that not work correctly?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1630#issuecomment-291159078
https://github.com/hail-is/hail/pull/1630#issuecomment-291173170:92,Modifiability,variab,variables,92,@tpoterba I thought the conversions would lift `Int`s to `Double`s before unifying the type variables. What are struct attributes? I am a unsure that our conversions would work for struct field types.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1630#issuecomment-291173170
https://github.com/hail-is/hail/pull/1632#issuecomment-290863636:94,Safety,detect,detected,94,```text; In [4]: hc.import_vcf('src/test/resources/sample.vcf'); hail: info: No multiallelics detected.; hail: info: Coerced sorted dataset; Out[4]: <hail.dataset.VariantDataset at 0x10b0d8ad0>. In [5]: hc.import_vcf('src/test/resources/sample2.vcf'); hail: info: Multiallelics detected. Some methods cannot be run without splitting or filtering multiallelics first.; hail: info: Coerced sorted dataset; Out[5]: <hail.dataset.VariantDataset at 0x10b0d8cd0>; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290863636
https://github.com/hail-is/hail/pull/1632#issuecomment-290863636:278,Safety,detect,detected,278,```text; In [4]: hc.import_vcf('src/test/resources/sample.vcf'); hail: info: No multiallelics detected.; hail: info: Coerced sorted dataset; Out[4]: <hail.dataset.VariantDataset at 0x10b0d8ad0>. In [5]: hc.import_vcf('src/test/resources/sample2.vcf'); hail: info: Multiallelics detected. Some methods cannot be run without splitting or filtering multiallelics first.; hail: info: Coerced sorted dataset; Out[5]: <hail.dataset.VariantDataset at 0x10b0d8cd0>; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290863636
https://github.com/hail-is/hail/pull/1632#issuecomment-290863636:36,Testability,test,test,36,```text; In [4]: hc.import_vcf('src/test/resources/sample.vcf'); hail: info: No multiallelics detected.; hail: info: Coerced sorted dataset; Out[4]: <hail.dataset.VariantDataset at 0x10b0d8ad0>. In [5]: hc.import_vcf('src/test/resources/sample2.vcf'); hail: info: Multiallelics detected. Some methods cannot be run without splitting or filtering multiallelics first.; hail: info: Coerced sorted dataset; Out[5]: <hail.dataset.VariantDataset at 0x10b0d8cd0>; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290863636
https://github.com/hail-is/hail/pull/1632#issuecomment-290863636:222,Testability,test,test,222,```text; In [4]: hc.import_vcf('src/test/resources/sample.vcf'); hail: info: No multiallelics detected.; hail: info: Coerced sorted dataset; Out[4]: <hail.dataset.VariantDataset at 0x10b0d8ad0>. In [5]: hc.import_vcf('src/test/resources/sample2.vcf'); hail: info: Multiallelics detected. Some methods cannot be run without splitting or filtering multiallelics first.; hail: info: Coerced sorted dataset; Out[5]: <hail.dataset.VariantDataset at 0x10b0d8cd0>; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290863636
https://github.com/hail-is/hail/pull/1632#issuecomment-290947359:19,Availability,failure,failure,19,Look into the test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290947359
https://github.com/hail-is/hail/pull/1632#issuecomment-290947359:14,Testability,test,test,14,Look into the test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290947359
https://github.com/hail-is/hail/pull/1632#issuecomment-291121033:14,Testability,test,test,14,still failing test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-291121033
https://github.com/hail-is/hail/pull/1634#issuecomment-294068059:64,Testability,log,logreg,64,"@danking back to you. I made progress on cleaning up linreg and logreg suites, there's more I can do in logreg especially but I'd like to keep that as a later PR. I made all changes except these:; ```; def readGt(flags: Int): Int; def skipAdDp(flags: Int): Unit; ```; I'd appreciate if you look more closely at what would be involved, I'm not sure it'd be an improvement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1634#issuecomment-294068059
https://github.com/hail-is/hail/pull/1634#issuecomment-294068059:104,Testability,log,logreg,104,"@danking back to you. I made progress on cleaning up linreg and logreg suites, there's more I can do in logreg especially but I'd like to keep that as a later PR. I made all changes except these:; ```; def readGt(flags: Int): Int; def skipAdDp(flags: Int): Unit; ```; I'd appreciate if you look more closely at what would be involved, I'm not sure it'd be an improvement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1634#issuecomment-294068059
https://github.com/hail-is/hail/pull/1637#issuecomment-296274513:33,Testability,test,tests,33,@cseed @tpoterba I added several tests around binding that make me feel much more comfortable with the correctness.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1637#issuecomment-296274513
https://github.com/hail-is/hail/pull/1637#issuecomment-298932795:29,Modifiability,rewrite,rewrite,29,Closing until I have time to rewrite using recursively passed EvalContexts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1637#issuecomment-298932795
https://github.com/hail-is/hail/pull/1638#issuecomment-291887240:217,Testability,test,test,217,"@danking Yeah I do. I also realized that the way I'm constructing the object now uses an apply method, not a constructor. So I'll have to fiddle with the reflection a bit too. Sorry about that. Could have sworn I hit test button before submitting.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1638#issuecomment-291887240
https://github.com/hail-is/hail/pull/1638#issuecomment-291888209:12,Testability,test,test,12,I never hit test before submitting. 😁,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1638#issuecomment-291888209
https://github.com/hail-is/hail/pull/1645#issuecomment-292253230:380,Performance,cache,cached,380,"This is ungodly slow. ```python; In [4]: %%timeit; ...: with hdfs_read('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:; 100 loops, best of 3: 8.51 ms per loop. In [5]: %%timeit; ...: with open('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:. The slowest run took 8.48 times longer than the fastest. This could mean that an intermediate result is being cached.; 100000 loops, best of 3: 12.7 µs per loop; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-292253230
https://github.com/hail-is/hail/pull/1645#issuecomment-292253230:77,Testability,log,log,77,"This is ungodly slow. ```python; In [4]: %%timeit; ...: with hdfs_read('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:; 100 loops, best of 3: 8.51 ms per loop. In [5]: %%timeit; ...: with open('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:. The slowest run took 8.48 times longer than the fastest. This could mean that an intermediate result is being cached.; 100000 loops, best of 3: 12.7 µs per loop; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-292253230
https://github.com/hail-is/hail/pull/1645#issuecomment-292253230:213,Testability,log,log,213,"This is ungodly slow. ```python; In [4]: %%timeit; ...: with hdfs_read('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:; 100 loops, best of 3: 8.51 ms per loop. In [5]: %%timeit; ...: with open('hail.log') as f:; ...: for line in f:; ...: pass; ...:; ...:. The slowest run took 8.48 times longer than the fastest. This could mean that an intermediate result is being cached.; 100000 loops, best of 3: 12.7 µs per loop; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-292253230
https://github.com/hail-is/hail/pull/1645#issuecomment-292270329:15,Testability,test,tests,15,Added docs and tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-292270329
https://github.com/hail-is/hail/pull/1645#issuecomment-294021662:13,Security,expose,expose,13,"hmm, need to expose this in sphinx",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-294021662
https://github.com/hail-is/hail/pull/1645#issuecomment-294269949:196,Availability,error,errored,196,"I think it was wrong -- the buffered thing probably already implements it in terms of write. I didn't even define flush on the java side, so it wasn't getting called in my tests (or it would have errored)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-294269949
https://github.com/hail-is/hail/pull/1645#issuecomment-294269949:172,Testability,test,tests,172,"I think it was wrong -- the buffered thing probably already implements it in terms of write. I didn't even define flush on the java side, so it wasn't getting called in my tests (or it would have errored)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-294269949
https://github.com/hail-is/hail/pull/1648#issuecomment-293415826:105,Testability,test,test,105,"Also, consider whether you can use RegressionUtils.toHWENormalizedGtArray in the main code, not just the test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1648#issuecomment-293415826
https://github.com/hail-is/hail/pull/1648#issuecomment-294171904:8,Testability,test,tests,8,"Failing tests likely due to recent commit, should be simple fix and re-push.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1648#issuecomment-294171904
https://github.com/hail-is/hail/pull/1648#issuecomment-294171904:53,Usability,simpl,simple,53,"Failing tests likely due to recent commit, should be simple fix and re-push.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1648#issuecomment-294171904
https://github.com/hail-is/hail/pull/1650#issuecomment-292925773:42,Integrability,interface,interface,42,"@tpoterba and I are discussing changes to interface, possibly to internals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1650#issuecomment-292925773
https://github.com/hail-is/hail/pull/1650#issuecomment-293427251:32,Integrability,interface,interface,32,Closing in favor of PR with new interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1650#issuecomment-293427251
https://github.com/hail-is/hail/pull/1655#issuecomment-292960736:108,Performance,perform,performance,108,"FYI, I create a `yDummy` of all zeros in order to very simply reuse the regression utils we have. Effect on performance is negligible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1655#issuecomment-292960736
https://github.com/hail-is/hail/pull/1655#issuecomment-292960736:55,Usability,simpl,simply,55,"FYI, I create a `yDummy` of all zeros in order to very simply reuse the regression utils we have. Effect on performance is negligible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1655#issuecomment-292960736
https://github.com/hail-is/hail/pull/1656#issuecomment-293058402:120,Integrability,depend,depends,120,"OK, now passes the tests. Not sure if `minRep` should be called in `VariantSubgen` or in `LoadBGenTest` (as I did). All depends on how `VariantSubgen` is used (e.g. if testing minRep)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402
https://github.com/hail-is/hail/pull/1656#issuecomment-293058402:90,Performance,Load,LoadBGenTest,90,"OK, now passes the tests. Not sure if `minRep` should be called in `VariantSubgen` or in `LoadBGenTest` (as I did). All depends on how `VariantSubgen` is used (e.g. if testing minRep)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402
https://github.com/hail-is/hail/pull/1656#issuecomment-293058402:19,Testability,test,tests,19,"OK, now passes the tests. Not sure if `minRep` should be called in `VariantSubgen` or in `LoadBGenTest` (as I did). All depends on how `VariantSubgen` is used (e.g. if testing minRep)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402
https://github.com/hail-is/hail/pull/1656#issuecomment-293058402:168,Testability,test,testing,168,"OK, now passes the tests. Not sure if `minRep` should be called in `VariantSubgen` or in `LoadBGenTest` (as I did). All depends on how `VariantSubgen` is used (e.g. if testing minRep)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402
https://github.com/hail-is/hail/issues/1670#issuecomment-316394897:26,Testability,test,test,26,"This means that we should test our various definitions of isInsertion, isDeletion, etc against htsjdk. I think I made this issue at your request, and don't feel super strongly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1670#issuecomment-316394897
https://github.com/hail-is/hail/issues/1677#issuecomment-294209899:13,Testability,test,testing,13,"We should be testing randomly generated INFO fields with `Gen`, that would have caught this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1677#issuecomment-294209899
https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:11,Availability,error,error,11,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615
https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:170,Deployability,update,update,170,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615
https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:74,Performance,load,load,74,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615
https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:110,Security,access,access,110,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615
https://github.com/hail-is/hail/pull/1691#issuecomment-295269806:99,Availability,down,download,99,Either sphinx cached incorrectly or it's not writing the added text: https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/18456:id/www/hail/expr/hail.expr.TInt.html#hail.expr.TInt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806
https://github.com/hail-is/hail/pull/1691#issuecomment-295269806:14,Performance,cache,cached,14,Either sphinx cached incorrectly or it's not writing the added text: https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/18456:id/www/hail/expr/hail.expr.TInt.html#hail.expr.TInt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806
https://github.com/hail-is/hail/issues/1694#issuecomment-365623725:113,Availability,error,error,113,"maybe we should check the result type in anything that could localize an unrealizable python data structure, and error? Places like collect/take/aggregate?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1694#issuecomment-365623725
https://github.com/hail-is/hail/pull/1695#issuecomment-296220668:91,Safety,unsafe,unsafe,91,"@cseed back to you, I inserted existential types everywhere. I managed this with only two `unsafe` functions: in let-binding and in lambda aggregators. In both cases, the type of the `Code[T]` should be correct if the compilation process is correct. Adding a `checkcast` would create a tremendous amount of unnecessary byte code. I suspect returning the type as a part of the compilation process would enable me to remove the let-binding `unsafe`. I'm not sure about the lambda aggregator case. I'm unconcerned since all that code is disappearing soon anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1695#issuecomment-296220668
https://github.com/hail-is/hail/pull/1695#issuecomment-296220668:439,Safety,unsafe,unsafe,439,"@cseed back to you, I inserted existential types everywhere. I managed this with only two `unsafe` functions: in let-binding and in lambda aggregators. In both cases, the type of the `Code[T]` should be correct if the compilation process is correct. Adding a `checkcast` would create a tremendous amount of unnecessary byte code. I suspect returning the type as a part of the compilation process would enable me to remove the let-binding `unsafe`. I'm not sure about the lambda aggregator case. I'm unconcerned since all that code is disappearing soon anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1695#issuecomment-296220668
https://github.com/hail-is/hail/pull/1696#issuecomment-297134678:6,Testability,test,test,6,"Added test + docs page, addressed all comments. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1696#issuecomment-297134678
https://github.com/hail-is/hail/pull/1697#issuecomment-297083904:69,Availability,failure,failure,69,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
https://github.com/hail-is/hail/pull/1697#issuecomment-297083904:64,Testability,test,test,64,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
https://github.com/hail-is/hail/pull/1697#issuecomment-297083904:91,Testability,test,testing,91,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
https://github.com/hail-is/hail/pull/1697#issuecomment-297083904:177,Testability,test,test,177,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
https://github.com/hail-is/hail/pull/1697#issuecomment-297083904:194,Testability,Test,Test,194,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
https://github.com/hail-is/hail/pull/1697#issuecomment-297083904:26,Usability,feedback,feedback,26,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
https://github.com/hail-is/hail/pull/1697#issuecomment-298819643:21,Modifiability,evolve,evolve,21,Closing as this will evolve in separate branch for a while yet.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-298819643
https://github.com/hail-is/hail/pull/1698#issuecomment-298988120:146,Testability,test,test,146,@cseed please hold off on this until I incorporate changes in the export branch. I want to make sure everything is working with the import/export test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1698#issuecomment-298988120
https://github.com/hail-is/hail/pull/1703#issuecomment-296328487:161,Testability,test,test,161,"The difference is consistent with numerical instability of inversion of singular fisher matrix, with different results under different natives. I've changed the test on constant dosage to:; ```; def assertConsistentWithConstant(converged: Annotation, pval: Annotation) {; assert(!converged.asInstanceOf[Boolean] || pval.asInstanceOf[Double].isNaN); ```; This assures that this difference does not engender deviant scientific results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1703#issuecomment-296328487
https://github.com/hail-is/hail/pull/1703#issuecomment-296328487:199,Testability,assert,assertConsistentWithConstant,199,"The difference is consistent with numerical instability of inversion of singular fisher matrix, with different results under different natives. I've changed the test on constant dosage to:; ```; def assertConsistentWithConstant(converged: Annotation, pval: Annotation) {; assert(!converged.asInstanceOf[Boolean] || pval.asInstanceOf[Double].isNaN); ```; This assures that this difference does not engender deviant scientific results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1703#issuecomment-296328487
https://github.com/hail-is/hail/pull/1703#issuecomment-296328487:272,Testability,assert,assert,272,"The difference is consistent with numerical instability of inversion of singular fisher matrix, with different results under different natives. I've changed the test on constant dosage to:; ```; def assertConsistentWithConstant(converged: Annotation, pval: Annotation) {; assert(!converged.asInstanceOf[Boolean] || pval.asInstanceOf[Double].isNaN); ```; This assures that this difference does not engender deviant scientific results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1703#issuecomment-296328487
https://github.com/hail-is/hail/pull/1708#issuecomment-296402338:6,Testability,benchmark,benchmark,6,"Cloud benchmark: with 20k-gene-annotated 1k genomes hardcall VDS (37.2 million variants, 2535 samples, 3 covariates), using 1000 partitions and 342 cores (24 workers, 16 cores each) on GCP, it takes about 60s to do linear burden test and output linregKT to TSV, and another 60s to output the sampleKT to TSV. ```; linreg_kt, sample_kt = (hc.read('gs://jbloom/ALL.1KG.qc.hardcalls.p1000.burden.vds'); .linreg_burden('gene',; 	 'va.geneSet',; 	 'gs.map(g => va.weight * g.gt).sum()',; 'sa.pheno',; ['sa.cov1', 'sa.cov2'])). linreg_kt.export('gs://jbloom/burden_linreg.tsv.gz'); sample_kt.export('gs://jbloom/burden_samples.tsv.gz'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-296402338
https://github.com/hail-is/hail/pull/1708#issuecomment-296402338:229,Testability,test,test,229,"Cloud benchmark: with 20k-gene-annotated 1k genomes hardcall VDS (37.2 million variants, 2535 samples, 3 covariates), using 1000 partitions and 342 cores (24 workers, 16 cores each) on GCP, it takes about 60s to do linear burden test and output linregKT to TSV, and another 60s to output the sampleKT to TSV. ```; linreg_kt, sample_kt = (hc.read('gs://jbloom/ALL.1KG.qc.hardcalls.p1000.burden.vds'); .linreg_burden('gene',; 	 'va.geneSet',; 	 'gs.map(g => va.weight * g.gt).sum()',; 'sa.pheno',; ['sa.cov1', 'sa.cov2'])). linreg_kt.export('gs://jbloom/burden_linreg.tsv.gz'); sample_kt.export('gs://jbloom/burden_samples.tsv.gz'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-296402338
https://github.com/hail-is/hail/pull/1708#issuecomment-296492792:11,Testability,test,tests,11,Added more tests and single_key option. Ready for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-296492792
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:73,Availability,error,error,73,"The call in the docs is wrong:; ```; [:makeHailDocs] Warning, treated as error:; [13:46:55]	[:makeHailDocs] WARNING: **********************************************************************; [13:46:55]	[:makeHailDocs] File ""hail.VariantDataset.rst"", line 16, in default; [13:46:55]	[:makeHailDocs] Failed example:; [13:46:55]	[:makeHailDocs] linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); [13:46:55]	[:makeHailDocs] .linreg_burden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1401,Availability,Error,Error,1401,"burden.vds'); [13:46:55]	[:makeHailDocs] .linreg_burden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1506,Availability,error,error,1506,":46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnectio",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1692,Availability,error,error,1692,"a.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnection.java:214); [13:46:55]	[:makeHailDocs] 	at java.lang.Thread.run(Thread.java:745); [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:2741,Availability,Error,Error,2741,"5]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnection.java:214); [13:46:55]	[:makeHailDocs] 	at java.lang.Thread.run(Thread.java:745); [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Hail version: devel-b94d386; [13:46:55]	[:makeHailDocs] Error summary: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] make: *** [doctest] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:2759,Availability,error,error,2759,"5]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnection.java:214); [13:46:55]	[:makeHailDocs] 	at java.lang.Thread.run(Thread.java:745); [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Hail version: devel-b94d386; [13:46:55]	[:makeHailDocs] Error summary: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] make: *** [doctest] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:2945,Availability,Error,Error,2945,"5]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnection.java:214); [13:46:55]	[:makeHailDocs] 	at java.lang.Thread.run(Thread.java:745); [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Hail version: devel-b94d386; [13:46:55]	[:makeHailDocs] Error summary: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] make: *** [doctest] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1430,Integrability,message,message,1430,"urden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:996,Testability,test,test,996,"The call in the docs is wrong:; ```; [:makeHailDocs] Warning, treated as error:; [13:46:55]	[:makeHailDocs] WARNING: **********************************************************************; [13:46:55]	[:makeHailDocs] File ""hail.VariantDataset.rst"", line 16, in default; [13:46:55]	[:makeHailDocs] Failed example:; [13:46:55]	[:makeHailDocs] linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); [13:46:55]	[:makeHailDocs] .linreg_burden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203
https://github.com/hail-is/hail/pull/1709#issuecomment-298405321:138,Modifiability,refactor,refactoring,138,"Agreed on plan with @astheeggeggs to use this branch for his immediate needs, while pulling in pieces of this code in new PR once 0.1 and refactoring of RegressionUtils is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1709#issuecomment-298405321
https://github.com/hail-is/hail/pull/1712#issuecomment-297784358:25,Deployability,update,update,25,"Code looks good. Can you update the VariantDataset.vep function documentation? In particular, note that plugin overrides human_ancestor and conservation file. plugin in cleaner. We should remove the latter when we start version 0.2: https://github.com/hail-is/hail/issues/1728",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358
https://github.com/hail-is/hail/pull/1712#issuecomment-297784358:104,Modifiability,plugin,plugin,104,"Code looks good. Can you update the VariantDataset.vep function documentation? In particular, note that plugin overrides human_ancestor and conservation file. plugin in cleaner. We should remove the latter when we start version 0.2: https://github.com/hail-is/hail/issues/1728",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358
https://github.com/hail-is/hail/pull/1712#issuecomment-297784358:159,Modifiability,plugin,plugin,159,"Code looks good. Can you update the VariantDataset.vep function documentation? In particular, note that plugin overrides human_ancestor and conservation file. plugin in cleaner. We should remove the latter when we start version 0.2: https://github.com/hail-is/hail/issues/1728",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358
https://github.com/hail-is/hail/pull/1712#issuecomment-298847712:5,Deployability,update,updated,5,I've updated the docs. ; One remaining issue is how to allow multiple `--plugin` options (http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html) ?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712
https://github.com/hail-is/hail/pull/1712#issuecomment-298847712:73,Modifiability,plugin,plugin,73,I've updated the docs. ; One remaining issue is how to allow multiple `--plugin` options (http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html) ?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712
https://github.com/hail-is/hail/pull/1712#issuecomment-299003723:22,Modifiability,plugin,plugin,22,"Will the additional --plugin options change the output schema? We fix the schema in the VEP command which is one reason we've resisted supporting general VEP options. To support a general schema, we probably need to impute the schema from the VEP JSON output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-299003723
https://github.com/hail-is/hail/issues/1717#issuecomment-305575523:151,Security,expose,exposed,151,"@konradjk This ended up being more work than expected since we had not yet had a function that took a lambda (only had methods that did so). I have it exposed on a branch, but it seems our uniroot functionality is a little bit unexpected. It always returns the right root when there is one, but it fails to identify situations where no root exists. Will require some investigation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-305575523
https://github.com/hail-is/hail/issues/1717#issuecomment-305807984:185,Availability,error,error,185,Right now it will return either a double if it finds one or NA if it doesn't. But it kind of seems like if I'm going to impose a restriction like the one above maybe it should throw an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-305807984
https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:96,Availability,avail,available,96,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826
https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:319,Availability,avail,available,319,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826
https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:171,Integrability,interface,interface,171,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826
https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:149,Security,expose,exposed,149,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826
https://github.com/hail-is/hail/issues/1717#issuecomment-388877364:45,Security,expose,exposed,45,"Thanks. @zaczap has a branch with a function exposed that uses it, so may want him in this to comment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388877364
https://github.com/hail-is/hail/pull/1720#issuecomment-297238390:6,Testability,test,test,6,"Added test, ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297238390
https://github.com/hail-is/hail/pull/1720#issuecomment-297446909:193,Testability,log,log,193,"I'm happy with the code here, and Jon and Alex have looked at math, so I think this is more or less good to go in. Will approve after discussing with Jon whether the possibility of the maximum log liklhood falling on a grid point is something worth worrying about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297446909
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1222,Deployability,integrat,integrate,1222,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1222,Integrability,integrat,integrate,1222,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:141,Testability,log,logDelta,141,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:177,Testability,log,logDeltaGrid,177,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:193,Testability,log,logLkhd,193,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:228,Testability,log,logLkhdVals,228,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:460,Testability,log,logDelta,460,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:502,Testability,log,logLkhd,502,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:539,Testability,log,logLkhd,539,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:625,Testability,log,logLkhd,625,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:648,Testability,log,logLkhd,648,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:677,Testability,log,logLkhd,677,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:702,Testability,log,logLkhd,702,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:735,Testability,log,logLkhd,735,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:862,Testability,log,logLkhd,862,"I did the check against MMAP results using this R code on the grids in global from lmmreg analysis:; ```; ##### delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1094,Testability,log,log,1094,"## delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1170,Testability,log,logLkhd,1170,"## delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1183,Testability,log,logLkhd,1183,"## delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1196,Testability,log,logLkhd,1196,"## delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1344,Testability,log,logLkhd,1344,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1413,Testability,log,logLkhd,1413,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1706,Testability,log,logDelta,1706,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1720,Testability,log,logLkhd,1720,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1779,Testability,log,logLkhd,1779,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1816,Testability,log,logLkhd,1816,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1884,Testability,log,logLkhd,1884,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:265,Deployability,pipeline,pipeline,265,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:312,Deployability,pipeline,pipeline,312,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:381,Deployability,pipeline,pipeline,381,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:1274,Deployability,pipeline,pipeline,1274,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:414,Modifiability,config,config,414,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:32,Performance,load,load,32,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:95,Testability,log,log,95,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:145,Testability,log,log,145,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:198,Testability,log,log,198,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:274,Testability,test,testGWAS,274,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:321,Testability,test,testGWAS,321,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:390,Testability,test,testGWAS,390,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:1283,Testability,test,testGWAS,1283,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527
https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:143,Availability,down,down,143,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790
https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:113,Integrability,interface,interface,113,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790
https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:24,Modifiability,Refactor,Refactored,24,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790
https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:79,Testability,test,tests,79,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790
https://github.com/hail-is/hail/pull/1727#issuecomment-301183677:28,Integrability,interface,interface,28,I'm happy with this initial interface: typecheck and typecheck_method. Back to you!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-301183677
https://github.com/hail-is/hail/pull/1733#issuecomment-299016246:39,Testability,test,test,39,Back to you. There's also an ExprSuite test failing due to the switch from Double to Int.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1733#issuecomment-299016246
https://github.com/hail-is/hail/issues/1743#issuecomment-299075862:296,Modifiability,config,config,296,"FWIW you don't actually need to parse it at the moment. This is my code that works:. ```; temp_file = 'hdfs:/kt.txt.bgz'; types_file = 'hdfs:/kt.types.txt'; kt.export(temp_file, types_file=types_file). with hail.hadoop_read(types_file) as f:; types = f.read(); kt = hc.import_keytable(temp_file, config=hail.TextTableConfig(types=types)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1743#issuecomment-299075862
https://github.com/hail-is/hail/pull/1748#issuecomment-298948635:60,Availability,error,error,60,Checked that PartitioningSuite doesn't get an out of memory error on bgen branch and ExprSuite.testImpexes takes less than a second now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298948635
https://github.com/hail-is/hail/pull/1748#issuecomment-298948635:95,Testability,test,testImpexes,95,Checked that PartitioningSuite doesn't get an out of memory error on bgen branch and ExprSuite.testImpexes takes less than a second now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298948635
https://github.com/hail-is/hail/pull/1748#issuecomment-298957026:36,Deployability,integrat,integration,36,@danking back to you. restarted the integration test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298957026
https://github.com/hail-is/hail/pull/1748#issuecomment-298957026:36,Integrability,integrat,integration,36,@danking back to you. restarted the integration test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298957026
https://github.com/hail-is/hail/pull/1748#issuecomment-298957026:48,Testability,test,test,48,@danking back to you. restarted the integration test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298957026
https://github.com/hail-is/hail/pull/1759#issuecomment-299068258:16,Performance,cache,cache,16,why? people can cache this themselves,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1759#issuecomment-299068258
https://github.com/hail-is/hail/pull/1759#issuecomment-299068662:200,Safety,avoid,avoided,200,"Because the linreg keytable is computed via mapAnnotations on sample keytable and then both are returned. So if the user then asks for sample keytable, it is recomputed. As we discussed, this will be avoided once variant annotations are keytables too, which will make it natural to pass in the sample_keytable to linreg_burden instead of the vds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1759#issuecomment-299068662
https://github.com/hail-is/hail/pull/1768#issuecomment-299443875:41,Integrability,interface,interface,41,@jigold we should probably have the same interface in terms of units. Which do you think is best?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1768#issuecomment-299443875
https://github.com/hail-is/hail/pull/1768#issuecomment-299478117:17,Integrability,interface,interface,17,"The python/scala interface should be the same, I agree with Jon",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1768#issuecomment-299478117
https://github.com/hail-is/hail/pull/1775#issuecomment-302124342:24,Availability,error,errors,24,Force pushed to resolve errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1775#issuecomment-302124342
https://github.com/hail-is/hail/pull/1778#issuecomment-300001063:66,Integrability,interface,interface,66,"When did responding to comments become a ""review""? Super annoying interface change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300001063
https://github.com/hail-is/hail/pull/1778#issuecomment-300169255:61,Deployability,configurat,configuration,61,"It's interesting, this reading problem is basically a little configuration language, but somehow it's really hard to write the performance we want without lots of code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255
https://github.com/hail-is/hail/pull/1778#issuecomment-300169255:61,Modifiability,config,configuration,61,"It's interesting, this reading problem is basically a little configuration language, but somehow it's really hard to write the performance we want without lots of code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255
https://github.com/hail-is/hail/pull/1778#issuecomment-300169255:127,Performance,perform,performance,127,"It's interesting, this reading problem is basically a little configuration language, but somehow it's really hard to write the performance we want without lots of code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255
https://github.com/hail-is/hail/pull/1778#issuecomment-302164339:108,Modifiability,rewrite,rewrite,108,Case class equals does a reference equality check as you expected. I switched to structural equality in the rewrite functions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-302164339
https://github.com/hail-is/hail/pull/1778#issuecomment-302273874:296,Testability,test,test,296,"@danking It looks like that github change you made means reviews are invalidated by further commits to the branch. Is that what we want? I misunderstood, I thought further commits to master invalidate the checks. It's really the latter I want. Either way, I need a fresh approval (minor fix to a test).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-302273874
https://github.com/hail-is/hail/pull/1778#issuecomment-302577366:170,Performance,race condition,race condition,170,"It looks like I accidentally ""comment"" reviewed rather than ""approve"" reviewed. Further commits to either feature or master will invalidate CI results, though there is a race condition that is only fixable by requiring everyone to rebase on top of latest master before merges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-302577366
https://github.com/hail-is/hail/pull/1779#issuecomment-299721946:716,Availability,error,error,716,"Also, I created `gs://hail-common/vep/vep/GRCh37`, `gs://hail-common/vep/vep/GRCh38`; directories with VEP configs and loftee data files, so you can now run ; ```; gcloud dataproc clusters create $CLUSTER; ...; --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-init.sh. or . --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh; ```; along with ; ```; gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-gcloud.properties. or . gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-gcloud.properties; ```. though the init.sh script ties the cluster to a particular genome build. . Also, it would be nice if hail could throw an error if trying to annotate a GRCh37 callset with GRCh38 VEP, etc. Would it make sense to put this check in the VEP command?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946
https://github.com/hail-is/hail/pull/1779#issuecomment-299721946:107,Modifiability,config,configs,107,"Also, I created `gs://hail-common/vep/vep/GRCh37`, `gs://hail-common/vep/vep/GRCh38`; directories with VEP configs and loftee data files, so you can now run ; ```; gcloud dataproc clusters create $CLUSTER; ...; --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-init.sh. or . --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh; ```; along with ; ```; gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-gcloud.properties. or . gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-gcloud.properties; ```. though the init.sh script ties the cluster to a particular genome build. . Also, it would be nice if hail could throw an error if trying to annotate a GRCh37 callset with GRCh38 VEP, etc. Would it make sense to put this check in the VEP command?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946
https://github.com/hail-is/hail/pull/1779#issuecomment-301174135:166,Deployability,pipeline,pipeline,166,"Alright awesome. I'm good to approve this, but if you could, please rebase this off of branch ""breakingbad"" and PR into that. We're currently putting all potentially pipeline breaking changes into that branch until we merge that whole thing into master at once for 0.1 release. breakingbad will be merged into master tonight or tomorrow in all liklihood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-301174135
https://github.com/hail-is/hail/pull/1779#issuecomment-301174135:269,Deployability,release,release,269,"Alright awesome. I'm good to approve this, but if you could, please rebase this off of branch ""breakingbad"" and PR into that. We're currently putting all potentially pipeline breaking changes into that branch until we merge that whole thing into master at once for 0.1 release. breakingbad will be merged into master tonight or tomorrow in all liklihood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-301174135
https://github.com/hail-is/hail/pull/1780#issuecomment-302461856:9,Modifiability,refactor,refactored,9,@cseed I refactored the resource stream into utils. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1780#issuecomment-302461856
https://github.com/hail-is/hail/issues/1785#issuecomment-300283835:221,Security,hash,hashed,221,"Yes - although would that ensure they're unique indices? I had using the variants in a feature vector in mind, which each index err...indexing the vector. (Could also work with samples.). When I did it outside of hail, I hashed the v.contig/v.start values and then used a String Indexer to get unique index values.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1785#issuecomment-300283835
https://github.com/hail-is/hail/pull/1789#issuecomment-302176856:31,Integrability,synchroniz,synchronized,31,@danking Can you look over the synchronized/volatile code here? You thought through this stuff originally. I reviewed everything else (mainly comments on the previous commit).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302176856
https://github.com/hail-is/hail/pull/1789#issuecomment-302184683:206,Performance,load,loaded,206,"You should add the reference to the VSM metadata on write. On read, if the reference field is missing, print a warning and default to the the GRCh37. You should then verify the global reference matches the loaded reference. Finally, do you have the data for h38? You should add that in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302184683
https://github.com/hail-is/hail/pull/1789#issuecomment-302429644:485,Integrability,rout,route,485,"I thought we were going to load the genome reference upon initializing the HailContext. What happens if someone has two VDS's in the same session, but one is GRCh37 and the other is 38? Then we would have to ensure the reference is set to the correct one for every operation on the vds. It wouldn't be too difficult to add a decorator to the Python VDS to set the global reference on each method call. Then for a Join we can check the references are the same explicitly. If we go this route, I think we should not have the HailContext have the reference parameter, and instead have it as an optional parameter on `import_vcf`, `import_plink`, ...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302429644
https://github.com/hail-is/hail/pull/1789#issuecomment-302429644:27,Performance,load,load,27,"I thought we were going to load the genome reference upon initializing the HailContext. What happens if someone has two VDS's in the same session, but one is GRCh37 and the other is 38? Then we would have to ensure the reference is set to the correct one for every operation on the vds. It wouldn't be too difficult to add a decorator to the Python VDS to set the global reference on each method call. Then for a Join we can check the references are the same explicitly. If we go this route, I think we should not have the HailContext have the reference parameter, and instead have it as an optional parameter on `import_vcf`, `import_plink`, ...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302429644
https://github.com/hail-is/hail/pull/1789#issuecomment-302433569:29,Performance,load,load,29,"> I thought we were going to load the genome reference upon initializing the HailContext. That is correct. I just want to double-check the VDS reference matches the global one. We won't support multiple datasets with different references in a single Hail session with this change. That's an intentional assumption to make the first version of reference support simpler. . Right, so you should also add a reference option the HailContext constructor (last argument, named with default to GRCh37 not to break backward compatibility) to set the reference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302433569
https://github.com/hail-is/hail/pull/1789#issuecomment-302433569:361,Usability,simpl,simpler,361,"> I thought we were going to load the genome reference upon initializing the HailContext. That is correct. I just want to double-check the VDS reference matches the global one. We won't support multiple datasets with different references in a single Hail session with this change. That's an intentional assumption to make the first version of reference support simpler. . Right, so you should also add a reference option the HailContext constructor (last argument, named with default to GRCh37 not to break backward compatibility) to set the reference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302433569
https://github.com/hail-is/hail/issues/1806#issuecomment-301357111:4,Availability,error,error,4,"The error occurred on a gzipped VCF, and went away after converting to bgzipped.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301357111
https://github.com/hail-is/hail/issues/1806#issuecomment-301426712:183,Performance,load,loaded,183,"This isn't _really_ a bug. It's a combination of two things:. 1. Parquet can't write partitions larger than 2**31 (2.1G or so); 2. gzipped VCF can't be split, so the entire thing was loaded as 1 partition",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301426712
https://github.com/hail-is/hail/issues/1806#issuecomment-301753639:77,Availability,error,error,77,"I see.. if there's a place where it's easy to check for this, I could add an error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639
https://github.com/hail-is/hail/issues/1806#issuecomment-301753639:83,Integrability,message,message,83,"I see.. if there's a place where it's easy to check for this, I could add an error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639
https://github.com/hail-is/hail/issues/1807#issuecomment-301447221:51,Integrability,interface,interface,51,"It sounds like you were using the old command-line interface. Hail has moved to a python API, documented here:; https://hail.is/hail/api.html; https://hail.is/hail/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807#issuecomment-301447221
https://github.com/hail-is/hail/pull/1811#issuecomment-302128830:68,Integrability,interface,interface,68,"I didn't look through everything, but skimmed through quickly. This interface is gonna be way nicer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1811#issuecomment-302128830
https://github.com/hail-is/hail/issues/1813#issuecomment-317562462:1,Integrability,contract,contract,1,"""contract types""? What's that mean?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1813#issuecomment-317562462
https://github.com/hail-is/hail/pull/1814#issuecomment-301555125:236,Performance,cache,cached,236,"Shuffles function as a persist in most cases. I don't completely understand when they don't... Is this a case when shuffle won't persist?. 1. Split multi, something tiny moves, it's put in memory; 2. Use this thing a couple times. It's cached; 3. Do a big shuffle with something else, which uses lots of memory and evicts the split-multi shuffled variants; 4. now you need to recompute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301555125
https://github.com/hail-is/hail/pull/1814#issuecomment-301566601:64,Deployability,pipeline,pipeline,64,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601
https://github.com/hail-is/hail/pull/1814#issuecomment-301566601:193,Energy Efficiency,green,green,193,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601
https://github.com/hail-is/hail/pull/1814#issuecomment-301566601:112,Usability,clear,clear,112,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601
https://github.com/hail-is/hail/issues/1818#issuecomment-301725415:231,Security,hash,hash,231,We changed the name of this function from `import_keytable` to `import_table` with the changes for the 0.1 stable build yesterday. Is your Hail code out of date? Try running:; ```python; >>> hc.version; ```; If it says `devel-<git hash>` instead of `0.1-<git hash>` it's out of date.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725415
https://github.com/hail-is/hail/issues/1818#issuecomment-301725415:259,Security,hash,hash,259,We changed the name of this function from `import_keytable` to `import_table` with the changes for the 0.1 stable build yesterday. Is your Hail code out of date? Try running:; ```python; >>> hc.version; ```; If it says `devel-<git hash>` instead of `0.1-<git hash>` it's out of date.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725415
https://github.com/hail-is/hail/issues/1818#issuecomment-301725642:94,Availability,error,error,94,"However, a `NameError` is surprising here: I would've thought that this would be an attribute error instead. Let us know any updates!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642
https://github.com/hail-is/hail/issues/1818#issuecomment-301725642:125,Deployability,update,updates,125,"However, a `NameError` is surprising here: I would've thought that this would be an attribute error instead. Let us know any updates!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642
https://github.com/hail-is/hail/issues/1818#issuecomment-302037736:16,Deployability,install,installed,16,"hi tpoterba.; I installed the new version of hail, following the instructions:; $ git clone https://github.com/broadinstitute/hail.git; $ cd hail; $ ./gradlew shadowJar; And all the commands went well.; **********************; $ ./gradlew shadowJar. ........ 24 warnings found; :processResources; :classes; :shadowJar. BUILD SUCCESSFUL; **********************; But when I tried to import hail, it went wrong; [root@tele-3 hail]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/hail/python/hail/__init__.py"", line 3, in <module>; from hail.context import HailContext; File ""/opt/Software/hail/python/hail/context.py"", line 3, in <module>; from hail.typecheck import *; File ""/opt/Software/hail/python/hail/typecheck/__init__.py"", line 1, in <module>; from check import *; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 1, in <module>; from decorator import decorator, getargspec; ImportError: cannot import name getargspec. How can I fix it?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-302037736
https://github.com/hail-is/hail/issues/1818#issuecomment-302064968:150,Deployability,install,install,150,"Hail depends on the `decorator` module, and in this case it looks like you've got it but it's out of date. The following should fix it:. ```bash; pip install -U decorator; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968
https://github.com/hail-is/hail/issues/1818#issuecomment-302064968:5,Integrability,depend,depends,5,"Hail depends on the `decorator` module, and in this case it looks like you've got it but it's out of date. The following should fix it:. ```bash; pip install -U decorator; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968
https://github.com/hail-is/hail/issues/1822#issuecomment-301754312:36,Deployability,install,install,36,"git hash looks like a standard Hail install, nevermind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301754312
https://github.com/hail-is/hail/issues/1822#issuecomment-301754312:4,Security,hash,hash,4,"git hash looks like a standard Hail install, nevermind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301754312
https://github.com/hail-is/hail/issues/1822#issuecomment-301756372:185,Availability,error,error,185,"it is stock hail, but I'm running with a new GRCh38 VEP config + files from; gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh. though I've run it on another VCF and didn't get that error. ; I'll rerun to make sure it wasn't a misconfiguration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372
https://github.com/hail-is/hail/issues/1822#issuecomment-301756372:56,Modifiability,config,config,56,"it is stock hail, but I'm running with a new GRCh38 VEP config + files from; gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh. though I've run it on another VCF and didn't get that error. ; I'll rerun to make sure it wasn't a misconfiguration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372
https://github.com/hail-is/hail/issues/1822#issuecomment-301759591:22,Availability,error,error,22,"Nope, seeing the same error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301759591
https://github.com/hail-is/hail/issues/1822#issuecomment-301916114:2310,Safety,detect,detect,2310,"anche level for SNP model at VQS Lod < -502107.0516"">; ##FILTER=<ID=VQSRTrancheSNP99.95to100.00,Description=""Truth sensitivity tranche level for SNP model at VQS Lod: -502107.0516 <= x < -22.1967"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=.,Type=String,Description=""PGT"">; ##FORMAT=<ID=PID,Number=.,Type=String,Description=""PID"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ... ~ 3000 lines of header + decoys; ##contig=<ID=HLA-DRB1*12:17,length=11260>; ##contig=<ID=HLA-DRB1*13:01:01,length=13935>; ##contig=<ID=HLA-DRB1*13:02:01,length=13941>; ##contig=<ID=HLA-DRB1*14:05:01,length=13933>; ##contig=<ID=HLA-DRB1*14:54:01,length=13936>; ##contig=<ID=HLA-DRB1*15:01:01:01,length=11080>; ##contig=<ID=HLA-DRB1*15:01:01:02,length=11571>; ##contig=<ID=HLA-DRB1*15:01:01:03,length=11056>; ##contig=<ID=HLA-DRB1*15:01:01:04,length=11056>; ##contig=<ID=HLA-DRB1*15:02:01,length=10313>; ##contig=<ID=HLA-DRB1*15:03:01:01,length=11567>; ##contig=<ID=HLA-DRB1*15:03:01:02,length=11569>; ##contig=<ID=HLA-DRB1*16:02:01,length=11005>; ##reference=file:///cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; #CHROM	POS	ID	RE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916114
https://github.com/hail-is/hail/issues/1822#issuecomment-301916114:2302,Testability,Test,Test,2302,"anche level for SNP model at VQS Lod < -502107.0516"">; ##FILTER=<ID=VQSRTrancheSNP99.95to100.00,Description=""Truth sensitivity tranche level for SNP model at VQS Lod: -502107.0516 <= x < -22.1967"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=.,Type=String,Description=""PGT"">; ##FORMAT=<ID=PID,Number=.,Type=String,Description=""PID"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ... ~ 3000 lines of header + decoys; ##contig=<ID=HLA-DRB1*12:17,length=11260>; ##contig=<ID=HLA-DRB1*13:01:01,length=13935>; ##contig=<ID=HLA-DRB1*13:02:01,length=13941>; ##contig=<ID=HLA-DRB1*14:05:01,length=13933>; ##contig=<ID=HLA-DRB1*14:54:01,length=13936>; ##contig=<ID=HLA-DRB1*15:01:01:01,length=11080>; ##contig=<ID=HLA-DRB1*15:01:01:02,length=11571>; ##contig=<ID=HLA-DRB1*15:01:01:03,length=11056>; ##contig=<ID=HLA-DRB1*15:01:01:04,length=11056>; ##contig=<ID=HLA-DRB1*15:02:01,length=10313>; ##contig=<ID=HLA-DRB1*15:03:01:01,length=11567>; ##contig=<ID=HLA-DRB1*15:03:01:02,length=11569>; ##contig=<ID=HLA-DRB1*16:02:01,length=11005>; ##reference=file:///cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; #CHROM	POS	ID	RE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916114
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:47,Availability,error,error,47,"while the 1kg vcf that generated the assertion error looks like ; ```; ##fileformat=VCFv4.2; ##ApplyRecalibration=""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:6991,Availability,down,downsampled,6991,"order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=Dels,Number=1,Type=Float,Description=""Fraction of Reads Containing Spanning Deletions"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=HaplotypeScore,Number=1,Type=Float,Description=""Consistency of the site with at most two segregating haplotypes"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=OriginalContig,Number=1,Type=String,Description=""The name of the source contig/chromosome prior to liftover."">; ##INFO=<ID=OriginalStart,Number=1,Type=String,Description=""The p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20364,Availability,mask,mask,20364,"ion=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 maskName=Mask missingValuesInExpressionsShouldEvaluateAsFailing=false invalidatePreviousFilters=false filter_mismatching_base_and_quals=false""; ##contig=<ID=1,length=248956422>; ##contig=<ID=2,length=242193529>; ##contig=<ID=3,length=198295559>; ##contig=<ID=4,length=190214555>; ##contig=<ID=5,length=181538259>; ##contig=<ID=6,length=170805979>; ##contig=<ID=7,length=159345973>; ##contig=<ID=8,length=145138636>; ##contig=<ID=",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20940,Availability,mask,maskExtension,20940,,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20956,Availability,mask,maskName,20956,,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20965,Availability,Mask,Mask,20965,,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:1162,Energy Efficiency,monitor,monitorThreadEfficiency,1162,"""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=Combine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:3138,Energy Efficiency,monitor,monitorThreadEfficiency,3138,"=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=[(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recalibrated.vcf), (RodBinding name=variant2 source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.filtered.vcf)] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub genotypemergeoption=UNSORTED filteredrecordsmergetype=KEEP_IF_ANY_UNFILTERED multipleallelesmergetype=BY_TYPE rod_priority_list=null printComplexMerges=false filteredAreUncalled=false minimalVCF=false",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:12316,Energy Efficiency,monitor,monitorThreadEfficiency,12316,"s=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL kee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:14744,Energy Efficiency,monitor,monitorThreadEfficiency,14744,e=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.bam.list] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/scatter/temp_0001_of_1200/scattered.intervals] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=75 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false genotype_likelihoods_model=BOTH pcr_error_rate=1.0E-4 computeSLOD=false annotateNDA=false pair_hmm_implementation=ORIGINAL min_base_quality_score=17 max_deletion_fraction=0.05 min_indel_count_for_genotyping=5 min_indel_fraction_per_sample=0.25 indel_heterozygosity=1.25E-4 indelGapContinuationPenalty=10 indelGapOpenPenalty=45 indelHaplotypeSize=80 indelDebug=false ignoreSNPAlleles=false allReadsSP=false ignoreLaneInfo=false reference_sample_calls=(RodBinding name= source=UNBOUND) reference_sample_name=null sample_ploidy=2 min_quality_score=1 max_quality_score=40 site_quality_prior=20 min_power_threshold_for_calling=0.95 min_reference_depth=100 exclude_filtered_reference_sites,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:17885,Energy Efficiency,monitor,monitorThreadEfficiency,17885,A read_filter=[] intervals=[/seq/references/HybSelOligos/whole_exome_agilent_1.1_refseq_plus_3_boosters/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.targets.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=50 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf) snpEffFile=(RodBinding name=snpEffFile source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snpeff.vcf) dbsnp=(RodBinding name= source=UNBOUND) comp=[] resource=[] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub annotation=[SnpEff] excludeAnnotation=[] group=[] expression=[] useAllAnnotations=false list=false alwaysAppendDbsnpId=false MendelViolationGeno,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20004,Energy Efficiency,monitor,monitorThreadEfficiency,20004,"analysis_type=VariantFiltration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:11091,Modifiability,config,config,11091,"t variant"">; ##INFO=<ID=SNPEFF_IMPACT,Number=1,Type=String,Description=""Impact of the highest-impact effect resulting from the current variant [MODIFIER, LOW, MODERATE, HIGH]"">; ##INFO=<ID=SNPEFF_TRANSCRIPT_ID,Number=1,Type=String,Description=""Transcript ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=STR,Number=0,Type=Flag,Description=""Variant is a short tandem repeat"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds ratio of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out"">; ##INFO=<ID=set,Number=1,Type=String,Description=""Source VCF for the merged record in CombineVariants"">; ##OriginalSnpEffCmd=""SnpEff eff -v -onlyCoding true -c /seq/references/Homo_sapiens_assembly19/v1/snpEff/Homo_sapiens_assembly19.snpEff.config -i vcf -o vcf GRCh37.64 /seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf ""; ##OriginalSnpEffVersion=""2.0.5 (build 2011-12-24), by Pablo Cingolani""; ##SelectVariants=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false pres",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:821,Performance,perform,performanceLog,821,"""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=Combine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:2797,Performance,perform,performanceLog,2797,"=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=[(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recalibrated.vcf), (RodBinding name=variant2 source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.filtered.vcf)] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub genotypemergeoption=UNSORTED filteredrecordsmergetype=KEEP_IF_ANY_UNFILTERED multipleallelesmergetype=BY_TYPE rod_priority_list=null printComplexMerges=false filteredAreUncalled=false minimalVCF=false",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:10755,Performance,perform,performing,10755," amino acid for the highest-impact effect resulting from the current variant (in HGVS style)"">; ##INFO=<ID=SNPEFF_CODON_CHANGE,Number=1,Type=String,Description=""Old/New codon for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_EFFECT,Number=1,Type=String,Description=""The highest-impact effect resulting from the current variant (or one of the highest-impact effects, if there is a tie)"">; ##INFO=<ID=SNPEFF_EXON_ID,Number=1,Type=String,Description=""Exon ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_FUNCTIONAL_CLASS,Number=1,Type=String,Description=""Functional class of the highest-impact effect resulting from the current variant: [NONE, SILENT, MISSENSE, NONSENSE]"">; ##INFO=<ID=SNPEFF_GENE_BIOTYPE,Number=1,Type=String,Description=""Gene biotype for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_GENE_NAME,Number=1,Type=String,Description=""Gene name for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_IMPACT,Number=1,Type=String,Description=""Impact of the highest-impact effect resulting from the current variant [MODIFIER, LOW, MODERATE, HIGH]"">; ##INFO=<ID=SNPEFF_TRANSCRIPT_ID,Number=1,Type=String,Description=""Transcript ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=STR,Number=0,Type=Flag,Description=""Variant is a short tandem repeat"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds ratio of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out"">; ##INFO=<ID=set,Number=1,Type=String,Description=""Source VCF for the merged record in CombineVariants"">; ##OriginalSnpEffCmd=""SnpEff eff -v -onlyCoding true -c /seq/references/Homo_sapiens_assembly19/v1/snpEff/Homo_sapiens_assembly19.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:11975,Performance,perform,performanceLog,11975,"s=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL kee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:14403,Performance,perform,performanceLog,14403,e=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.bam.list] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/scatter/temp_0001_of_1200/scattered.intervals] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=75 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false genotype_likelihoods_model=BOTH pcr_error_rate=1.0E-4 computeSLOD=false annotateNDA=false pair_hmm_implementation=ORIGINAL min_base_quality_score=17 max_deletion_fraction=0.05 min_indel_count_for_genotyping=5 min_indel_fraction_per_sample=0.25 indel_heterozygosity=1.25E-4 indelGapContinuationPenalty=10 indelGapOpenPenalty=45 indelHaplotypeSize=80 indelDebug=false ignoreSNPAlleles=false allReadsSP=false ignoreLaneInfo=false reference_sample_calls=(RodBinding name= source=UNBOUND) reference_sample_name=null sample_ploidy=2 min_quality_score=1 max_quality_score=40 site_quality_prior=20 min_power_threshold_for_calling=0.95 min_reference_depth=100 exclude_filtered_reference_sites,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:17544,Performance,perform,performanceLog,17544,A read_filter=[] intervals=[/seq/references/HybSelOligos/whole_exome_agilent_1.1_refseq_plus_3_boosters/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.targets.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=50 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf) snpEffFile=(RodBinding name=snpEffFile source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snpeff.vcf) dbsnp=(RodBinding name= source=UNBOUND) comp=[] resource=[] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub annotation=[SnpEff] excludeAnnotation=[] group=[] expression=[] useAllAnnotations=false list=false alwaysAppendDbsnpId=false MendelViolationGeno,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:19663,Performance,perform,performanceLog,19663,"analysis_type=VariantFiltration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:1085,Safety,unsafe,unsafe,1085,"""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=Combine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:3061,Safety,unsafe,unsafe,3061,"=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=[(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recalibrated.vcf), (RodBinding name=variant2 source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.filtered.vcf)] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub genotypemergeoption=UNSORTED filteredrecordsmergetype=KEEP_IF_ANY_UNFILTERED multipleallelesmergetype=BY_TYPE rod_priority_list=null printComplexMerges=false filteredAreUncalled=false minimalVCF=false",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:7287,Safety,detect,detect,7287,"order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=Dels,Number=1,Type=Float,Description=""Fraction of Reads Containing Spanning Deletions"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=HaplotypeScore,Number=1,Type=Float,Description=""Consistency of the site with at most two segregating haplotypes"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=OriginalContig,Number=1,Type=String,Description=""The name of the source contig/chromosome prior to liftover."">; ##INFO=<ID=OriginalStart,Number=1,Type=String,Description=""The p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:12239,Safety,unsafe,unsafe,12239,"s=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL kee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:14667,Safety,unsafe,unsafe,14667,e=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.bam.list] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/scatter/temp_0001_of_1200/scattered.intervals] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=75 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false genotype_likelihoods_model=BOTH pcr_error_rate=1.0E-4 computeSLOD=false annotateNDA=false pair_hmm_implementation=ORIGINAL min_base_quality_score=17 max_deletion_fraction=0.05 min_indel_count_for_genotyping=5 min_indel_fraction_per_sample=0.25 indel_heterozygosity=1.25E-4 indelGapContinuationPenalty=10 indelGapOpenPenalty=45 indelHaplotypeSize=80 indelDebug=false ignoreSNPAlleles=false allReadsSP=false ignoreLaneInfo=false reference_sample_calls=(RodBinding name= source=UNBOUND) reference_sample_name=null sample_ploidy=2 min_quality_score=1 max_quality_score=40 site_quality_prior=20 min_power_threshold_for_calling=0.95 min_reference_depth=100 exclude_filtered_reference_sites,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:17808,Safety,unsafe,unsafe,17808,A read_filter=[] intervals=[/seq/references/HybSelOligos/whole_exome_agilent_1.1_refseq_plus_3_boosters/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.targets.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=50 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf) snpEffFile=(RodBinding name=snpEffFile source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snpeff.vcf) dbsnp=(RodBinding name= source=UNBOUND) comp=[] resource=[] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub annotation=[SnpEff] excludeAnnotation=[] group=[] expression=[] useAllAnnotations=false list=false alwaysAppendDbsnpId=false MendelViolationGeno,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:19927,Safety,unsafe,unsafe,19927,"analysis_type=VariantFiltration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:37,Testability,assert,assertion,37,"while the 1kg vcf that generated the assertion error looks like ; ```; ##fileformat=VCFv4.2; ##ApplyRecalibration=""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:1725,Testability,stub,stubs,1725,"y=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_qu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:1810,Testability,stub,stubs,1810,"s=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658
