id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://hail.is/docs/0.2/methods/genetics.html:5620,Performance,cache,cache,5620,"nment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are consequence and tolerate_parse_error which are user-defined parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - Str",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:16650,Performance,perform,performs,16650,"ce]; Calculate call concordance with another dataset. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Note; Requires the dataset to contain only diploid and unphased genotype calls.; Use call() to recode genotype calls or missing() to set genotype; calls to missing. Examples; Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:; >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; This method computes the genotype call concordance (from the entry; field GT) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be diploid and unphased.; It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with “no data”. For example, if a variant is only in one dataset,; then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. No Data (missing variant or filtered en",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:16804,Performance,perform,performs,16804," a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Note; Requires the dataset to contain only diploid and unphased genotype calls.; Use call() to recode genotype calls or missing() to set genotype; calls to missing. Examples; Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:; >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; This method computes the genotype call concordance (from the entry; field GT) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be diploid and unphased.; It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with “no data”. For example, if a variant is only in one dataset,; then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. No Data (missing variant or filtered entry); No Call (missing genotype call); Hom Ref; Heterozygous; Hom Var. The first index is the state in the left dataset and the second index is; the ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:20696,Performance,load,loaded,20696,"e called homozygous variant on the right. Parameters:. left (MatrixTable) – First dataset to compare.; right (MatrixTable) – Second dataset to compare. Returns:; (list of list of int, Table, Table) – The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. hail.methods.filter_intervals(ds, intervals, keep=True)[source]; Filter rows with a list of intervals.; Examples; Filter to loci falling within one interval:; >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:; >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; Based on the keep argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges.; When keep=True, partitions that don’t overlap any supplied interval; will not be loaded at all. This enables filter_intervals() to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters:. ds (MatrixTable or Table) – Dataset to filter.; intervals (ArrayExpression of type tinterval) – Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep (bool) – If True, keep only rows that fall within any interval in intervals.; If False, keep only rows that fall outside all intervals in; intervals. Returns:; MatrixTable or Table. hail.methods.filter_alleles(mt, f)[source]; Filter alternate alleles. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Keep SNPs:; >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:; >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[al",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:20774,Performance,latency,latency,20774,"able) – Second dataset to compare. Returns:; (list of list of int, Table, Table) – The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. hail.methods.filter_intervals(ds, intervals, keep=True)[source]; Filter rows with a list of intervals.; Examples; Filter to loci falling within one interval:; >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:; >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; Based on the keep argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges.; When keep=True, partitions that don’t overlap any supplied interval; will not be loaded at all. This enables filter_intervals() to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters:. ds (MatrixTable or Table) – Dataset to filter.; intervals (ArrayExpression of type tinterval) – Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep (bool) – If True, keep only rows that fall within any interval in intervals.; If False, keep only rows that fall outside all intervals in; intervals. Returns:; MatrixTable or Table. hail.methods.filter_alleles(mt, f)[source]; Filter alternate alleles. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Keep SNPs:; >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:; >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[allele_index - 1] > 0). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC =",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:28766,Performance,load,loadings,28766,",5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In; PLINK/GCTA the denominator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing g",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:29924,Performance,load,loadings,29924,"tics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In; PLINK/GCTA the denominator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The g",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:30598,Performance,load,loadings,30598,"minator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The genetic relationship matrix (GRM) \(G\) encodes genetic correlation; between each pair of samples. It is defined by \(G = MM^T\) where; \(M\) is a standardized version of the genotype matrix, computed as; follows. Let \(C\) be the \(n \times m\) matrix of raw genotypes; in the variant dataset, with rows indexed by \(n\) samples and columns; indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the; number of alternate alleles of variant \(j\) carried by sample; \(i\), which can be 0, 1, 2, or missing. For each variant \(j\),; the sample alternate allele frequency \(p_j\) is computed as half the; mean of the non-missing entries of column \(j\). Entries of \(M\",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:30712,Performance,load,loadings,30712,"ap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The genetic relationship matrix (GRM) \(G\) encodes genetic correlation; between each pair of samples. It is defined by \(G = MM^T\) where; \(M\) is a standardized version of the genotype matrix, computed as; follows. Let \(C\) be the \(n \times m\) matrix of raw genotypes; in the variant dataset, with rows indexed by \(n\) samples and columns; indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the; number of alternate alleles of variant \(j\) carried by sample; \(i\), which can be 0, 1, 2, or missing. For each variant \(j\),; the sample alternate allele frequency \(p_j\) is computed as half the; mean of the non-missing entries of column \(j\). Entries of \(M\); are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\s",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:44461,Performance,queue,queue,44461,"d_prune(). If the dataset contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to ld_prune().; >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient \(r^2\) of any pair at most bp_window_size; base pairs apart is strictly less than r2. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, phase information is; ignored. Variants that do not vary across samples are dropped.; The method prunes variants in linkage disequilibrium in three stages. The first, “local pruning” stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; memory_per_core. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matri",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:44522,Performance,queue,queue,44522,"lit before being passed to ld_prune().; >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient \(r^2\) of any pair at most bp_window_size; base pairs apart is strictly less than r2. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, phase information is; ignored. Variants that do not vary across samples are dropped.; The method prunes variants in linkage disequilibrium in three stages. The first, “local pruning” stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; memory_per_core. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regar",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:45976,Performance,queue,queue,45976,"ocally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regard to memory and Hadoop replication errors. Parameters:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_r",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:47045,Performance,load,loaded,47045,"th higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multialleli",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:47132,Performance,load,load,47132,"th higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multialleli",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:52807,Performance,throughput,throughput,52807,"le, Table, Table, Table). hail.methods.de_novo(mt, pedigree, pop_frequency_prior, *, min_gq=20, min_p=0.05, max_parent_ab=0.05, min_child_ab=0.2, min_dp_ratio=0.1, ignore_in_sample_allele_frequency=False)[source]; Call putative de novo events from trio data. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Call de novo events:; >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree, pop_frequency_prior=priors[dataset.row_key].AF). Notes; This method assumes the GATK high-throughput sequencing fields exist:; GT, AD, DP, GQ, PL.; This method replicates the functionality of Kaitlin Samocha’s de novo; caller. The version; corresponding to git commit bde3e40 is implemented in Hail with her; permission and assistance.; This method produces a Table with the following fields:. locus (locus) – Variant locus.; alleles (array<str>) – Variant alleles.; id (str) – Proband sample ID.; prior (float64) – Site frequency prior. It is the maximum of:; the computed dataset alternate allele frequency, the; pop_frequency_prior parameter, and the global prior; 1 / 3e7. If the ignore_in_sample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:56660,Performance,throughput,throughput,56660,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:58312,Performance,throughput,throughput,58312," in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:; (AB > 0.2). HIGH-quality indel:; (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:; (p > 0.5) AND (AB > 0.3) AND (AC < 10). LOW-quality indel:; (AB > 0.2). Additionally, de novo candidates are not considered if the proband GQ is; smaller than the min_gq parameter, if the proband allele balance is; lower than the min_child_ab parameter, if the depth ratio between the; proband and parents is smaller than the min_depth_ratio parameter, if; the allele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nir",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:60068,Performance,cache,cache,60068,"experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopSc",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:60088,Performance,cache,cache,60088,"experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopSc",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:60464,Performance,cache,cache,60464,"data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: array<str>,; medGenIds: array<s",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:75168,Performance,perform,perform,75168,"eturned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximu",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:79389,Performance,scalab,scalable,79389,"l an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the max_size parameter to skip; groups larger than max_size. Warning; skat() considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which all covariates are defined.; For each row, missing values of x are mean-imputed over these columns.; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; This method provides a scalable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:86800,Performance,throughput,throughput,86800,"_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following cod",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:103172,Performance,cache,cache,103172,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:40964,Safety,avoid,avoid,40964,"result of row_correlation() using; linalg.utils.locus_windows() and; BlockMatrix.sparsify_row_intervals(); in order to only compute linkage disequilibrium between nearby; variants. Use row_correlation() directly to calculate correlation; without windowing.; More precisely, variants are 0-indexed by their order in the matrix table; (see add_row_index()). Each variant is regarded as a vector of; elements defined by entry_expr, typically the number of alternate alleles; or genotype dosage. Missing values are mean-imputed within variant.; The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants \(i\) and \(j\) are on the; same contig and within radius base pairs (inclusive) then the; \((i, j)\) element is their; Pearson correlation coefficient.; Otherwise, the \((i, j)\) element is 0.0.; Rows with a constant value (i.e., zero variance) will result in nan; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; aggregators.stats()).; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that’s; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; See the warnings in row_correlation(). In particular, for large; matrices it may be preferable to run its stages separately.; entry_expr and locus_expr are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Para",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:86270,Safety,avoid,avoids,86270,"(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm.",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:91542,Safety,avoid,avoids,91542," to remap these; values. Here is an example:; >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.annotate_rows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') . The info field AC in data/export.vcf will have Number=1.; New Fields; split_multi_hts() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2. See also; split_multi(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root (str) – Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table – A biallelic variant dataset. hail.methods.summarize_variants(mt, show=True, *, handler=None)[source]; Summarize the variants present in a dataset and print the results.; Examples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants p",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:56604,Security,validat,validation,56604,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:56841,Security,validat,validation,56841,"igned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:; (AB > 0.2). HIGH-quality indel:; (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:; (p > 0.5) AND (AB > 0.3) AND (AC < 10). ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:2660,Testability,log,logistic,2660,"s matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(datase",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:2697,Testability,test,test,2697,"s matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(datase",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:2816,Testability,log,logistic,2816,"x(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:2830,Testability,test,test,2830,"x(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:3392,Testability,test,test,3392,"e, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when gen",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:25877,Testability,log,log,25877,"r>) – The old alleles, before filtering and; computing the minimal representation.; old_to_new (array<int32>) – An array that maps old allele index to; new allele index. Its length is the same as old_alleles. Alleles that; are filtered are missing.; new_to_old (array<int32>) – An array that maps new allele index to; the old allele index. Its length is the same as the modified alleles; field. Downcode algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; split_multi_hts().; The downcode algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Subset algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference alle",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:46563,Testability,test,tested,46563,"ers:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:59109,Testability,test,tested,59109,"ele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotatio",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:68036,Testability,log,logistic,68036,"r of SNP alternate alleles.; n_insertion (int64) – Number of insertion alternate alleles.; n_deletion (int64) – Number of deletion alternate alleles.; n_singleton (int64) – Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; n_transition (int64) – Number of transition (A-G, C-T) alternate alleles.; n_transversion (int64) – Number of transversion alternate alleles.; n_star (int64) – Number of star (upstream deletion) alleles.; r_ti_tv (float64) – Transition/Transversion ratio.; r_het_hom_var (float64) – Het/HomVar call ratio.; r_insertion_deletion (float64) – Insertion/Deletion allele ratio. Missing values NA may result from division by zero. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable – Dataset with a new column-indexed field name. hail.methods._logistic_skat(group, weight, y, x, covariates, max_size=46340, null_max_iterations=25, null_tolerance=1e-06, accuracy=1e-06, iterations=10000)[source]; The logistic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null mode",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:68073,Testability,test,test,68073,"r of SNP alternate alleles.; n_insertion (int64) – Number of insertion alternate alleles.; n_deletion (int64) – Number of deletion alternate alleles.; n_singleton (int64) – Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; n_transition (int64) – Number of transition (A-G, C-T) alternate alleles.; n_transversion (int64) – Number of transversion alternate alleles.; n_star (int64) – Number of star (upstream deletion) alleles.; r_ti_tv (float64) – Transition/Transversion ratio.; r_het_hom_var (float64) – Het/HomVar call ratio.; r_insertion_deletion (float64) – Insertion/Deletion allele ratio. Missing values NA may result from division by zero. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable – Dataset with a new column-indexed field name. hail.methods._logistic_skat(group, weight, y, x, covariates, max_size=46340, null_max_iterations=25, null_tolerance=1e-06, accuracy=1e-06, iterations=10000)[source]; The logistic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null mode",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:68101,Testability,test,tests,68101," n_singleton (int64) – Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; n_transition (int64) – Number of transition (A-G, C-T) alternate alleles.; n_transversion (int64) – Number of transversion alternate alleles.; n_star (int64) – Number of star (upstream deletion) alleles.; r_ti_tv (float64) – Transition/Transversion ratio.; r_het_hom_var (float64) – Het/HomVar call ratio.; r_insertion_deletion (float64) – Insertion/Deletion allele ratio. Missing values NA may result from division by zero. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable – Dataset with a new column-indexed field name. hail.methods._logistic_skat(group, weight, y, x, covariates, max_size=46340, null_max_iterations=25, null_tolerance=1e-06, accuracy=1e-06, iterations=10000)[source]; The logistic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textr",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:68391,Testability,log,logit,68391,"t64) – Number of transversion alternate alleles.; n_star (int64) – Number of star (upstream deletion) alleles.; r_ti_tv (float64) – Transition/Transversion ratio.; r_het_hom_var (float64) – Het/HomVar call ratio.; r_insertion_deletion (float64) – Insertion/Deletion allele ratio. Missing values NA may result from division by zero. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable – Dataset with a new column-indexed field name. hail.methods._logistic_skat(group, weight, y, x, covariates, max_size=46340, null_max_iterations=25, null_tolerance=1e-06, accuracy=1e-06, iterations=10000)[source]; The logistic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}\]; Therefore \(r_i\), the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also no",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:68495,Testability,test,tests,68495," r_het_hom_var (float64) – Het/HomVar call ratio.; r_insertion_deletion (float64) – Insertion/Deletion allele ratio. Missing values NA may result from division by zero. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable – Dataset with a new column-indexed field name. hail.methods._logistic_skat(group, weight, y, x, covariates, max_size=46340, null_max_iterations=25, null_tolerance=1e-06, accuracy=1e-06, iterations=10000)[source]; The logistic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}\]; Therefore \(r_i\), the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. Each sample’s phenotype is Bernoulli distributed with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a sy",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:69022,Testability,log,logit,69022,"stic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}\]; Therefore \(r_i\), the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. Each sample’s phenotype is Bernoulli distributed with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:69134,Testability,log,logit,69134,") covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}\]; Therefore \(r_i\), the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. Each sample’s phenotype is Bernoulli distributed with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:71639,Testability,test,test,71639,"egin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}\]; The null hypothesis test tests for the probability of observing even larger values of \(Q\).; The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. Rare-variant association testing for; sequencing data with the sequence kernel association test. Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; Generate a dataset with a phenotype noisily computed from the genotypes:; >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:; >>> skat = hl._",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:71644,Testability,test,tests,71644,"egin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}\]; The null hypothesis test tests for the probability of observing even larger values of \(Q\).; The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. Rare-variant association testing for; sequencing data with the sequence kernel association test. Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; Generate a dataset with a phenotype noisily computed from the genotypes:; >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:; >>> skat = hl._",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:71829,Testability,test,testing,71829,"} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}\]; The null hypothesis test tests for the probability of observing even larger values of \(Q\).; The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. Rare-variant association testing for; sequencing data with the sequence kernel association test. Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; Generate a dataset with a phenotype noisily computed from the genotypes:; >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:71895,Testability,test,test,71895,"} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}\]; The null hypothesis test tests for the probability of observing even larger values of \(Q\).; The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. Rare-variant association testing for; sequencing data with the sequence kernel association test. Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; Generate a dataset with a phenotype noisily computed from the genotypes:; >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:73066,Testability,test,test,73066,"t with a phenotype noisily computed from the genotypes:; >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76027,Testability,test,test,76027,"; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault fl",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76215,Testability,log,logistic,76215,"ple size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the resid",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76293,Testability,log,logisitic,76293,"AT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covar",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76869,Testability,test,test,76869,"t term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:77684,Testability,log,log-likelihood,77684," parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[source]; Test each keyed group of rows for association by linear or logistic; SKAT test.; Examples; Test each gene for association using the linear sequence kernel association; test:; >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). Caution; By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.;",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:77924,Testability,log,logistic,77924," tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[source]; Test each keyed group of rows for association by linear or logistic; SKAT test.; Examples; Test each gene for association using the linear sequence kernel association; test:; >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). Caution; By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this c",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:78059,Testability,log,logistic,78059," tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[source]; Test each keyed group of rows for association by linear or logistic; SKAT test.; Examples; Test each gene for association using the linear sequence kernel association; test:; >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). Caution; By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this c",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:78074,Testability,test,test,78074," tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[source]; Test each keyed group of rows for association by linear or logistic; SKAT test.; Examples; Test each gene for association using the linear sequence kernel association; test:; >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). Caution; By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this c",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:78168,Testability,test,test,78168,"ssing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[source]; Test each keyed group of rows for association by linear or logistic; SKAT test.; Examples; Test each gene for association using the linear sequence kernel association; test:; >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). Caution; By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the max_size parameter to skip; groups larger than max_size. Warning; skat() considers the same set of columns (i.e., samples, points) for; every gro",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:79452,Testability,test,test,79452,"l an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the max_size parameter to skip; groups larger than max_size. Warning; skat() considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which all covariates are defined.; For each row, missing values of x are mean-imputed over these columns.; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; This method provides a scalable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:79991,Testability,log,logistic,79991,"iders the same set of columns (i.e., samples, points) for; every group, namely those columns for which all covariates are defined.; For each row, missing values of x are mean-imputed over these columns.; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; This method provides a scalable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:80836,Testability,log,logistic,80836," allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing valu",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81033,Testability,log,logistic,81033,"nse y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Ex",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81746,Testability,log,logistic,81746,"2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82075,Testability,log,logistic,82075,"s; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower a",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82144,Testability,test,test,82144,"s; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower a",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82167,Testability,log,logistic,82167,"ning the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). ha",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82176,Testability,test,test,82176,"ning the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). ha",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82203,Testability,log,logistic,82203,"ning the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). ha",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82290,Testability,log,logistic,82290,"e; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82299,Testability,test,test,82299,"e; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82466,Testability,test,test,82466,"d parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. Warning; In order to support a wide variety of data types, this function splits only; the variants on a Ma",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82878,Testability,test,tested,82878,"erty.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. Warning; In order to support a wide variety of data types, this function splits only; the variants on a MatrixTable, but not the genotypes. Use; split_multi_hts() if possible, or split the genotypes yourself using; one of the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:86994,Testability,log,logic,86994,"gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:93662,Testability,test,test,93662,"===========; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ==============================. Parameters:. mt (MatrixTable or Table) – Matrix table with a variant (locus / alleles) row key.; show (bool) – If True, print results instead of returning them.; handler. Notes; The result returned if show is False is a Struct with; five fields:. n_variants (int): Number of variants present in the matrix table.; allele_types (dict [str, int]): Number of alternate alleles in; each allele allele category.; contigs (dict [str, int]): Number of variants on each contig.; allele_counts (dict [int, int]): Number of variants broken down; by number of alleles (biallelic is 2, for example).; r_ti_tv (float): Ratio of transition alternate alleles to; transversion alternate alleles. Returns:; None or Struct – Returns None if show is True, or returns results as a struct. hail.methods.transmission_disequilibrium_test(dataset, pedigree)[source]; Performs the transmission disequilibrium test on trios. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Compute TDT association statistics and show the first two results:; >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) ; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+----------+----------+; | 1:246714629 | [""C"",""A""] | 0 | 4 ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:94956,Testability,test,test,94956,"remove; them. Examples; Compute TDT association statistics and show the first two results:; >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) ; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+----------+----------+; | 1:246714629 | [""C"",""A""] | 0 | 4 | 4.00e+00 | 4.55e-02 |; | 2:167262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which ma",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:100428,Testability,test,test,100428," Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; AC / AN.; AC (array<int32>) – Calculated allele count, one element per; allele, including the reference. Sums to AN.; AN (int32) – Total number of called alleles.; homozygote_count (array<int32>) – Number of homozygotes per; allele. One element per allele, including the reference.; call_rate (float64) – Fraction of calls neither missing nor filtered.; Equivalent to n_called / count_cols().; n_called (int64) – Number of samples with a defined GT.; n_not_called (int64) – Number of samples with a missing GT.; n_filtered (int64) – Number of filtered entries.; n_het (int64) – Number of heterozygous samples.; n_non_ref (int64) – Number of samples with at least one called; non-reference allele.; het_freq_hwe (float64) – Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; functions.hardy_weinberg_test() for details.; p_value_hwe (float64) – p-value from two-sided test of Hardy-Weinberg; equilibrium. See functions.hardy_weinberg_test() for details.; p_value_excess_het (float64) – p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Vari",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:100569,Testability,test,test,100569,"lele count, one element per; allele, including the reference. Sums to AN.; AN (int32) – Total number of called alleles.; homozygote_count (array<int32>) – Number of homozygotes per; allele. One element per allele, including the reference.; call_rate (float64) – Fraction of calls neither missing nor filtered.; Equivalent to n_called / count_cols().; n_called (int64) – Number of samples with a defined GT.; n_not_called (int64) – Number of samples with a missing GT.; n_filtered (int64) – Number of filtered entries.; n_het (int64) – Number of heterozygous samples.; n_non_ref (int64) – Number of samples with at least one called; non-reference allele.; het_freq_hwe (float64) – Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; functions.hardy_weinberg_test() for details.; p_value_hwe (float64) – p-value from two-sided test of Hardy-Weinberg; equilibrium. See functions.hardy_weinberg_test() for details.; p_value_excess_het (float64) – p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(d",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:100844,Testability,test,test,100844," / count_cols().; n_called (int64) – Number of samples with a defined GT.; n_not_called (int64) – Number of samples with a missing GT.; n_filtered (int64) – Number of filtered entries.; n_het (int64) – Number of heterozygous samples.; n_non_ref (int64) – Number of samples with at least one called; non-reference allele.; het_freq_hwe (float64) – Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; functions.hardy_weinberg_test() for details.; p_value_hwe (float64) – p-value from two-sided test of Hardy-Weinberg; equilibrium. See functions.hardy_weinberg_test() for details.; p_value_excess_het (float64) – p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP).",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:29298,Usability,simpl,simply,29298,"ion, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In; PLINK/GCTA the denominator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:33960,Usability,simpl,simply,33960,"_expr)[source]; Computes the realized relationship matrix (RRM).; Examples; >>> rrm = hl.realized_relationship_matrix(dataset.GT). Notes; The realized relationship matrix (RRM) is defined as follows. Consider the; \(n \times m\) matrix \(C\) of raw genotypes, with rows indexed by; \(n\) samples and columns indexed by the \(m\) bialellic autosomal; variants; \(C_{ij}\) is the number of alternate alleles of variant; \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For; each variant \(j\), the sample alternate allele frequency \(p_j\) is; computed as half the mean of the non-missing entries of column \(j\).; Entries of \(M\) are then mean-centered and variance-normalized as. \[M_{ij} =; \frac{C_{ij}-2p_j}; {\sqrt{\frac{m}{n} \sum_{k=1}^n (C_{ij}-2p_j)^2}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype; imputation). This scaling normalizes each variant column to have empirical; variance \(1/m\), which gives each sample row approximately unit total; variance (assuming linkage equilibrium) and yields the \(n \times n\); sample correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the realized relationship matrix and; the genetic relatedness matrix (GRM) used in; realized_relationship_matrix() is the variant (column) normalization:; where RRM uses empirical variance, GRM uses expected variance under; Hardy-Weinberg Equilibrium.; This method drops variants with zero variance before computing kinship. Parameters:; call_expr (CallExpression) – Entry-indexed call expression on matrix table with columns corresponding; to samples. Returns:; BlockMatrix – Realized relationship matrix for all samples. Row and column indices; correspond to matrix table column index. hail.methods.impute_sex(call, aaf_threshold=0.0, include_par=False, female_threshold=0.2, male_threshold=0.8, aaf=None)[source]; Impute sex of samples by calculating inbreeding coefficient on the; X chromosome. Note; R",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:54360,Usability,simpl,simplifying,54360,"ample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry (struct) – Father entry fields from mt.; proband_entry (struct) – Mother entry fields from mt.; is_female (bool) – True if proband is female.; p_de_novo (float64) – Unfiltered posterior probability; that the event is de novo rather than a missed heterozygous; event in a parent.; confidence (str) Validation confidence. One of: 'HIGH',; 'MEDIUM', 'LOW'. The key of the table is ['locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:70507,Usability,simpl,simplify,70507," with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z^T Z\). In the expressions below, we elide transpositions; of symmetric matrices:. \[\begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}\]; Before substituting the definition of \(P_0\), simplify it using the reduced QR; decomposition:. \[\begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:70924,Usability,simpl,simplified,70924,"stribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z^T Z\). In the expressions below, we elide transpositions; of symmetric matrices:. \[\begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}\]; Before substituting the definition of \(P_0\), simplify it using the reduced QR; decomposition:. \[\begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}\]; The null hypothesis test tests for the probability of observing even larger values of \(Q\).; The SKAT method was originally described in:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/impex.html:2848,Availability,toler,tolerance,2848," data in textual representations into a Hail; MatrixTable. Finally, it is possible to create a Hail Table; from a pandas DataFrame with Table.from_pandas(). import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_lines(paths[, min_partitions, ...]); Import lines of file(s) as a Table of strings. Genetics; Hail has several functions to import genetics-specific file formats into Hail; MatrixTable or Table objects:. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_bed(path[, reference_genome, ...]); Import a UCSC BED file as a Table. import_bgen(path, entry_fields[, ...]); Import BGEN file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_fam(path[, quant_pheno, delimiter, ...]); Import a PLINK FAM file into a Table. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. Export. General purpose; Some of the most widely-used export functionality is found as class methods; on the Table and Expression objects:. Table.export(): Used to write a Table to a text table (TSV).; Expression.export(): Used to write an expression to a text file. For; one-dimensional expressions (table row fields, matrix table row or column fields),; this is very similar to Table.export(). For two-dimensional expressions; (entry expressions on matrix tables), a text matrix representation that can be; imported with import_matrix_table() will be produced.; Table.to_pandas(): Used to convert a Hail table to a pandas; DataFrame. Genetics; Hail can export to some of the genetics-specific file formats:. export_vcf(dataset, output[, ...]); Export a MatrixTable ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:15405,Availability,error,error,15405,"p glob patterns in file names.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. hail.methods.import_fam(path, quant_pheno=False, delimiter='\\\\s+', missing='NA')[source]; Import a PLINK FAM file into a Table.; Examples; Import a tab-separated; FAM file; with a case-control phenotype:; >>> fam_kt = hl.import_fam('data/case_control_study.fam'). Import a FAM file with a quantitative phenotype:; >>> fam_kt = hl.import_fam('data/quantitative_study.fam', quant_pheno=True). Notes; In Hail, unlike PLINK, the user must explicitly distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype with quant_pheno=False will return an error; (unless all values happen to be 0, 1, 2, or -9):; The resulting Table will have fields, types, and values that are interpreted as missing. fam_id (tstr) – Family ID (missing = “0”); id (tstr) – Sample ID (key column); pat_id (tstr) – Paternal ID (missing = “0”); mat_id (tstr) – Maternal ID (missing = “0”); is_female (tstr) – Sex (missing = “NA”, “-9”, “0”). One of:. is_case (tbool) – Case-control phenotype (missing = “0”, “-9”,; non-numeric or the missing argument, if given.; quant_pheno (tfloat64) – Quantitative phenotype (missing = “NA” or; the missing argument, if given. Warning; Hail will interpret the value “-9” as a valid quantitative phenotype, which; differs from default PLINK behavior. Use missing='-9' to interpret this; value as missing. Parameters:. path (str) – Path to FAM file.; quant_pheno (bool) – If True, phenotype is interpreted as quantitative.; delimiter (str) – Field delimiter regex.; missing (str) – The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also t",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:16524,Availability,toler,tolerance,16524,"alues that are interpreted as missing. fam_id (tstr) – Family ID (missing = “0”); id (tstr) – Sample ID (key column); pat_id (tstr) – Paternal ID (missing = “0”); mat_id (tstr) – Maternal ID (missing = “0”); is_female (tstr) – Sex (missing = “NA”, “-9”, “0”). One of:. is_case (tbool) – Case-control phenotype (missing = “0”, “-9”,; non-numeric or the missing argument, if given.; quant_pheno (tfloat64) – Quantitative phenotype (missing = “NA” or; the missing argument, if given. Warning; Hail will interpret the value “-9” as a valid quantitative phenotype, which; differs from default PLINK behavior. Use missing='-9' to interpret this; value as missing. Parameters:. path (str) – Path to FAM file.; quant_pheno (bool) – If True, phenotype is interpreted as quantitative.; delimiter (str) – Field delimiter regex.; missing (str) – The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also treated as missing. Returns:; Table. hail.methods.import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None, reference_genome='default', contig_recoding=None, skip_invalid_loci=False)[source]; Import GEN file(s) as a MatrixTable.; Examples; >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; For more information on the GEN file format, see here.; If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the chromosome parameter.; To load multiple files at the same time, use Hadoop Glob Patterns.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file. Row Fields. locus (tlocus or tstruct) – Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:18424,Availability,toler,tolerance,18424,"n by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input [0.98, 0.0, 0.0] will be normalized to; [1.0, 0.0, 0.0]. Parameters:. path (str or list of str) – GEN files to import.; sample_file (str) – Sample file to import.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions (int, optional) – Number of partitions.; chromosome (str, optional) – Chromosome if not included in the GEN file; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:18702,Availability,toler,tolerance,18702,"riant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input [0.98, 0.0, 0.0] will be normalized to; [1.0, 0.0, 0.0]. Parameters:. path (str or list of str) – GEN files to import.; sample_file (str) – Sample file to import.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions (int, optional) – Number of partitions.; chromosome (str, optional) – Chromosome if not included in the GEN file; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs)[source]; Import a locus interval list as a Table.; Examples; Add the row field capture_region indicating inclusion in; at least one locus interval from capture_intervals.txt:; >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_gen",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:18812,Availability,toler,tolerance,18812,"he first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input [0.98, 0.0, 0.0] will be normalized to; [1.0, 0.0, 0.0]. Parameters:. path (str or list of str) – GEN files to import.; sample_file (str) – Sample file to import.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions (int, optional) – Number of partitions.; chromosome (str, optional) – Chromosome if not included in the GEN file; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs)[source]; Import a locus interval list as a Table.; Examples; Add the row field capture_region indicating inclusion in; at least one locus interval from capture_intervals.txt:; >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_def",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:35717,Availability,error,error,35717," that a chromosome field will be of type tstr. Setting; impute=True and types={'Chromosome': hl.tstr} solves this problem. Parameters:. paths (str or list of str) – Files to import.; key (str or list of str) – Key fields(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – If True`, assume the file has no header and name the N fields f0,; f1, … fN (0-indexed).; impute (bool) – If True, Impute field types from the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Re",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:45024,Availability,down,downstream,45024," and the alternate alleles (ALT field) are; the subsequent elements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elem",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:50222,Availability,down,downstream,50222,"will be; determined from their corresponding Hail types. To output a desired; Description, Number, and/or Type value in a FORMAT or INFO field or to; specify FILTER lines, use the metadata parameter to supply a dictionary; with the relevant information. See; get_vcf_metadata() for how to obtain the; dictionary corresponding to the original VCF, and for info on how this; dictionary should be structured.; The output VCF header will also contain CONTIG lines; with ID, length, and assembly fields derived from the reference genome of; the dataset.; The output VCF header will not contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; append_to_header parameter. Warning; INFO fields stored at VCF import are not automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating info, downstream; tools which may produce erroneous results. The solution is to create new; fields in info or overwrite existing fields. For example, in order to; produce an accurate AC field, one can run variant_qc() and copy; the variant_qc.AC field to info.AC as shown below.; >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the head",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:50767,Deployability,pipeline,pipeline,50767,"ence genome of; the dataset.; The output VCF header will not contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; append_to_header parameter. Warning; INFO fields stored at VCF import are not automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating info, downstream; tools which may produce erroneous results. The solution is to create new; fields in info or overwrite existing fields. For example, in order to; produce an accurate AC field, one can run variant_qc() and copy; the variant_qc.AC field to info.AC as shown below.; >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:61482,Deployability,update,updated,61482,"d False is output as '1'. The; default and missing values are 'NA'.; varid (StringExpression, optional) – Expression for the variant ID (2nd column of the BIM file). The default; value is hl.delimit([dataset.locus.contig, hl.str(dataset.locus.position), dataset.alleles[0], dataset.alleles[1]], ':'); cm_position (Float64Expression, optional) – Expression for the 3rd column of the BIM file (position in centimorgans).; The default value is 0.0. The missing value is 0.0. hail.methods.get_vcf_metadata(path)[source]; Extract metadata from VCF header.; Examples; >>> hl.get_vcf_metadata('data/example2.vcf.bgz') ; {'filter': {'LowQual': {'Description': ''}, ...},; 'format': {'AD': {'Description': 'Allelic depths for the ref and alt alleles in the order listed',; 'Number': 'R',; 'Type': 'Integer'}, ...},; 'info': {'AC': {'Description': 'Allele count in genotypes, for each ALT allele, in the same order as listed',; 'Number': 'A',; 'Type': 'Integer'}, ...}}. Notes; This method parses the VCF header to extract the ID, Number,; Type, and Description fields from FORMAT and INFO lines as; well as ID and Description for FILTER lines. For example,; given the following header lines:; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">. The resulting Python dictionary returned would be; metadata = {'filter': {'LowQual': {'Description': 'Low quality'}},; 'format': {'DP': {'Description': 'Read Depth',; 'Number': '1',; 'Type': 'Integer'}},; 'info': {'MQ': {'Description': 'RMS Mapping Quality',; 'Number': '1',; 'Type': 'Float'}}}. which can be used with export_vcf() to fill in the relevant fields in the header. Parameters:; path (str) – VCF file(s) to read. If more than one file is given, the first; file is used. Returns:; dict of str to (dict of str to (dict of str to str)). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:30158,Integrability,depend,dependent,30158,"enotype, which; differs from default PLINK behavior. Use missing='-9' to interpret this; value as missing. Parameters:. bed (str) – PLINK BED file.; bim (str) – PLINK BIM file.; fam (str) – PLINK FAM file.; min_partitions (int, optional) – Minimum number of partitions. Useful in conjunction with block_size.; missing (str) – String used to denote missing values only for the phenotype field.; This is in addition to “-9”, “0”, and “N/A” for case-control; phenotypes.; delimiter (str) – FAM file field delimiter regex.; quant_pheno (bool) – If True, FAM phenotype is interpreted as quantitative.; a2_reference (bool) – If True, A2 is treated as the reference allele. If False, A1 is treated; as the reference allele.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome. If None, the; default is dependent on the reference_genome. For “GRCh37”, the default; is {'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}. For “GRCh38”, the; default is {'1': 'chr1', ..., '22': 'chr22', '23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'}.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; n_partitions (int, optional) – Number of partitions. If both n_partitions and block_size; are specified, n_partitions will be used.; block_size (int, optional) – Block size, in MB. Default: 128MB blocks. Returns:; MatrixTable. hail.methods.import_table(paths, key=None, min_partitions=None, impute=False, no_header=False, comment=(), delimiter='\t', missing='NA', types={}, quote=None, skip_blank_lines=False, force_bgz=False, filter=None, find_replace=None, force=False, source_file_field=None)[source]; Import delimited text file (text table) as Table.; The resulting Table will have no key fields. Use; Table.key_by() to specify keys. See also:; import_matrix_table().; Ex",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:51602,Integrability,interface,interface,51602,"= ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, outp",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:6133,Modifiability,parameteriz,parameterized,6133," UCSC BED file as a Table.; Examples; The file formats are; $ cat data/file1.bed; track name=""BedTest""; 20 1 14000000; 20 17000000 18000000; ... $ cat file2.bed; track name=""BedTest""; 20 1 14000000 cnv1; 20 17000000 18000000 cnv2; ... Add the row field cnv_region indicating inclusion in; at least one interval of the three-column BED file:; >>> bed = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(cnv_region = hl.is_defined(bed[dataset.locus])). Add a row field cnv_id with the value given by the; fourth column of a BED file:; >>> bed = hl.import_bed('data/file2.bed'); >>> result = dataset.annotate_rows(cnv_id = bed[dataset.locus].target). Notes; The table produced by this method has one of two possible structures. If; the .bed file has only three fields (chrom, chromStart, and; chromEnd), then the produced table has only one column:. interval (tinterval) - Row key. Genomic interval. If; reference_genome is defined, the point type of the interval will be; tlocus parameterized by the reference_genome. Otherwise,; the point type is a tstruct with two fields: contig with; type tstr and position with type tint32. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. interval (tinterval) - Row key. Genomic interval. Same schema as above.; target (tstr) - Fourth column of .bed file. UCSC bed files can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; Intervals in UCSC BED files are 0-indexed and half open.; The line “5 100 105” correpsonds to the interval [5:101-5:106) in Hail’s; 1-indexed notation. Details; here. Parameters:. path (str) – Path to .bed file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recodi",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:10452,Modifiability,parameteriz,parameterized,10452,"gen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. GT (tcall) – The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; GP (tarray o",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:17522,Modifiability,parameteriz,parameterized,17522,"le_file=None, tolerance=0.2, min_partitions=None, chromosome=None, reference_genome='default', contig_recoding=None, skip_invalid_loci=False)[source]; Import GEN file(s) as a MatrixTable.; Examples; >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; For more information on the GEN file format, see here.; If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the chromosome parameter.; To load multiple files at the same time, use Hadoop Glob Patterns.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file. Row Fields. locus (tlocus or tstruct) – Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:20289,Modifiability,parameteriz,parameterized,20289,"rue, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs)[source]; Import a locus interval list as a Table.; Examples; Add the row field capture_region indicating inclusion in; at least one locus interval from capture_intervals.txt:; >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. contig:start-end; contig  start  end (tab-separated); contig  start  end  direction  target (tab-separated). A file in either of the first two formats produces a table with one; field:. interval (tinterval) - Row key. Genomic interval. If; reference_genome is defined, the point type of the interval will be; tlocus parameterized by the reference_genome. Otherwise,; the point type is a tstruct with two fields: contig with; type tstr and position with type tint32. A file in the third format (with a “target” column) produces a table with two; fields:. interval (tinterval) - Row key. Same schema as above.; target (tstr). If reference_genome is defined AND the file has one field, intervals; are parsed with parse_locus_interval(). See the documentation for; valid inputs.; If reference_genome is NOT defined and the file has one field,; intervals are parsed with the regex `""([^:]*):(\d+)\-(\d+)""; where contig, start, and end match each of the three capture groups.; start and end match positions inclusively, e.g.; start <= position <= end.; For files with three or five fields, start and end match positions; inclusively, e.g. start <= position <= end. Parameters:. path (str) – Path to file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_in",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:27750,Modifiability,parameteriz,parameterized,27750,"; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the --make-bed option.; Hail uses the individual ID (column 2 in FAM file) as the sample id (s).; The individual IDs must be unique.; The resulting MatrixTable has the following fields:. Row fields:. locus (tlocus or tstruct) – Row key. The; chromosome and position. If reference_genome is defined, the type; will be tlocus parameterized by reference_genome.; Otherwise, the type will be a tstruct with two fields:; contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference allele (A2; if a2_reference is True) is the first element in the array.; rsid (tstr) – Column 2 in the BIM file.; cm_position (tfloat64) – Column 3 in the BIM file,; the position in centimorgans. Column fields:. s (tstr) – Column 2 in the Fam file (key field).; fam_id (tstr) – Column 1 in the FAM file. Set to; missing if ID equals “0”.; pat_id (tstr) – Column 3 in the FAM file. Set to; missing if ID equals “0”.; mat_id (tstr) – Column 4 in the FAM file. Set to; missing if ID equals “0”.; is_female (tstr) – Column 5 in the FAM file. Set to; missing if value equals “-9”, “0”, or “N/A”. Set to true if value; equals “2”. Set to false if value equals “1”.; is_case (tbool) – Column 6 in the FAM file. Only; present if quant_pheno equals Fa",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:43763,Modifiability,parameteriz,parameterized,43763,"y; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the; filters row field. This annotation is a set<str> and can be; queried for filter membership with expressions like; ds.filters.contains(""VQSRTranche99.5...""). Variants that are flagged; as “PASS” will have no filters applied; for these variants,; hl.len(ds.filters) is 0. Thus, filtering to PASS variants; can be done with MatrixTable.filter_rows() as follows:; >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). Column Fields. s (tstr) – Column key. This is the sample ID. Row Fields. locus (tlocus or tstruct) – Row key. The; chromosome (CHROM field) and position (POS field). If reference_genome; is defined, the type will be tlocus parameterized by; reference_genome. Otherwise, the type will be a tstruct with; two fields: contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (REF field) is; the first element in the array and the alternate alleles (ALT field) are; the subsequent elements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and oth",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:51737,Modifiability,config,config,51737," being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, output; + '.bgen' will be a directory containing many BGEN files. In either case, the SAMPLE file is; written to output + '.sample'. For",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:52197,Modifiability,config,config,52197,"(one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, output; + '.bgen' will be a directory containing many BGEN files. In either case, the SAMPLE file is; written to output + '.sample'. For example,; >>> hl.export_bgen(mt, '/path/to/dataset') . Will write two files: /path/to/dataset.bgen and /path/to/dataset.sample. In contrast,; >>> hl.export_bgen(mt, '/path/to/dataset', parallel='header_per_shard') . Will create /path/to/dataset.sample and will create mt.n_partitions() files into the; directory /path/to/dataset.bgen/.; Notes; The export_bgen() function requires genotype probabilities, either as an entry; field of mt (of t",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:905,Performance,perform,performance,905,"﻿. Hail | ; Import / Export. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Import / Export. View page source. Import / Export. This page describes functionality for moving data in and out of Hail.; Hail has a suite of functionality for importing and exporting data to and from; general-purpose, genetics-specific, and high-performance native file formats. Native file formats; When saving data to disk with the intent to later use Hail, we highly recommend; that you use the native file formats to store Table and; MatrixTable objects. These binary formats not only smaller than other formats; (especially textual ones) in most cases, but also are significantly faster to; read into Hail later.; These files can be created with methods on the Table and; MatrixTable objects:. Table.write(); MatrixTable.write(). These files can be read into a Hail session later using the following methods:. read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Import. General purpose; The import_table() function is widely-used to import textual data; into a Hail Table. import_matrix_table() is used to import; two-dimensional matrix data in textual representations into a Hail; MatrixTable. Finally, it is possible to create a Hail Table; from a pandas DataFrame with Table.from_pandas(). import",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:7188,Performance,load,loaded,7188,"the reference_genome. Otherwise,; the point type is a tstruct with two fields: contig with; type tstr and position with type tint32. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. interval (tinterval) - Row key. Genomic interval. Same schema as above.; target (tstr) - Fourth column of .bed file. UCSC bed files can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; Intervals in UCSC BED files are 0-indexed and half open.; The line “5 100 105” correpsonds to the interval [5:101-5:106) in Hail’s; 1-indexed notation. Details; here. Parameters:. path (str) – Path to .bed file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding (dict of (str, str)) – Mapping from contig name in BED to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; **kwargs – Additional optional arguments to import_table() are valid arguments here except:; no_header, delimiter, impute, skip_blank_lines, types, and comment as these; are used by import_bed. Returns:; Table – Interval-keyed table. hail.methods.import_bgen(path, entry_fields, sample_file=None, n_partitions=None, block_size=None, index_file_map=None, variants=None, _row_fields=['varid', 'rsid'])[source]; Import BGEN file(s) as a MatrixTable.; Examples; Import a BGEN file as a matrix table with GT and GP entry fields:; >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:; >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:9516,Performance,load,load,9516,"e(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by ‘locus’ and ‘alleles’ from a BGEN file:; >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; Hail supports importing data from v1.2 of the BGEN file format.; Genotypes must be unphased and diploid, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic.; Each BGEN file must have a corresponding index file, which can be generated; with index_bgen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: con",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:10158,Performance,perform,performance,10158," genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic.; Each BGEN file must have a corresponding index file, which can be generated; with index_bgen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; acce",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:11018,Performance,perform,performance,11018,"ocus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. GT (tcall) – The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the BGEN file spec. For bi-allelic variants, the array has; three elements giving the probabilities of homozygous reference,; heterozygous, and homozygous alternate genotype, in that order.; dosage (tfloat64) – The expected value of the number of; alternate alleles, given by the probability of heterozygous genotype plus; twice the probability of homozygous alternate genotype. All variants must; be bi-allelic. See also; index_bgen(). Parameters:. path (str or list of str) – BGEN file(s) to read.; entry_fields (list of str) – List of entry fields to cre",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:17075,Performance,load,load,17075,"K behavior. Use missing='-9' to interpret this; value as missing. Parameters:. path (str) – Path to FAM file.; quant_pheno (bool) – If True, phenotype is interpreted as quantitative.; delimiter (str) – Field delimiter regex.; missing (str) – The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also treated as missing. Returns:; Table. hail.methods.import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None, reference_genome='default', contig_recoding=None, skip_invalid_loci=False)[source]; Import GEN file(s) as a MatrixTable.; Examples; >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; For more information on the GEN file format, see here.; If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the chromosome parameter.; To load multiple files at the same time, use Hadoop Glob Patterns.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file. Row Fields. locus (tlocus or tstruct) – Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:21496,Performance,load,loaded,21496,"d format (with a “target” column) produces a table with two; fields:. interval (tinterval) - Row key. Same schema as above.; target (tstr). If reference_genome is defined AND the file has one field, intervals; are parsed with parse_locus_interval(). See the documentation for; valid inputs.; If reference_genome is NOT defined and the file has one field,; intervals are parsed with the regex `""([^:]*):(\d+)\-(\d+)""; where contig, start, and end match each of the three capture groups.; start and end match positions inclusively, e.g.; start <= position <= end.; For files with three or five fields, start and end match positions; inclusively, e.g. start <= position <= end. Parameters:. path (str) – Path to file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding (dict of (str, str)) – Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; **kwargs – Additional optional arguments to import_table() are valid; arguments here except: no_header, comment, impute, and; types, as these are used by import_locus_intervals(). Returns:; Table – Interval-keyed table. hail.methods.import_matrix_table(paths, row_fields={}, row_key=[], entry_type=dtype('int32'), missing='NA', min_partitions=None, no_header=False, force_bgz=False, sep=None, delimiter=None, comment=())[source]; Import tab-delimited file(s) as a MatrixTable.; Examples; Consider the following file containing counts from a RNA sequencing; dataset:; $ cat data/matrix1.tsv; Barcode Tissue Days GENE1 GENE2 GENE3 GENE4; TTAGCCA brain 1.0 0 0 1 0; ATCACTT kidney 5.5 3 0 2 0; CTCTTCT kidney 2.5 0 0 0 1; CTATATA brain 7.0 0 0 3 0. The field Days contains floating-point numbers and each of the ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:26219,Performance,load,load,26219,"x.); The header information for row fields is allowed to be missing, if the; column IDs are present, but the header must then consist only of tab-delimited; column IDs (no row field names).; The column IDs will never be missing, even if the missing string appears; in the column IDs. Parameters:. paths (str or list of str) – Files to import.; row_fields (dict of str to HailType) – Columns to take as row fields in the MatrixTable. They must be located; before all entry columns.; row_key (str or list of str) – Key fields(s). If empty, creates an index row_id to use as key.; entry_type (HailType) – Type of entries in matrix table. Must be one of: tint32,; tint64, tfloat32, tfloat64, or; tstr. Default: tint32.; missing (str) – Identifier to be treated as missing. Default: NA; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – If True, assume the file has no header and name the row fields f0,; f1, … fK (0-indexed) and the column keys 0, 1, … N.; force_bgz (bool) – If True, load .gz files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep (str) – This parameter is a deprecated name for delimiter, please use that; instead.; delimiter (str) – A single character string which separates values in the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns:; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:35780,Performance,load,load,35780,": hl.tstr} solves this problem. Parameters:. paths (str or list of str) – Files to import.; key (str or list of str) – Key fields(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – If True`, assume the file has no header and name the N fields f0,; f1, … fN (0-indexed).; impute (bool) – If True, Impute field types from the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:36365,Performance,load,load,36365,"rs or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of par",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:37410,Performance,load,load,37410,"ely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of partitions.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition (bool) – If True, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if True and min_partitions is less than; the number of files. Returns:; Table – Table constructed from imported data. hail.methods.import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, call_fields=['PGT'], reference_genome='default', contig_recoding=None, array_elements_required=True, skip_invalid_loci=False, entry_float_type=dtype('float64'), filter=None, find_replace=None, n_partitions=None, block_size=Non",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:37696,Performance,load,load,37696,"ng glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of partitions.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition (bool) – If True, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if True and min_partitions is less than; the number of files. Returns:; Table – Table constructed from imported data. hail.methods.import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, call_fields=['PGT'], reference_genome='default', contig_recoding=None, array_elements_required=True, skip_invalid_loci=False, entry_float_type=dtype('float64'), filter=None, find_replace=None, n_partitions=None, block_size=None, _create_row_uids=False, _create_col_uids=False)[source]; Import VCF file(s) as a MatrixTable.; Examples; Import a standard bgzipped VCF with GRCh37 as the reference genome.; >>> ds = hl.import_vcf('data/example2.vcf.bgz', reference_genome='GRCh37'). Impo",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:41921,Performance,load,load,41921,"-+----------+--------------+; | 1:123456 | [""A"",""C""] | NA | NA | NA | [1,NA] |; +---------------+------------+------+---------+----------+--------------+. +------------------+----------------+----------------+--------------+; | info.B | info.C | info.D | 'SAMPLE1'.GT |; +------------------+----------------+----------------+--------------+; | array<float64> | array<float64> | array<float64> | call |; +------------------+----------------+----------------+--------------+; | [NA,2.00e+00,NA] | NA | NA | 0/0 |; +------------------+----------------+----------------+--------------+. +--------------+--------------+--------------+; | 'SAMPLE1'.X | 'SAMPLE1'.Y | 'SAMPLE1'.Z |; +--------------+--------------+--------------+; | array<int32> | array<int32> | array<int32> |; +--------------+--------------+--------------+; | [1,NA,1] | NA | NA |; +--------------+--------------+--------------+. Notes; Hail is designed to be maximally compatible with files in the VCF v4.2; spec.; import_vcf() takes a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as Hadoop glob; patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you; have a large compressed VCF that ends in .vcf.gz, it is likely that the; file is actually block-compressed, and you should rename the file to; .vcf.bgz accordingly. If you are unable to rename this file, please use; force_bgz=True to ignore the extension and treat this file as; block-gzipped.; If you have a non-block (aka standard) gzipped file, you may use; force=True; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:42798,Performance,perform,perform,42798,"es a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as Hadoop glob; patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you; have a large compressed VCF that ends in .vcf.gz, it is likely that the; file is actually block-compressed, and you should rename the file to; .vcf.bgz accordingly. If you are unable to rename this file, please use; force_bgz=True to ignore the extension and treat this file as; block-gzipped.; If you have a non-block (aka standard) gzipped file, you may use; force=True; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the; filters row field. This annotation is a set<str> and can be; queried for filter membership with expressions like; ds.filters.contains(""VQSRTranche99.5...""). Variants that are flagged; as “PASS” will have no filters applied; for these variants,; hl.len(ds.filters) is 0. Thus, filtering to PASS variants; can be done with MatrixTable.filter_rows() as follows:; >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). Column Fields. s (tstr) – Column key. This is the sample ID. Row Fields. locus (tlocus or tstruct) – Row key. The; chromosome (CHROM field) and position (POS field). If reference_genome; is defined, the type will be tlocus parameterized by; reference_genome. Otherwise, the type will be a tstruct with; two fields: contig with type tstr and position with t",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:44992,Performance,load,load,44992,"the variant. The reference allele (REF field) is; the first element in the array and the alternate alleles (ALT field) are; the subsequent elements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatte",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:45135,Performance,load,load,45135,"ements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:45462,Performance,load,load,45462," VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not c",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:45542,Performance,load,load,45542,"cf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:45625,Performance,load,load,45625,"declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfl",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:45649,Performance,load,loaded,45649,"are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfloat64.; filter (str, optional) – Line filter regex. A p",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:45861,Performance,load,loaded,45861,"ers:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfloat64.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitutio",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:52147,Safety,avoid,avoid,52147,"(one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, output; + '.bgen' will be a directory containing many BGEN files. In either case, the SAMPLE file is; written to output + '.sample'. For example,; >>> hl.export_bgen(mt, '/path/to/dataset') . Will write two files: /path/to/dataset.bgen and /path/to/dataset.sample. In contrast,; >>> hl.export_bgen(mt, '/path/to/dataset', parallel='header_per_shard') . Will create /path/to/dataset.sample and will create mt.n_partitions() files into the; directory /path/to/dataset.bgen/.; Notes; The export_bgen() function requires genotype probabilities, either as an entry; field of mt (of t",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:27156,Testability,test,test,27156,"(0-indexed) and the column keys 0, 1, … N.; force_bgz (bool) – If True, load .gz files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep (str) – This parameter is a deprecated name for delimiter, please use that; instead.; delimiter (str) – A single character string which separates values in the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns:; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the --make-bed option.; Hail uses the individual ID (column 2 in FAM file) as the sample id (s).; The individual IDs must be unique.; The resulting MatrixTable has the following fields:. Row fields:. locus (tlocus or tstruct) – Row key. The; chromosome and position. If reference_genome is defined, the type; will be tlocus parameterized by reference_genome.; Otherwise, the type will be a tstruct with two fields:; contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference allele (A2; if a2_reference is True) is the first element in the array.; rsid (tstr) – Column 2 in the BIM file.; cm_position (tfloat64) – Column 3 in ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:27182,Testability,test,test,27182,"0, 1, … N.; force_bgz (bool) – If True, load .gz files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep (str) – This parameter is a deprecated name for delimiter, please use that; instead.; delimiter (str) – A single character string which separates values in the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns:; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the --make-bed option.; Hail uses the individual ID (column 2 in FAM file) as the sample id (s).; The individual IDs must be unique.; The resulting MatrixTable has the following fields:. Row fields:. locus (tlocus or tstruct) – Row key. The; chromosome and position. If reference_genome is defined, the type; will be tlocus parameterized by reference_genome.; Otherwise, the type will be a tstruct with two fields:; contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference allele (A2; if a2_reference is True) is the first element in the array.; rsid (tstr) – Column 2 in the BIM file.; cm_position (tfloat64) – Column 3 in the BIM file,; the position in c",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/impex.html:27208,Testability,test,test,27208,"l) – If True, load .gz files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep (str) – This parameter is a deprecated name for delimiter, please use that; instead.; delimiter (str) – A single character string which separates values in the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns:; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the --make-bed option.; Hail uses the individual ID (column 2 in FAM file) as the sample id (s).; The individual IDs must be unique.; The resulting MatrixTable has the following fields:. Row fields:. locus (tlocus or tstruct) – Row key. The; chromosome and position. If reference_genome is defined, the type; will be tlocus parameterized by reference_genome.; Otherwise, the type will be a tstruct with two fields:; contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference allele (A2; if a2_reference is True) is the first element in the array.; rsid (tstr) – Column 2 in the BIM file.; cm_position (tfloat64) – Column 3 in the BIM file,; the position in centimorgans. Column fields",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
https://hail.is/docs/0.2/methods/index.html:2475,Availability,toler,tolerance,2475,"edness; identity_by_descent(); king(); pc_relate(); simulate_random_mating(). Miscellaneous; grep(); maximal_independent_set(); rename_duplicates(); segment_intervals(). Import / Export. export_elasticsearch(t, host, port, index, ...); Export a Table to Elasticsearch. export_gen(dataset, output[, precision, gp, ...]); Export a MatrixTable as GEN and SAMPLE files. export_bgen(mt, output[, gp, varid, rsid, ...]); Export MatrixTable as MatrixTable as BGEN 1.2 file with 8 bits of per probability. export_plink(dataset, output[, call, ...]); Export a MatrixTable as PLINK2 BED, BIM and FAM files. export_vcf(dataset, output[, ...]); Export a MatrixTable or Table as a VCF file. get_vcf_metadata(path); Extract metadata from VCF header. import_bed(path[, reference_genome, ...]); Import a UCSC BED file as a Table. import_bgen(path, entry_fields[, ...]); Import BGEN file(s) as a MatrixTable. import_fam(path[, quant_pheno, delimiter, ...]); Import a PLINK FAM file into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association ",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:5443,Availability,error,errors,5443,"ataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, ",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:4651,Deployability,update,update,4651,"gression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:8850,Deployability,update,updated,8850,"ate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness estimates between individuals using a KING variant. pc_relate(call_expr, min_individual_maf, *); Compute relatedness estimates between individuals using a variant of the PC-Relate method. Miscellaneous. grep(regex, path[, max_count, show, force, ...]); Searches given paths for all lines containing regex matches. maximal_independent_set(i, j[, keep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. [1]; Purcell, Shaun et al. “PLINK: a tool set for whole-genome association and; population-based linkage analyses.” American journal of human genetics; vol. 81,3 (2007):; 559-75. doi:10.1086/519795. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838/. [2]; Manichaikul, Ani et al. “Robust relationship inference in genome-wide; association studies.” Bioinformatics (Oxford, England) vol. 26,22 (2010):; 2867-73. doi:10.1093/bioinformatics/btq559. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/. [3]; Conomos, Matthew P et al. “Model-free Estimation of Recent Genetic; Relatedness.” American journal of human genetics vol. 98,1 (2016):; 127-48. doi:10.1016/j.ajhg.2015.11.022. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3440,Modifiability,variab,variable,3440,"le into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call conc",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3570,Modifiability,variab,variable,3570," list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3609,Modifiability,variab,variables,3609," list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3722,Modifiability,variab,variable,3722," fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr);",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3770,Modifiability,variab,variable,3770," fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr);",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3883,Modifiability,variab,variable,3883,"as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized g",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3930,Modifiability,variab,variable,3930,"as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized g",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:5610,Modifiability,config,config,5610,"ternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant ",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:6684,Modifiability,config,config,6684,"hip_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness estimates between individuals using a KING variant. pc_relate(call_expr, min_individual_maf, *); Compute relatedness estimates between individuals using a variant of the PC-Relate method. Miscellaneous. grep(regex, path[, max_count, show, force, ...]); Searches given paths for all lines con",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:6289,Performance,throughput,throughput,6289,"arly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_d",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3426,Testability,test,test,3426,"le into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call conc",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3556,Testability,test,test,3556," list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3669,Testability,test,test,3669," fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr);",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3708,Testability,test,test,3708," fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr);",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3785,Testability,log,logistic,3785," fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr);",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3830,Testability,test,test,3830,"as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized g",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:3869,Testability,test,test,3869,"as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized g",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:5946,Testability,log,logistic,5946,"ix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_des",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:5960,Testability,test,test,5960,"ix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_des",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/index.html:6414,Testability,test,test,6414," mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness ",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
https://hail.is/docs/0.2/methods/misc.html:7647,Deployability,update,updated,7647,"ction must satisfy the following property:; tie_breaker(l, r) == -tie_breaker(r, l).; When multiple nodes have the same degree, this algorithm will order the; nodes according to tie_breaker and remove the largest node.; If keyed is False, then a node may appear twice in the resulting; table. Parameters:. i (Expression) – Expression to compute one endpoint of an edge.; j (Expression) – Expression to compute another endpoint of an edge.; keep (bool) – If True, return vertices in set. If False, return vertices removed.; tie_breaker (function) – Function used to order nodes with equal degree.; keyed (bool) – If True, key the resulting table by the node field, this requires; a sort. Returns:; Table – Table with the set of independent vertices. The table schema is one row; field node which has the same type as input expressions i and j. hail.methods.rename_duplicates(dataset, name='unique_id')[source]; Rename duplicate column keys. Note; Requires the column key to be one field of type tstr. Examples; >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; This method produces a new column field from the string column key by; appending a unique suffix _N as necessary. For example, if the column; key “NA12878” appears three times in the dataset, the first will produce; “NA12878”, the second will produce “NA12878_1”, and the third will produce; “NA12878_2”. The name of this new field is parameterized by name. Parameters:. dataset (MatrixTable) – Dataset.; name (str) – Name of new field. Returns:; MatrixTable. hail.methods.segment_intervals(ht, points)[source]; Segment the interval keys of ht at a given set of points. Parameters:. ht (Table) – Table with interval keys.; points (Table or ArrayExpression) – Points at which to segment the intervals, a table or an array. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/misc.html
https://hail.is/docs/0.2/methods/misc.html:1842,Modifiability,sandbox,sandbox,1842,"ep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. hail.methods.grep(regex, path, max_count=100, *, show=True, force=False, force_bgz=False)[source]; Searches given paths for all lines containing regex matches.; Examples; Print all lines containing the string hello in file.txt:; >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; grep() mimics the basic functionality of Unix grep in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses Java regular expression; patterns.; The RegExr sandbox may be helpful. Parameters:. regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return; show (bool) – When True, show the values on stdout. When False, return a; dictionary mapping file names to lines.; force_bgz (bool) – If True, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; dict of str to list of str. hail.methods.maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True)[source]; Return a table containing the vertices in a near; maximal independent set; of an undirecte",MatchSource.WIKI,docs/0.2/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/misc.html
https://hail.is/docs/0.2/methods/misc.html:7185,Modifiability,parameteriz,parameterized,7185,"ction must satisfy the following property:; tie_breaker(l, r) == -tie_breaker(r, l).; When multiple nodes have the same degree, this algorithm will order the; nodes according to tie_breaker and remove the largest node.; If keyed is False, then a node may appear twice in the resulting; table. Parameters:. i (Expression) – Expression to compute one endpoint of an edge.; j (Expression) – Expression to compute another endpoint of an edge.; keep (bool) – If True, return vertices in set. If False, return vertices removed.; tie_breaker (function) – Function used to order nodes with equal degree.; keyed (bool) – If True, key the resulting table by the node field, this requires; a sort. Returns:; Table – Table with the set of independent vertices. The table schema is one row; field node which has the same type as input expressions i and j. hail.methods.rename_duplicates(dataset, name='unique_id')[source]; Rename duplicate column keys. Note; Requires the column key to be one field of type tstr. Examples; >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; This method produces a new column field from the string column key by; appending a unique suffix _N as necessary. For example, if the column; key “NA12878” appears three times in the dataset, the first will produce; “NA12878”, the second will produce “NA12878_1”, and the third will produce; “NA12878_2”. The name of this new field is parameterized by name. Parameters:. dataset (MatrixTable) – Dataset.; name (str) – Name of new field. Returns:; MatrixTable. hail.methods.segment_intervals(ht, points)[source]; Segment the interval keys of ht at a given set of points. Parameters:. ht (Table) – Table with interval keys.; points (Table or ArrayExpression) – Points at which to segment the intervals, a table or an array. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/misc.html
https://hail.is/docs/0.2/methods/misc.html:1842,Testability,sandbox,sandbox,1842,"ep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. hail.methods.grep(regex, path, max_count=100, *, show=True, force=False, force_bgz=False)[source]; Searches given paths for all lines containing regex matches.; Examples; Print all lines containing the string hello in file.txt:; >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; grep() mimics the basic functionality of Unix grep in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses Java regular expression; patterns.; The RegExr sandbox may be helpful. Parameters:. regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return; show (bool) – When True, show the values on stdout. When False, return a; dictionary mapping file names to lines.; force_bgz (bool) – If True, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; dict of str to list of str. hail.methods.maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True)[source]; Return a table containing the vertices in a near; maximal independent set; of an undirecte",MatchSource.WIKI,docs/0.2/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/misc.html
https://hail.is/docs/0.2/methods/relatedness.html:14015,Availability,down,down,14015,"ed_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of a sample’s first k; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as descri",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:17635,Availability,avail,available,17635," given by:. \[\widehat{k^{(2)}_{ij}} \coloneqq; \frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}\]; The estimator for identity-by-descent zero is given by:. \[\widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:19101,Availability,reliab,reliably,19101,"thm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that \(g_{is}\) is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first.; The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the statistics parameter:. i (col_key.dtype) – First sample. (key field); j (col_key.dtype) – Second sample. (key field); kin (tfloat64) – Kinship estimate, \(\widehat{\phi_{ij}}\).; ibd2 (tfloat64) – IBD2 estimate, \(\widehat{k^{(2)}_{ij}}\).; ibd0 (tfloat64) – IBD0 estimate, \(\widehat{k^{(0)}_{ij}}\).; ibd1 (tfloat64) – IBD1 estimate, \(\widehat{k^{(1)}_{ij}}\). Here col_key refers to the column key of the source matrix table,; and col_key.dtype is a struct containing the column key fields.; There is one row for each pair of distinct samples (columns), whe",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:20388,Availability,down,downstream,20388,"otype. To enforce the latter,; either filter or split multi-allelic variants first.; The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the statistics parameter:. i (col_key.dtype) – First sample. (key field); j (col_key.dtype) – Second sample. (key field); kin (tfloat64) – Kinship estimate, \(\widehat{\phi_{ij}}\).; ibd2 (tfloat64) – IBD2 estimate, \(\widehat{k^{(2)}_{ij}}\).; ibd0 (tfloat64) – IBD0 estimate, \(\widehat{k^{(0)}_{ij}}\).; ibd1 (tfloat64) – IBD1 estimate, \(\widehat{k^{(1)}_{ij}}\). Here col_key refers to the column key of the source matrix table,; and col_key.dtype is a struct containing the column key fields.; There is one row for each pair of distinct samples (columns), where i; corresponds to the column of smaller column index. In particular, if the; same column key value exists for \(n\) columns, then the resulting; table will have \(\binom{n-1}{2}\) rows with both key fields equal to; that column key value. This may result in unexpected behavior in downstream; processing. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; min_individual_maf (float) – The minimum individual-specific minor allele frequency.; If either individual-specific minor allele frequency for a pair of; individuals is below this threshold, then the variant will not; be used to estimate relatedness for the pair.; k (int, optional) – If set, k principal component scores are computed and used.; Exactly one of k and scores_expr must be specified.; scores_expr (ArrayNumericExpression, optional) – Column-indexed expression of principal component scores, with the same; source as call_expr. All array values must have the same positive length,; corresponding to the number of principal components, and all scores must; be non-missing. Exactly one of k and scores_expr must be specified.; min_kinship (float, optional) – If set, pairs of samples with kinship lower than min_kinship are excluded; from the results.; statistics (str) – Set of st",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:7191,Deployability,configurat,configurations,7191,"leotide variants for which both; individuals \(i\) and \(j\) have a non-missing genotype.; \(X_{i,s}\) be the genotype score matrix. Each entry corresponds to; the genotype of individual \(i\) at variant; \(s\). Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. \(X_{i,s}\) is calculated by invoking; n_alt_alleles() on the call_expr. The three counts above, \(N^{Aa}\), \(N^{Aa,Aa}\), and; \(N^{AA,aa}\), exclude variants where one or both individuals have; missing genotypes.; In terms of the symbols above, we can define \(d\), the genetic distance; between two samples. We can interpret \(d\) as an unnormalized; measurement of the genetic material not shared identically-by-descent:. \[d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2\]; In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. \((X_{i,s} - X_{j,s})^2\); homref; het; homalt. homref; 0; 1; 4. het; 1; 0; 1. homalt; 4; 1; 0. which leads to this expression for genetic distance:. \[d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}\]; The first term, \(4 N^{AA,aa}_{i,j}\), accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case of a pair of heteroyzgous genotypes. We offset this; with the fourth and final term.; The genetic distance, \(d_{i,j}\), ranges between zero and four times; the number of variants in the dataset. In the supplement to Manichaikul,; et. al, the authors demonstrate that the kinship coefficient,; \(\phi_{i,j}\), between two individuals from the same population is; rel",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:23253,Deployability,update,updated,23253,"with kinship lower than min_kinship are excluded; from the results.; statistics (str) – Set of statistics to compute.; If 'kin', only estimate the kinship statistic.; If 'kin2', estimate the above and IBD2.; If 'kin20', estimate the above and IBD0.; If 'all', estimate the above and IBD1.; block_size (int, optional) – Block size of block matrices used in the algorithm.; Default given by BlockMatrix.default_block_size().; include_self_kinship (bool) – If True, include entries for an individual’s estimated kinship with; themselves. Defaults to False. Returns:; Table – A Table mapping pairs of samples to their pair-wise statistics. hail.methods.simulate_random_mating(mt, n_rounds=1, generation_size_multiplier=1.0, keep_founders=True)[source]; Simulate random diploid mating to produce new individuals. Parameters:. mt; n_rounds (int) – Number of rounds of mating.; generation_size_multiplier (float) – Ratio of number of offspring to current population for each round of mating.; keep_founders :obj:`bool` – If true, keep all founders and intermediate generations in the final sample list. If; false, keep only offspring in the last generation. Returns:; MatrixTable. [1]; Purcell, Shaun et al. “PLINK: a tool set for whole-genome association and; population-based linkage analyses.” American journal of human genetics; vol. 81,3 (2007):; 559-75. doi:10.1086/519795. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838/. [2]; Manichaikul, Ani et al. “Robust relationship inference in genome-wide; association studies.” Bioinformatics (Oxford, England) vol. 26,22 (2010):; 2867-73. doi:10.1093/bioinformatics/btq559. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/. [3]; Conomos, Matthew P et al. “Model-free Estimation of Recent Genetic; Relatedness.” American journal of human genetics vol. 98,1 (2016):; 127-48. doi:10.1016/j.ajhg.2015.11.022. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:12472,Energy Efficiency,efficient,efficient,12472,"indexed call expression.; block_size (int, optional) – Block size of block matrices used in the algorithm.; Default given by BlockMatrix.default_block_size(). Returns:; MatrixTable – A MatrixTable whose rows and columns are keys are taken from; call-expr’s column keys. It has one entry field, phi. hail.methods.pc_relate(call_expr, min_individual_maf, *, k=None, scores_expr=None, min_kinship=None, statistics='all', block_size=None, include_self_kinship=False)[source]; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Note; Requires the dataset to contain only diploid genotype calls. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) . Only compute the kinship statistic. This is more efficient than; computing all statistics.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') . Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\rig",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:12671,Energy Efficiency,efficient,efficient,12671,"umns are keys are taken from; call-expr’s column keys. It has one entry field, phi. hail.methods.pc_relate(call_expr, min_individual_maf, *, k=None, scores_expr=None, min_kinship=None, statistics='all', block_size=None, include_self_kinship=False)[source]; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Note; Requires the dataset to contain only diploid genotype calls. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) . Only compute the kinship statistic. This is more efficient than; computing all statistics.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') . Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:19525,Integrability,depend,depending,19525,"are all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that \(g_{is}\) is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first.; The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the statistics parameter:. i (col_key.dtype) – First sample. (key field); j (col_key.dtype) – Second sample. (key field); kin (tfloat64) – Kinship estimate, \(\widehat{\phi_{ij}}\).; ibd2 (tfloat64) – IBD2 estimate, \(\widehat{k^{(2)}_{ij}}\).; ibd0 (tfloat64) – IBD0 estimate, \(\widehat{k^{(0)}_{ij}}\).; ibd1 (tfloat64) – IBD1 estimate, \(\widehat{k^{(1)}_{ij}}\). Here col_key refers to the column key of the source matrix table,; and col_key.dtype is a struct containing the column key fields.; There is one row for each pair of distinct samples (columns), where i; corresponds to the column of smaller column index. In particular, if the; same column key value exists for \(n\) columns, then the resulting; table will have \(\binom{n-1}{2}\) rows with both key fields equal to; that column key value. This may result in unexpected behavior in downstream; processing. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; min_individual_maf (float) ",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:930,Modifiability,inherit,inherited,930,"﻿. Hail | ; Relatedness. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Relatedness. View page source. Relatedness; The relatedness of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by ",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:1010,Modifiability,inherit,inherited,1010," Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Relatedness. View page source. Relatedness; The relatedness of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:1065,Modifiability,inherit,inherited,1065," Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Relatedness. View page source. Relatedness; The relatedness of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:1879,Modifiability,inherit,inherited,1879,"and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple; homogeneous populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness estimates between individuals using a KING variant. pc_relate(call_expr, min_individual_maf, *); Compute relatedness estimates between individuals using a variant of the PC-Relate method. simulate_random_mating(mt[, n_rounds, ...]); Simulate random diploid mating to produce new in",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:7191,Modifiability,config,configurations,7191,"leotide variants for which both; individuals \(i\) and \(j\) have a non-missing genotype.; \(X_{i,s}\) be the genotype score matrix. Each entry corresponds to; the genotype of individual \(i\) at variant; \(s\). Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. \(X_{i,s}\) is calculated by invoking; n_alt_alleles() on the call_expr. The three counts above, \(N^{Aa}\), \(N^{Aa,Aa}\), and; \(N^{AA,aa}\), exclude variants where one or both individuals have; missing genotypes.; In terms of the symbols above, we can define \(d\), the genetic distance; between two samples. We can interpret \(d\) as an unnormalized; measurement of the genetic material not shared identically-by-descent:. \[d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2\]; In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. \((X_{i,s} - X_{j,s})^2\); homref; het; homalt. homref; 0; 1; 4. het; 1; 0; 1. homalt; 4; 1; 0. which leads to this expression for genetic distance:. \[d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}\]; The first term, \(4 N^{AA,aa}_{i,j}\), accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case of a pair of heteroyzgous genotypes. We offset this; with the fourth and final term.; The genetic distance, \(d_{i,j}\), ranges between zero and four times; the number of variants in the dataset. In the supplement to Manichaikul,; et. al, the authors demonstrate that the kinship coefficient,; \(\phi_{i,j}\), between two individuals from the same population is; rel",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:13919,Modifiability,inherit,inherited,13919,"t than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as ",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:4003,Performance,perform,perform,4003,"scent estimates. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:; >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in \([0.2, 0.9]\), using minor allele frequencies stored in; the row field panel_maf:; >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; The dataset must have a column field named s which is a StringExpression; and which uniquely identifies a column.; The implementation is based on the IBD algorithm described in the PLINK; paper.; identity_by_descent() requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first.; The resulting Table entries have the type: { i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }. The key list is: *i: String, j:; String*.; Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form; i j ibd.Z0 ibd.Z1 ibd.Z2 ibd.PI_HAT ibs0 ibs1 ibs2; sample1 sample2 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample3 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample4 0.6807 0.0000 0.3193 0.3193 ...; sample1 sample5 0.1966 0.0000 0.8034 0.8034 ... Parameters:. dataset (MatrixTable) – Variant-keyed and sample-keyed MatrixTable containing genotype information.; maf (Float64Expression, optional) – Row-indexed expression for the minor allele frequency.; bounded (bool) – Forces the estimations for Z0, Z1, Z2, and PI_HAT to take; on biologically meani",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:17955,Performance,load,loadings,17955,"}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Thi",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:17982,Performance,load,loadings,17982,"}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Thi",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:18061,Performance,perform,perform,18061,"^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pair",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/relatedness.html:14149,Usability,simpl,simply,14149,"hip=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of a sample’s first k; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as described by principal; component coordinates) and population allele frequency is linear. The estimators for kinship, and identity-by-descent zero, one, and two; follow. Let:. \(S",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
https://hail.is/docs/0.2/methods/stats.html:3574,Availability,error,error,3574,"taset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each array.; If y is a list of lists of expressions, then n and sum_x are of type; array<float64>, and the last five fields are of type; array<array<float64>>. Index into these arrays with; a[index_in_outer_list, index_in_inner_list]. For example, if; y=[[a], [b, c]] then the p-value for b is p_value[1][0].; In the statistical genetics example above, the input variable x encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathr",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6241,Availability,toler,tolerance,6241,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7220,Availability,toler,tolerance,7220,"ass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7530,Availability,toler,tolerance,7530,"result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9335,Availability,error,error,9335,"st (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:11675,Availability,error,errors,11675," For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15206,Availability,toler,tolerance,15206,"ts will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15347,Availability,toler,tolerance,15347,"ts will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15373,Availability,toler,tolerance,15373,"gh parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, opt",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15509,Availability,toler,tolerance,15509,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:16353,Availability,toler,tolerance,16353,"on_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed nume",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:16494,Availability,toler,tolerance,16494,"on_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed nume",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:16520,Availability,toler,tolerance,16520,"ne)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:17178,Availability,error,error,17178,"y list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:23013,Deployability,update,updated,23013," each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; BlockMatrix.write_from_entry_expr().; The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; ld_matrix().; To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12807,Energy Efficiency,reduce,reduces,12807,"omplete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Hei",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:22403,Energy Efficiency,efficient,efficiently,22403," each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; BlockMatrix.write_from_entry_expr().; The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; ld_matrix().; To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9135,Integrability,depend,depends,9135,"esent values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:10992,Integrability,depend,dependent,10992,"um likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:873,Modifiability,variab,variable,873,"﻿. Hail | ; Statistics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, par",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1003,Modifiability,variab,variable,1003," 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1042,Modifiability,variab,variables,1042," 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1155,Modifiability,variab,variable,1155,"ion; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1203,Modifiability,variab,variable,1203,"ion; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1316,Modifiability,variab,variable,1316,"; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[sourc",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1363,Modifiability,variab,variable,1363,"; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[sourc",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:2074,Modifiability,variab,variable,2074,"r association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:2351,Modifiability,variab,variable,2351," for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:2391,Modifiability,variab,variables,2391," for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:2852,Modifiability,variab,variable,2852,"2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each a",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:2914,Modifiability,variab,variables,2914,"2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each a",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:4240,Modifiability,variab,variable,4240,"ny) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each array.; If y is a list of lists of expressions, then n and sum_x are of type; array<float64>, and the last five fields are of type; array<array<float64>>. Index into these arrays with; a[index_in_outer_list, index_in_inner_list]. For example, if; y=[[a], [b, c]] then the p-value for b is p_value[1][0].; In the statistical genetics example above, the input variable x encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include a",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:5496,Modifiability,variab,variable,5496,"{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6295,Modifiability,variab,variable,6295,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6344,Modifiability,variab,variable,6344,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7702,Modifiability,variab,variables,7702,"dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7975,Modifiability,variab,variable,7975,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8031,Modifiability,variab,variable,8031,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8094,Modifiability,variab,variable,8094," at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statis",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8417,Modifiability,variab,variable,8417,"set.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statist",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:14938,Modifiability,variab,variable,14938," approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['r",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15563,Modifiability,variab,variable,15563,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15611,Modifiability,variab,variable,15611,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:16143,Modifiability,variab,variable,16143,"clude in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformation",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:5643,Performance,perform,perform,5643,", \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=d",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:5727,Performance,perform,performance,5727,"d as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7917,Performance,perform,performs,7917,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:18125,Performance,load,loadings,18125,"porated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int)",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:18240,Performance,load,loadings,18240," numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, ta",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:18553,Performance,load,loadings,18553,"mal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v'",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:18569,Performance,load,loadings,18569,"mal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v'",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:18856,Performance,load,loadings,18856,"elding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T'",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:18924,Performance,load,loadings,18924,"elding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T'",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:19001,Performance,load,loadings,19001,"times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'b', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:19217,Performance,load,loadings,19217,"first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'b', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd',",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:19331,Performance,load,loadings,19331,"while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'b', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v:",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7987,Safety,predict,predicting,7987,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:20894,Safety,avoid,avoid,20894,"v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v: str, s: str, GT: call}')); >>> mt = ht.to_matrix_table(row_key=['v'], col_key=['s']). Compute genotype correlation between all pairs of variants:; >>> ld = hl.row_correlation(mt.GT.n_alt_alleles()); >>> ld.to_numpy(); array([[ 1. , -0.85280287, 0.42640143],; [-0.85280287, 1. , -0.5 ],; [ 0.42640143, -0.5 , 1. ]]). Compute genotype correlation between consecutively-indexed variants:; >>> ld.sparsify_band(lower=0, upper=1).to_numpy(); array([[ 1. , -0.85280287, 0. ],; [ 0. , 1. , -0.5 ],; [ 0. , 0. , 1. ]]). Warning; Rows with a constant value (i.e., zero variance) will result nan; correlation values. To avoid this, first check that all rows vary or filter; out constant rows (for example, with the help of aggregators.stats()). Notes; In this method, each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:859,Testability,test,test,859,"﻿. Hail | ; Statistics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, par",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:989,Testability,test,test,989," 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1102,Testability,test,test,1102,"ion; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1141,Testability,test,test,1141,"ion; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1218,Testability,log,logistic,1218,"ion; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1263,Testability,test,test,1263,"; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[sourc",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:1302,Testability,test,test,1302,"; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[sourc",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:2060,Testability,test,test,2060,"r association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:2337,Testability,test,test,2337," for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:4355,Testability,test,tested,4355,"64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each array.; If y is a list of lists of expressions, then n and sum_x are of type; array<float64>, and the last five fields are of type; array<array<float64>>. Index into these arrays with; a[index_in_outer_list, index_in_inner_list]. For example, if; y=[[a], [b, c]] then the p-value for b is p_value[1][0].; In the statistical genetics example above, the input variable x encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – On",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6176,Testability,test,test,6176,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6281,Testability,test,test,6281,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6359,Testability,log,logistic,6359,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6399,Testability,log,logistic,6399,"y-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.phe",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6424,Testability,test,test,6424,"y-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.phe",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6583,Testability,test,test,6583,"ns.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() conside",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6735,Testability,log,logistic,6735,"de in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6760,Testability,test,test,6760,"de in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:6933,Testability,test,test,6933,"oing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row,",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7290,Testability,test,test,7290,"riable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood rati",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:7956,Testability,test,test,7956,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8054,Testability,log,logistic,8054,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8267,Testability,test,test,8267," ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_erro",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8299,Testability,test,test,8299," ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_erro",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8324,Testability,test,test,8324," ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_erro",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:8350,Testability,test,test,8350," ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_erro",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9150,Testability,test,test,9150,"esent values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9503,Testability,test,testing,9503,"put values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; f",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9701,Testability,test,testing,9701,"\beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9812,Testability,test,testing,9812,"gma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence ma",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9871,Testability,test,tests,9871,"or; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned mat",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:9892,Testability,log,logistic,9892,"or; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned mat",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:10056,Testability,test,test,10056,"ets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:10732,Testability,test,testing,10732," Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not m",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:11203,Testability,test,testing,11203,"s:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from t",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:11319,Testability,log,logistic,11319,"default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial())",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:11468,Testability,test,testing,11468,"loded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:11767,Testability,log,logistic,11767," always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; signifi",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:11823,Testability,test,tests,11823," always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; signifi",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12087,Testability,log,logistic,12087,"s with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full mod",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12103,Testability,log,logistic,12103,"s with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full mod",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12199,Testability,log,logistf,12199,"s with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full mod",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12219,Testability,log,logistf,12219,"s with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full mod",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12326,Testability,log,logfit,12326,"s with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full mod",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12379,Testability,log,logistf,12379,"s with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full mod",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12802,Testability,test,test,12802,"omplete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Hei",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:12960,Testability,test,test,12960,"1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests corr",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:13245,Testability,test,testing,13245,"rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EP",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:13347,Testability,log,logreg,13347,"logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:13512,Testability,test,testing,13512,"value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:13585,Testability,log,logistic,13585,"value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:13623,Testability,test,tests,13623,"value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:13700,Testability,test,tests,13700,"373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed r",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:13969,Testability,log,logistic,13969,"rey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:14004,Testability,log,logistic,14004,"er variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:14024,Testability,test,tests,14024,"er variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:14076,Testability,test,tests,14076,"is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expre",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:14561,Testability,test,test,14561,"tic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association wit",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:14616,Testability,test,test,14616,"tic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Heinze and Schemper further analyze Firth’s approach in; A solution to the problem of separation in logistic regression, 2002.; Hail’s logistic regression tests correspond to the b.wald,; b.lrt, and b.score tests in EPACTS. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association wit",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15446,Testability,test,test,15446,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15549,Testability,test,test,15549,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/methods/stats.html:15714,Testability,test,tests,15714,"indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
https://hail.is/docs/0.2/nd/index.html:15053,Deployability,update,updated,15053,"l be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.maximum(a, b)); array([2, 5, 4], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.maximum(a, b)); array([nan, 5., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – Nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; NDArrayExpression – Element-wise maximums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. hail.nd.minimum(nd1, nd2)[source]; Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index.; If an array element being compared has the value NaN,; the minimum for that index will be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; min_array (NDArrayExpression) – Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:6807,Energy Efficiency,reduce,reduced,6807,"hape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (nd",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:6882,Energy Efficiency,reduce,reduced,6882,"; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. h",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:7100,Energy Efficiency,reduce,reduced,7100,"ndent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. hail.nd.svd(nd, full_matrices=True, compute_uv=True)[source]; Performs a singular value decomposition. Parameters:. nd (NDArrayNumericExpression) – A",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:7619,Energy Efficiency,reduce,reduced,7619,"ar matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. hail.nd.svd(nd, full_matrices=True, compute_uv=True)[source]; Performs a singular value decomposition. Parameters:. nd (NDArrayNumericExpression) – A 2 dimensional ndarray, shape(M, N).; full_matrices (bool) – If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv (bool) – If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns:. - u (NDArrayNumericExpression) – The left singular vectors.; - s (NDArrayNumericExpression) – The singular values.; - vt",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:7669,Energy Efficiency,reduce,reduced,7669,"rns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. hail.nd.svd(nd, full_matrices=True, compute_uv=True)[source]; Performs a singular value decomposition. Parameters:. nd (NDArrayNumericExpression) – A 2 dimensional ndarray, shape(M, N).; full_matrices (bool) – If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv (bool) – If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns:. - u (NDArrayNumericExpression) – The left singular vectors.; - s (NDArrayNumericExpression) – The singular values.; - vt (NDArrayNumericExpression) – The right singular vectors. hail.nd.inv(nd",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:877,Integrability,interface,interface,877,"﻿. Hail | ; nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; nd. View page source. nd. NDArray Functions. Notes; This is a recently added, experimental module. We would love to hear what use cases you have for this as we expand this functionality.; As much as possible, we try to mimic the numpy array interface. array(input_array[, dtype]); Construct an NDArrayExpression. arange(start[, stop, step]); Returns a 1-dimensions ndarray of integers from start to stop by step. full(shape, value[, dtype]); Creates a hail NDArrayNumericExpression full of the specified value. zeros(shape[, dtype]); Creates a hail NDArrayNumericExpression full of zeros. ones(shape[, dtype]); Creates a hail NDArrayNumericExpression full of ones. diagonal(nd); Gets the diagonal of a 2 dimensional NDArray. solve(a, b[, no_crash]); Solve a linear system. solve_triangular(A, b[, lower, no_crash]); Solve a triangular linear system Ax = b for x. qr(nd[, mode]); Performs a QR decomposition. svd(nd[, full_matrices, compute_uv]); Performs a singular value decomposition. inv(nd); Performs a matrix inversion. concatenate(nds[, axis]); Join a sequence of arrays along an existing axis. hstack(arrs); Stack arrays in sequence horizontally (column wise). vstack(arrs); Stack arrays in sequence vertically (row wise). eye(N[, M, dtype]); Construct a 2-D NDArrayExpression with ones on the main diagonal and zeros elsewhere. identity(N[, dtype]); Constru",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:6139,Modifiability,variab,variables,6139,"NDArrayNumericExpression full of ones.; Examples; Create a 5 by 7 NDArray of type tfloat64 ones.; >>> hl.nd.ones((5, 7)). It is possible to specify a type other than tfloat64 with the dtype argument.; >>> hl.nd.ones((5, 7), dtype=hl.tfloat32). Parameters:. shape (tuple or TupleExpression) – Desired shape.; dtype (HailType) – Desired hail type. Default: float64. See also; full(). Returns:; NDArrayNumericExpression – ndarray of the specified size full of ones. hail.nd.diagonal(nd)[source]; Gets the diagonal of a 2 dimensional NDArray.; Examples; >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters:; nd (NDArrayNumericExpression) – A 2 dimensional NDArray, shape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/nd/index.html:6521,Modifiability,variab,variables,6521,"Returns:; NDArrayNumericExpression – ndarray of the specified size full of ones. hail.nd.diagonal(nd)[source]; Gets the diagonal of a 2 dimensional NDArray.; Examples; >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters:; nd (NDArrayNumericExpression) – A 2 dimensional NDArray, shape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
https://hail.is/docs/0.2/overview/expressions.html:9046,Deployability,update,updated,9046,"r -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ... .when(""indel"", True); ... .or_missing()); <BooleanExpression of type bool>. As with case statements, missingness will propagate up through a switch; statement. If we changed the value of csq to the missing value; hl.missing(hl.tstr), then the result of the switch statement above would also; be missing. Missingness; In Hail, all expressions can be missing. An expression representing a missing; value of a given type can be generated with the missing() function, which; takes the type as its single argument.; An example of generating a Float64Expression that is missing is:; >>> hl.missing('float64'); <Float64Expression of type float64>. These can be used with conditional statements to set values to missing if they; don’t satisfy a condition:; >>> hl.if_else(x > 2.0, x, hl.missing(hl.tfloat)); <Float64Expression of type float64>. The Python representation of a missing value is None. For example, if; we define cnull to be a missing value with type tcall, calling; the method is_het will return None and not False.; >>> cnull = hl.missing('call'); >>> hl.eval(cnull.is_het()); None. Functions; In addition to the methods exposed on each Expression, Hail also has; numerous functions that can be applied to expressions, which also return an; expression.; Take a look at the Functions page for full documentation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
https://hail.is/docs/0.2/overview/expressions.html:3151,Integrability,wrap,wrapped,3151,"utations into native code, and running them in parallel.; The result of the expression is computed only when it is needed. So z is; an expression representing the computation of x + y, but not the actual; value.; To peek at the value of this computation, there are two options:; eval(), which returns a Python value, and Expression.show(),; which prints a human-readable representation of an expression.; >>> hl.eval(z); 11; >>> z.show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 11 |; +--------+. Hail’s expressions are especially important for interacting with fields in; tables and matrix tables. Throughout Hail documentation and tutorials, you will; see code like this:; >>> ht2 = ht.annotate(C4 = ht.C3 + 3 * ht.C2 ** 2). This snippet of code is adding a field, C4, to a table, ht, and; returning the result as a new table, ht2. The code passed to the; Table.annotate() method is an expression that references the fields; C3 and C2 in ht.; Notice that 3 and 2 are not wrapped in constructor functions like; hl.int32(3). In the same way that Hail expressions can be combined together; via operations like addition and multiplication, they can also be combined with; Python objects.; For example, we can add a Python int to an Int32Expression.; >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an Int32Expression to an; int.; >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like numpy; or scipy.; Hail has many subclasses of Expression – one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail ArrayNumericExpression with; array():; >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. Expression objects keep track of their data type, which is; why we can see that a is of type array<int32> in the output above. An; expression’s type can also be ",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
https://hail.is/docs/0.2/overview/expressions.html:6676,Integrability,depend,depending,6676," 0. In the above conditional, the condition is x > 0, the consequent is 1,; and the alternate is 0.; Here is the Hail expression equivalent with if_else():; >>> hl.if_else(x > 0, 1, 0); <Int32Expression of type int32>. This example returns an Int32Expression which can be used in more; computations. We can add the conditional expression to our array a from; earlier:; >>> a + hl.if_else(x > 0, 1, 0); <ArrayNumericExpression of type array<int32>>. Case Statements; More complicated conditional statements can be constructed with case().; For example, we might want to return 1 if x < -1, 2 if; -1 <= x <= 2 and 3 if x > 2.; >>> (hl.case(); ... .when(x < -1, 1); ... .when((x >= -1) & (x <= 2), 2); ... .when(x > 2, 3); ... .or_missing()); <Int32Expression of type int32>. Notice that this expression ends with a call to or_missing(),; which means that if none of the conditions are met, a missing value is returned.; Cases started with case() can end with a call to; or_missing(), default(), or; or_error(), depending on what you want to happen if none; of the when clauses are met.; It’s important to note that missingness propagates up in Hail, so if the value; of the discriminant in a case statement is missing, then the result will be; missing as well.; >>> y = hl.missing(hl.tint32); >>> result = hl.case().when(y > 0, 1).default(-1); >>> hl.eval(result). The value of result will be missing, not 1 or -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ...",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
https://hail.is/docs/0.2/overview/expressions.html:4157,Security,access,accessed,4157,"rapped in constructor functions like; hl.int32(3). In the same way that Hail expressions can be combined together; via operations like addition and multiplication, they can also be combined with; Python objects.; For example, we can add a Python int to an Int32Expression.; >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an Int32Expression to an; int.; >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like numpy; or scipy.; Hail has many subclasses of Expression – one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail ArrayNumericExpression with; array():; >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. Expression objects keep track of their data type, which is; why we can see that a is of type array<int32> in the output above. An; expression’s type can also be accessed with Expression.dtype().; >>> a.dtype; dtype('array<int32>'). Hail arrays can be indexed and sliced like Python lists or numpy arrays:; >>> a[1]; <Int32Expression of type int32>. >>> a[1:-1]; <ArrayNumericExpression of type array<int32>>. In addition to constructor methods like array() and bool(),; Hail expressions can also be constructed with the literal() method,; which will impute the type of of the expression.; >>> hl.literal([0,1,2]); <ArrayNumericExpression of type array<int32>>. Boolean Logic; Unlike Python, a Hail BooleanExpression cannot be used with the Python; keywords and, or, and not. The Hail substitutes are &, |,; and ~.; >>> s1 = hl.int32(3) == 4; >>> s2 = hl.int32(3) != 4. >>> s1 & s2; <BooleanExpression of type bool>. >>> s1 | s2; <BooleanExpression of type bool>. >>> ~s1; <BooleanExpression of type bool>. Remember that you can use eval(): to evaluate the expression.; >>> hl.eval(~s1); True. Caution; The operator precedence of & and | is different from ",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
https://hail.is/docs/0.2/overview/expressions.html:8798,Security,expose,exposed,8798,"r -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ... .when(""indel"", True); ... .or_missing()); <BooleanExpression of type bool>. As with case statements, missingness will propagate up through a switch; statement. If we changed the value of csq to the missing value; hl.missing(hl.tstr), then the result of the switch statement above would also; be missing. Missingness; In Hail, all expressions can be missing. An expression representing a missing; value of a given type can be generated with the missing() function, which; takes the type as its single argument.; An example of generating a Float64Expression that is missing is:; >>> hl.missing('float64'); <Float64Expression of type float64>. These can be used with conditional statements to set values to missing if they; don’t satisfy a condition:; >>> hl.if_else(x > 2.0, x, hl.missing(hl.tfloat)); <Float64Expression of type float64>. The Python representation of a missing value is None. For example, if; we define cnull to be a missing value with type tcall, calling; the method is_het will return None and not False.; >>> cnull = hl.missing('call'); >>> hl.eval(cnull.is_het()); None. Functions; In addition to the methods exposed on each Expression, Hail also has; numerous functions that can be applied to expressions, which also return an; expression.; Take a look at the Functions page for full documentation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
https://hail.is/docs/0.2/overview/index.html:1256,Deployability,update,updated,1256,"﻿. Hail | ; Hail Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview. View page source. Hail Overview; Hail is a library for analyzing structured tabular and matrix data. Hail; contains a collection of primitives for operating on data in parallel, as well; as a suite of functionality for processing genetic data.; This section of Hail’s documentation contains detailed explanations of Hail’s; architecture, primitives, classes, and libraries. Expressions; What is an Expression?; Boolean Logic; Conditional Expressions; Missingness; Functions. Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables; Keys; Referencing Fields; Import; Common Operations; Aggregation; Group-By; Joins; Interacting with Matrix Tables Locally. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/index.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:6134,Deployability,update,update,6134,"------------------------; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>; ----------------------------------------. Common Operations; Like tables, Hail provides a number of methods for manipulating data in a; matrix table.; Filter; MatrixTable has three methods to filter based on expressions:. MatrixTable.filter_rows(); MatrixTable.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schem",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:6907,Deployability,update,update,6907,"column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. MatrixTable has four select methods that select and create new fields:. MatrixTable.select_globals(); MatrixTable.select_rows(); MatrixTable.select_cols(); MatrixTable.select_entries(). Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; KEY=VALUE to compute new fields. The Python unpack operator ** can be; used to specify that all fields",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:14109,Deployability,update,updated,14109," while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matrix Tables Locally; Some useful methods to interact with matrix tables locally are; MatrixTable.describe(), MatrixTable.head(), and; MatrixTable.sample_rows(). describe prints out the schema for all row; fields, column fields, entry fields, and global fields as well as the row keys; and column keys. head returns a new matrix table with only the first N rows.; sample_rows returns a new matrix table where the rows are randomly sampled with; frequency p.; To get the dimensions of the matrix table, use MatrixTable.count_rows(); and MatrixTable.count_cols(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:1697,Energy Efficiency,efficient,efficiently,1697,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:10347,Energy Efficiency,efficient,efficient,10347,"----------+------------+---------------+; | 20:10019093 | [""A"",""G""] | 1 |; | 20:10019093 | [""A"",""G""] | 2 |; | 20:10026348 | [""A"",""G""] | 1 |; | 20:10026348 | [""A"",""G""] | 2 |; | 20:10026357 | [""T"",""C""] | 1 |; | 20:10026357 | [""T"",""C""] | 2 |; | 20:10030188 | [""T"",""A""] | 1 |; | 20:10030188 | [""T"",""A""] | 2 |; | 20:10030452 | [""G"",""A""] | 1 |; | 20:10030452 | [""G"",""A""] | 2 |; +---------------+------------+---------------+; showing top 10 rows. Aggregation; MatrixTable has three methods to compute aggregate statistics. MatrixTable.aggregate_rows(); MatrixTable.aggregate_cols(); MatrixTable.aggregate_entries(). These methods take an aggregated expression and evaluate it, returning; a Python value.; An example of querying entries is to compute the global mean of field GQ:; >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) ; 67.73196915777027. It is possible to compute multiple values simultaneously by; creating a tuple or struct. This is encouraged, because grouping two; computations together is far more efficient by traversing the dataset only once; rather than twice.; >>> mt.aggregate_entries((hl.agg.stats(mt.DP), hl.agg.stats(mt.GQ))) ; (Struct(mean=41.83915800445897, stdev=41.93057654787303, min=0.0, max=450.0, n=34537, sum=1444998.9999999995),; Struct(mean=67.73196915777027, stdev=29.80840934057741, min=0.0, max=99.0, n=33720, sum=2283922.0000000135)). See the Aggregators page for the complete list of aggregator; functions. Group-By; Matrix tables can be aggregated along the row or column axis to produce a new; matrix table. MatrixTable.group_rows_by(); MatrixTable.group_cols_by(). First let’s add a random phenotype as a new column field case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_s",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:1729,Integrability,interface,interfaces,1729,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:11968,Performance,perform,performing,11968,"eld case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_n",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:12103,Performance,perform,performs,12103," hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; ",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:12494,Performance,perform,performed,12494,"4,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matri",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:1713,Security,expose,exposes,1713,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:2386,Security,access,accessed,2386,"hile MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:2727,Security,expose,exposed,2727,"d exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt ; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus ; <LocusExpression of type locus<GRCh37>>. Likewise, mt.DP is an Int32Expression with type int32; and is an entry field of mt.; Hail expressions can also Expression.describe() themselves, providing; information about their source matrix table or table and which",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:2917,Security,access,access,2917," form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt ; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus ; <LocusExpression of type locus<GRCh37>>. Likewise, mt.DP is an Int32Expression with type int32; and is an entry field of mt.; Hail expressions can also Expression.describe() themselves, providing; information about their source matrix table or table and which keys index the; expression, if any. For example, mt.DP.describe() tells us that mt.DP; has type int32 and is an entry field of mt, since it is indexed; by both rows and columns:; >>> mt.DP.describe() ; ---------------------------------",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:7794,Security,access,accessing,7794,"all_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. MatrixTable has four select methods that select and create new fields:. MatrixTable.select_globals(); MatrixTable.select_rows(); MatrixTable.select_cols(); MatrixTable.select_entries(). Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; KEY=VALUE to compute new fields. The Python unpack operator ** can be; used to specify that all fields of a Struct should become top level fields.; However, be aware that all top-level field names must be unique. In the; following example, **mt[‘info’] would fail if DP already exists as an entry; field.; >>> mt_new = mt.select_rows(**mt['info']) . The example below adds two new row fields. Keys are always preserved, so the; row keys locus and alleles will also be present in the new table.; AC = mt.info.AC turns the subfield AC into a top-level field.; >>> mt_new = mt.select_rows(AC = mt.info.AC,; ... n_filters = hl.len(mt['filters'])). The order of the fields entered as arguments will be maintained in the new; matrix table.; Drop; The complement of select methods, MatrixTable.drop() can remove any top; level field. An example of removing the GQ entry field is:; >>> mt_new = mt.drop('GQ'). Explode; Explode operations can is",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table-1.html:6441,Usability,simpl,simplest,6441,"le.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty());",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
https://hail.is/docs/0.2/overview/matrix_table.html:6134,Deployability,update,update,6134,"------------------------; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>; ----------------------------------------. Common Operations; Like tables, Hail provides a number of methods for manipulating data in a; matrix table.; Filter; MatrixTable has three methods to filter based on expressions:. MatrixTable.filter_rows(); MatrixTable.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schem",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:6907,Deployability,update,update,6907,"column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. MatrixTable has four select methods that select and create new fields:. MatrixTable.select_globals(); MatrixTable.select_rows(); MatrixTable.select_cols(); MatrixTable.select_entries(). Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; KEY=VALUE to compute new fields. The Python unpack operator ** can be; used to specify that all fields",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:14109,Deployability,update,updated,14109," while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matrix Tables Locally; Some useful methods to interact with matrix tables locally are; MatrixTable.describe(), MatrixTable.head(), and; MatrixTable.sample_rows(). describe prints out the schema for all row; fields, column fields, entry fields, and global fields as well as the row keys; and column keys. head returns a new matrix table with only the first N rows.; sample_rows returns a new matrix table where the rows are randomly sampled with; frequency p.; To get the dimensions of the matrix table, use MatrixTable.count_rows(); and MatrixTable.count_cols(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:1697,Energy Efficiency,efficient,efficiently,1697,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:10347,Energy Efficiency,efficient,efficient,10347,"----------+------------+---------------+; | 20:10019093 | [""A"",""G""] | 1 |; | 20:10019093 | [""A"",""G""] | 2 |; | 20:10026348 | [""A"",""G""] | 1 |; | 20:10026348 | [""A"",""G""] | 2 |; | 20:10026357 | [""T"",""C""] | 1 |; | 20:10026357 | [""T"",""C""] | 2 |; | 20:10030188 | [""T"",""A""] | 1 |; | 20:10030188 | [""T"",""A""] | 2 |; | 20:10030452 | [""G"",""A""] | 1 |; | 20:10030452 | [""G"",""A""] | 2 |; +---------------+------------+---------------+; showing top 10 rows. Aggregation; MatrixTable has three methods to compute aggregate statistics. MatrixTable.aggregate_rows(); MatrixTable.aggregate_cols(); MatrixTable.aggregate_entries(). These methods take an aggregated expression and evaluate it, returning; a Python value.; An example of querying entries is to compute the global mean of field GQ:; >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) ; 67.73196915777027. It is possible to compute multiple values simultaneously by; creating a tuple or struct. This is encouraged, because grouping two; computations together is far more efficient by traversing the dataset only once; rather than twice.; >>> mt.aggregate_entries((hl.agg.stats(mt.DP), hl.agg.stats(mt.GQ))) ; (Struct(mean=41.83915800445897, stdev=41.93057654787303, min=0.0, max=450.0, n=34537, sum=1444998.9999999995),; Struct(mean=67.73196915777027, stdev=29.80840934057741, min=0.0, max=99.0, n=33720, sum=2283922.0000000135)). See the Aggregators page for the complete list of aggregator; functions. Group-By; Matrix tables can be aggregated along the row or column axis to produce a new; matrix table. MatrixTable.group_rows_by(); MatrixTable.group_cols_by(). First let’s add a random phenotype as a new column field case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_s",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:1729,Integrability,interface,interfaces,1729,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:11968,Performance,perform,performing,11968,"eld case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_n",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:12103,Performance,perform,performs,12103," hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; ",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:12494,Performance,perform,performed,12494,"4,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matri",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:1713,Security,expose,exposes,1713,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:2386,Security,access,accessed,2386,"hile MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:2727,Security,expose,exposed,2727,"d exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt ; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus ; <LocusExpression of type locus<GRCh37>>. Likewise, mt.DP is an Int32Expression with type int32; and is an entry field of mt.; Hail expressions can also Expression.describe() themselves, providing; information about their source matrix table or table and which",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:2917,Security,access,access,2917," form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt ; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus ; <LocusExpression of type locus<GRCh37>>. Likewise, mt.DP is an Int32Expression with type int32; and is an entry field of mt.; Hail expressions can also Expression.describe() themselves, providing; information about their source matrix table or table and which keys index the; expression, if any. For example, mt.DP.describe() tells us that mt.DP; has type int32 and is an entry field of mt, since it is indexed; by both rows and columns:; >>> mt.DP.describe() ; ---------------------------------",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:7794,Security,access,accessing,7794,"all_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. MatrixTable has four select methods that select and create new fields:. MatrixTable.select_globals(); MatrixTable.select_rows(); MatrixTable.select_cols(); MatrixTable.select_entries(). Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; KEY=VALUE to compute new fields. The Python unpack operator ** can be; used to specify that all fields of a Struct should become top level fields.; However, be aware that all top-level field names must be unique. In the; following example, **mt[‘info’] would fail if DP already exists as an entry; field.; >>> mt_new = mt.select_rows(**mt['info']) . The example below adds two new row fields. Keys are always preserved, so the; row keys locus and alleles will also be present in the new table.; AC = mt.info.AC turns the subfield AC into a top-level field.; >>> mt_new = mt.select_rows(AC = mt.info.AC,; ... n_filters = hl.len(mt['filters'])). The order of the fields entered as arguments will be maintained in the new; matrix table.; Drop; The complement of select methods, MatrixTable.drop() can remove any top; level field. An example of removing the GQ entry field is:; >>> mt_new = mt.drop('GQ'). Explode; Explode operations can is",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/matrix_table.html:6441,Usability,simpl,simplest,6441,"le.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty());",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
https://hail.is/docs/0.2/overview/table.html:8903,Availability,down,downstream,8903,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:4439,Deployability,update,update,4439,"cussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the rows of; a dataset, Hail provides an Table.aggregate() method which can be passed; a wide variety of aggregator functions (see Aggregators):; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'F')); 0.5. We also might want to compute the mean value of HT for each sex. This is; possible with a combination of Table.group_by() and; GroupedTable.aggregate():; >>> ht_agg = (ht.group_by(ht.SEX); ... .aggregate(mean = hl.agg.mean(ht.HT))); >>> ht_agg.show(); +-----+----------+; | SEX | mean |; +-----+----------+; | str | float64 |; +-----+----------+; | ""F"" | 6.50e+01 |; | ""M"" | 6.85e+01 |; +-----+----------+. Note that the result of ht.group_by(...).aggregate(...) is a new; Table while the result of ht.aggregate(...) is a Python value. Joi",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:8843,Deployability,pipeline,pipelines,8843,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:9400,Deployability,update,updated,9400,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:1340,Modifiability,variab,variable,1340,"oins; Interacting with Tables Locally. MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview; Table Overview. View page source. Table Overview; A Table is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the import_table() function.; >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; import_locus_intervals(), import_fam(), and import_bed().; Many Hail methods also return tables.; An example of a table is below. We recommend ht as a variable name for; tables, referring to a “Hail table”.; >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; In addition to row fields, Hail tables also have global fields. You can think of; globals as extra fields in the table whose values are identical for every row.; For example, the same table above with the global field G = 5 can be thought; of as; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | G |; +-------+-------+-----+-------+----",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:988,Performance,load,load,988,"ble Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview; Table Overview. View page source. Table Overview; A Table is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the import_table() function.; >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; import_locus_intervals(), import_fam(), and import_bed().; Many Hail methods also return tables.; An example of a table is below. We recommend ht as a variable name for; tables, referring to a “Hail table”.; >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; In addition to row fields, Hail tables also have global fie",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:7297,Performance,perform,perform,7297,"-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | A | B |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | 65 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | 72 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | 70 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | 60 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+. In addition to the Table.join() method, Hail provides another; join syntax using Python’s bracket indexing syntax. The syntax looks like; right_table[left_table.key], which will return an Expression; instead of a Table. This expression is a dictionary mapping the; keys in the left table to the rows in the right table.; We can annotate the left table with this expression to perform a left join:; left_table.annotate(x = right_table[left_table.key].x]. For example, below; we add the field ‘B’ from ht2 to ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:3621,Security,access,accessed,3621,"| 5 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. but the value 5 is only stored once for the entire dataset and NOT once per; row of the table. The output of Table.describe() lists what all of the row; fields and global fields are.; >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; Row fields can be specified to be the key of the table with the method; Table.key_by(). Keys are important for joining tables together (discussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:3728,Security,access,accessed,3728,"-----+-------+-----+-------+-------+-------+-------+-------+-------+. but the value 5 is only stored once for the entire dataset and NOT once per; row of the table. The output of Table.describe() lists what all of the row; fields and global fields are.; >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; Row fields can be specified to be the key of the table with the method; Table.key_by(). Keys are important for joining tables together (discussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:3835,Security,access,accessing,3835,"tput of Table.describe() lists what all of the row; fields and global fields are.; >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; Row fields can be specified to be the key of the table with the method; Table.key_by(). Keys are important for joining tables together (discussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the rows of; a dataset, Hail provides an Table.aggregate() method which can be passed; a wide variety of aggregator functions (see Aggregators):; >>> ht.aggregate(hl.agg.fract",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:8690,Security,access,accessed,8690,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:9078,Security,access,accessed,9078,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:9166,Security,access,accessed,9166,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:9264,Security,access,accessed,9264,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/overview/table.html:8835,Testability,test,testing,8835,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
https://hail.is/docs/0.2/stats/hail.stats.LinearMixedModel.html:946,Deployability,update,updated,946,"﻿. Hail | ; LinearMixedModel. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats; LinearMixedModel. View page source. LinearMixedModel. class hail.stats.LinearMixedModel[source]; Class representing a linear mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. Attributes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/stats/hail.stats.LinearMixedModel.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/stats/hail.stats.LinearMixedModel.html
https://hail.is/docs/0.2/stats/index.html:795,Deployability,update,updated,795,"﻿. Hail | ; stats. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats. View page source. stats. Classes. LinearMixedModel; Class representing a linear mixed model. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/stats/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/stats/index.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1527,Availability,avail,available,1527," Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.s",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1809,Availability,error,error,1809,"Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and t",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2141,Availability,down,downsampling,2141,", and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, ",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2377,Availability,down,downloads,2377,"aulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this function",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2925,Availability,down,downstream,2925,"use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTable.; We can use rows along with select to pull out 5 variants. The select method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.; Use the show method to display the variants. [6]:. mt.rows().select().show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""];",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:15914,Availability,error,error,15914,"start. [28]:. p = hl.plot.histogram(mt.sample_qc.call_rate, range=(.88,1), legend='Call Rate'); show(p). [Stage 24:> (0 + 1) / 1]. [29]:. p = hl.plot.histogram(mt.sample_qc.gq_stats.mean, range=(10,70), legend='Mean Sample GQ'); show(p). [Stage 27:> (0 + 1) / 1]. Often, these metrics are correlated. [30]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 30:> (0 + 1) / 1]. Removing outliers from the dataset will generally improve association results. We can make arbitrary cutoffs and use them to filter:. [31]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); print('After filter, %d/284 samples remain.' % mt.count_cols()). [Stage 32:> (0 + 1) / 1]. After filter, 250/284 samples remain. Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:16517,Availability,down,downstream,16517," (0 + 1) / 1]. After filter, 250/284 samples remain. Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. [33]:. mt = hl.variant_qc(mt). [34]:. mt.row.describe(). --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; rsid: str,; qual: float64,; filters: set<str>,; info: struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; },; variant_qc: struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:16643,Availability,down,downstream,16643,"eads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. [33]:. mt = hl.variant_qc(mt). [34]:. mt.row.describe(). --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; rsid: str,; qual: float64,; filters: set<str>,; info: struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; },; variant_qc: struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozyg",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:18413,Availability,error,error,18413,"m: float64,; set: str; },; variant_qc: struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64,; p_value_excess_het: float64; }; }; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7f0460fbcb50>; Index:; ['row']; --------------------------------------------------------. These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; str",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:18822,Availability,down,downsampled,18822,"e_excess_het: float64; }; }; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7f0460fbcb50>; Index:; ['row']; --------------------------------------------------------. These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields fo",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:19859,Availability,error,error,19859," already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Gen",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:20804,Availability,error,error,20804,"the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.; The pca function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca function does the same, using HWE-normalized genotypes for the PCA. [41]:. eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). [Stage 158:> (0 + 1) / 1]. [42]:. pprint(eigenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-0",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:22289,Availability,recover,recover,22289,"igenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-01,1.03e-02,2.65e-02,-8.51e-02,2.49e-02,5.67e-02,-8.31e-03]; showing top 5 rows. Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2218,Deployability,integrat,integrate,2218,"t(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:11825,Deployability,release,release,11825,"ing Python’s Counter class. [23]:. from collections import Counter; counts = Counter(snp_counts); counts.most_common(). [23]:. [(Struct(ref='C', alt='T'), 2418),; (Struct(ref='G', alt='A'), 2367),; (Struct(ref='A', alt='G'), 1929),; (Struct(ref='T', alt='C'), 1864),; (Struct(ref='C', alt='A'), 494),; (Struct(ref='G', alt='T'), 477),; (Struct(ref='T', alt='G'), 466),; (Struct(ref='A', alt='C'), 451),; (Struct(ref='C', alt='G'), 150),; (Struct(ref='G', alt='C'), 111),; (Struct(ref='T', alt='A'), 77),; (Struct(ref='A', alt='T'), 75)]. It’s nice to see that we can actually uncover something biological from this small dataset: we see that these frequencies come in pairs. C/T and G/A are actually the same mutation, just viewed from from opposite strands. Likewise, T/A and A/T are the same mutation on opposite strands. There’s a 30x difference between the frequency of C/T and A/T SNPs. Why?; The same Python, R, and Unix tools could do this work as well, but we’re starting to hit a wall - the latest gnomAD release publishes about 250 million variants, and that won’t fit in memory on a single computer.; What about genotypes? Hail can query the collection of all genotypes in the dataset, and this is getting large even for our tiny dataset. Our 284 samples and 10,000 variants produce 10 million unique genotypes. The gnomAD dataset has about 5 trillion unique genotypes.; Hail plotting functions allow Hail fields as arguments, so we can pass in the DP field directly here. If the range and bins arguments are not set, this function will compute the range based on minimum and maximum values of the field and use the default 50 bins. [24]:. p = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p). [Stage 22:> (0 + 1) / 1]. Quality Control; QC is where analysts spend most of their time with sequencing datasets. QC is an iterative process, and is different for every project: there is no “push-button” solution for QC. Each time the Broad collects a",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:27018,Deployability,update,updated,27018,"2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). mt = mt.annotate_cols(scores = pcs[mt.s].scores); gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 310:> (0 + 1) / 1]. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:19082,Energy Efficiency,consumption,consumption,19082,"tistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check wheth",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:20336,Energy Efficiency,consumption,consumption,20336,"------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.; The pca function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca function does the same, using HWE-normalized genotypes for the PCA. [41]:. eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). [Stage 158:> (0 + 1) / 1]. [42]:. pprint(eigenvalues). [18.084111467840707,; 9.9840764",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:23208,Energy Efficiency,power,powered,23208,"s well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the Matrix",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:23335,Energy Efficiency,consumption,consumption,23335,"cs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the MatrixTable.entries method to convert our matrix table to a table (with one row for each sample for each variant). In this representation, it is easy to aggregate over any fields we like, which is often ",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2218,Integrability,integrat,integrate,2218,"t(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:9843,Integrability,interface,interfaces,9843,"1,; min=-1.0,; max=10.0,; n=3500,; sum=13943.0). However, these metrics aren’t perfectly representative of the samples in our dataset. Here’s why:. [18]:. table.count(). [18]:. 3500. [19]:. mt.count_cols(). [19]:. 284. Since there are fewer samples in our dataset than in the full thousand genomes cohort, we need to look at annotations on the dataset. We can use aggregate_cols to get the metrics for only the samples in our dataset. [20]:. mt.aggregate_cols(hl.agg.counter(mt.pheno.SuperPopulation)). [20]:. {'AFR': 76, 'AMR': 34, 'EAS': 72, 'EUR': 47, 'SAS': 55}. [21]:. pprint(mt.aggregate_cols(hl.agg.stats(mt.pheno.CaffeineConsumption))). Struct(mean=4.415492957746479,; stdev=1.577763427465917,; min=0.0,; max=9.0,; n=284,; sum=1254.0). The functionality demonstrated in the last few cells isn’t anything especially new: it’s certainly not difficult to ask these questions with Pandas or R dataframes, or even Unix tools like awk. But Hail can use the same interfaces and query language to analyze collections that are much larger, like the set of variants.; Here we calculate the counts of each of the 12 possible unique SNPs (4 choices for the reference base * 3 choices for the alternate base).; To do this, we need to get the alternate allele of each variant and then count the occurences of each unique ref/alt pair. This can be done with Hail’s counter function. [22]:. snp_counts = mt.aggregate_rows(hl.agg.counter(hl.Struct(ref=mt.alleles[0], alt=mt.alleles[1]))); pprint(snp_counts). {Struct(ref='G', alt='A'): 2367,; Struct(ref='C', alt='T'): 2418,; Struct(ref='T', alt='A'): 77,; Struct(ref='C', alt='G'): 150,; Struct(ref='G', alt='C'): 111,; Struct(ref='G', alt='T'): 477,; Struct(ref='T', alt='G'): 466,; Struct(ref='T', alt='C'): 1864,; Struct(ref='A', alt='G'): 1929,; Struct(ref='C', alt='A'): 494,; Struct(ref='A', alt='T'): 75,; Struct(ref='A', alt='C'): 451}. We can list the counts in descending order using Python’s Counter class. [23]:. from collections import Counter; c",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:3134,Modifiability,variab,variable,3134,"et, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTable.; We can use rows along with select to pull out 5 variants. The select method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.; Use the show method to display the variants. [6]:. mt.rows().select().show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; showing top 5 rows. Alternatively:. [7]:. mt.row_key.show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; showing to",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:22683,Modifiability,variab,variable,22683,"8e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-01,1.03e-02,2.65e-02,-8.51e-02,2.49e-02,5.67e-02,-8.31e-03]; showing top 5 rows. Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:22756,Modifiability,variab,variable,22756,"8e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-01,1.03e-02,2.65e-02,-8.51e-02,2.49e-02,5.67e-02,-8.31e-03]; showing top 5 rows. Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1284,Performance,load,load,1284,"ing data from VCF; Getting to know our data; Adding column fields; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Epilogue. Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; ",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2456,Performance,load,load,2456,"er for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTa",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:21071,Performance,load,loadings,21071,"t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.; The pca function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca function does the same, using HWE-normalized genotypes for the PCA. [41]:. eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). [Stage 158:> (0 + 1) / 1]. [42]:. pprint(eigenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:15969,Safety,detect,detect,15969," 1) / 1]. Often, these metrics are correlated. [30]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 30:> (0 + 1) / 1]. Removing outliers from the dataset will generally improve association results. We can make arbitrary cutoffs and use them to filter:. [31]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); print('After filter, %d/284 samples remain.' % mt.count_cols()). [Stage 32:> (0 + 1) / 1]. After filter, 250/284 samples remain. Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. [33]:. mt = hl.variant_qc(mt). [34]:. mt.row.describe(). --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; rsid: str,; qual: float",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:22289,Safety,recover,recover,22289,"igenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-01,1.03e-02,2.65e-02,-8.51e-02,2.49e-02,5.67e-02,-8.31e-03]; showing top 5 rows. Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:25313,Safety,detect,detect,25313,"iant). In this representation, it is easy to aggregate over any fields we like, which is often the first step of rare variant analysis.; What if we want to group by minor allele frequency bin and hair color, and calculate the mean GQ?. [51]:. entries = entries.annotate(maf_bin = hl.if_else(entries.info.AF[0]<0.01, ""< 1%"",; hl.if_else(entries.info.AF[0]<0.05, ""1%-5%"", "">5%""))). results2 = (entries.group_by(af_bin = entries.maf_bin, purple_hair = entries.pheno.PurpleHair); .aggregate(mean_gq = hl.agg.stats(entries.GQ).mean,; mean_dp = hl.agg.stats(entries.DP).mean)). [52]:. results2.show(). [Stage 193:> (0 + 1) / 1]. af_binpurple_hairmean_gqmean_dpstrboolfloat64float64; ""1%-5%""False2.48e+017.43e+00; ""1%-5%""True2.46e+017.47e+00; ""< 1%""False2.35e+017.55e+00; ""< 1%""True2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:25602,Safety,detect,detect,25602,"e the mean GQ?. [51]:. entries = entries.annotate(maf_bin = hl.if_else(entries.info.AF[0]<0.01, ""< 1%"",; hl.if_else(entries.info.AF[0]<0.05, ""1%-5%"", "">5%""))). results2 = (entries.group_by(af_bin = entries.maf_bin, purple_hair = entries.pheno.PurpleHair); .aggregate(mean_gq = hl.agg.stats(entries.GQ).mean,; mean_dp = hl.agg.stats(entries.DP).mean)). [52]:. results2.show(). [Stage 193:> (0 + 1) / 1]. af_binpurple_hairmean_gqmean_dpstrboolfloat64float64; ""1%-5%""False2.48e+017.43e+00; ""1%-5%""True2.46e+017.47e+00; ""< 1%""False2.35e+017.55e+00; ""< 1%""True2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1116,Testability,test,test,1116,"; ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Download public 1000 Genomes data; Importing data from VCF; Getting to know our data; Adding column fields; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Epilogue. Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1373,Testability,log,logger,1373,"Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Epilogue. Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cel",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1774,Testability,log,log,1774,"Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is natur",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:18954,Testability,test,tests,18954,"0x7f0460fbcb50>; Index:; ['row']; --------------------------------------------------------. These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p =",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:23693,Testability,test,test,23693,"with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the MatrixTable.entries method to convert our matrix table to a table (with one row for each sample for each variant). In this representation, it is easy to aggregate over any fields we like, which is often the first step of rare variant analysis.; What if we want to group by minor allele frequency bin and hair color, and calculate the mean GQ?. [51]:. entries = entries.annotate(maf_bin = hl.if_else(entries.info.AF[0]<0.01, ""< 1%"",; hl.if_else(entries.info.AF[0]<0.05, ""1%-5%"", "">5%""))). results2 = (entries.g",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:23358,Usability,simpl,simply,23358,"cs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the MatrixTable.entries method to convert our matrix table to a table (with one row for each sample for each variant). In this representation, it is easy to aggregate over any fields we like, which is often ",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:25706,Usability,learn,learn,25706,"; .aggregate(mean_gq = hl.agg.stats(entries.GQ).mean,; mean_dp = hl.agg.stats(entries.DP).mean)). [52]:. results2.show(). [Stage 193:> (0 + 1) / 1]. af_binpurple_hairmean_gqmean_dpstrboolfloat64float64; ""1%-5%""False2.48e+017.43e+00; ""1%-5%""True2.46e+017.47e+00; ""< 1%""False2.35e+017.55e+00; ""< 1%""True2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). mt = mt.annotate_cols(scores = pcs[mt.s].",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
https://hail.is/docs/0.2/tutorials/03-tables.html:1399,Availability,down,download,1399,"l; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Tutorial. View page source. Table Tutorial; Table is Hail’s distributed analogue of a data frame or SQL table. It will be familiar if you’ve used R or pandas, but Table differs in 3 important ways:. It is distributed. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/03-tables.html:2049,Availability,avail,available,2049,"Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are di",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/03-tables.html:4956,Deployability,update,updated,4956,"y: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. You can count the rows of a table. [6]:. users.count(). [6]:. 943. You can access fields of tables with the Python attribute notation table.field, or with index notation table['field']. The latter is useful when the field names are not valid Python identifiers (if a field name includes a space, for example). [7]:. users.occupation.describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. [8]:. users['occupation'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. users.occupation and users['occupation'] are Hail Expressions; Lets peak at their using show. Notice that the key is shown as well!. [9]:. users.occupation.show(). idoccupationint32str; 1""technician""; 2""other""; 3""writer""; 4""technician""; 5""other""; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. Exercise; The movie dataset has two other tables: movies.ht and ratings.ht. Load these tables and have a quick look around. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/03-tables.html:1806,Performance,load,load,1806," familiar if you’ve used R or pandas, but Table differs in 3 important ways:. It is distributed. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; ",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/03-tables.html:2358,Performance,load,load,2358,"r Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/03-tables.html:3604,Security,access,access,3604," Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. You can count the rows of a table. [6]:. users.count(). [6]:. 943. You can access fields of tables with the Python attribute notation table.field, or with index notation table['field']. The latter is useful when the field names are not valid Python identifiers (if a field name includes a space, for example). [7]:. users.occupation.describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. [8]:. users['occupation'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. users.occupation and users['occupation'] are Hail Expressions; Lets peak at their using show. Notice that the key is shown as well!. [9]:. users.occupation.show(). idoccupationint32str; 1""technician""; 2""oth",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/03-tables.html:1895,Testability,log,logger,1895,"uted. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/03-tables.html:2296,Testability,log,log,2296,"ead_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""9404",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:2315,Availability,avail,available,2315,"ow to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:6353,Availability,error,errors,6353,"ntheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, 12, 14, 20, 19, 20, 20, 6, 12, 4, 11, 6, 9, 3, 3, 9, 3, 2, 3, 2, 3, 1, 0, 2, 5], n_smaller=1, n_larger=1). [12]:. p = hl.plot.histogram(hist, legend='Age'); show(p). take and collect; There are a few aggregators for collecting values. take localizes a few values into an array. It has an optional ordering.; collect localizes all values into an array.; collect_as_set localizes all unique values into a set. [13]:. users.aggregate(hl.agg.take(users.occupation, 5)). [13]:. ['technician', 'other', 'writer', 'technician', 'other']. [14]:. users.aggregate(hl.agg.take(users.age, 5, ordering=-users.age)). [14]:. [73, 70, 70, 70, 69]. Warning! Aggregators like collect and counter return Python objects and can fail with out of memory errors if you apply them to collections that are too large (e.g. all 50 trillion genotypes in the UK Biobank dataset). [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:6535,Deployability,update,updated,6535,"ntheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, 12, 14, 20, 19, 20, 20, 6, 12, 4, 11, 6, 9, 3, 3, 9, 3, 2, 3, 2, 3, 1, 0, 2, 5], n_smaller=1, n_larger=1). [12]:. p = hl.plot.histogram(hist, legend='Age'); show(p). take and collect; There are a few aggregators for collecting values. take localizes a few values into an array. It has an optional ordering.; collect localizes all values into an array.; collect_as_set localizes all unique values into a set. [13]:. users.aggregate(hl.agg.take(users.occupation, 5)). [13]:. ['technician', 'other', 'writer', 'technician', 'other']. [14]:. users.aggregate(hl.agg.take(users.age, 5, ordering=-users.age)). [14]:. [73, 70, 70, 70, 69]. Warning! Aggregators like collect and counter return Python objects and can fail with out of memory errors if you apply them to collections that are too large (e.g. all 50 trillion genotypes in the UK Biobank dataset). [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:2072,Performance,load,load,2072,"erage age? Youngest age? Oldest age?; What are all the occupations that appear, and how many times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum.",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:2684,Performance,load,load,2684,"in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. [5]:. users.aggregate(hl.agg.stats(users.age)). [5]:. Struct(mean=34.05196182396607, stdev=12.186273150937211, min=7.0, max=73.0, n=943, sum=32111.0). counter; What about non-numeric data, like the occupation field?; counter is modeled on the Python Counter object: it",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:2161,Testability,log,logger,2161,"y times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:2562,Testability,log,log,2562,"bine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. [5]:. users.aggregate(hl.agg.stats(users.age)). [5]:. Struct(mean=34.05196182396607, stdev=12.186273150937211, min=7.0, max=73.0, n=943, sum=32111.0). counter; ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:4607,Testability,log,logical,4607,"ion field?; counter is modeled on the Python Counter object: it counts the number of times each distinct value occurs in the collection of values being aggregated. [6]:. users.aggregate(hl.agg.counter(users.occupation)). [6]:. {'administrator': 79,; 'artist': 28,; 'doctor': 7,; 'educator': 95,; 'engineer': 67,; 'entertainment': 18,; 'executive': 32,; 'healthcare': 16,; 'homemaker': 7,; 'lawyer': 12,; 'librarian': 51,; 'marketing': 26,; 'none': 9,; 'other': 105,; 'programmer': 66,; 'retired': 14,; 'salesman': 12,; 'scientist': 31,; 'student': 196,; 'technician': 27,; 'writer': 45}. filter; You can filter elements of a collection before aggregation by using filter. [7]:. users.aggregate(hl.agg.filter(users.sex == 'M', hl.agg.count())). [7]:. 670. The argument to filter should be a Boolean expression. [8]:. users.aggregate(hl.agg.count_where(users.sex == 'M')). [8]:. 670. Boolean expressions can be compound, but be sure to use parentheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:4644,Testability,log,logical,4644,"ion field?; counter is modeled on the Python Counter object: it counts the number of times each distinct value occurs in the collection of values being aggregated. [6]:. users.aggregate(hl.agg.counter(users.occupation)). [6]:. {'administrator': 79,; 'artist': 28,; 'doctor': 7,; 'educator': 95,; 'engineer': 67,; 'entertainment': 18,; 'executive': 32,; 'healthcare': 16,; 'homemaker': 7,; 'lawyer': 12,; 'librarian': 51,; 'marketing': 26,; 'none': 9,; 'other': 105,; 'programmer': 66,; 'retired': 14,; 'salesman': 12,; 'scientist': 31,; 'student': 196,; 'technician': 27,; 'writer': 45}. filter; You can filter elements of a collection before aggregation by using filter. [7]:. users.aggregate(hl.agg.filter(users.sex == 'M', hl.agg.count())). [7]:. 670. The argument to filter should be a Boolean expression. [8]:. users.aggregate(hl.agg.count_where(users.sex == 'M')). [8]:. 670. Boolean expressions can be compound, but be sure to use parentheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:1368,Usability,learn,learn,1368,"able Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Aggregation Tutorial. View page source. Aggregation Tutorial; In the last section, we inspected the structure of the data and displayed a few example values.; How do we get a deeper feel for the data? One of the most natural things to do is to create a summary of a large number of values. For example, you could ask:. How many women are in the dataset? How many men?; What is the average age? Youngest age? Oldest age?; What are all the occupations that appear, and how many times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/04-aggregation.html:1746,Usability,simpl,simplest,1746,"ast section, we inspected the structure of the data and displayed a few example values.; How do we get a deeper feel for the data? One of the most natural things to do is to create a summary of a large number of values. For example, you could ask:. How many women are in the dataset? How many men?; What is the average age? Youngest age? Oldest age?; What are all the occupations that appear, and how many times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-oper",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1367,Availability,avail,available,1367,"ixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 66. Annotate; You can add new fields to a table with annotate. As an example, let’s create a new column called cleaned_occupation that replaces missing entries in the occupation field labeled as ‘other’ with ‘non",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:7289,Deployability,update,updated,7289,"ssing_occupations = hl.set(['other', 'none']). t = users.select(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idcleaned_occupationint32str; 1""technician""; 2NA; 3""writer""; 4""technician""; 5NA; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. [11]:. missing_occupations = hl.set(['other', 'none']). t = users.transmute(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idagesexzipcodecleaned_occupationint32int32strstrstr; 124""M""""85711""""technician""; 253""F""""94043""NA; 323""M""""32067""""writer""; 424""M""""43537""""technician""; 533""F""""15213""NA; 642""M""""98101""""executive""; 757""M""""91344""""administrator""; 836""M""""05201""""administrator""; 929""M""""01002""""student""; 1053""M""""90703""""lawyer""; showing top 10 rows. Global Fields; Finally, you can add global fields with annotate_globals. Globals are useful for storing metadata about a dataset or storing small data structures like sets and maps. [12]:. t = users.annotate_globals(cohort = 5, cloudable = hl.set(['sample1', 'sample10', 'sample15'])); t.describe(). ----------------------------------------; Global fields:; 'cohort': int32; 'cloudable': set<str>; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. [13]:. t.cloudable. [13]:. <SetExpression of type set<str>>. [14]:. hl.eval(t.cloudable). [14]:. {'sample1', 'sample10', 'sample15'}. Exercises. Z-score normalize the age field of users.; Convert zip to an integer. Hint: Not all zipcodes are US zipcodes! Use hl.int32 to convert a string to an integer. Use StringExpression.matches to see if a string matches a regular expression. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1124,Performance,load,load,1124,"Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Filter; Annotate; Select and Transmute; Global Fields; Exercises. Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1759,Performance,load,load,1759,"urce. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 66. Annotate; You can add new fields to a table with annotate. As an example, let’s create a new column called cleaned_occupation that replaces missing entries in the occupation field labeled as ‘other’ with ‘none.’. [5]:. missing_occupations = hl.set(['other', 'none']). t = users.annotate(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idagesexoccupationzipcodecleaned_occupationint32int32strstrstrstr; 124""M""""technician""""85711""""technician""; 253""F""""other""",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1213,Testability,log,logger,1213," Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Filter; Annotate; Select and Transmute; Global Fields; Exercises. Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 6",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1614,Testability,log,log,1614,"elopers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 66. Annotate; You can add new fields to a table with annotate. As an example, let’s create a new column called cleaned_occupation that replaces missing entries in the occupation field labeled as ‘other’ with ‘none.’. [5]:. missing_occupations = hl.set(['other', 'none']). t = users.annotate(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation))",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
https://hail.is/docs/0.2/tutorials/06-joins.html:1789,Availability,avail,available,1789,"illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. key is a struct expression of all of the key fields, so we can refer to the key of a table without explicitly specifying",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:4536,Availability,down,down,4536,"’ll use the Table.parallelize() method to create two small tables, t1 and t2. [4]:. t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). [5]:. t1.show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. abstrint32; ""bar""2; ""bar""2; ""foo""1. [6]:. t2.show(). txstrfloat64; ""bar""2.78e+00; ""bar""-1.00e+00; ""foo""3.14e+00; ""quam""0.00e+00. Now, we can join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new joined table j has a field t2_x that comes from the rows of t2. The tables could be joined, because they shared the same number of keys (1) and the same key type (string). The keys do not need to share the same name. Notice that the rows with keys present in t2 but not in t1 do not show up in the final result.; This join syntax performs a left join. Tables also have a SQL-style inner/left/right/outer join() method.; The magic of keys is that they can be used to create a mapping, like a Python dictionary, between the keys of one table and the row value",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:14316,Deployability,update,updated,14316,"ngineer""""Charade (1963)""5.00e+00; ""entertainment""""American in Paris, An (1951)""5.00e+00; ""executive""""A Chef in Love (1996)""5.00e+00; ""healthcare""""39 Steps, The (1935)""5.00e+00; ""homemaker""""Beautiful Girls (1996)""5.00e+00; ""lawyer""""Anastasia (1997)""5.00e+00; showing top 10 rows. Let’s try to get a deeper understanding of this result. Notice that every movie displayed has an average rating of 5, which means that every person gave these movies the highest rating. Is that unlikely? We can determine how many people rated each of these movies by working backwards and filtering our original movie_data table by fields in highest_rated.; Note that in the second line below, we are taking advantage of the fact that Hail tables are keyed. [19]:. highest_rated = highest_rated.key_by(; highest_rated.occupation, highest_rated.movie). counts_temp = movie_data.filter(; hl.is_defined(highest_rated[movie_data.occupation, movie_data.movie])). counts = counts_temp.group_by(counts_temp.occupation, counts_temp.movie).aggregate(; counts = hl.agg.count()). counts.show(). [Stage 108:> (0 + 1) / 1]. occupationmoviecountsstrstrint64; ""administrator""""A Chef in Love (1996)""1; ""artist""""39 Steps, The (1935)""1; ""doctor""""Alien (1979)""1; ""educator""""Aparajito (1956)""2; ""engineer""""Charade (1963)""1; ""entertainment""""American in Paris, An (1951)""1; ""executive""""A Chef in Love (1996)""1; ""healthcare""""39 Steps, The (1935)""1; ""homemaker""""Beautiful Girls (1996)""1; ""lawyer""""Anastasia (1997)""1; showing top 10 rows. So it looks like the highest rated movies, when computed naively, mostly have a single viewer rating them. To get a better understanding of the data, we can recompute this list but only include movies which have more than 1 viewer (left as an exercise). Exercises. What is the favorite movie for each occupation, conditional on there being more than one viewer?; What genres are rated most differently by men and women?. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:1231,Performance,load,load,1231,"tudy (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; The Key to Understanding Joins; Joining Tables; Exercises. MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:1546,Performance,load,load,1546,"ts; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; ",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:3969,Performance,load,load,3969,"e:; struct {; id: int32; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f5beee034f0>; Index:; ['row']; --------------------------------------------------------. Keys need not be unique or non-missing, although in many applications they will be both.; When tables are joined in Hail, they are joined based on their keys. In order to join two tables, they must share the same number of keys, same key types (i.e. string vs integer), and the same order of keys.; Let’s look at a simple example of a join. We’ll use the Table.parallelize() method to create two small tables, t1 and t2. [4]:. t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). [5]:. t1.show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. abstrint32; ""bar""2; ""bar""2; ""foo""1. [6]:. t2.show(). txstrfloat64; ""bar""2.78e+00; ""bar""-1.00e+00; ""foo""3.14e+00; ""quam""0.00e+00. Now, we can join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:5299,Performance,perform,performs,5299,"an join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new joined table j has a field t2_x that comes from the rows of t2. The tables could be joined, because they shared the same number of keys (1) and the same key type (string). The keys do not need to share the same name. Notice that the rows with keys present in t2 but not in t1 do not show up in the final result.; This join syntax performs a left join. Tables also have a SQL-style inner/left/right/outer join() method.; The magic of keys is that they can be used to create a mapping, like a Python dictionary, between the keys of one table and the row values of another table: table[expr] will refer to the row of table that has a key value of expr. If the row is not unique, one such row is chosen arbitrarily.; Here’s a subtle bit: if expr is an expression indexed by a row of table2, then table[expr] is also an expression indexed by a row of table2.; Also note that while they look similar, table['field'] and table1[table2.key] are doing very different things!; table['field'] selects a field from the table, while table1[table2.key] creates a mapping between the keys of table2 and the rows of table1. [9]:. t1['a'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f5bee73d130>; Index:; ['row']",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:1635,Testability,log,logger,1635,"ge Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; -----",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:2036,Testability,log,log,2036,"pation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. key is a struct expression of all of the key fields, so we can refer to the key of a table without explicitly specifying the names of the key fields. [3]:. users.key.describe(). --------------------------------------------------------; Type:; struct {; id: int32; }; -----------------------------------------------------",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:825,Usability,simpl,simple,825,"﻿. Hail | ; Table Joins Tutorial. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; The Key to Understanding Joins; Joining Tables; Exercises. MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/06-joins.html:3498,Usability,simpl,simple,3498,"; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. key is a struct expression of all of the key fields, so we can refer to the key of a table without explicitly specifying the names of the key fields. [3]:. users.key.describe(). --------------------------------------------------------; Type:; struct {; id: int32; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f5beee034f0>; Index:; ['row']; --------------------------------------------------------. Keys need not be unique or non-missing, although in many applications they will be both.; When tables are joined in Hail, they are joined based on their keys. In order to join two tables, they must share the same number of keys, same key types (i.e. string vs integer), and the same order of keys.; Let’s look at a simple example of a join. We’ll use the Table.parallelize() method to create two small tables, t1 and t2. [4]:. t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). [5]:. t1.show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. abstrint32; ""bar""2; ""bar""2; ""foo""1. [6]:. t2.show(). txstrfloat64; ""bar""2.78e+00; ""bar""-1.00e+00; ""foo""3.14e+00; ""quam""0.00e+00. Now, we can join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""1",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:3019,Availability,avail,available,3019,".; Column fields are stored once per column. These can contain information about the columns, or summary data calculated per column.; Entry fields are the piece that makes this structure a matrix – there is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry f",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:10599,Deployability,update,updated,10599,"; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; 'call_rate': float64; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. [12]:. p = hl.plot.histogram(mt2.call_rate, range=(0,1.0), bins=100,; title='Variant Call Rate Histogram', legend='Call Rate'); show(p). Exercise: GQ vs DP; In this exercise, you’ll use Hail to investigate a strange property of sequencing datasets.; The DP field is the sequencing depth (the number of reads).; Let’s first plot a histogram of DP:. [13]:. p = hl.plot.histogram(mt.DP, range=(0,40), bins=40, title='DP Histogram', legend='DP'); show(p). [Stage 9:> (0 + 1) / 1]. Now, let’s do the same thing for GQ.; The GQ field is the phred-scaled “genotype quality”. The formula to convert to a linear-scale confidence (0 to 1) is 10 ** -(mt.GQ / 10). GQ is truncated to lie between 0 and 99. [14]:. p = hl.plot.histogram(mt.GQ, range=(0,100), bins=100, title='GQ Histogram', legend='GQ'); show(p). [Stage 10:> (0 + 1) / 1]. Whoa! That’s a strange distribution! There’s a big spike at 100. The rest of the values have roughly the same shape as the DP distribution, but form a Dimetrodon. Use Hail to figure out what’s going on!. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:966,Performance,scalab,scalability,966,"﻿. Hail | ; MatrixTable Tutorial. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; MatrixTable Anatomy; Importing and Reading; MatrixTable operations; Exercise: GQ vs DP. Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; MatrixTable Tutorial. View page source. MatrixTable Tutorial; If you’ve gotten this far, you’re probably thinking:. “Can’t I do all of this in pandas or R?”; “What does this have to do with biology?”. The two crucial features that Hail adds are scalability and the domain-specific primitives needed to work easily with biological data. Fear not! You’ve learned most of the basic concepts of Hail and now are ready for the bit that makes it possible to represent and compute on genetic matrices: the MatrixTable.; In the last example of the Table Joins Tutorial, the ratings table had a compound key: movie_id and user_id. The ratings were secretly a movie-by-user matrix!; However, since this matrix is very sparse, it is reasonably represented in a so-called “coordinate form” Table, where each row of the table is an entry of the sparse matrix. For large and dense matrices (like sequencing data), the per-row overhead of coordinate reresentations is untenable. That’s why we built MatrixTable, a 2-dimensional generalization of Table. MatrixTable Anatomy; Recall that Table has two kinds of fields:. global fields; row fields. MatrixTable has four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. Th",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:2776,Performance,load,load,2776,"tomy; Recall that Table has two kinds of fields:. global fields; row fields. MatrixTable has four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. These can contain information about the rows, or summary data calculated per row.; Column fields are stored once per column. These can contain information about the columns, or summary data calculated per column.; Entry fields are the piece that makes this structure a matrix – there is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingR",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:6655,Performance,load,load,6655,"tries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[""C"",""A""]; showing top 10 rows. [7]:. mt.s.show(). sstr; ""HG00096""; ""HG00099""; ""HG00105""; ""HG00118""; ""HG00129""; ""HG00148""; ""HG00177""; ""HG00182""; ""HG00242""; ""HG00254""; showing top 10 rows. Let’s investigate the genotypes and the call rate. Let’s look at the first few genotypes:. [8]:. mt.GT.show(). 'HG00096''HG00099''HG00105''HG00118'locusallelesGTGTGTGTlocus<GRCh37>array<str>callcallcallcall; 1:904165[""G"",""A""]0/00/00/00/0; 1:909917[""G"",""A""]0/00/00/00/0; 1:986963[""C"",""T""]0/00/00/00/0; 1:1563691[""T"",""G""]NA0/00/00/0; 1:1707740[""T"",""G""]0/10/10/10/0; 1:2252970[""C"",""T""]0/0NA0/00/0; ",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:6040,Security,access,accessed,6040,"--------------------------------. [4]:. mt.GT.describe(). --------------------------------------------------------; Type:; call; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7efd1f44dc10>; Index:; ['row', 'column']; --------------------------------------------------------. MatrixTable operations; We belabored the operations on tables because they all have natural analogs (sometimes several) on matrix tables. For example:. count => count_{rows, cols} (and count which returns both); filter => filter_{rows, cols, entries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:6096,Security,access,accessed,6096,"--------------------------------. [4]:. mt.GT.describe(). --------------------------------------------------------; Type:; call; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7efd1f44dc10>; Index:; ['row', 'column']; --------------------------------------------------------. MatrixTable operations; We belabored the operations on tables because they all have natural analogs (sometimes several) on matrix tables. For example:. count => count_{rows, cols} (and count which returns both); filter => filter_{rows, cols, entries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:6175,Security,access,accessed,6175,"----------; Type:; call; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7efd1f44dc10>; Index:; ['row', 'column']; --------------------------------------------------------. MatrixTable operations; We belabored the operations on tables because they all have natural analogs (sometimes several) on matrix tables. For example:. count => count_{rows, cols} (and count which returns both); filter => filter_{rows, cols, entries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[""C"",""A""]; showing top 10 rows. [7]:. mt.s.show(). sstr; ""HG00096""; ""HG00099""; ""HG00105""; ""HG00118""; ""HG",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:2865,Testability,log,logger,2865,"s four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. These can contain information about the rows, or summary data calculated per row.; Column fields are stored once per column. These can contain information about the columns, or summary data calculated per column.; Entry fields are the piece that makes this structure a matrix – there is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingC",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:3266,Testability,log,log,3266,"here is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/07-matrixtable.html:1074,Usability,learn,learned,1074,"2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; MatrixTable Anatomy; Importing and Reading; MatrixTable operations; Exercise: GQ vs DP. Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; MatrixTable Tutorial. View page source. MatrixTable Tutorial; If you’ve gotten this far, you’re probably thinking:. “Can’t I do all of this in pandas or R?”; “What does this have to do with biology?”. The two crucial features that Hail adds are scalability and the domain-specific primitives needed to work easily with biological data. Fear not! You’ve learned most of the basic concepts of Hail and now are ready for the bit that makes it possible to represent and compute on genetic matrices: the MatrixTable.; In the last example of the Table Joins Tutorial, the ratings table had a compound key: movie_id and user_id. The ratings were secretly a movie-by-user matrix!; However, since this matrix is very sparse, it is reasonably represented in a so-called “coordinate form” Table, where each row of the table is an entry of the sparse matrix. For large and dense matrices (like sequencing data), the per-row overhead of coordinate reresentations is untenable. That’s why we built MatrixTable, a 2-dimensional generalization of Table. MatrixTable Anatomy; Recall that Table has two kinds of fields:. global fields; row fields. MatrixTable has four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. These can contain information about the rows, or summary data calculated per row.; Column fields are stored once per column. These can contain in",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:1353,Availability,avail,available,1353,"; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': str; 'SuperPopulation': str; 'isFemale': bool; 'PurpleHair': bool; 'CaffeineConsumption': int32; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:5750,Availability,down,downsample,5750,"an also pass in a Hail field as a label argument, which determines how to color the data points. [7]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for down",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:5917,Availability,down,downsampled,5917,"ts. [7]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value,",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:6139,Availability,down,downsampled,6139,"r_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover ",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:6792,Availability,down,downsampling,6792," (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None); show(p). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:7220,Availability,down,downsample,7220," (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None); show(p). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:7472,Deployability,update,updated,7472," (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None); show(p). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:6311,Modifiability,variab,variables,6311,"ariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None)",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:6541,Modifiability,variab,variables,6541,"ariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None)",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:1110,Performance,load,load,1110,"ence. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; Histogram; Cumulative Histogram; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:1860,Performance,load,load,1860,"amples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': str; 'SuperPopulation': str; 'isFemale': bool; 'PurpleHair': bool; 'CaffeineConsumption': int32; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:5237,Performance,cache,cache,5237," value of 50. [4]:. p = hl.plot.histogram(mt.DP, range=(0, 30), bins=30); show(p). [Stage 4:> (0 + 1) / 1]. Cumulative Histogram; The cumulative_histogram() method works in a similar way to histogram(). [5]:. p = hl.plot.cumulative_histogram(mt.DP, range=(0,30), bins=30); show(p). Scatter; The scatter() method can also take in either Python types or Hail fields as arguments for x and y. [6]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 7:> (0 + 1) / 1]. We can also pass in a Hail field as a label argument, which determines how to color the data points. [7]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:1199,Testability,log,logger,1199,"enome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; Histogram; Cumulative Histogram; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': st",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/08-plotting.html:1600,Testability,log,log,1600,"tabase; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': str; 'SuperPopulation': str; 'isFemale': bool; 'PurpleHair': bool; 'CaffeineConsumption': int32; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_hom_ref: int64,; n_het: int6",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:2025,Availability,avail,available,2025,"ighly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = gg",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:5923,Deployability,continuous,continuous,5923,"’s filter the data to 2007 for our first experiments. [10]:. gp_2007 = gp.filter(gp.year == 2007). If we want to see how many countries from each continent we have, we can use geom_bar, which just takes in an x aesthetic and then implicitly counts how many values of each x there are. [11]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(). [11]:. To make it a little prettier, let’s color per continent as well. We use fill to specify color of shapes (as opposed to color for points and lines. color on a bar chart sets the color of the bar outline.). [12]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent)). [12]:. Maybe we instead want to see not the number of countries per continent, but the number of people living on each continent. We can do this with geom_bar as well by specifying a weight. [13]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent, weight=gp_2007.pop)). [13]:. Histograms are similar to bar plots, except they break a continuous x axis into bins. Let’s import the iris dataset for this. [14]:. iris = hl.Table.from_pandas(plotly.data.iris()); iris.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sepal_length': float64; 'sepal_width': float64; 'petal_length': float64; 'petal_width': float64; 'species': str; 'species_id': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s make a histogram:. [15]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(). [15]:. By default histogram plots groups stacked on top of each other, which is not always easy to interpret. We can specify the position argument to histogram to get different behavior. ""dodge"" puts the bars next to each other:. [16]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""dodge""). [16]:. And ""identity"" plots them over each other. It he",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:7590,Deployability,update,updated,7590,"ies per continent, but the number of people living on each continent. We can do this with geom_bar as well by specifying a weight. [13]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent, weight=gp_2007.pop)). [13]:. Histograms are similar to bar plots, except they break a continuous x axis into bins. Let’s import the iris dataset for this. [14]:. iris = hl.Table.from_pandas(plotly.data.iris()); iris.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sepal_length': float64; 'sepal_width': float64; 'petal_length': float64; 'petal_width': float64; 'species': str; 'species_id': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s make a histogram:. [15]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(). [15]:. By default histogram plots groups stacked on top of each other, which is not always easy to interpret. We can specify the position argument to histogram to get different behavior. ""dodge"" puts the bars next to each other:. [16]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""dodge""). [16]:. And ""identity"" plots them over each other. It helps to set an alpha value to make them slightly transparent in these cases. [17]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""identity"", alpha=0.8). [17]:. Labels and Axes; It’s always a good idea to label your axes. This can be done most easily with xlab and ylab. We can also use ggtitle to add a title. Let’s pull in the same plot from above, and add labels. [18]:. (ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) +; geom_histogram(position=""identity"", alpha=0.8) +; xlab(""Sepal Length"") + ylab(""Number of samples"") + ggtitle(""Sepal length by flower type""); ). [18]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:2683,Integrability,interface,interface,2683,"nitializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:1012,Modifiability,flexible,flexible,1012,". 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial; The Grammar of Graphics; Geoms that group; Labels and Axes. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GGPlot Tutorial. View page source. GGPlot Tutorial. [1]:. import hail as hl; from hail.ggplot import *. import plotly. Loading BokehJS ... The Hail team has implemented a plotting module for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI ava",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:4081,Modifiability,inherit,inherits,4081,"dx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'country': str; 'continent': str; 'year': int32; 'lifeExp': float64; 'pop': int32; 'gdpPercap': float64; 'iso_alpha': str; 'iso_num': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s filter the data to 2007 for our first experiments. [10]:. gp_2007 = gp.filter(gp.year == 2007). If we want to see how many countries from each continent we have, we can use geom_bar, which just ",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:1782,Performance,load,load,1782,"t import *. import plotly. Loading BokehJS ... The Hail team has implemented a plotting module for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:2294,Performance,load,load,2294,"ive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to str",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:3655,Safety,interlock,interlocking,3655,"n x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; -------------------------------------",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:3655,Security,interlock,interlocking,3655,"n x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; -------------------------------------",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:1871,Testability,log,logger,1871,"le for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or poi",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/tutorials/09-ggplot.html:2272,Testability,log,log,2272,"ive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to str",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
https://hail.is/docs/0.2/utils/index.html:5639,Availability,error,error,5639,"er than 50 MB). Examples; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: ; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt') as f: ; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:5854,Availability,error,error,5854," file stored in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt') as f: ; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', '",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:7577,Availability,error,error,7577," Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: ; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_dir(path)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_stat(path)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. hail.utils.hadoop_ls(path)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[sour",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:8014,Availability,error,error,8014,"tr) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_dir(path)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_stat(path)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. hail.utils.hadoop_ls(path)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:11129,Availability,down,download,11129,"tional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next ",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:11446,Availability,down,download,11446,". Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:11721,Availability,down,download,11721,". Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:12176,Deployability,update,updated,12176,". Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10033,Performance,optimiz,optimized,10033,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10059,Performance,perform,performance,10059,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10754,Performance,optimiz,optimized,10754,"artitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.;",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10780,Performance,perform,performance,10780,"artitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.;",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:3058,Security,access,accessing,3058,"ions. get_movie_lens(output_dir[, overwrite]); Download public Movie Lens dataset. class hail.utils.Interval(start, end, includes_start=True, includes_end=False, point_type=None)[source]; An object representing a range of values between start and end.; >>> interval2 = hl.Interval(3, 6). Parameters:. start (any type) – Object with type point_type.; end (any type) – Object with type point_type.; includes_start (bool) – Interval includes start.; includes_end (bool) – Interval includes end. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.interval.take(5). This is rare; it is much; more common to manipulate the IntervalExpression object, which is; constructed using the following functions:. interval(); locus_interval(); parse_locus_interval(). class hail.utils.Struct(**kwargs)[source]; Nested annotation structure.; >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:; >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:; attributes – Field names and values. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.info.take(5). This is rare; it is much; more common to manipulate the StructExpression object, which is; constructed using the struct() function. class hail.utils.frozendict(d)[source]; An object representing an immutable dictionary.; >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a frozendict:; >>> dict(frozendict({'a': 1, 'b': 2})). Note; This object refers to the Pyth",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:3251,Security,access,accessed,3251,"source]; An object representing a range of values between start and end.; >>> interval2 = hl.Interval(3, 6). Parameters:. start (any type) – Object with type point_type.; end (any type) – Object with type point_type.; includes_start (bool) – Interval includes start.; includes_end (bool) – Interval includes end. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.interval.take(5). This is rare; it is much; more common to manipulate the IntervalExpression object, which is; constructed using the following functions:. interval(); locus_interval(); parse_locus_interval(). class hail.utils.Struct(**kwargs)[source]; Nested annotation structure.; >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:; >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:; attributes – Field names and values. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.info.take(5). This is rare; it is much; more common to manipulate the StructExpression object, which is; constructed using the struct() function. class hail.utils.frozendict(d)[source]; An object representing an immutable dictionary.; >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a frozendict:; >>> dict(frozendict({'a': 1, 'b': 2})). Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.my_dict.take(5). This is rare; it is much; more common to manipulate the DictExpression object, which is; cons",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:6305,Security,access,access,6305,"ruct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: ; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:1574,Testability,log,log,1574,"nd Version Policy. menu; Hail. Python API; Hail Query Python API; utils. View page source. utils. ANY_REGION; Built-in mutable sequence. Interval(start, end[, includes_start, ...]); An object representing a range of values between start and end. Struct(**kwargs); Nested annotation structure. frozendict(d); An object representing an immutable dictionary. hadoop_open(path[, mode, buffer_size]); Open a file through the Hadoop filesystem API. hadoop_copy(src, dest); Copy a file through the Hadoop filesystem API. hadoop_exists(path); Returns True if path exists. hadoop_is_file(path); Returns True if path both exists and is a file. hadoop_is_dir(path); Returns True if path both exists and is a directory. hadoop_stat(path); Returns information about the file or directory at a given path. hadoop_ls(path); Returns information about files at path. hadoop_scheme_supported(scheme); Returns True if the Hadoop filesystem supports URLs with the given scheme. copy_log(path); Attempt to copy the session log to a hadoop-API-compatible location. range_table(n[, n_partitions]); Construct a table with the row index and no other fields. range_matrix_table(n_rows, n_cols[, ...]); Construct a matrix table with row and column indices and no entry fields. get_1kg(output_dir[, overwrite]); Download subset of the 1000 Genomes dataset and sample annotations. get_hgdp(output_dir[, overwrite]); Download subset of the Human Genome Diversity Panel dataset and sample annotations. get_movie_lens(output_dir[, overwrite]); Download public Movie Lens dataset. class hail.utils.Interval(start, end, includes_start=True, includes_end=False, point_type=None)[source]; An object representing a range of values between start and end.; >>> interval2 = hl.Interval(3, 6). Parameters:. start (any type) – Object with type point_type.; end (any type) – Object with type point_type.; includes_start (bool) – Interval includes start.; includes_end (bool) – Interval includes end. Note; This object refers to the Python val",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9003,Testability,log,log,9003," Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant f",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9129,Testability,log,log,9129,"ined in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitio",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9151,Testability,log,log,9151,"ined in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitio",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9192,Testability,log,log,9192,"ursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9273,Testability,log,log,9273,"bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9334,Testability,log,log,9334,"n bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and c",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9377,Testability,log,log,9377," last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_t",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9474,Testability,log,logs,9474," last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_t",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9535,Testability,log,log,9535,"op_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table con",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9614,Testability,log,log,9614,"RLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32)",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:9658,Testability,log,log,9658,"; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It cont",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10000,Testability,test,testing,10000,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10721,Testability,test,testing,10721,"artitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.;",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:6764,Usability,simpl,simpler,6764,"Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: ; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_dir(path)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_stat(path)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_ti",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10012,Usability,learn,learning,10012,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/utils/index.html:10733,Usability,learn,learning,10733,"artitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.;",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.load_combiner.html:911,Deployability,update,updated,911,"﻿. Hail | ; hail.vds.combiner.load_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.load_combiner. View page source. hail.vds.combiner.load_combiner. hail.vds.combiner.load_combiner(path)[source]; Load a VariantDatasetCombiner from path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.load_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.load_combiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html:1419,Deployability,update,updated,1419,"﻿. Hail | ; hail.vds.combiner.new_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.new_combiner. View page source. hail.vds.combiner.new_combiner. hail.vds.combiner.new_combiner(*, output_path, temp_path, save_path=None, gvcf_paths=None, vds_paths=None, vds_sample_counts=None, intervals=None, import_interval_size=None, use_genome_default_intervals=False, use_exome_default_intervals=False, gvcf_external_header=None, gvcf_sample_names=None, gvcf_info_to_keep=None, gvcf_reference_entry_fields_to_keep=None, call_fields=['PGT'], branch_factor=100, target_records=24000, gvcf_batch_size=None, batch_size=None, reference_genome='default', contig_recoding=None, force=False)[source]; Create a new VariantDatasetCombiner or load one from save_path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.new_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html:1337,Performance,load,load,1337,"﻿. Hail | ; hail.vds.combiner.new_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.new_combiner. View page source. hail.vds.combiner.new_combiner. hail.vds.combiner.new_combiner(*, output_path, temp_path, save_path=None, gvcf_paths=None, vds_paths=None, vds_sample_counts=None, intervals=None, import_interval_size=None, use_genome_default_intervals=False, use_exome_default_intervals=False, gvcf_external_header=None, gvcf_sample_names=None, gvcf_info_to_keep=None, gvcf_reference_entry_fields_to_keep=None, call_fields=['PGT'], branch_factor=100, target_records=24000, gvcf_batch_size=None, batch_size=None, reference_genome='default', contig_recoding=None, force=False)[source]; Create a new VariantDatasetCombiner or load one from save_path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.new_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:813,Availability,failure,failure-tolerant,813,"﻿. Hail | ; VariantDatasetCombiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VariantDatasetCombiner. View page source. VariantDatasetCombiner. class hail.vds.combiner.VariantDatasetCombiner[source]; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets.; Examples; A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='g",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:2361,Availability,avail,available,2361,"new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: use_exome_default_intervals=True and; use_genome_default_intervals=True.; The combiner serializes itself to save_path so that it can be restarted after failure. Parameters:. save_path (str) – The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path (str) – The location to store the new VariantDataset.; temp_path (str) – The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome (ReferenceGenome) – The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor (int) – The number of Variant Datasets to combine at once.; target_records (int) – The target number of variants per partition.; gvcf_batch_size (int) – The number of GVCFs to ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:2654,Availability,failure,failure,2654,"e extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: use_exome_default_intervals=True and; use_genome_default_intervals=True.; The combiner serializes itself to save_path so that it can be restarted after failure. Parameters:. save_path (str) – The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path (str) – The location to store the new VariantDataset.; temp_path (str) – The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome (ReferenceGenome) – The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor (int) – The number of Variant Datasets to combine at once.; target_records (int) – The target number of variants per partition.; gvcf_batch_size (int) – The number of GVCFs to combine into a Variant Dataset at once.; contig_recoding (dict mapping str to str or None) – This mapping is applied to GVCF contigs before importing them into Hail. This is used to; handle GVCFs containing invalid contig names. For example, GRCh38 GVCFs which contain the; co",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:6476,Deployability,update,updated,6476,"nfo_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. to_dict()[source]; A serializable representation of this combiner. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:2236,Integrability,depend,depends,2236,"_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: use_exome_default_intervals=True and; use_genome_default_intervals=True.; The combiner serializes itself to save_path so that it can be restarted after failure. Parameters:. save_path (str) – The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path (str) – The location to store the new VariantDataset.; temp_path (str) – The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome (ReferenceGenome) – The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor (int) – The number of Variant Datasets to combine at once.",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:5329,Performance,load,load,5329,"o partition the GVCF files. The same partitioning is used; for all GVCF files. Finer partitioning yields more parallelism but less work per task.; gvcf_info_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. t",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:6021,Performance,load,load,6021,"nfo_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. to_dict()[source]; A serializable representation of this combiner. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
https://hail.is/docs/0.2/vds/hail.vds.combiner.VDSMetadata.html:1965,Deployability,update,updated,1965,"﻿. Hail | ; VDSMetadata. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VDSMetadata. View page source. VDSMetadata. class hail.vds.combiner.VDSMetadata[source]; The path to a Variant Dataset and the number of samples within. Parameters:. path (str) – Path to the variant dataset.; n_samples (int) – Number of samples contained within the Variant Dataset at path. Attributes. n_samples; Alias for field number 1. path; Alias for field number 0. Methods. __add__(value, /); Return self+value. __class_getitem__(); See PEP 585. __contains__(key, /); Return key in self. __eq__(value, /); Return self==value. __ge__(value, /); Return self>=value. __getitem__(key, /); Return self[key]. __getnewargs__(); Return self as a plain tuple. Used by copy and pickle. __gt__(value, /); Return self>value. __iter__(); Implement iter(self). __le__(value, /); Return self<=value. __len__(); Return len(self). __lt__(value, /); Return self<value. __mul__(value, /); Return self*value. __ne__(value, /); Return self!=value. __rmul__(value, /); Return value*self. count(value, /); Return number of occurrences of value. index(value, start=0, stop=9223372036854775807, /); Return first index of value.; Raises ValueError if the value is not present. n_samples; Alias for field number 1. path; Alias for field number 0. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VDSMetadata.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VDSMetadata.html
https://hail.is/docs/0.2/vds/hail.vds.filter_chromosomes.html:1821,Deployability,update,updated,1821,"﻿. Hail | ; hail.vds.filter_chromosomes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_chromosomes. View page source. hail.vds.filter_chromosomes. hail.vds.filter_chromosomes(vds, *, keep=None, remove=None, keep_autosomes=False)[source]; Filter chromosomes of a VariantDataset in several possible modes.; Notes; There are three modes for filter_chromosomes(), based on which argument is passed; to the function. Exactly one of the below arguments must be passed by keyword. keep: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with only those; chromosomes.; remove: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with those chromosomes; removed.; keep_autosomes: This argument expects the value True, and returns a dataset without; sex and mitochondrial chromosomes. Parameters:. vds (VariantDataset) – Dataset.; keep – Keep a specified list of contigs.; remove – Remove a specified list of contigs; keep_autosomes – If true, keep only autosomal chromosomes. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_chromosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_chromosomes.html
https://hail.is/docs/0.2/vds/hail.vds.filter_intervals.html:1476,Deployability,update,updated,1476,"﻿. Hail | ; hail.vds.filter_intervals. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_intervals. View page source. hail.vds.filter_intervals. hail.vds.filter_intervals(vds, intervals, *, split_reference_blocks=False, keep=True)[source]; Filter intervals in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; intervals (Table or ArrayExpression of type tinterval) – Intervals to filter on.; split_reference_blocks (bool) – If true, remove reference data outside the given intervals by segmenting reference; blocks at interval boundaries. Results in a smaller result, but this filter mode; is more computationally expensive to evaluate.; keep (bool) – Whether to keep, or filter out (default) rows that fall within any; interval in intervals. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_intervals.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_intervals.html
https://hail.is/docs/0.2/vds/hail.vds.filter_samples.html:1321,Deployability,update,updated,1321,"﻿. Hail | ; hail.vds.filter_samples. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_samples. View page source. hail.vds.filter_samples. hail.vds.filter_samples(vds, samples, *, keep=True, remove_dead_alleles=False)[source]; Filter samples in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; samples (Table or list of str) – Samples to keep or remove.; keep (bool) – Whether to keep (default), or filter out the samples from samples_table.; remove_dead_alleles (bool) – If true, remove alleles observed in no samples. Alleles with AC == 0 will be; removed, and LA values recalculated. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_samples.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_samples.html
https://hail.is/docs/0.2/vds/hail.vds.filter_variants.html:1185,Deployability,update,updated,1185,"﻿. Hail | ; hail.vds.filter_variants. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_variants. View page source. hail.vds.filter_variants. hail.vds.filter_variants(vds, variants_table, *, keep=True)[source]; Filter variants in a VariantDataset, without removing reference; data. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; variants_table (Table) – Variants to filter on.; keep (bool) – Whether to keep (default), or filter out the variants from variants_table. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_variants.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_variants.html
https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html:1972,Deployability,update,updated,1972,"﻿. Hail | ; hail.vds.impute_sex_chromosome_ploidy. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chromosome_ploidy. View page source. hail.vds.impute_sex_chromosome_ploidy. hail.vds.impute_sex_chromosome_ploidy(vds, calling_intervals, normalization_contig, use_variant_dataset=False)[source]; Impute sex chromosome ploidy from depth of reference or variant data within calling intervals.; Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. vds (vds: VariantDataset) – Dataset.; calling_intervals (Table or ArrayExpression) – Calling intervals with consistent read coverage (for exomes, trim the capture intervals).; normalization_contig (str) – Autosomal contig for depth comparison.; use_variant_dataset (bool) – Whether to use depth of variant data within calling intervals instead of reference data. Default will use reference data. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html
https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html:2002,Deployability,update,updated,2002,"x_chr_ploidy_from_interval_coverage. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chr_ploidy_from_interval_coverage. View page source. hail.vds.impute_sex_chr_ploidy_from_interval_coverage. hail.vds.impute_sex_chr_ploidy_from_interval_coverage(mt, normalization_contig)[source]; Impute sex chromosome ploidy from a precomputed interval coverage MatrixTable.; The input MatrixTable must have the following row fields:. interval (interval): Genomic interval of interest.; interval_size (int32): Size of interval, in bases. And the following entry fields:. sum_dp (int64): Sum of depth values by base across the interval. Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. mt (MatrixTable) – Interval-by-sample MatrixTable with sum of depth values across the interval.; normalization_contig (str) – Autosomal contig for depth comparison. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html
https://hail.is/docs/0.2/vds/hail.vds.interval_coverage.html:3031,Deployability,update,updated,3031,"enomic interval of interest.; interval_size (int32): Size of interval, in bases. Computes the following entry fields:. bases_over_gq_threshold (tuple of int64): Number of bases in the interval; over each GQ threshold.; fraction_over_gq_threshold (tuple of float64): Fraction of interval (in bases); above each GQ threshold. Computed by dividing each member of bases_over_gq_threshold; by interval_size.; bases_over_dp_threshold (tuple of int64): Number of bases in the interval; over each DP threshold.; fraction_over_dp_threshold (tuple of float64): Fraction of interval (in bases); above each DP threshold. Computed by dividing each member of bases_over_dp_threshold; by interval_size.; sum_dp (int64): Sum of depth values by base across the interval.; mean_dp (float64): Mean depth of bases across the interval. Computed by dividing; sum_dp by interval_size. If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Note; The metrics computed by this method are computed only from reference blocks. Most; variant callers produce data where non-reference calls interrupt reference blocks, and; so the metrics computed here are slight underestimates of the true values (which would; include the quality/depth of non-reference calls). This is likely a negligible difference,; but is something to be aware of, especially as it interacts with samples of; ancestral backgrounds with more or fewer non-reference calls. Parameters:. vds (VariantDataset); intervals (Table) – Table of intervals. Must be start-inclusive, and cannot span contigs.; gq_thresholds (tuple of int) – GQ thresholds.; dp_field (str, optional) – Field for depth calculation. Uses DP or MIN_DP by default (with priority for DP if present). Returns:; MatrixTable – Interval-by-sample matrix. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.interval_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.interval_coverage.html
https://hail.is/docs/0.2/vds/hail.vds.lgt_to_gt.html:988,Deployability,update,updated,988,"﻿. Hail | ; hail.vds.lgt_to_gt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.lgt_to_gt. View page source. hail.vds.lgt_to_gt. hail.vds.lgt_to_gt(lgt, la)[source]; Transform LGT into GT using local alleles array. Parameters:. lgt (CallExpression) – LGT value.; la (ArrayExpression) – Local alleles array. Returns:; CallExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.lgt_to_gt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.lgt_to_gt.html
https://hail.is/docs/0.2/vds/hail.vds.local_to_global.html:2325,Deployability,update,updated,2325,"t; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.local_to_global. View page source. hail.vds.local_to_global. hail.vds.local_to_global(array, local_alleles, n_alleles, fill_value, number)[source]; Reindex a locally-indexed array to globally-indexed.; Examples; >>> local_alleles = hl.array([0, 2]); >>> local_ad = hl.array([9, 10]); >>> local_pl = hl.array([94, 0, 123]). >>> hl.eval(local_to_global(local_ad, local_alleles, n_alleles=3, fill_value=0, number='R')); [9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; The number parameter matches the VCF specification; number definitions:. A indicates one value per allele, excluding the reference.; R indicates one value per allele, including the reference.; G indicates one value per unique diploid genotype. Warning; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample – with 100,000 samples, nearly 50GB per variant!. See also; lgt_to_gt(). Parameters:. array (ArrayExpression) – Array to reindex.; local_alleles (ArrayExpression) – Local alleles array.; n_alleles (Int32Expression) – Total number of alleles to reindex to.; fill_value – Value to fill in at global indices with no data.; number (str) – One of ‘A’, ‘R’, ‘G’. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.local_to_global.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.local_to_global.html
https://hail.is/docs/0.2/vds/hail.vds.merge_reference_blocks.html:1837,Deployability,update,updated,1837,"﻿. Hail | ; hail.vds.merge_reference_blocks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.merge_reference_blocks. View page source. hail.vds.merge_reference_blocks. hail.vds.merge_reference_blocks(ds, equivalence_function, merge_functions=None)[source]; Merge adjacent reference blocks according to user equivalence criteria.; Examples; Coarsen GQ granularity into bins of 10 and merges blocks with the same GQ in order to; compress reference data.; >>> rd = vds.reference_data ; >>> vds.reference_data = rd.annotate_entries(GQ = rd.GQ - rd.GQ % 10) ; >>> vds2 = hl.vds.merge_reference_blocks(vds,; ... equivalence_function=lambda block1, block2: block1.GQ == block2.GQ),; ... merge_functions={'MIN_DP': 'min'}) . Notes; The equivalence_function argument expects a function from two reference blocks to a; boolean value indicating whether they should be combined. Adjacency checks are builtin; to the method (two reference blocks are ‘adjacent’ if the END of one block is one base; before the beginning of the next).; The merge_functions. Parameters:; ds (VariantDataset or MatrixTable) – Variant dataset or reference block matrix table. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.merge_reference_blocks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.merge_reference_blocks.html
https://hail.is/docs/0.2/vds/hail.vds.read_vds.html:1057,Deployability,update,updated,1057,"﻿. Hail | ; hail.vds.read_vds. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.read_vds. View page source. hail.vds.read_vds. hail.vds.read_vds(path, *, intervals=None, n_partitions=None, _assert_reference_type=None, _assert_variant_type=None, _warn_no_ref_block_max_length=True)[source]; Read in a VariantDataset written with VariantDataset.write(). Parameters:; path (str). Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.read_vds.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.read_vds.html
https://hail.is/docs/0.2/vds/hail.vds.sample_qc.html:1546,Deployability,update,updated,1546,"﻿. Hail | ; hail.vds.sample_qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.sample_qc. View page source. hail.vds.sample_qc. hail.vds.sample_qc(vds, *, gq_bins=(0, 20, 60), dp_bins=(0, 1, 10, 20, 30), dp_field=None)[source]; Compute sample quality metrics about a VariantDataset.; If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; gq_bins (tuple of int) – Tuple containing cutoffs for genotype quality (GQ) scores.; dp_bins (tuple of int) – Tuple containing cutoffs for depth (DP) scores.; dp_field (str) – Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns:; Table – Hail Table of results, keyed by sample. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.sample_qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.sample_qc.html
https://hail.is/docs/0.2/vds/hail.vds.split_multi.html:1054,Availability,error,error,1054,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.split_multi.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html
https://hail.is/docs/0.2/vds/hail.vds.split_multi.html:1144,Deployability,update,updated,1144,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.split_multi.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html
https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html:1151,Availability,down,downstream,1151,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html
https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html:1059,Deployability,patch,patch,1059,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html
https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html:1616,Deployability,update,updated,1616,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html
https://hail.is/docs/0.2/vds/hail.vds.to_dense_mt.html:1055,Deployability,update,updated,1055,"﻿. Hail | ; hail.vds.to_dense_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_dense_mt. View page source. hail.vds.to_dense_mt. hail.vds.to_dense_mt(vds)[source]; Creates a single, dense MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in dense MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.to_dense_mt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_dense_mt.html
https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html:1136,Deployability,update,updated,1136,"﻿. Hail | ; hail.vds.to_merged_sparse_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_merged_sparse_mt. View page source. hail.vds.to_merged_sparse_mt. hail.vds.to_merged_sparse_mt(vds, *, ref_allele_function=None)[source]; Creates a single, merged sparse MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in the merged sparse MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.to_merged_sparse_mt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html
https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html:1714,Deployability,patch,patch,1714,"Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.truncate_reference_blocks. View page source. hail.vds.truncate_reference_blocks. hail.vds.truncate_reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None)[source]; Cap reference blocks at a maximum length in order to permit faster interval filtering.; Examples; Truncate reference blocks to 5 kilobases:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) . Truncate the longest 1% of reference blocks to the length of the 99th percentile block:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) . Notes; After this function has been run, the reference blocks have a known maximum length ref_block_max_length,; stored in the global fields, which permits vds.filter_intervals() to filter to intervals of the reference; data by reading ref_block_max_length bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work.; It is also possible to patch an existing VDS to store the max reference block length with vds.store_ref_block_max_length(). See also; vds.store_ref_block_max_length(). Parameters:. vds (VariantDataset or MatrixTable); max_ref_block_base_pairs – Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction – Fraction of reference block length distribution to truncate / winsorize. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.truncate_reference_blocks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html
https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html:2188,Deployability,update,updated,2188,"Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.truncate_reference_blocks. View page source. hail.vds.truncate_reference_blocks. hail.vds.truncate_reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None)[source]; Cap reference blocks at a maximum length in order to permit faster interval filtering.; Examples; Truncate reference blocks to 5 kilobases:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) . Truncate the longest 1% of reference blocks to the length of the 99th percentile block:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) . Notes; After this function has been run, the reference blocks have a known maximum length ref_block_max_length,; stored in the global fields, which permits vds.filter_intervals() to filter to intervals of the reference; data by reading ref_block_max_length bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work.; It is also possible to patch an existing VDS to store the max reference block length with vds.store_ref_block_max_length(). See also; vds.store_ref_block_max_length(). Parameters:. vds (VariantDataset or MatrixTable); max_ref_block_base_pairs – Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction – Fraction of reference block length distribution to truncate / winsorize. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.truncate_reference_blocks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html
https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:1294,Availability,checkpoint,checkpoint,1294,"alg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VariantDataset. View page source. VariantDataset. class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a datas",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:1677,Availability,checkpoint,checkpoint,1677,"class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; W",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:2785,Deployability,update,updated,2785,"ss facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; Write to path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:1580,Security,validat,validate,1580,"n API; Hail Query Python API; Variant Dataset; VariantDataset. View page source. VariantDataset. class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; E",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:2577,Security,validat,validate,2577,"ss facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; Write to path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
https://hail.is/docs/0.2/vds/index.html:3404,Availability,failure,failure-tolerant,3404,"me ploidy from a precomputed interval coverage MatrixTable. to_dense_mt(vds); Creates a single, dense MatrixTable from the split VariantDataset representation. to_merged_sparse_mt(vds, *[, ...]); Creates a single, merged sparse MatrixTable from the split VariantDataset representation. truncate_reference_blocks(ds, *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:7008,Availability,error,error,7008,"eles make it possible to keep the data small to; match its inherent information content. Component tables; The VariantDataset is made up of two component matrix tables – the; reference_data and the variant_data.; The reference_data matrix table is a sparse matrix of reference blocks. The; reference_data matrix table has row key locus, but; does not have an alleles key or field. The column key is the sample ID. The; entries indicate regions of reference calls with similar sequencing metadata; (depth, quality, etc), starting from vds.reference_data.locus.position and; ending at vds.reference_data.END (inclusive!). There is no GT call field; because all calls in the reference data are implicitly homozygous reference (in; the future, a table of ploidy by interval may be included to allow for proper; representation of structural variation, but there is no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrog",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:7939,Deployability,pipeline,pipelines,7939,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:8191,Deployability,pipeline,pipelines,8191,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:8403,Deployability,pipeline,pipelines,8403,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:8798,Deployability,update,updated,8798,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:1209,Integrability,interface,interfaces,1209,"ssions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset. View page source. Variant Dataset; The VariantDataset is an extra layer of abstraction of the Hail Matrix Table for working; with large sequencing datasets. It was initially developed in response to the gnomAD project’s need; to combine, represent, and analyze 150,000 whole genomes. It has since been used on datasets as; large as 955,000 whole exomes. The VariantDatasetCombiner produces a; VariantDataset by combining any number of GVCF and/or VariantDataset files. Warning; Hail 0.1 also had a Variant Dataset class. Although pieces of the interfaces are similar, they should not; be considered interchangeable and do not represent the same data. Variant Dataset. VariantDataset; Class for representing cohort-level genomic data. read_vds(path, *[, intervals, n_partitions, ...]); Read in a VariantDataset written with VariantDataset.write(). filter_samples(vds, samples, *[, keep, ...]); Filter samples in a VariantDataset. filter_variants(vds, variants_table, *[, keep]); Filter variants in a VariantDataset, without removing reference data. filter_intervals(vds, intervals, *[, ...]); Filter intervals in a VariantDataset. filter_chromosomes(vds, *[, keep, remove, ...]); Filter chromosomes of a VariantDataset in several possible modes. sample_qc(vds, *[, gq_bins, dp_bins, dp_field]); Compute sample quality metrics about a VariantDataset. split_multi(vds, *[, filter_changed_loci]); Split the multiallelic variants in a VariantDataset. interval_coverage(vds, intervals[, ...]); Compute statistics about base coverage by interval. impute_sex_chromosome_ploidy(vds, ...[, ...])",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:7486,Integrability,depend,depending,7486,"ata.locus.position and; ending at vds.reference_data.END (inclusive!). There is no GT call field; because all calls in the reference data are implicitly homozygous reference (in; the future, a table of ploidy by interval may be included to allow for proper; representation of structural variation, but there is no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:3569,Performance,load,load,3569,"taset representation. to_merged_sparse_mt(vds, *[, ...]); Creates a single, merged sparse MatrixTable from the split VariantDataset representation. truncate_reference_blocks(ds, *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference informa",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:3768,Performance,scalab,scalable,3768,", *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denot",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:3926,Performance,scalab,scalable,3926,"blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain info",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:4060,Performance,scalab,scalable,4060,"ocally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the inf",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:4803,Performance,scalab,scalable,4803," Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the information content; does not change. To avoid this superlinear scaling, the SVCR renames these; fields to their “local” versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list.; For instance, if a sample has the following information in its GVCF:; Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample’s; entry would look like:; LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The “1” allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (C), but rather the allele whose i",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:5119,Safety,avoid,avoid,5119,"intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the information content; does not change. To avoid this superlinear scaling, the SVCR renames these; fields to their “local” versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list.; For instance, if a sample has the following information in its GVCF:; Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample’s; entry would look like:; LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The “1” allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (C), but rather the allele whose index is in position 1 of the LA list.; The index at position 2 of the LA list is 2, and the allele with absolute; index 2 is T. Local alleles make it possible to keep the data small to; match its inherent information content. Component tables; The VariantDataset is made up of two component matrix tables – the; reference_data and the variant_data.; The reference_",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/vds/index.html:8071,Testability,test,tests,8071,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
https://hail.is/docs/0.2/_modules/index.html:470,Availability,avail,available,470,﻿. Hail | ; Overview: module code. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.c,MatchSource.WIKI,docs/0.2/_modules/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/index.html
https://hail.is/docs/0.2/_modules/index.html:2200,Deployability,update,updated,2200,"onfiguration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.combiner.variant_dataset_combiner; hail.vds.functions; hail.vds.methods; hail.vds.sample_qc; hail.vds.variant_dataset; hailtop.frozendict; hailtop.fs.fs_utils. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/index.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_autosomes.html:11127,Deployability,update,updated,11127,": int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_autosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_autosomes.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrMT.html:10725,Deployability,update,updated,10725,"Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; 1000_Genomes_chrMT. View page source. 1000_Genomes_chrMT. Versions: phase_3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: int32,; VT: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_chrMT.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrMT.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrX.html:11112,Deployability,update,updated,11112,": int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_chrX.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrX.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrY.html:10939,Deployability,update,updated,10939,"ixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_chrY.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrY.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html:12998,Deployability,update,updated,12998,"oat64,; AF_EAS: float64,; AF_AMR: float64,; AF_SAS: float64,; AF_AFR: float64,; HWE_EUR: float64,; HWE_EAS: float64,; HWE_AMR: float64,; HWE_SAS: float64,; HWE_AFR: float64,; HWE: float64,; ExcHet_EUR: float64,; ExcHet_EAS: float64,; ExcHet_AMR: float64,; ExcHet_SAS: float64,; ExcHet_AFR: float64,; ExcHet: float64,; ME: float64,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AC_Hom_EUR_unrel: int32,; AC_Hom_EAS_unrel: int32,; AC_Hom_AMR_unrel: int32,; AC_Hom_SAS_unrel: int32,; AC_Hom_AFR_unrel: int32,; AC_Het_EUR_unrel: int32,; AC_Het_EAS_unrel: int32,; AC_Het_AMR_unrel: int32,; AC_Het_SAS_unrel: int32,; AC_Het_AFR_unrel: int32,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64,; HWE_EUR_unrel: float64,; HWE_EAS_unrel: float64,; HWE_AMR_unrel: float64,; HWE_SAS_unrel: float64,; HWE_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrX.html:12983,Deployability,update,updated,12983,"oat64,; AF_EAS: float64,; AF_AMR: float64,; AF_SAS: float64,; AF_AFR: float64,; HWE_EUR: float64,; HWE_EAS: float64,; HWE_AMR: float64,; HWE_SAS: float64,; HWE_AFR: float64,; HWE: float64,; ExcHet_EUR: float64,; ExcHet_EAS: float64,; ExcHet_AMR: float64,; ExcHet_SAS: float64,; ExcHet_AFR: float64,; ExcHet: float64,; ME: float64,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AC_Hom_EUR_unrel: int32,; AC_Hom_EAS_unrel: int32,; AC_Hom_AMR_unrel: int32,; AC_Hom_SAS_unrel: int32,; AC_Hom_AFR_unrel: int32,; AC_Het_EUR_unrel: int32,; AC_Het_EAS_unrel: int32,; AC_Het_AMR_unrel: int32,; AC_Het_SAS_unrel: int32,; AC_Het_AFR_unrel: int32,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64,; HWE_EUR_unrel: float64,; HWE_EAS_unrel: float64,; HWE_AMR_unrel: float64,; HWE_SAS_unrel: float64,; HWE_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrX.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrX.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html:12118,Deployability,update,updated,12118,"32,; AF: float64,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; END: int32,; ExcessHet: float64,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: int32,; MLEAF: float64,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; RAW_MQ: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; VariantType: str,; culprit: str,; AN_EAS: int32,; AN_AMR: int32,; AN_EUR: int32,; AN_AFR: int32,; AN_SAS: int32,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EAS: int32,; AC_AMR: int32,; AC_EUR: int32,; AC_AFR: int32,; AC_SAS: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AF_EAS: float64,; AF_AMR: float64,; AF_EUR: float64,; AF_AFR: float64,; AF_SAS: float64,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html:11463,Deployability,update,updated,11463,": array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: array<str>,; RSID_REMOVED: array<str>,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrX.html:11448,Deployability,update,updated,11448,": array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: array<str>,; RSID_REMOVED: array<str>,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrX.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrX.html
https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html:11261,Deployability,update,updated,11261,"population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: str,; RSID_REMOVED: str,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html
https://hail.is/docs/0.2/datasets/schemas/CADD.html:9446,Deployability,update,updated,9446,"ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; CADD. View page source. CADD. Versions: 1.4, 1.6; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (1.4, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'raw_score': float64; 'PHRED_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/CADD.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/CADD.html
https://hail.is/docs/0.2/datasets/schemas/clinvar_gene_summary.html:9550,Deployability,update,updated,9550,"d_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_gene_summary. View page source. clinvar_gene_summary. Versions: 2019-07; Reference genome builds: None; Type: hail.Table. Schema (2019-07, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'GeneID': int32; 'Total_submissions': int32; 'Total_alleles': int32; 'Submissions_reporting_this_gene': int32; 'Alleles_reported_Pathogenic_Likely_pathogenic': int32; 'Gene_MIM_number': int32; 'Number_uncertain': int32; 'Number_with_conflicts': int32; 'gene_name': str; ----------------------------------------; Key: ['gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/clinvar_gene_summary.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_gene_summary.html
https://hail.is/docs/0.2/datasets/schemas/clinvar_variant_summary.html:9926,Deployability,update,updated,9926,"v_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_variant_summary. View page source. clinvar_variant_summary. Versions: 2019-07; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2019-07, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'Type': str; 'Name': str; 'GeneID': int32; 'GeneSymbol': str; 'HGNC_ID': str; 'ClinicalSignificance': str; 'ClinSigSimple': int32; 'LastEvaluated': str; 'RS# (dbSNP)': int32; 'nsv/esv (dbVar)': str; 'RCVaccession': str; 'PhenotypeIDS': str; 'PhenotypeList': str; 'Origin': str; 'OriginSimple': str; 'Assembly': str; 'ChromosomeAccession': str; 'ReferenceAllele': str; 'AlternateAllele': str; 'Cytogenetic': str; 'ReviewStatus': str; 'NumberSubmitters': int32; 'Guidelines': str; 'TestedInGTR': str; 'OtherIDs': str; 'SubmitterCategories': int32; 'VariationID': int32; 'interval': interval<locus<GRCh37>>; 'AlleleID': int32; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/clinvar_variant_summary.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_variant_summary.html
https://hail.is/docs/0.2/datasets/schemas/DANN.html:9416,Deployability,update,updated,9416,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; DANN. View page source. DANN. Versions: None; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (None, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/DANN.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/DANN.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:12380,Deployability,update,updated,12380,"C_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype': str; 'ZFIN_zebrafish_gene': str; 'ZFIN_zebrafish_structure': str; 'ZFIN_zebrafish_phenotype_quality': str; 'ZFIN_zebrafish_phenotype_tag': str; ----------------------------------------; Key: ['Gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:10772,Safety,predict,prediction,10772,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:10831,Safety,predict,prediction,10831,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:10900,Safety,predict,prediction,10900,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:10968,Safety,predict,prediction,10968,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:11036,Safety,predict,prediction,11036,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:11099,Safety,predict,prediction,11099,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:11161,Safety,predict,prediction,11161,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:11223,Safety,predict,prediction,11223,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:11289,Safety,predict,prediction,11289,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:11361,Safety,predict,prediction,11361,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
https://hail.is/docs/0.2/datasets/schemas/dbNSFP_variants.html:20991,Deployability,update,updated,20991,"es_POPMAX_AF': float64; 'gnomAD_genomes_POPMAX_nhomalt': int32; 'gnomAD_genomes_controls_AC': int32; 'gnomAD_genomes_controls_AN': int32; 'gnomAD_genomes_controls_AF': float64; 'gnomAD_genomes_controls_nhomalt': int32; 'gnomAD_genomes_controls_AFR_AC': int32; 'gnomAD_genomes_controls_AFR_AN': int32; 'gnomAD_genomes_controls_AFR_AF': float64; 'gnomAD_genomes_controls_AFR_nhomalt': int32; 'gnomAD_genomes_controls_AMR_AC': int32; 'gnomAD_genomes_controls_AMR_AN': int32; 'gnomAD_genomes_controls_AMR_AF': float64; 'gnomAD_genomes_controls_AMR_nhomalt': int32; 'gnomAD_genomes_controls_ASJ_AC': int32; 'gnomAD_genomes_controls_ASJ_AN': int32; 'gnomAD_genomes_controls_ASJ_AF': float64; 'gnomAD_genomes_controls_ASJ_nhomalt': int32; 'gnomAD_genomes_controls_EAS_AC': int32; 'gnomAD_genomes_controls_EAS_AN': int32; 'gnomAD_genomes_controls_EAS_AF': float64; 'gnomAD_genomes_controls_EAS_nhomalt': int32; 'gnomAD_genomes_controls_FIN_AC': int32; 'gnomAD_genomes_controls_FIN_AN': int32; 'gnomAD_genomes_controls_FIN_AF': float64; 'gnomAD_genomes_controls_FIN_nhomalt': int32; 'gnomAD_genomes_controls_NFE_AC': int32; 'gnomAD_genomes_controls_NFE_AN': int32; 'gnomAD_genomes_controls_NFE_AF': float64; 'gnomAD_genomes_controls_NFE_nhomalt': int32; 'gnomAD_genomes_controls_POPMAX_AC': int32; 'gnomAD_genomes_controls_POPMAX_AN': int32; 'gnomAD_genomes_controls_POPMAX_AF': float64; 'gnomAD_genomes_controls_POPMAX_nhomalt': int32; 'clinvar_id': int32; 'clinvar_clnsig': str; 'clinvar_trait': str; 'clinvar_review': str; 'clinvar_hgvs': str; 'clinvar_var_source': str; 'clinvar_MedGen_id': str; 'clinvar_OMIM_id': str; 'clinvar_Orphanet_id': str; 'Interpro_domain': str; 'GTEx_V7_gene': str; 'GTEx_V7_tissue': str; 'Geuvadis_eQTL_target_gene': str; 'locus': locus<GRCh37>; 'alleles': array<str>; 'chr': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_variants.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_variants.html
https://hail.is/docs/0.2/datasets/schemas/dbSNP.html:10632,Deployability,update,updated,10632,"quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP. View page source. dbSNP. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; RS: int32,; GENEINFO: str,; PSEUDOGENEINFO: str,; dbSNPBuildID: int32,; SAO: int32,; SSR: int32,; VC: str,; PM: bool,; NSF: bool,; NSM: bool,; NSN: bool,; SYN: bool,; U3: bool,; U5: bool,; ASS: bool,; DSS: bool,; INT: bool,; R3: bool,; R5: bool,; GNO: bool,; PUB: bool,; FREQ: struct {; _GENOME_DK: float64,; _TWINSUK: float64,; _dbGaP_PopFreq: float64,; _Siberian: float64,; _Chileans: float64,; _FINRISK: float64,; _HapMap: float64,; _Estonian: float64,; _ALSPAC: float64,; _GoESP: float64,; _TOPMED: float64,; _PAGE_STUDY: float64,; _1000Genomes: float64,; _Korea1K: float64,; _ChromosomeY: float64,; _ExAC: float64,; _Qatari: float64,; _GoNL: float64,; _MGP: float64,; _GnomAD: float64,; _Vietnamese: float64,; _GnomAD_exomes: float64,; _PharmGKB: float64,; _KOREAN: float64,; _Daghestan: float64,; _HGDP_Stanford: float64,; _NorthernSweden: float64,; _SGDP_PRJ: float64; },; COMMON: bool,; CLNHGVS: array<str>,; CLNVI: array<str>,; CLNORIGIN: array<str>,; CLNSIG: array<str>,; CLNDISDB: array<str>,; CLNDN: array<str>,; CLNREVSTAT: array<str>,; CLNACC: array<str>; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbSNP.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP.html
https://hail.is/docs/0.2/datasets/schemas/dbSNP_rsid.html:9426,Deployability,update,updated,9426,"mad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP_rsid. View page source. dbSNP_rsid. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbSNP_rsid.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP_rsid.html
https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html:9499,Deployability,update,updated,9499,"es_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_low_complexity_regions. View page source. Ensembl_homo_sapiens_low_complexity_regions. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html
https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html:9490,Deployability,update,updated,9490,"cores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_reference_genome. View page source. Ensembl_homo_sapiens_reference_genome. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'reference_allele': str; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html
https://hail.is/docs/0.2/datasets/schemas/gencode.html:9698,Deployability,update,updated,9698,"ces_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gencode. View page source. gencode. Versions: v19, v31, v35; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (v35, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh38>>; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; 'level': int32; 'gene_id': str; 'gene_type': str; 'ccdsid': str; 'exon_id': str; 'exon_number': int32; 'havana_gene': str; 'transcript_type': str; 'protein_id': str; 'gene_name': str; 'transcript_name': str; 'transcript_id': str; 'transcript_support_level': str; 'hgnc_id': str; 'ont': str; 'havana_transcript': str; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gencode.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gencode.html
https://hail.is/docs/0.2/datasets/schemas/gerp_elements.html:9425,Deployability,update,updated,9425,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_elements. View page source. gerp_elements. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; 'S': float64; 'p_value': float64; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gerp_elements.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_elements.html
https://hail.is/docs/0.2/datasets/schemas/gerp_scores.html:9397,Deployability,update,updated,9397,"_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_scores. View page source. gerp_scores. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'N': float64; 'S': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gerp_scores.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_scores.html
https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html:9570,Deployability,update,updated,9570,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AFR. View page source. giant_bmi_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html
https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html:9563,Deployability,update,updated,9563,"; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_ALL. View page source. giant_bmi_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html
https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html:9570,Deployability,update,updated,9570,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AMR. View page source. giant_bmi_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html
https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html:9570,Deployability,update,updated,9570,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EAS. View page source. giant_bmi_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html
https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html:9570,Deployability,update,updated,9570,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EUR. View page source. giant_bmi_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html
https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html:9570,Deployability,update,updated,9570,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_SAS. View page source. giant_bmi_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html
https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AFR.html:9579,Deployability,update,updated,9579,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AFR. View page source. giant_height_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AFR.html
https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_ALL.html:9572,Deployability,update,updated,9572,"ad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_ALL. View page source. giant_height_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_ALL.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_ALL.html
https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AMR.html:9579,Deployability,update,updated,9579,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AMR. View page source. giant_height_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AMR.html
https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EAS.html:9579,Deployability,update,updated,9579,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EAS. View page source. giant_height_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EAS.html
https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EUR.html:9579,Deployability,update,updated,9579,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EUR. View page source. giant_height_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EUR.html
https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_SAS.html:9579,Deployability,update,updated,9579,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_SAS. View page source. giant_height_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_SAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_SAS.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html:9603,Deployability,update,updated,9603,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Add. View page source. giant_whr_exome_C_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html:9603,Deployability,update,updated,9603,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Rec. View page source. giant_whr_exome_C_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html:9610,Deployability,update,updated,9610,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Add. View page source. giant_whr_exome_C_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html:9610,Deployability,update,updated,9610,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Rec. View page source. giant_whr_exome_C_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html:9603,Deployability,update,updated,9603,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Add. View page source. giant_whr_exome_M_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html:9603,Deployability,update,updated,9603,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Rec. View page source. giant_whr_exome_M_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html:9610,Deployability,update,updated,9610,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Add. View page source. giant_whr_exome_M_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html:9610,Deployability,update,updated,9610,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Rec. View page source. giant_whr_exome_M_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html:9603,Deployability,update,updated,9603,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Add. View page source. giant_whr_exome_W_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html:9603,Deployability,update,updated,9603,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Rec. View page source. giant_whr_exome_W_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html:9610,Deployability,update,updated,9610,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Add. View page source. giant_whr_exome_W_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html
https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html:9610,Deployability,update,updated,9610,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Rec. View page source. giant_whr_exome_W_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_annotation_pext.html:14370,Deployability,update,updated,14370,"4,; over_15: float64,; over_20: float64,; over_25: float64,; over_30: float64,; over_50: float64,; over_100: float64; }; }; 'gerp': float64; 'tx_annotation': array<struct {; ensg: str,; csq: str,; symbol: str,; lof: str,; lof_flag: str,; Cells_Transformedfibroblasts: float64,; Prostate: float64,; Spleen: float64,; Brain_FrontalCortex_BA9_: float64,; SmallIntestine_TerminalIleum: float64,; MinorSalivaryGland: float64,; Artery_Coronary: float64,; Skin_SunExposed_Lowerleg_: float64,; Cells_EBV_transformedlymphocytes: float64,; Brain_Hippocampus: float64,; Esophagus_Muscularis: float64,; Brain_Nucleusaccumbens_basalganglia_: float64,; Artery_Tibial: float64,; Brain_Hypothalamus: float64,; Adipose_Visceral_Omentum_: float64,; Cervix_Ectocervix: float64,; Brain_Spinalcord_cervicalc_1_: float64,; Brain_CerebellarHemisphere: float64,; Nerve_Tibial: float64,; Breast_MammaryTissue: float64,; Liver: float64,; Skin_NotSunExposed_Suprapubic_: float64,; AdrenalGland: float64,; Vagina: float64,; Pancreas: float64,; Lung: float64,; FallopianTube: float64,; Pituitary: float64,; Muscle_Skeletal: float64,; Colon_Transverse: float64,; Artery_Aorta: float64,; Heart_AtrialAppendage: float64,; Adipose_Subcutaneous: float64,; Esophagus_Mucosa: float64,; Heart_LeftVentricle: float64,; Brain_Cerebellum: float64,; Brain_Cortex: float64,; Thyroid: float64,; Brain_Substantianigra: float64,; Kidney_Cortex: float64,; Uterus: float64,; Stomach: float64,; WholeBlood: float64,; Bladder: float64,; Brain_Anteriorcingulatecortex_BA24_: float64,; Brain_Putamen_basalganglia_: float64,; Brain_Caudate_basalganglia_: float64,; Colon_Sigmoid: float64,; Cervix_Endocervix: float64,; Ovary: float64,; Esophagus_GastroesophagealJunction: float64,; Testis: float64,; Brain_Amygdala: float64,; mean_proportion: float64; }>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_annotation_pext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_annotation_pext.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_base_pext.html:10933,Deployability,update,updated,10933,". Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ensg': str; 'locus': locus<GRCh37>; 'symbol': str; 'Cells_Transformedfibroblasts': float64; 'Prostate': float64; 'Spleen': float64; 'Brain_FrontalCortex_BA9_': float64; 'SmallIntestine_TerminalIleum': float64; 'MinorSalivaryGland': float64; 'Artery_Coronary': float64; 'Skin_SunExposed_Lowerleg_': float64; 'Cells_EBV_transformedlymphocytes': float64; 'Brain_Hippocampus': float64; 'Esophagus_Muscularis': float64; 'Brain_Nucleusaccumbens_basalganglia_': float64; 'Artery_Tibial': float64; 'Brain_Hypothalamus': float64; 'Adipose_Visceral_Omentum_': float64; 'Cervix_Ectocervix': float64; 'Brain_Spinalcord_cervicalc_1_': float64; 'Brain_CerebellarHemisphere': float64; 'Nerve_Tibial': float64; 'Breast_MammaryTissue': float64; 'Liver': float64; 'Skin_NotSunExposed_Suprapubic_': float64; 'AdrenalGland': float64; 'Vagina': float64; 'Pancreas': float64; 'Lung': float64; 'FallopianTube': float64; 'Pituitary': float64; 'Muscle_Skeletal': float64; 'Colon_Transverse': float64; 'Artery_Aorta': float64; 'Heart_AtrialAppendage': float64; 'Adipose_Subcutaneous': float64; 'Esophagus_Mucosa': float64; 'Heart_LeftVentricle': float64; 'Brain_Cerebellum': float64; 'Brain_Cortex': float64; 'Thyroid': float64; 'Brain_Substantianigra': float64; 'Kidney_Cortex': float64; 'Uterus': float64; 'Stomach': float64; 'WholeBlood': float64; 'Bladder': float64; 'Brain_Anteriorcingulatecortex_BA24_': float64; 'Brain_Putamen_basalganglia_': float64; 'Brain_Caudate_basalganglia_': float64; 'Colon_Sigmoid': float64; 'Cervix_Endocervix': float64; 'Ovary': float64; 'Esophagus_GastroesophagealJunction': float64; 'Testis': float64; 'Brain_Amygdala': float64; 'mean_proportion': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_base_pext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_base_pext.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_coverage.html:9371,Deployability,update,updated,9371,"ad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_chrM_coverage. View page source. gnomad_chrM_coverage. Versions: 3.1; Reference genome builds: GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'mean': float64; 'median': int32; 'over_100': float64; 'over_1000': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_chrM_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_coverage.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_sites.html:19008,Deployability,update,updated,19008,"hen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'rsid': set<str>; 'common_low_heteroplasmy': bool; 'base_qual_hist': array<int64>; 'position_hist': array<int64>; 'strand_bias_hist': array<int64>; 'weak_evidence_hist': array<int64>; 'contamination_hist': array<int64>; 'heteroplasmy_below_min_het_threshold_hist': array<int64>; 'excluded_AC': int64; 'AN': int64; 'AC_hom': int64; 'AC_het': int64; 'hl_hist': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_all': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_alt': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_mean': float64; 'mq_mean': float64; 'tlod_mean': float64; 'AF_hom': float32; 'AF_het': float32; 'max_hl': float64; 'hap_AN': array<int64>; 'hap_AC_het': array<int64>; 'hap_AC_hom': array<int64>; 'hap_AF_hom': array<float32>; 'hap_AF_het': array<float32>; 'hap_hl_hist': array<array<int64>>; 'hap_faf_hom': array<float64>; 'hapmax_AF_hom': str; 'hapmax_AF_het': str; 'faf_hapmax_hom': float64; 'pop_AN': array<int64>; 'pop_AC_het': array<int64>; 'pop_AC_hom': array<int64>; 'pop_AF_hom': array<float32>; 'pop_AF_het': array<float32>; 'pop_hl_hist': array<array<int64>>; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_chrM_sites.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_sites.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_coverage.html:9527,Deployability,update,updated,9527,"d_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_exome_coverage. View page source. gnomad_exome_coverage. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_exome_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_coverage.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_sites.html:15398,Deployability,update,updated,15398,"ce: str,; motif_feature_consequences: array<struct {; allele_num: int32,; consequence_terms: array<str>,; high_inf_pos: str,; impact: str,; minimised: int32,; motif_feature_id: str,; motif_name: str,; motif_pos: int32,; motif_score_change: float64,; strand: int32,; variant_allele: str; }>,; regulatory_feature_consequences: array<struct {; allele_num: int32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'allele_info': struct {; BaseQRankSum: float64,; ClippingRankSum: float64,; DB: bool,; DP: int32,; DS: bool,; END: int32,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MQ: float64,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; culprit: str; }; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_exome_sites.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_sites.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_coverage.html:9545,Deployability,update,updated,9545,"gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_genome_coverage. View page source. gnomad_genome_coverage. Versions: 2.1, 3.0.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_genome_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_coverage.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_sites.html:14673,Deployability,update,updated,14673,"_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'vqsr': struct {; AS_VQSLOD: float64,; AS_culprit: str,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool; }; 'region_flag': struct {; lcr: bool,; segdup: bool; }; 'allele_info': struct {; variant_type: str,; allele_type: str,; n_alt_alleles: int32,; was_mixed: bool; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_genome_sites.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_sites.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html:34895,Deployability,update,updated,34895," n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Entry fields:; 'DP': int32; 'GQ': int32; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'GT': call; 'PGT': call; 'AD': array<int32>; 'PL': array<int32>; 'adj': bool; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html:21063,Deployability,update,updated,21063," n_non_ref: int64,; n_snp: int64,; n_transition: int64,; n_transversion: int64,; r_het_hom_var: float64,; r_insertion_deletion: float64,; r_ti_tv: float64; }; 'gnomad_sex_imputation': struct {; chr20_mean_dp: float32,; chrX_mean_dp: float32,; chrY_mean_dp: float32,; chrX_ploidy: float32,; chrY_ploidy: float32,; X_karyotype: str,; Y_karyotype: str,; sex_karyotype: str,; f_stat: float64,; n_called: int64,; expected_homs: float64,; observed_homs: int64; }; 'gnomad_population_inference': struct {; pca_scores: array<float64>,; pop: str,; prob_afr: float64,; prob_ami: float64,; prob_amr: float64,; prob_asj: float64,; prob_eas: float64,; prob_fin: float64,; prob_mid: float64,; prob_nfe: float64,; prob_oth: float64,; prob_sas: float64; }; 'gnomad_sample_qc_residuals': struct {; n_snp_residual: float64,; r_ti_tv_residual: float64,; r_insertion_deletion_residual: float64,; n_insertion_residual: float64,; n_deletion_residual: float64,; r_het_hom_var_residual: float64,; n_transition_residual: float64,; n_transversion_residual: float64; }; 'gnomad_sample_filters': struct {; hard_filters: set<str>,; hard_filtered: bool,; release_related: bool,; qc_metrics_filters: set<str>; }; 'gnomad_high_quality': bool; 'gnomad_release': bool; 'relatedness_inference': struct {; related_samples: set<struct {; s: str,; kin: float64,; ibd0: float64,; ibd1: float64,; ibd2: float64; }>,; related: bool; }; 'hgdp_tgp_meta': struct {; project: str,; study_region: str,; population: str,; genetic_region: str,; latitude: float64,; longitude: float64,; hgdp_technical_meta: struct {; source: str,; library_type: str; },; global_pca_scores: array<float64>,; subcontinental_pca: struct {; pca_scores: array<float64>,; pca_scores_outliers_removed: array<float64>,; outlier: bool; },; gnomad_labeled_subpop: str; }; 'high_quality': bool; ----------------------------------------; Key: ['s']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html:9925,Deployability,update,updated,9925,"ad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_hgdp_1kg_subset_sparse. View page source. gnomad_hgdp_1kg_subset_sparse. Versions: 3.1.2; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (3.1.2, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'DP': int32; 'END': int32; 'GQ': int32; 'LA': array<int32>; 'LAD': array<int32>; 'LGT': call; 'LPGT': call; 'LPL': array<int32>; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'gvcf_info': struct {; ClippingRankSum: float64,; BaseQRankSum: float64,; MQ: float64,; MQRankSum: float64,; MQ_DP: int32,; QUALapprox: int32,; RAW_MQ: float64,; ReadPosRankSum: float64,; VarDP: int32; }; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html:22976,Deployability,update,updated,22976,"ruct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; 'AS_lowqual': bool; 'telomere_or_centromere': bool; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html:9475,Deployability,update,updated,9475,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_afr. View page source. gnomad_ld_scores_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html:9475,Deployability,update,updated,9475,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_amr. View page source. gnomad_ld_scores_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html:9475,Deployability,update,updated,9475,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_asj. View page source. gnomad_ld_scores_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html:9475,Deployability,update,updated,9475,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_eas. View page source. gnomad_ld_scores_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_est.html:9475,Deployability,update,updated,9475,"7). gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_est. View page source. gnomad_ld_scores_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_est.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_est.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html:9475,Deployability,update,updated,9475,"; Schema (2.1.1, GRCh37). gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_fin. View page source. gnomad_ld_scores_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html:9475,Deployability,update,updated,9475,"; gnomad_ld_scores_nfe; Schema (2.1.1, GRCh37). gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nfe. View page source. gnomad_ld_scores_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html:9475,Deployability,update,updated,9475,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; Schema (2.1.1, GRCh37). gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nwe. View page source. gnomad_ld_scores_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html:9475,Deployability,update,updated,9475,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; Schema (2.1.1, GRCh37). gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_seu. View page source. gnomad_ld_scores_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_afr. View page source. gnomad_ld_variant_indices_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_amr. View page source. gnomad_ld_variant_indices_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_asj. View page source. gnomad_ld_variant_indices_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_eas. View page source. gnomad_ld_variant_indices_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_est. View page source. gnomad_ld_variant_indices_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_fin. View page source. gnomad_ld_variant_indices_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html:10454,Deployability,update,updated,10454,"ant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nfe. View page source. gnomad_ld_variant_indices_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'min_af': float64; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nwe. View page source. gnomad_ld_variant_indices_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html:10435,Deployability,update,updated,10435,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_seu. View page source. gnomad_ld_variant_indices_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_lof_metrics.html:10941,Deployability,update,updated,10941,".1.1, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_lof_metrics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_lof_metrics.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; Schema (2.1, GRCh37). gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d01. View page source. gnomad_mnv_genome_d01. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; Schema (2.1, GRCh37). gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d02. View page source. gnomad_mnv_genome_d02. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; Schema (2.1, GRCh37). gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d03. View page source. gnomad_mnv_genome_d03. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; Schema (2.1, GRCh37). gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d04. View page source. gnomad_mnv_genome_d04. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; Schema (2.1, GRCh37). gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d05. View page source. gnomad_mnv_genome_d05. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; Schema (2.1, GRCh37). gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d06. View page source. gnomad_mnv_genome_d06. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; Schema (2.1, GRCh37). gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d07. View page source. gnomad_mnv_genome_d07. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; Schema (2.1, GRCh37). gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d08. View page source. gnomad_mnv_genome_d08. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; Schema (2.1, GRCh37). gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d09. View page source. gnomad_mnv_genome_d09. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html:9486,Deployability,update,updated,9486,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; Schema (2.1, GRCh37). gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d10. View page source. gnomad_mnv_genome_d10. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html:9409,Deployability,update,updated,9409,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html:9195,Performance,load,loadings,9195,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html:10970,Deployability,update,updated,10970,".1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html:13328,Deployability,update,updated,13328,"al': array<int64>; 'exp_lof_afr': array<float64>; 'obs_lof_afr': array<int64>; 'exp_lof_amr': array<float64>; 'obs_lof_amr': array<int64>; 'exp_lof_eas': array<float64>; 'obs_lof_eas': array<int64>; 'exp_lof_nfe': array<float64>; 'obs_lof_nfe': array<int64>; 'exp_lof_sas': array<float64>; 'obs_lof_sas': array<int64>; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': set<str>; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int64; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int64; 'n_sites_array': array<int64>; 'classic_caf': float64; 'max_af': float64; 'classic_caf_array': array<float64>; 'no_lofs': int64; 'obs_het_lof': int64; 'obs_hom_lof': int64; 'defined': int64; 'pop_no_lofs': dict<str, int64>; 'pop_obs_het_lof': dict<str, int64>; 'pop_obs_hom_lof': dict<str, int64>; 'pop_defined': dict<str, int64>; 'p': float64; 'pop_p': dict<str, float64>; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': str; 'cds_length': int64; 'num_coding_exons': int64; 'interval': interval<locus<GRCh37>>; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': float64; ----------------------------------------; Key: ['gene', 'transcript']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html
https://hail.is/docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html:10199,Deployability,update,updated,10199,"hema (2.1.1, GRCh37). ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_variant_co-occurrence. View page source. gnomad_variant_co-occurrence. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'max_freq': float64; 'least_consequence': str; 'same_haplotype_em_probability_cutoff': float64; 'different_haplotypes_em_probability_cutoff': float64; 'global_annotation_descriptions': struct {; max_freq: str,; least_consequence: str,; same_haplotype_em_probability_cutoff: str,; different_haplotypes_em_probability_cutoff: str; }; 'row_annotation_descriptions': struct {; locus1: str,; alleles1: str,; locus2: str,; alleles2: str,; phase_info: struct {; description: str,; gt_counts: str,; em: struct {; hap_counts: str,; p_chet: str,; same_haplotype: str,; different_haplotype: str; }; }; }; ----------------------------------------; Row fields:; 'locus1': locus<GRCh37>; 'alleles1': array<str>; 'locus2': locus<GRCh37>; 'alleles2': array<str>; 'phase_info': dict<str, struct {; gt_counts: array<int32>,; em: struct {; hap_counts: array<float64>,; p_chet: float64,; same_haplotype: bool,; different_haplotype: bool; }; }>; ----------------------------------------; Key: ['locus1', 'alleles1', 'locus2', 'alleles2']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html:9718,Deployability,update,updated,9718,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html:9730,Deployability,update,updated,9730,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html:9697,Deployability,update,updated,9697,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html:9801,Deployability,update,updated,9801,"d_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_all_snp_gene_associations. View page source. GTEx_eQTL_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'tissue': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'tss_distance': int32; ----------------------------------------; Entry fields:; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Column key: ['tissue']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html:9694,Deployability,update,updated,9694,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html:9703,Deployability,update,updated,9703,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html:9697,Deployability,update,updated,9697,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html:9700,Deployability,update,updated,9700,"iant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html:9766,Deployability,update,updated,9766,"est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html:9739,Deployability,update,updated,9739,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html:9739,Deployability,update,updated,9739,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html:9706,Deployability,update,updated,9706,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html:9694,Deployability,update,updated,9694,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html:9730,Deployability,update,updated,9730,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html:9709,Deployability,update,updated,9709,"ndices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html:9712,Deployability,update,updated,9712,"ices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html:9769,Deployability,update,updated,9769,"t; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html:9739,Deployability,update,updated,9739,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html:9748,Deployability,update,updated,9748,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html:9724,Deployability,update,updated,9724,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html:9721,Deployability,update,updated,9721,"as; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html:9736,Deployability,update,updated,9736,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html:9757,Deployability,update,updated,9757,"dices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html:9697,Deployability,update,updated,9697,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html:9706,Deployability,update,updated,9706,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:9763,Deployability,update,updated,9763,"s_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html:9706,Deployability,update,updated,9706,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html:9718,Deployability,update,updated,9718,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:9724,Deployability,update,updated,9724,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html:9718,Deployability,update,updated,9718,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html:9697,Deployability,update,updated,9697,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html:9673,Deployability,update,updated,9673,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Liver_all_snp_gene_associations. View page source. GTEx_eQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html:9670,Deployability,update,updated,9670,"s_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Lung_all_snp_gene_associations. View page source. GTEx_eQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html:9718,Deployability,update,updated,9718,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html:9703,Deployability,update,updated,9703,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html:9694,Deployability,update,updated,9694,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html:9673,Deployability,update,updated,9673,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Ovary_all_snp_gene_associations. View page source. GTEx_eQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html:9682,Deployability,update,updated,9682,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_eQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html:9685,Deployability,update,updated,9685,"mad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_eQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html:9682,Deployability,update,updated,9682,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Prostate_all_snp_gene_associations. View page source. GTEx_eQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:9751,Deployability,update,updated,9751,"t_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:9736,Deployability,update,updated,9736,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:9748,Deployability,update,updated,9748,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html:9676,Deployability,update,updated,9676,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Spleen_all_snp_gene_associations. View page source. GTEx_eQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html:9679,Deployability,update,updated,9679," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Stomach_all_snp_gene_associations. View page source. GTEx_eQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html:9676,Deployability,update,updated,9676,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Testis_all_snp_gene_associations. View page source. GTEx_eQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html:9679,Deployability,update,updated,9679," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_eQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html:9676,Deployability,update,updated,9676,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Uterus_all_snp_gene_associations. View page source. GTEx_eQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html:9676,Deployability,update,updated,9676,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Vagina_all_snp_gene_associations. View page source. GTEx_eQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html:9691,Deployability,update,updated,9691,"ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_eQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html:11118,Deployability,update,updated,11118,"e: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'read_count': int32; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html:11092,Deployability,update,updated,11092,"genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': float64; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html:11193,Deployability,update,updated,11193,"; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'junction_id': str; 'junction_interval': interval<locus<GRCh37>>; 'gene_id': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'gene_symbol': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': int32; ----------------------------------------; Column key: ['s']; Row key: ['junction_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html:9775,Deployability,update,updated,9775,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html:9787,Deployability,update,updated,9787,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html:9754,Deployability,update,updated,9754,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html:9751,Deployability,update,updated,9751,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html:9760,Deployability,update,updated,9760,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html:9754,Deployability,update,updated,9754,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html:9757,Deployability,update,updated,9757,"d_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html:9823,Deployability,update,updated,9823,"ices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html:9796,Deployability,update,updated,9796,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html:9796,Deployability,update,updated,9796,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html:9763,Deployability,update,updated,9763,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html:9751,Deployability,update,updated,9751,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html:9787,Deployability,update,updated,9787,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html:9766,Deployability,update,updated,9766,"ant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html:9769,Deployability,update,updated,9769,"t_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html:9826,Deployability,update,updated,9826,"es_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html:9796,Deployability,update,updated,9796,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html:9805,Deployability,update,updated,9805,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html:9781,Deployability,update,updated,9781,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html:9778,Deployability,update,updated,9778,"ces_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html:9793,Deployability,update,updated,9793,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html:9814,Deployability,update,updated,9814,"nt_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html:9754,Deployability,update,updated,9754,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html:9763,Deployability,update,updated,9763,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:9820,Deployability,update,updated,9820,"ndices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html:9763,Deployability,update,updated,9763,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html:9775,Deployability,update,updated,9775,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:9781,Deployability,update,updated,9781,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html:9775,Deployability,update,updated,9775,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html:9754,Deployability,update,updated,9754,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html:9730,Deployability,update,updated,9730,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Liver_all_snp_gene_associations. View page source. GTEx_sQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html:9727,Deployability,update,updated,9727,"ndices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Lung_all_snp_gene_associations. View page source. GTEx_sQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html:9775,Deployability,update,updated,9775,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html:9760,Deployability,update,updated,9760,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html:9751,Deployability,update,updated,9751,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html:9730,Deployability,update,updated,9730,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Ovary_all_snp_gene_associations. View page source. GTEx_sQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html:9739,Deployability,update,updated,9739,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_sQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html:9742,Deployability,update,updated,9742,"; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_sQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html:9739,Deployability,update,updated,9739,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Prostate_all_snp_gene_associations. View page source. GTEx_sQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:9808,Deployability,update,updated,9808,"ariant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:9793,Deployability,update,updated,9793,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:9805,Deployability,update,updated,9805,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html:9733,Deployability,update,updated,9733,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Spleen_all_snp_gene_associations. View page source. GTEx_sQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html:9736,Deployability,update,updated,9736,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Stomach_all_snp_gene_associations. View page source. GTEx_sQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html:9733,Deployability,update,updated,9733,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Testis_all_snp_gene_associations. View page source. GTEx_sQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html:9736,Deployability,update,updated,9736,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_sQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html:9733,Deployability,update,updated,9733,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Uterus_all_snp_gene_associations. View page source. GTEx_sQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html:9733,Deployability,update,updated,9733,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Vagina_all_snp_gene_associations. View page source. GTEx_sQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html:9748,Deployability,update,updated,9748,"omad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_sQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html
https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html:12941,Deployability,update,updated,12941,"L2': float64; 'UTR_5_UCSC.flanking.500L2': float64; 'WeakEnhancer_HoffmanL2': float64; 'WeakEnhancer_Hoffman.flanking.500L2': float64; 'GERP.NSL2': float64; 'GERP.RSsup4L2': float64; 'MAFbin1L2': float64; 'MAFbin2L2': float64; 'MAFbin3L2': float64; 'MAFbin4L2': float64; 'MAFbin5L2': float64; 'MAFbin6L2': float64; 'MAFbin7L2': float64; 'MAFbin8L2': float64; 'MAFbin9L2': float64; 'MAFbin10L2': float64; 'MAF_Adj_Predicted_Allele_AgeL2': float64; 'MAF_Adj_LLD_AFRL2': float64; 'Recomb_Rate_10kbL2': float64; 'Nucleotide_Diversity_10kbL2': float64; 'Backgrd_Selection_StatL2': float64; 'CpG_Content_50kbL2': float64; 'MAF_Adj_ASMCL2': float64; 'GTEx_eQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K27acQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K4me1QTL_MaxCPPL2': float64; 'BLUEPRINT_DNA_methylation_MaxCPPL2': float64; 'synonymousL2': float64; 'non_synonymousL2': float64; 'Conserved_Vertebrate_phastCons46wayL2': float64; 'Conserved_Vertebrate_phastCons46way.flanking.500L2': float64; 'Conserved_Mammal_phastCons46wayL2': float64; 'Conserved_Mammal_phastCons46way.flanking.500L2': float64; 'Conserved_Primate_phastCons46wayL2': float64; 'Conserved_Primate_phastCons46way.flanking.500L2': float64; 'BivFlnkL2': float64; 'BivFlnk.flanking.500L2': float64; 'Human_Promoter_VillarL2': float64; 'Human_Promoter_Villar.flanking.500L2': float64; 'Human_Enhancer_VillarL2': float64; 'Human_Enhancer_Villar.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_PromoterL2': float64; 'Ancient_Sequence_Age_Human_Promoter.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_EnhancerL2': float64; 'Ancient_Sequence_Age_Human_Enhancer.flanking.500L2': float64; 'Human_Enhancer_Villar_Species_Enhancer_CountL2': float64; 'Human_Promoter_Villar_ExACL2': float64; 'Human_Promoter_Villar_ExAC.flanking.500L2': float64; 'locus': locus<GRCh37>; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html
https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html:9646,Deployability,update,updated,9646,"_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; Schema (2.2, GRCh37). panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; ldsc_baselineLD_ldscores. View page source. ldsc_baselineLD_ldscores. Versions: 2.2, 1.1; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (2.2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'annotation': str; 'M_5_50': int32; 'M': int32; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'SNP': str; ----------------------------------------; Entry fields:; 'x': float64; ----------------------------------------; Column key: ['annotation']; Row key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html:9391,Deployability,update,updated,9391,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; Schema (0.2, GRCh37). panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AFR. View page source. panukb_ld_scores_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html:9391,Deployability,update,updated,9391,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; Schema (0.2, GRCh37). panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AMR. View page source. panukb_ld_scores_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html:9391,Deployability,update,updated,9391,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; Schema (0.2, GRCh37). panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_CSA. View page source. panukb_ld_scores_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html:9391,Deployability,update,updated,9391,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; Schema (0.2, GRCh37). panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EAS. View page source. panukb_ld_scores_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html:9391,Deployability,update,updated,9391,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; Schema (0.2, GRCh37). panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EUR. View page source. panukb_ld_scores_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_MID.html:9391,Deployability,update,updated,9391,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; Schema (0.2, GRCh37). panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_MID. View page source. panukb_ld_scores_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_MID.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_MID.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html:9395,Deployability,update,updated,9395,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; Schema (0.2, GRCh37). panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AFR. View page source. panukb_ld_variant_indices_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html:9395,Deployability,update,updated,9395,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; Schema (0.2, GRCh37). panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AMR. View page source. panukb_ld_variant_indices_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html:9395,Deployability,update,updated,9395,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; Schema (0.2, GRCh37). panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_CSA. View page source. panukb_ld_variant_indices_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html:9395,Deployability,update,updated,9395,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; Schema (0.2, GRCh37). panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EAS. View page source. panukb_ld_variant_indices_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html:9395,Deployability,update,updated,9395,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; Schema (0.2, GRCh37). panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EUR. View page source. panukb_ld_variant_indices_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html
https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html:9395,Deployability,update,updated,9395,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; Schema (0.2, GRCh37). panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_MID. View page source. panukb_ld_variant_indices_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html
https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html:14753,Deployability,update,updated,14753,"t32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'freq': array<struct {; pop: str,; ac: float64,; af: float64,; an: int64,; gnomad_exomes_ac: int32,; gnomad_exomes_af: float64,; gnomad_exomes_an: int32,; gnomad_genomes_ac: int32,; gnomad_genomes_af: float64,; gnomad_genomes_an: int32; }>; 'pass_gnomad_exomes': bool; 'pass_gnomad_genomes': bool; 'n_passing_populations': int32; 'high_quality': bool; 'nearest_genes': str; 'info': float64; ----------------------------------------; Entry fields:; 'meta_analysis': array<struct {; BETA: float64,; SE: float64,; Pvalue: float64,; Q: float64,; Pvalue_het: float64,; N: int32,; N_pops: int32,; AF_Allele2: float64,; AF_Cases: float64,; AF_Controls: float64; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html
https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html:14747,Deployability,update,updated,14747,"t32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'freq': array<struct {; pop: str,; ac: float64,; af: float64,; an: int64,; gnomad_exomes_ac: int32,; gnomad_exomes_af: float64,; gnomad_exomes_an: int32,; gnomad_genomes_ac: int32,; gnomad_genomes_af: float64,; gnomad_genomes_an: int32; }>; 'pass_gnomad_exomes': bool; 'pass_gnomad_genomes': bool; 'n_passing_populations': int32; 'high_quality': bool; 'nearest_genes': str; 'info': float64; ----------------------------------------; Entry fields:; 'meta_analysis': array<struct {; BETA: float64,; SE: float64,; Pvalue: float64,; Q: float64,; Pvalue_het: float64,; N: int32,; N_pops: int32,; AF_Allele2: float64,; AF_Cases: float64,; AF_Controls: float64; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html
https://hail.is/docs/0.2/datasets/schemas/panukb_summary_stats.html:10227,Deployability,update,updated,10227,"lineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats; Schema (0.3, GRCh37). Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_summary_stats. View page source. panukb_summary_stats. Versions: 0.3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (0.3, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'trait_type': str; 'phenocode': str; 'pheno_sex': str; 'coding': str; 'modifier': str; 'pheno_data': array<struct {; n_cases: int32,; n_controls: int32,; heritability: float64,; saige_version: str,; inv_normalized: bool,; pop: str; }>; 'description': str; 'description_more': str; 'coding_description': str; 'category': str; 'n_cases_full_cohort_both_sexes': int64; 'n_cases_full_cohort_females': int64; 'n_cases_full_cohort_males': int64; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'gene': str; 'annotation': str; ----------------------------------------; Entry fields:; 'summary_stats': array<struct {; AF_Allele2: float64,; imputationInfo: float64,; BETA: float64,; SE: float64,; `p.value.NA`: float64,; `AF.Cases`: float64,; `AF.Controls`: float64,; Pvalue: float64,; low_confidence: bool; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_summary_stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_summary_stats.html
https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html:10509,Deployability,update,updated,10509,"ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_both_sexes. View page source. UK_Biobank_Rapid_GWAS_both_sexes. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html
https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html:10497,Deployability,update,updated,10497," panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_female. View page source. UK_Biobank_Rapid_GWAS_female. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html
https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html:10491,Deployability,update,updated,10491,"CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_male. View page source. UK_Biobank_Rapid_GWAS_male. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html
https://hail.is/docs/0.2/linalg/utils/index.html:4883,Deployability,update,updated,4883,"hl.linalg.utils.locus_windows(ht.locus, 1); (array([0, 0, 2, 3, 3, 5]), array([2, 2, 3, 5, 5, 6])). Windows with 1cm radius:; >>> hl.linalg.utils.locus_windows(ht.locus, 1.0, coord_expr=ht.cm); (array([0, 1, 1, 3, 3, 5]), array([1, 3, 3, 5, 5, 6])). Notes; This function returns two 1-dimensional ndarrays of integers,; starts and stops, each of size equal to the number of rows.; By default, for all indices i, [starts[i], stops[i]) is the maximal; range of row indices j such that contig[i] == contig[j] and; position[i] - radius <= position[j] <= position[i] + radius.; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that has; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the function will fail.; The last example above uses centimorgan coordinates, so; [starts[i], stops[i]) is the maximal range of row indices j such; that contig[i] == contig[j] and; cm[i] - radius <= cm[j] <= cm[i] + radius.; Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; BlockMatrix.sparsify_row_intervals(). Parameters:. locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table.; radius (int) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value.; Must be on the same table or matrix table as locus_expr.; By default, the row value is given by the locus position. Returns:; (numpy.ndarray of int, numpy.ndarray of int) – Tuple of start indices array and stop indices array. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/utils/index.html
https://hail.is/docs/0.2/_modules/hail/context.html:8825,Availability,avail,available,8825,"sh` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:12067,Availability,error,error,12067," of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; dr",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:6629,Deployability,configurat,configuration,6629,"))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_con",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:6973,Deployability,configurat,configuration,6973,"_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKI",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:7160,Deployability,configurat,configuration,7160,"ores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkCo",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:8350,Deployability,pipeline,pipeline,8350," # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:8988,Deployability,configurat,configuration,8988,"hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Sp",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:10003,Deployability,configurat,configuration,10003,"lass:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:10105,Deployability,configurat,configuration,10105,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:11610,Deployability,configurat,configuration,11610,"rocess. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; defa",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:12206,Deployability,configurat,configuration,12206,"vided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:12769,Deployability,release,release,12769,"w_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configur",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:20273,Deployability,install,installed,20273,"'GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {ve",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:20474,Deployability,install,installed,20474," hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:20777,Deployability,install,installed,20777,"L_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if Env._hc:; Env.hc().stop(). [docs]def spark_context():; """"""Returns the active Spark context. Returns; -------; :class:`pyspark.SparkContext`; """"""; return Env.spark_backend('spark_context').sc. [docs]def tmp_dir() -> str:; """"""Returns the Hail shared temporary directory. Returns; -------; :class:`str`; """"""; return",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:20829,Deployability,release,releases,20829,"; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if Env._hc:; Env.hc().stop(). [docs]def spark_context():; """"""Returns the active Spark context. Returns; -------; :class:`pyspark.SparkContext`; """"""; return Env.spark_backend('spark_context').sc. [docs]def tmp_dir() -> str:; """"""Returns the Hail shared temporary directory. Returns; -------; :class:`str`; """"""; return Env.hc()._tmpdir. class _TemporaryFilenameManager:; def __in",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:27839,Deployability,update,updated,27839,"ndle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**flags):; Env.backend().set_flags(**flags). def _get_flags(*flags):; return Env.backend().get_flags(*flags). @contextmanager; def _with_flags(**flags):; before = _get_flags(*flags); try:; _set_flags(**flags); yield; finally:; _set_flags(**before). def debug_info():; from hail.backend.backend import local_jar_information; from hail.backend.spark_backend import SparkBackend. spark_conf = None; if isinstance(Env.backend(), SparkBackend):; spark_conf = spark_context()._conf.getAll(); return {'spark_conf': spark_conf, 'local_jar_information': local_jar_information(), 'version': version()}. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:9179,Integrability,message,messages,9179,"; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`s",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:6506,Modifiability,config,configure,6506,"),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; backend=nullable(enumeration(*BackendType.__args__)),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:6629,Modifiability,config,configuration,6629,"))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_con",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:6973,Modifiability,config,configuration,6973,"_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKI",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:7089,Modifiability,variab,variable,7089,"ion=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buck",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:7126,Modifiability,config,config,7126,"None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:7160,Modifiability,config,configuration,7160,"ores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkCo",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:7987,Modifiability,config,config,7987,"is function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local pat",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:8039,Modifiability,config,config,8039,"is function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local pat",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:8988,Modifiability,config,configuration,8988,"hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Sp",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:10003,Modifiability,config,configuration,10003,"lass:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:10105,Modifiability,config,configuration,10105,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:11089,Modifiability,config,configure,11089,"obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_d",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:11217,Modifiability,config,configure,11217,"e. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.';",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:11610,Modifiability,config,configuration,11610,"rocess. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; defa",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:11652,Modifiability,config,config,11652,"kend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" back",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:12206,Modifiability,config,configuration,12206,"vided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:26173,Performance,load,loaded,26173," reference genome (``'GRCh37'`` by default).; With an argument, sets the default reference genome to the argument. Returns; -------; :class:`.ReferenceGenome`; """"""; if new_default_reference is not None:; Env.hc().default_reference = new_default_reference; return None; return Env.hc().default_reference. [docs]def get_reference(name) -> ReferenceGenome:; """"""Returns the reference genome corresponding to `name`. Notes; -----. Hail's built-in references are ``'GRCh37'``, ``GRCh38'``, ``'GRCm38'``, and; ``'CanFam3'``.; The contig names and lengths come from the GATK resource bundle:; `human_g1k_v37.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**f",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:7544,Security,access,accessing,7544,"ault arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifyin",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:7742,Security,access,accessing,7742,"and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:1899,Testability,log,log,1899,", nullable, oneof, sequenceof, sized_tupleof, typecheck, typecheck_method; from hail.utils import get_env_or_default; from hail.utils.java import BackendType, Env, choose_backend, warning; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:1909,Testability,log,log,1909,", nullable, oneof, sequenceof, sized_tupleof, typecheck, typecheck_method; from hail.utils import get_env_or_default; from hail.utils.java import BackendType, Env, choose_backend, warning; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2033,Testability,log,log,2033,"fault; from hail.utils.java import BackendType, Env, choose_backend, warning; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, back",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2119,Testability,log,log,2119,"loud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2133,Testability,log,log,2133,"loud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2468,Testability,log,log,2468,"pdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during th",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2638,Testability,log,log,2638,"pdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during th",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2642,Testability,log,log,2642,"pdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during th",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2845,Testability,log,log,2845,"ion(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_non",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:2975,Testability,log,log,2975,"ion(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_non",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:3041,Testability,assert,assert,3041,"ion(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_non",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:3073,Testability,log,log,3073,"h.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_nonce = False; else:; backend.set_flags(rng_nonce=hex(global_seed)); Env._hc = self. def initialize_references(self, default_reference)",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:3708,Testability,log,log,3708,"l_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nullable(int), backend=Backend; ); def __init__(self, log, quiet, append, tmpdir, local_tmpdir, global_seed, backend):; assert not Env._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_nonce = False; else:; backend.set_flags(rng_nonce=hex(global_seed)); Env._hc = self. def initialize_references(self, default_reference):; assert self._backend; self._backend.initialize_references(); if default_reference in BUILTIN_REFERENCES:; self._default_ref = self._backend.get_reference(default_reference); else:; self._default_ref = ReferenceGenome.read(default_reference). @property; def default_reference(self) -> ReferenceGenome:; assert self._default_ref is not None, '_default_ref should have been initialized in HailContext.create'; return self._default_ref. @default_reference.setter; def default_reference(self, value):; if not isinstance(value, ReferenceGenome):; raise TypeError(f'{value} is {type(value)} not a ReferenceGenome'); self._default_",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:4075,Testability,assert,assert,4075,"v._hc. self._log = log. self._tmpdir = tmpdir; self._local_tmpdir = local_tmpdir. self._backend = backend. self._warn_cols_order = True; self._warn_entries_order = True. self._default_ref: Optional[ReferenceGenome] = None. if not quiet:; py_version = version(); sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_nonce = False; else:; backend.set_flags(rng_nonce=hex(global_seed)); Env._hc = self. def initialize_references(self, default_reference):; assert self._backend; self._backend.initialize_references(); if default_reference in BUILTIN_REFERENCES:; self._default_ref = self._backend.get_reference(default_reference); else:; self._default_ref = ReferenceGenome.read(default_reference). @property; def default_reference(self) -> ReferenceGenome:; assert self._default_ref is not None, '_default_ref should have been initialized in HailContext.create'; return self._default_ref. @default_reference.setter; def default_reference(self, value):; if not isinstance(value, ReferenceGenome):; raise TypeError(f'{value} is {type(value)} not a ReferenceGenome'); self._default_ref = value. def stop(self):; assert self._backend; self._backend.stop(); self._backend = None; Env._hc = None; Env._dummy_table = None; Env._seed_generator = None; hail.ir.clear_session_functions(). [docs]@typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; mi",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:4377,Testability,assert,assert,4377," __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_nonce = False; else:; backend.set_flags(rng_nonce=hex(global_seed)); Env._hc = self. def initialize_references(self, default_reference):; assert self._backend; self._backend.initialize_references(); if default_reference in BUILTIN_REFERENCES:; self._default_ref = self._backend.get_reference(default_reference); else:; self._default_ref = ReferenceGenome.read(default_reference). @property; def default_reference(self) -> ReferenceGenome:; assert self._default_ref is not None, '_default_ref should have been initialized in HailContext.create'; return self._default_ref. @default_reference.setter; def default_reference(self, value):; if not isinstance(value, ReferenceGenome):; raise TypeError(f'{value} is {type(value)} not a ReferenceGenome'); self._default_ref = value. def stop(self):; assert self._backend; self._backend.stop(); self._backend = None; Env._hc = None; Env._dummy_table = None; Env._seed_generator = None; hail.ir.clear_session_functions(). [docs]@typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:4728,Testability,assert,assert,4728,"ser_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_nonce = False; else:; backend.set_flags(rng_nonce=hex(global_seed)); Env._hc = self. def initialize_references(self, default_reference):; assert self._backend; self._backend.initialize_references(); if default_reference in BUILTIN_REFERENCES:; self._default_ref = self._backend.get_reference(default_reference); else:; self._default_ref = ReferenceGenome.read(default_reference). @property; def default_reference(self) -> ReferenceGenome:; assert self._default_ref is not None, '_default_ref should have been initialized in HailContext.create'; return self._default_ref. @default_reference.setter; def default_reference(self, value):; if not isinstance(value, ReferenceGenome):; raise TypeError(f'{value} is {type(value)} not a ReferenceGenome'); self._default_ref = value. def stop(self):; assert self._backend; self._backend.stop(); self._backend = None; Env._hc = None; Env._dummy_table = None; Env._seed_generator = None; hail.ir.clear_session_functions(). [docs]@typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; backend=nullable(enumeration(*BackendType.__args__)),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullabl",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:5005,Testability,log,log,5005,"ce in BUILTIN_REFERENCES:; self._default_ref = self._backend.get_reference(default_reference); else:; self._default_ref = ReferenceGenome.read(default_reference). @property; def default_reference(self) -> ReferenceGenome:; assert self._default_ref is not None, '_default_ref should have been initialized in HailContext.create'; return self._default_ref. @default_reference.setter; def default_reference(self, value):; if not isinstance(value, ReferenceGenome):; raise TypeError(f'{value} is {type(value)} not a ReferenceGenome'); self._default_ref = value. def stop(self):; assert self._backend; self._backend.stop(); self._backend = None; Env._hc = None; Env._dummy_table = None; Env._seed_generator = None; hail.ir.clear_session_functions(). [docs]@typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; backend=nullable(enumeration(*BackendType.__args__)),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:5873,Testability,log,log,5873,"),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; backend=nullable(enumeration(*BackendType.__args__)),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:9004,Testability,log,log,9004,"nd `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:9044,Testability,log,log,9044,"nd `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:9175,Testability,log,log,9175,"; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`s",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:9237,Testability,log,log,9237,"only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on dri",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:10097,Testability,log,logging,10097,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:12020,Testability,log,log,12020," of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; dr",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:12885,Testability,log,log,12885," default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:12889,Testability,log,log,12889," default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:13551,Testability,log,log,13551," default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:13555,Testability,log,log,13555," default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:13940,Testability,log,log,13940," default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:13944,Testability,log,log,13944," default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:14349,Testability,log,log,14349,"n,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; copy_log_on_error=nullable(bool),; ); def init_spark(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference='GRCh37',; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; copy_log_on_error: bool = False,; ):; from hail.backend.py4j_backend import connect_logger; from hail.backend.spark_backend import SparkBackend. log = _get_log(log); tmpdir = _get_tmpdir(tmp_dir); local_tmpdir = _get_local_tmpdir(local_tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). app_name = app_name or 'Hail'; (; gcs_requester_pays_project,; gcs_requester_pays_buckets,; ) = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; get_gcs_request",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:14911,Testability,log,log,14911,"n,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; copy_log_on_error=nullable(bool),; ); def init_spark(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference='GRCh37',; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; copy_log_on_error: bool = False,; ):; from hail.backend.py4j_backend import connect_logger; from hail.backend.spark_backend import SparkBackend. log = _get_log(log); tmpdir = _get_tmpdir(tmp_dir); local_tmpdir = _get_local_tmpdir(local_tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). app_name = app_name or 'Hail'; (; gcs_requester_pays_project,; gcs_requester_pays_buckets,; ) = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; get_gcs_request",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:15404,Testability,log,log,15404,"_REFERENCES),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; copy_log_on_error=nullable(bool),; ); def init_spark(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference='GRCh37',; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; copy_log_on_error: bool = False,; ):; from hail.backend.py4j_backend import connect_logger; from hail.backend.spark_backend import SparkBackend. log = _get_log(log); tmpdir = _get_tmpdir(tmp_dir); local_tmpdir = _get_local_tmpdir(local_tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). app_name = app_name or 'Hail'; (; gcs_requester_pays_project,; gcs_requester_pays_buckets,; ) = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; get_gcs_requester_pays_configuration(; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); ); backend = SparkBackend(; idempotent,; sc,; spark_conf,; app_name,; master,; local,; log,; quiet,; append,; min_block_size,; branching_factor,; tmpdir,; local_tmpdir,; skip_logging_configuration,; optimizer_iterations,; gcs_requester_pays_project=gcs_requester_pays_project,; gcs_requester_pays_buckets=gcs_requester_pays_buckets,; copy_log_on_error=copy_log_on_error,; ); if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). @typechec",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:15419,Testability,log,log,15419,"_REFERENCES),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; copy_log_on_error=nullable(bool),; ); def init_spark(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference='GRCh37',; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; copy_log_on_error: bool = False,; ):; from hail.backend.py4j_backend import connect_logger; from hail.backend.spark_backend import SparkBackend. log = _get_log(log); tmpdir = _get_tmpdir(tmp_dir); local_tmpdir = _get_local_tmpdir(local_tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). app_name = app_name or 'Hail'; (; gcs_requester_pays_project,; gcs_requester_pays_buckets,; ) = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; get_gcs_requester_pays_configuration(; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); ); backend = SparkBackend(; idempotent,; sc,; spark_conf,; app_name,; master,; local,; log,; quiet,; append,; min_block_size,; branching_factor,; tmpdir,; local_tmpdir,; skip_logging_configuration,; optimizer_iterations,; gcs_requester_pays_project=gcs_requester_pays_project,; gcs_requester_pays_buckets=gcs_requester_pays_buckets,; copy_log_on_error=copy_log_on_error,; ); if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). @typechec",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:15959,Testability,log,log,15959,"se,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference='GRCh37',; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; copy_log_on_error: bool = False,; ):; from hail.backend.py4j_backend import connect_logger; from hail.backend.spark_backend import SparkBackend. log = _get_log(log); tmpdir = _get_tmpdir(tmp_dir); local_tmpdir = _get_local_tmpdir(local_tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). app_name = app_name or 'Hail'; (; gcs_requester_pays_project,; gcs_requester_pays_buckets,; ) = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; get_gcs_requester_pays_configuration(; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); ); backend = SparkBackend(; idempotent,; sc,; spark_conf,; app_name,; master,; local,; log,; quiet,; append,; min_block_size,; branching_factor,; tmpdir,; local_tmpdir,; skip_logging_configuration,; optimizer_iterations,; gcs_requester_pays_project=gcs_requester_pays_project,; gcs_requester_pays_buckets=gcs_requester_pays_buckets,; copy_log_on_error=copy_log_on_error,; ); if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). @typecheck(; billing_project=nullable(str),; remote_tmpdir=nullable(str),; log=nullable(str),; quiet=bool,; append=bool,; tmpdir=nullable(str),; local_tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; disable_progress_bar=nullable(bool),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; name",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:16327,Testability,log,log,16327,"mport SparkBackend. log = _get_log(log); tmpdir = _get_tmpdir(tmp_dir); local_tmpdir = _get_local_tmpdir(local_tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). app_name = app_name or 'Hail'; (; gcs_requester_pays_project,; gcs_requester_pays_buckets,; ) = convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; get_gcs_requester_pays_configuration(; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); ); backend = SparkBackend(; idempotent,; sc,; spark_conf,; app_name,; master,; local,; log,; quiet,; append,; min_block_size,; branching_factor,; tmpdir,; local_tmpdir,; skip_logging_configuration,; optimizer_iterations,; gcs_requester_pays_project=gcs_requester_pays_project,; gcs_requester_pays_buckets=gcs_requester_pays_buckets,; copy_log_on_error=copy_log_on_error,; ); if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). @typecheck(; billing_project=nullable(str),; remote_tmpdir=nullable(str),; log=nullable(str),; quiet=bool,; append=bool,; tmpdir=nullable(str),; local_tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; disable_progress_bar=nullable(bool),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; name_prefix=nullable(str),; token=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(sequenceof(str)),; ); async def init_batch(; *,; billing_project: Optional[str] = None,; remote_tmpdir: Optional[str] = None,; log: Optional[str] = None,; quiet: bool = False,; append: bool = False,; tmpdir: Optional[str] = None,; local_tmpdir: Op",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:16568,Testability,log,log,16568,"or=copy_log_on_error,; ); if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). @typecheck(; billing_project=nullable(str),; remote_tmpdir=nullable(str),; log=nullable(str),; quiet=bool,; append=bool,; tmpdir=nullable(str),; local_tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; disable_progress_bar=nullable(bool),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; name_prefix=nullable(str),; token=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(sequenceof(str)),; ); async def init_batch(; *,; billing_project: Optional[str] = None,; remote_tmpdir: Optional[str] = None,; log: Optional[str] = None,; quiet: bool = False,; append: bool = False,; tmpdir: Optional[str] = None,; local_tmpdir: Optional[str] = None,; default_reference: str = 'GRCh37',; global_seed: Optional[int] = None,; disable_progress_bar: Optional[bool] = None,; driver_cores: Optional[Union[str, int]] = None,; driver_memory: Optional[str] = None,; worker_cores: Optional[Union[str, int]] = None,; worker_memory: Optional[str] = None,; name_prefix: Optional[str] = None,; token: Optional[str] = None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[List[str]] = None,; ):; from hail.backend.service_backend import ServiceBackend. # FIXME: pass local_tmpdir and use on worker and driver; backend = await ServiceBackend.create(; billing_project=billing_project,; remote_tmpdir=remote_tmpdir,; disable_progress_bar=disable_progress_bar,; driver_cores=driver",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:17265,Testability,log,log,17265,"or=copy_log_on_error,; ); if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). @typecheck(; billing_project=nullable(str),; remote_tmpdir=nullable(str),; log=nullable(str),; quiet=bool,; append=bool,; tmpdir=nullable(str),; local_tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; disable_progress_bar=nullable(bool),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; name_prefix=nullable(str),; token=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(sequenceof(str)),; ); async def init_batch(; *,; billing_project: Optional[str] = None,; remote_tmpdir: Optional[str] = None,; log: Optional[str] = None,; quiet: bool = False,; append: bool = False,; tmpdir: Optional[str] = None,; local_tmpdir: Optional[str] = None,; default_reference: str = 'GRCh37',; global_seed: Optional[int] = None,; disable_progress_bar: Optional[bool] = None,; driver_cores: Optional[Union[str, int]] = None,; driver_memory: Optional[str] = None,; worker_cores: Optional[Union[str, int]] = None,; worker_memory: Optional[str] = None,; name_prefix: Optional[str] = None,; token: Optional[str] = None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[List[str]] = None,; ):; from hail.backend.service_backend import ServiceBackend. # FIXME: pass local_tmpdir and use on worker and driver; backend = await ServiceBackend.create(; billing_project=billing_project,; remote_tmpdir=remote_tmpdir,; disable_progress_bar=disable_progress_bar,; driver_cores=driver",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:18506,Testability,log,log,18506,"es: Optional[Union[str, int]] = None,; driver_memory: Optional[str] = None,; worker_cores: Optional[Union[str, int]] = None,; worker_memory: Optional[str] = None,; name_prefix: Optional[str] = None,; token: Optional[str] = None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[List[str]] = None,; ):; from hail.backend.service_backend import ServiceBackend. # FIXME: pass local_tmpdir and use on worker and driver; backend = await ServiceBackend.create(; billing_project=billing_project,; remote_tmpdir=remote_tmpdir,; disable_progress_bar=disable_progress_bar,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=name_prefix,; credentials_token=token,; regions=regions,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ). log = _get_log(log); if tmpdir is None:; tmpdir = backend.remote_tmpdir + 'tmp/hail/' + secret_alnum_string(); local_tmpdir = _get_local_tmpdir(local_tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend). @typecheck(; log=nullable(str),; quiet=bool,; append=bool,; branching_factor=int,; tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backen",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:18521,Testability,log,log,18521,"es: Optional[Union[str, int]] = None,; driver_memory: Optional[str] = None,; worker_cores: Optional[Union[str, int]] = None,; worker_memory: Optional[str] = None,; name_prefix: Optional[str] = None,; token: Optional[str] = None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[List[str]] = None,; ):; from hail.backend.service_backend import ServiceBackend. # FIXME: pass local_tmpdir and use on worker and driver; backend = await ServiceBackend.create(; billing_project=billing_project,; remote_tmpdir=remote_tmpdir,; disable_progress_bar=disable_progress_bar,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=name_prefix,; credentials_token=token,; regions=regions,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ). log = _get_log(log); if tmpdir is None:; tmpdir = backend.remote_tmpdir + 'tmp/hail/' + secret_alnum_string(); local_tmpdir = _get_local_tmpdir(local_tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend). @typecheck(; log=nullable(str),; quiet=bool,; append=bool,; branching_factor=int,; tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backen",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:18684,Testability,log,log,18684,"r] = None,; token: Optional[str] = None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[List[str]] = None,; ):; from hail.backend.service_backend import ServiceBackend. # FIXME: pass local_tmpdir and use on worker and driver; backend = await ServiceBackend.create(; billing_project=billing_project,; remote_tmpdir=remote_tmpdir,; disable_progress_bar=disable_progress_bar,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=name_prefix,; credentials_token=token,; regions=regions,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ). log = _get_log(log); if tmpdir is None:; tmpdir = backend.remote_tmpdir + 'tmp/hail/' + secret_alnum_string(); local_tmpdir = _get_local_tmpdir(local_tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend). @typecheck(; log=nullable(str),; quiet=bool,; append=bool,; branching_factor=int,; tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:18781,Testability,log,log,18781,"oject,; remote_tmpdir=remote_tmpdir,; disable_progress_bar=disable_progress_bar,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=name_prefix,; credentials_token=token,; regions=regions,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ). log = _get_log(log); if tmpdir is None:; tmpdir = backend.remote_tmpdir + 'tmp/hail/' + secret_alnum_string(); local_tmpdir = _get_local_tmpdir(local_tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend). @typecheck(; log=nullable(str),; quiet=bool,; append=bool,; branching_factor=int,; tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_refer",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:19170,Testability,log,log,19170,"oject,; remote_tmpdir=remote_tmpdir,; disable_progress_bar=disable_progress_bar,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=name_prefix,; credentials_token=token,; regions=regions,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ). log = _get_log(log); if tmpdir is None:; tmpdir = backend.remote_tmpdir + 'tmp/hail/' + secret_alnum_string(); local_tmpdir = _get_local_tmpdir(local_tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend). @typecheck(; log=nullable(str),; quiet=bool,; append=bool,; branching_factor=int,; tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_refer",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:19573,Testability,log,log,19573,"dir(local_tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend). @typecheck(; log=nullable(str),; quiet=bool,; append=bool,; branching_factor=int,; tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:19588,Testability,log,log,19588,"dir(local_tmpdir). HailContext.create(log, quiet, append, tmpdir, local_tmpdir, default_reference, global_seed, backend). @typecheck(; log=nullable(str),; quiet=bool,; append=bool,; branching_factor=int,; tmpdir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:19845,Testability,log,log,19845,"r=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; global_seed=nullable(int),; skip_logging_configuration=bool,; jvm_heap_size=nullable(str),; _optimizer_iterations=nullable(int),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:20073,Testability,log,log,20073,", sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" aut",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/context.html:27134,Testability,test,test,27134,"ndle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**flags):; Env.backend().set_flags(**flags). def _get_flags(*flags):; return Env.backend().get_flags(*flags). @contextmanager; def _with_flags(**flags):; before = _get_flags(*flags); try:; _set_flags(**flags); yield; finally:; _set_flags(**before). def debug_info():; from hail.backend.backend import local_jar_information; from hail.backend.spark_backend import SparkBackend. spark_conf = None; if isinstance(Env.backend(), SparkBackend):; spark_conf = spark_context()._conf.getAll(); return {'spark_conf': spark_conf, 'local_jar_information': local_jar_information(), 'version': version()}. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8479,Availability,down,downstream,8479,"on(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise Not",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:56793,Availability,down,downstream,56793,"ix. Parameters; ----------; expr : bool or :class:`.BooleanExpression`; Filter expression.; keep : bool; Keep entries where `expr` is true. Returns; -------; :class:`.MatrixTable`; Filtered matrix table. Examples; --------. Keep entries where the sum of `AD` is greater than 10 and `GQ` is greater than 20:. >>> dataset_result = dataset.filter_entries((hl.sum(dataset.AD) > 10) & (dataset.GQ > 20)). Warning; -------; When `expr` evaluates to missing, the entry will be removed regardless of; `keep`. Note; ----; This method does not support aggregation. Notes; -----; The expression `expr` will be evaluated for every entry of the table.; If `keep` is ``True``, then entries where `expr` evaluates to ``True``; will be kept (the filter removes the entries where the predicate; evaluates to ``False``). If `keep` is ``False``, then entries where; `expr` evaluates to ``True`` will be removed (the filter keeps the; entries where the predicate evaluates to ``False``). Filtered entries are removed entirely from downstream operations. This; means that the resulting matrix table has sparsity -- that is, that the; number of entries is **smaller** than the product of :meth:`count_rows`; and :meth:`count_cols`. To re-densify a filtered matrix table, use the; :meth:`unfilter_entries` method to restore filtered entries, populated; all fields with missing values. Below are some properties of an; entry-filtered matrix table. 1. Filtered entries are not included in the :meth:`entries` table. >>> mt_range = hl.utils.range_matrix_table(10, 10); >>> mt_range = mt_range.annotate_entries(x = mt_range.row_idx + mt_range.col_idx); >>> mt_range.count(); (10, 10). >>> mt_range.entries().count(); 100. >>> mt_filt = mt_range.filter_entries(mt_range.x % 2 == 0); >>> mt_filt.count(); (10, 10). >>> mt_filt.count_rows() * mt_filt.count_cols(); 100. >>> mt_filt.entries().count(); 50. 2. Filtered entries are not included in aggregation. >>> mt_filt.aggregate_entries(hl.agg.count()); 50. >>> mt_filt = mt_filt",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58820,Availability,down,downstream,58820,"l_n.take(5); [5, 5, 5, 5, 5]. >>> mt_filt = mt_filt.annotate_rows(row_n = hl.agg.count()); >>> mt_filt.row_n.take(5); [5, 5, 5, 5, 5]. 3. Annotating a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; --",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:80170,Availability,checkpoint,checkpoint,80170,"calize=True) -> int:; """"""Count the number of columns in the matrix. Examples; --------. Count the number of columns:. >>> n_cols = dataset.count_cols(). Returns; -------; :obj:`int`; Number of columns in the matrix.; """"""; count_ir = ir.TableCount(ir.MatrixColsTable(self._mir)); if _localize:; return Env.backend().execute(count_ir); else:; return construct_expr(ir.LiftMeOut(count_ir), hl.tint64). [docs] def count(self) -> Tuple[int, int]:; """"""Count the number of rows and columns in the matrix. Examples; --------. >>> dataset.count(). Returns; -------; :obj:`int`, :obj:`int`; Number of rows, number of cols.; """"""; count_ir = ir.MatrixCount(self._mir); return Env.backend().execute(count_ir). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; _drop_cols=bool,; _drop_rows=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint(",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:81223,Availability,checkpoint,checkpoint,81223,"y: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_matrix_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _drop_cols=_drop_cols,; _drop_rows=_drop_rows,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(; output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str), _partitions=nullable(expr_any); ); def write(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _partitions=None,; ):; """"""Write to disk. Examples; --------. >>> dataset.write('output/dataset.mt'). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_matrix_table`. P",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:106508,Availability,avail,available,106508,". [docs] @typecheck_method(indices=sequenceof(int)); def choose_cols(self, indices: List[int]) -> 'MatrixTable':; """"""Choose a new set of columns from a list of old column indices. Examples; --------. Randomly shuffle column order:. >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:. >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters; ----------; indices : :obj:`list` of :obj:`int`; List of old column indices. Returns; -------; :class:`.MatrixTable`; """"""; n_cols = self.count_cols(); for i in indices:; if not 0 <= i < n_cols:; raise ValueError(f""'choose_cols': expect indices between 0 and {n_cols}, found {i}""); return MatrixTable(ir.MatrixChooseCols(self._mir, indices)). [docs] def n_partitions(self) -> int:; """"""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:106664,Availability,resilien,resilient-distributed-datasets-rdds,106664,"= list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:. >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters; ----------; indices : :obj:`list` of :obj:`int`; List of old column indices. Returns; -------; :class:`.MatrixTable`; """"""; n_cols = self.count_cols(); for i in indices:; if not 0 <= i < n_cols:; raise ValueError(f""'choose_cols': expect indices between 0 and {n_cols}, found {i}""); return MatrixTable(ir.MatrixChooseCols(self._mir, indices)). [docs] def n_partitions(self) -> int:; """"""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:107405,Availability,avail,available,107405,"----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108005,Availability,resilien,resilient-distributed-datasets-rdds,108005," of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write t",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108811,Availability,checkpoint,checkpoint,108811,"e advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108901,Availability,checkpoint,checkpoint,108901,"ark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; m",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108999,Availability,checkpoint,checkpoint,108999,"distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:109052,Availability,checkpoint,checkpoint,109052,"`shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111092,Availability,redundant,redundant,111092,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:113376,Availability,toler,tolerance,113376,"xed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of rows. Parameters; ----------; name : :class:`str`; Name for row index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_rows(**{name: hl.scan.count()}). [docs] @typecheck_method(name=str); def add_col_index(self, name: str = 'col_idx') -> 'MatrixTable':; """"""Add the integer index of each column as a new column field. Examples; --------. >>> dataset_result = dataset.add_col_index(). Notes; -----; The field added is type :py:data:`.tint32`. The column index is 0-indexed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of columns. Parameters; ----------; name: :class:`str`; Name for column index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_cols(**{name: hl.scan.count()}). @typecheck_method(other=matrix_table_type, tolerance=numeric, absolute=bool, reorder_fields=bool); def _same(self, other, tolerance=1e-6, absolute=False, reorder_fields=False) -> bool:; entries_name = Env.get_uid('entries_'); cols_name = Env.get_uid('columns_'). fd_f = set if reorder_fields else list. if fd_f(self.row) != fd_f(other.row):; print(f'Different row fields: \n {list(self.row)}\n {list(other.row)}'); return False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False; if fd_f(self.col) != fd_f(other.col):; print(f'Different col fields: \n {list(self.col)}\n {list(other.col)}'); return False; if fd_f(self.entry) != fd_f(other.entry):; print(f'Different row fields: \n {list(self.entry)}\n {list(other.entry)}'); return False. if reorder_fields:; entry_order = list(self.entry); if list(other.entry) != entry_order:; other = other.select_entries(*entry_order). globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). col_order",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:113455,Availability,toler,tolerance,113455,"xed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of rows. Parameters; ----------; name : :class:`str`; Name for row index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_rows(**{name: hl.scan.count()}). [docs] @typecheck_method(name=str); def add_col_index(self, name: str = 'col_idx') -> 'MatrixTable':; """"""Add the integer index of each column as a new column field. Examples; --------. >>> dataset_result = dataset.add_col_index(). Notes; -----; The field added is type :py:data:`.tint32`. The column index is 0-indexed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of columns. Parameters; ----------; name: :class:`str`; Name for column index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_cols(**{name: hl.scan.count()}). @typecheck_method(other=matrix_table_type, tolerance=numeric, absolute=bool, reorder_fields=bool); def _same(self, other, tolerance=1e-6, absolute=False, reorder_fields=False) -> bool:; entries_name = Env.get_uid('entries_'); cols_name = Env.get_uid('columns_'). fd_f = set if reorder_fields else list. if fd_f(self.row) != fd_f(other.row):; print(f'Different row fields: \n {list(self.row)}\n {list(other.row)}'); return False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False; if fd_f(self.col) != fd_f(other.col):; print(f'Different col fields: \n {list(self.col)}\n {list(other.col)}'); return False; if fd_f(self.entry) != fd_f(other.entry):; print(f'Different row fields: \n {list(self.entry)}\n {list(other.entry)}'); return False. if reorder_fields:; entry_order = list(self.entry); if list(other.entry) != entry_order:; other = other.select_entries(*entry_order). globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). col_order",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:114880,Availability,toler,tolerance,114880,"{list(other.globals)}'); return False; if fd_f(self.col) != fd_f(other.col):; print(f'Different col fields: \n {list(self.col)}\n {list(other.col)}'); return False; if fd_f(self.entry) != fd_f(other.entry):; print(f'Different row fields: \n {list(self.entry)}\n {list(other.entry)}'); return False. if reorder_fields:; entry_order = list(self.entry); if list(other.entry) != entry_order:; other = other.select_entries(*entry_order). globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). col_order = list(self.col); if list(other.col) != col_order:; other = other.select_cols(*col_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select_rows(*row_order). if list(self.col_key) != list(other.col_key):; print(f'different col keys:\n {list(self.col_key)}\n {list(other.col_key)}'); return False. return self._localize_entries(entries_name, cols_name)._same(; other._localize_entries(entries_name, cols_name), tolerance, absolute; ). @typecheck_method(caller=str, s=expr_struct()); def _select_entries(self, caller, s) -> 'MatrixTable':; base, cleanup = self._process_joins(s); analyze(caller, s, self._entry_indices); return cleanup(MatrixTable(ir.MatrixMapEntries(base._mir, s._ir))). @typecheck_method(caller=str, row=expr_struct()); def _select_rows(self, caller, row) -> 'MatrixTable':; analyze(caller, row, self._row_indices, {self._col_axis}); base, cleanup = self._process_joins(row); return cleanup(MatrixTable(ir.MatrixMapRows(base._mir, row._ir))). @typecheck_method(caller=str, col=expr_struct(), new_key=nullable(sequenceof(str))); def _select_cols(self, caller, col, new_key=None) -> 'MatrixTable':; analyze(caller, col, self._col_indices, {self._row_axis}); base, cleanup = self._process_joins(col); return cleanup(MatrixTable(ir.MatrixMapCols(base._mir, col._ir, new_key))). @typecheck_method(caller=str, s=expr_struct()); def _select_globals(self, caller, s) -> 'MatrixTable':; base,",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:7871,Deployability,pipeline,pipeline,7871,"s); return self._copy(col_keys=col_key, computed_col_key=computed_key). def _check_bindings(self, caller, new_bindings, indices):; empty = []. def iter_option(o):; return o if o is not None else empty. if indices == self._parent._row_indices:; fixed_fields = [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8142,Deployability,pipeline,pipeline,8142," [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----;",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58811,Deployability,pipeline,pipeline,58811,"l_n.take(5); [5, 5, 5, 5, 5]. >>> mt_filt = mt_filt.annotate_rows(row_n = hl.agg.count()); >>> mt_filt.row_n.take(5); [5, 5, 5, 5, 5]. 3. Annotating a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; --",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58960,Deployability,pipeline,pipeline,58960,"a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; -------; :class:`.MatrixTable`. Notes; -----; Adds a new row field, `row_field`, and a new column field, `col_field`,; each of which are structs with t",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58983,Deployability,pipeline,pipeline,58983,"a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; -------; :class:`.MatrixTable`. Notes; -----; Adds a new row field, `row_field`, and a new column field, `col_field`,; each of which are structs with t",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:89787,Deployability,pipeline,pipeline,89787,"h more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""; if Env.hc()._warn_entries_order and len(self.col_key) > 0:; warning(; ""entries(): Resulting entries table is sorted by '(row_key, col_key)'.""; ""\n To preserve row-major matrix table order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_entries_order = False. return Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(i",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111151,Deployability,pipeline,pipelines,111151,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:135577,Deployability,update,updated,135577,"t())))""; ). duplicates = [k for k, count in counts.items() if count > 1]; if duplicates:; raise ValueError(f""column keys must be unique, found duplicates: {', '.join(duplicates)}""). entries_uid = Env.get_uid(); cols_uid = Env.get_uid(). t = self; t = t._localize_entries(entries_uid, cols_uid). def fmt(f, col_key):; if f:; return col_key + separator + f; else:; return col_key. t = t.annotate(**{; fmt(f, col_keys[i]): t[entries_uid][i][j] for i in range(len(col_keys)) for j, f in enumerate(self.entry); }); t = t.drop(cols_uid, entries_uid). return t. [docs] @typecheck_method(rows=bool, cols=bool, entries=bool, handler=nullable(anyfunc)); def summarize(self, *, rows=True, cols=True, entries=True, handler=None):; """"""Compute and print summary information about the fields in the matrix table. .. include:: _templates/experimental.rst. Parameters; ----------; rows : :obj:`bool`; Compute summary for the row fields.; cols : :obj:`bool`; Compute summary for the column fields.; entries : :obj:`bool`; Compute summary for the entry fields.; """""". if handler is None:; handler = default_handler(); if cols:; handler(self.col._summarize(header='Columns', top=True)); if rows:; handler(self.row._summarize(header='Rows', top=True)); if entries:; handler(self.entry._summarize(header='Entries', top=True)). def _write_block_matrix(self, path, overwrite, entry_field, block_size):; mt = self; mt = mt._select_all(entry_exprs={entry_field: mt[entry_field]}). writer = ir.MatrixBlockMatrixWriter(path, overwrite, entry_field, block_size); Env.backend().execute(ir.MatrixWrite(self._mir, writer)). def _calculate_new_partitions(self, n_partitions):; """"""returns a set of range bounds that can be passed to write""""""; ht = self.rows(); ht = ht.select().select_globals(); return Env.backend().execute(; ir.TableToValueApply(ht._tir, {'name': 'TableCalculateNewPartitions', 'nPartitions': n_partitions}); ). matrix_table_type.set(MatrixTable). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8377,Energy Efficiency,reduce,reduces,8377,"on(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise Not",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:80522,Energy Efficiency,efficient,efficient,80522,"calize=True) -> int:; """"""Count the number of columns in the matrix. Examples; --------. Count the number of columns:. >>> n_cols = dataset.count_cols(). Returns; -------; :obj:`int`; Number of columns in the matrix.; """"""; count_ir = ir.TableCount(ir.MatrixColsTable(self._mir)); if _localize:; return Env.backend().execute(count_ir); else:; return construct_expr(ir.LiftMeOut(count_ir), hl.tint64). [docs] def count(self) -> Tuple[int, int]:; """"""Count the number of rows and columns in the matrix. Examples; --------. >>> dataset.count(). Returns; -------; :obj:`int`, :obj:`int`; Number of rows, number of cols.; """"""; count_ir = ir.MatrixCount(self._mir); return Env.backend().execute(count_ir). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; _drop_cols=bool,; _drop_rows=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint(",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:81075,Energy Efficiency,efficient,efficient,81075,"ls=bool,; _drop_cols=bool,; _drop_rows=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_matrix_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _drop_cols=_drop_cols,; _drop_rows=_drop_rows,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(; output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str), _partitions=nullable(expr_any); ); def write(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _partitions=None,; ):; """"""Write to disk. Examples; --------. >>> data",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:66080,Integrability,depend,dependent,66080,"ields_referenced)). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_rows(self, expr, _localize=True) -> Any:; """"""Aggregate over rows to a local value. Examples; --------; Aggregate over rows:. >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by row. This method should be thought of as a more convenient alternative to; the following:. >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; base, _ = self._process_joins(expr); analyze('MatrixTable.aggregate_rows', expr, self._global_indices, {self._row_axis}); rows_table = ir.MatrixRowsTable(base._mir); subst_query = ir.subst(expr._ir, {}, {'va': ir.Ref('row', rows_table.typ.row_type)}). agg_ir = ir.TableAggregate(rows_table, subst_query); if _localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_cols(self, expr, _localize=True) -> Any:; """"""Aggregate over columns to a local value. Examples; --------; Aggregate over columns:. >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful ref",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:67617,Integrability,depend,dependent,67617,"ef aggregate_cols(self, expr, _localize=True) -> Any:; """"""Aggregate over columns to a local value. Examples; --------; Aggregate over columns:. >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by column. This method should be thought of as a more convenient alternative to; the following:. >>> cols_table = dataset.cols(); >>> cols_table.aggregate(; ... hl.struct(fraction_female=hl.agg.fraction(cols_table.pheno.is_female),; ... case_ratio=hl.agg.count_where(cols_table.is_case) / hl.agg.count())). Note; ----; This method supports (and expects!) aggregation over columns. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; base, _ = self._process_joins(expr); analyze('MatrixTable.aggregate_cols', expr, self._global_indices, {self._col_axis}). cols_field = Env.get_uid(); globals = base.localize_entries(columns_array_field_name=cols_field).index_globals(); if len(self._col_key) == 0:; cols = globals[cols_field]; else:; if Env.hc()._warn_cols_order:; warning(; ""aggregate_cols(): Aggregates over cols ordered by 'col_key'.""; ""\n To preserve matrix table column order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_cols_order = False; cols = hl.sorted(globals[cols_field], key=lambda x: x.select(*self._col_key.keys())). agg_ir = ir.Let('global', globals.drop(cols_field)._ir, ir.StreamAgg(ir.ToStream(cols._ir), 'sa', expr._ir)). if _localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(agg_ir, expr.dtype). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_entries(self, expr, _localize=True):; """"""",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:69388,Integrability,depend,dependent,69388,"localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(agg_ir, expr.dtype). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_entries(self, expr, _localize=True):; """"""Aggregate over entries to a local value. Examples; --------; Aggregate over entries:. >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; -----; This method should be thought of as a more convenient alternative to; the following:. >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; ----; This method supports (and expects!) aggregation over entries. Parameters; ----------; expr : :class:`.Expression`; Aggregation expressions. Returns; -------; any; Aggregated value dependent on `expr`.; """""". base, _ = self._process_joins(expr); analyze('MatrixTable.aggregate_entries', expr, self._global_indices, {self._row_axis, self._col_axis}); agg_ir = ir.MatrixAggregate(base._mir, expr._ir); if _localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(field_expr=oneof(str, Expression)); def explode_rows(self, field_expr) -> 'MatrixTable':; """"""Explodes a row field of type array or set, copying the entire row for each element. Examples; --------; Explode rows by annotated genes:. >>> dataset_result = dataset.explode_rows(dataset.gene). Notes; -----; The new matrix table will have `N` copies of each row, where `N` is the number; of elements that row contains for the field denoted by `field_expr`. The field; referenced in `field_expr` is replaced in the sequence of duplicated rows by the; sequence of elements in the array or set. All other fields remain t",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:89769,Integrability,depend,depending,89769,"h more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""; if Env.hc()._warn_entries_order and len(self.col_key) > 0:; warning(; ""entries(): Resulting entries table is sorted by '(row_key, col_key)'.""; ""\n To preserve row-major matrix table order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_entries_order = False. return Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(i",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:13640,Modifiability,extend,extend,13640,".gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See Also; --------; :meth:`.aggregate`. Returns; -------; :class:`.MatrixTable`; Aggregated matrix table.; """"""; assert self._row_keys is not None or self._col_keys is not None. defined_exprs = []; for e in [self._row_fields, self._col_fields, self._entry_fields]:; if e is not None:; defined_exprs.append(e); for e in [self._computed_row_key, self._computed_col_key]:; if e is not None:; defined_exprs.extend(e.values()). def promote_none(e):; return hl.struct() if e is None else e. entry_exprs = promote_none(self._entry_fields); if len(entry_exprs) == 0:; warning(""'GroupedMatrixTable.result': No entry fields were defined.""). base, cleanup = self._parent._process_joins(*defined_exprs). if self._col_keys is not None:; cck = self._computed_col_key or {}; computed_key_uids = {k: Env.get_uid() for k in cck}; modified_keys = [computed_key_uids.get(k, k) for k in self._col_keys]; mt = MatrixTable(; ir.MatrixAggregateColsByKey(; ir.MatrixMapCols(; base._mir,; self._parent.col.annotate(**{computed_key_uids[k]: v for k, v in cck.items()})._ir,; modified_keys,; ),; entry_exprs._ir,; promote_none(self._col_fields)._ir,; ); ); if cck:; mt = mt.rename({v: k for k, v in computed_key_uids.items()}); else:; cck = self._computed_row_key or {}; computed_key_uids = {k: Env.get_uid() for k in cck}; modified_keys = [computed_key_uids.get(k, k) for k in self._row_keys]; mt = MatrixTable(; ir.MatrixAggregateRow",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:37681,Modifiability,variab,variable-length,37681,"o compute them. Returns; -------; :class:`.MatrixTable`; Matrix table with new row-and-column-indexed field(s).; """"""; caller = ""MatrixTable.annotate_entries""; check_annotate_exprs(caller, named_exprs, self._entry_indices, set()); return self._select_entries(caller, s=self.entry.annotate(**named_exprs)). [docs] def select_globals(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing global fields or create new fields by name, dropping the rest. Examples; --------; Select one existing field and compute a new one:. >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; -----; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified global fields.; """""". caller = 'MatrixTable.select_globals'; new_global = get_select_exprs(caller, exprs, named_exprs, self._global_indices, self._globals); return self._select_globals(caller, new_global). [docs] def select_rows(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing row fields or create new fields by name, dropping all; other non-key fields. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; -----; This method creates new row fields. If a created field shares its name; with a ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:39184,Modifiability,variab,variable-length,39184,"[docs] def select_rows(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing row fields or create new fields by name, dropping all; other non-key fields. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; -----; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail. Row keys are preserved. To drop or change a row key field, use; :meth:`MatrixTable.key_rows_by`. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method supports aggregation over columns. For instance, the usage:. >>> dataset_result = dataset.select_rows(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per row. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified row fields.; """"""; caller = 'MatrixTable.select_rows'; new_row = get_select_exprs(caller, exprs, named_exprs, self._row_indices, self._rvrow); return self._select_rows(caller, new_row). [docs] def select_cols(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing column fields or create new fields by name, dropping the rest. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_cols(; ... dataset.sample_qc,; ... dataset.pheno.age,; ... isCohort1 = dataset.pheno.cohort_name == 'Cohort1'). Notes; -----; This method creates new column fields. If a created field shares its name; with a differently-indexed field of the table, the method wi",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:40517,Modifiability,variab,variable-length,40517,"able.select_rows'; new_row = get_select_exprs(caller, exprs, named_exprs, self._row_indices, self._rvrow); return self._select_rows(caller, new_row). [docs] def select_cols(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing column fields or create new fields by name, dropping the rest. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_cols(; ... dataset.sample_qc,; ... dataset.pheno.age,; ... isCohort1 = dataset.pheno.cohort_name == 'Cohort1'). Notes; -----; This method creates new column fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method supports aggregation over rows. For instance, the usage:. >>> dataset_result = dataset.select_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified column fields.; """"""; caller = 'MatrixTable.select_cols'; new_col = get_select_exprs(caller, exprs, named_exprs, self._col_indices, self._col); return self._select_cols(caller, new_col). [docs] def select_entries(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing entry fields or create new fields by name, dropping the rest. Examples; --------; Drop all entry fields aside from `GT`:. >>> dataset_result = dataset.select_entries(dataset.GT). Notes; -----; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` metho",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:41615,Modifiability,variab,variable-length,41615," field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified column fields.; """"""; caller = 'MatrixTable.select_cols'; new_col = get_select_exprs(caller, exprs, named_exprs, self._col_indices, self._col); return self._select_cols(caller, new_col). [docs] def select_entries(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing entry fields or create new fields by name, dropping the rest. Examples; --------; Drop all entry fields aside from `GT`:. >>> dataset_result = dataset.select_entries(dataset.GT). Notes; -----; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified entry fields.; """"""; caller = 'MatrixTable.select_entries'; new_entry = get_select_exprs(caller, exprs, named_exprs, self._entry_indices, self._entry); return self._select_entries(caller, new_entry). [docs] @typecheck_method(exprs=oneof(str, Expression)); def drop(self, *exprs) -> 'MatrixTable':; """"""Drop fields. Examples; --------. Drop fields `PL` (an entry field), `info` (a row field), and `pheno` (a column; field): using strings:. >>> dataset_result = dataset.drop('PL', 'info', 'pheno'). Drop fields `PL` (an entry field), `info` (a row field), and `pheno` (a column; field): using field references:. >>> dataset_result = dataset.drop(dataset.PL, dataset.info, dataset.pheno). Drop a list of fie",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:90339,Modifiability,variab,variables,90339," always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""; if Env.hc()._warn_entries_order and len(self.col_key) > 0:; warning(; ""entries(): Resulting entries table is sorted by '(row_key, col_key)'.""; ""\n To preserve row-major matrix table order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_entries_order = False. return Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(ir.MatrixRowsTable(self._mir)), self.globals.dtype). [docs] def index_rows(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:. >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:91208,Modifiability,variab,variable-length,91208,"eturn Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(ir.MatrixRowsTable(self._mir)), self.globals.dtype). [docs] def index_rows(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:. >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Notes; -----; ``index_rows(exprs)`` is equivalent to ``rows().index(exprs)``; or ``rows()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.row_value`. Returns; -------; :class:`.Expression`; """"""; try:; return self.rows()._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index matrix table with given expressions:\n""; f"" MatrixTable row key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). [docs] def index_cols(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the column values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_cols(pheno =",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:92394,Modifiability,variab,variable-length,92394,"xprs)`` is equivalent to ``rows().index(exprs)``; or ``rows()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.row_value`. Returns; -------; :class:`.Expression`; """"""; try:; return self.rows()._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index matrix table with given expressions:\n""; f"" MatrixTable row key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). [docs] def index_cols(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the column values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.s).pheno). Or equivalently:. >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.col_key).pheno). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Notes; -----; ``index_cols(cols)`` is equivalent to ``cols().index(exprs)``; or ``cols()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.col_value`. Returns; -------; :class:`.Expression`; """"""; try:; return self.cols()._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index matrix table with given expressions:\n""; f"" MatrixTable col key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). [docs] def index_entries(self, row_exprs, col_exprs):; """"""Expose the entries as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2.index_entries(",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:132296,Modifiability,inherit,inherits,132296,"). t = t.add_index(index_uid); unique_cols = t.aggregate(; hl.agg.group_by(hl.struct(**{f: t[f] for f in col_key_fields}), hl.agg.take(t[index_uid], 1)); ); unique_cols = sorted([v[0] for _, v in unique_cols.items()]). return self.choose_cols(unique_cols). [docs] @deprecated(version=""0.2.129""); @typecheck_method(separator=str); def make_table(self, separator='.') -> Table:; """"""Make a table from a matrix table with one field per sample. .. deprecated:: 0.2.129; use :meth:`.localize_entries` instead because it supports more; columns. Parameters; ----------; separator : :class:`str`; Separator between sample IDs and entry field names. Returns; -------; :class:`.Table`. See Also; --------; :meth:`.localize_entries`. Notes; -----; The table has one row for each row of the input matrix. The; per sample and entry fields are formed by concatenating the; sample ID with the entry field name using `separator`. If the; entry field name is empty, the separator is omitted. The table inherits the globals from the matrix table. Examples; --------; Consider a matrix table with the following schema:. .. code-block:: text. Global fields:; 'batch': str; Column fields:; 's': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; Entry fields:; 'GT': call; 'GQ': int32; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>. and three sample IDs: `A`, `B` and `C`. Then the result of; :meth:`.make_table`:. >>> ht = mt.make_table() # doctest: +SKIP. has the original row fields along with 6 additional fields,; one for each sample and entry field:. .. code-block:: text. Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>; """"""; if not (len(self.col_key) == 1 and self.col_key[0].dtype == hl.tstr):; raise ValueError(""column key must be a single field of type str""). col_keys = self.col_key",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8072,Performance,optimiz,optimizer,8072," [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----;",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:88772,Performance,perform,performance,88772,"ls_by()'""; ); Env.hc()._warn_cols_order = False. return Table(ir.MatrixColsTable(self._mir)). [docs] def entries(self) -> Table:; """"""Returns a matrix in coordinate table form. Examples; --------; Extract the entry table:. >>> entries_table = dataset.entries(). Notes; -----; The coordinate table representation of the source matrix table contains; one row for each **non-filtered** entry of the matrix -- if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; ``M * N`` rows, which can be **a very large number**. This representation can be useful for aggregating over both axes of a matrix table; at the same time -- it is not possible to aggregate over a matrix table using; :meth:`group_rows_by` and :meth:`group_cols_by` at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with :meth:`entries`,; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:107658,Performance,perform,performance,107658,"org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Reparti",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:110279,Performance,cache,cache,110279,"ethod(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:110418,Performance,cache,cache,110418," partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111007,Performance,cache,cache,111007,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111130,Performance,perform,performance,111130,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:120046,Performance,perform,performed,120046,"_right_row_fields=bool); def union_cols(; self, other: 'MatrixTable', row_join_type: str = 'inner', drop_right_row_fields: bool = True; ) -> 'MatrixTable':; """"""Take the union of dataset columns. Warning; -------. This method does not preserve the global fields from the other matrix table. Examples; --------. Union the columns of two datasets:. >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; -----. In order to combine two datasets, three requirements must be met:. - The row keys must match.; - The column key schemas and column schemas must match.; - The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match. This method creates a :class:`.MatrixTable` which contains all columns; from both input datasets. The set of rows included in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are ",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:120899,Performance,perform,perform,120899,"d in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns; -------; :class:`.MatrixTable`; Dataset with columns from both datasets.; """"""; if self.entry.dtype != other.entry.dtype:; raise ValueError(; f'entry types differ:\n' f' left: {self.entry.dtype}\n' f' right: {other.entry.dtype}'; ); if self.col.dtype != other.col.dtype:; raise ValueError(f'column types differ:\n' f' left: {self.col.dtype}\n' f' right: {other.col.dtype}'); if self.col_key.keys() != other.col_key.keys():; raise ValueError(; f'column key fields differ:\n'; f' left: {"", "".join(self.col_key.keys())}\n'; f' right: {"", "".join(other.col_key.keys())}'; ); if list(self.row_key.dtype.values()) != list(other.row_key.dtype.values()):; raise ValueError(; f'row key types differ:\n'; f' left: {"", "".j",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:120942,Performance,perform,perform,120942,"d in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns; -------; :class:`.MatrixTable`; Dataset with columns from both datasets.; """"""; if self.entry.dtype != other.entry.dtype:; raise ValueError(; f'entry types differ:\n' f' left: {self.entry.dtype}\n' f' right: {other.entry.dtype}'; ); if self.col.dtype != other.col.dtype:; raise ValueError(f'column types differ:\n' f' left: {self.col.dtype}\n' f' right: {other.col.dtype}'); if self.col_key.keys() != other.col_key.keys():; raise ValueError(; f'column key fields differ:\n'; f' left: {"", "".join(self.col_key.keys())}\n'; f' right: {"", "".join(other.col_key.keys())}'; ); if list(self.row_key.dtype.values()) != list(other.row_key.dtype.values()):; raise ValueError(; f'row key types differ:\n'; f' left: {"", "".j",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:28681,Safety,unsafe,unsafe,28681,"""Key columns by a new set of fields. See :meth:`.Table.key_by` for more information on defining a key. Parameters; ----------; keys : varargs of :class:`str` or :class:`.Expression`.; Column fields to key by.; named_keys : keyword args of :class:`.Expression`.; Column fields to key by.; Returns; -------; :class:`.MatrixTable`; """"""; key_fields, computed_keys = get_key_by_exprs(""MatrixTable.key_cols_by"", keys, named_keys, self._col_indices). if not computed_keys:; return MatrixTable(ir.MatrixMapCols(self._mir, self._col._ir, key_fields)); else:; new_col = self.col.annotate(**computed_keys); base, cleanup = self._process_joins(new_col). return cleanup(MatrixTable(ir.MatrixMapCols(base._mir, new_col._ir, key_fields))). @typecheck_method(new_key=str); def _key_rows_by_assert_sorted(self, *new_key):; rk_names = list(self.row_key); i = 0; while i < min(len(new_key), len(rk_names)):; if new_key[i] != rk_names[i]:; break; i += 1. if i < 1:; raise ValueError(; f'cannot implement an unsafe sort with no shared key:\n new key: {new_key}\n old key: {rk_names}'; ). return MatrixTable(ir.MatrixKeyRowsBy(self._mir, list(new_key), is_sorted=True)). [docs] @typecheck_method(keys=oneof(str, Expression), named_keys=expr_any); def key_rows_by(self, *keys, **named_keys) -> 'MatrixTable':; """"""Key rows by a new set of fields. Examples; --------. >>> dataset_result = dataset.key_rows_by('locus'); >>> dataset_result = dataset.key_rows_by(dataset['locus']); >>> dataset_result = dataset.key_rows_by(**dataset.row_key.drop('alleles')). All of these expressions key the dataset by the 'locus' field, dropping; the 'alleles' field from the row key. >>> dataset_result = dataset.key_rows_by(contig=dataset['locus'].contig,; ... position=dataset['locus'].position,; ... alleles=dataset['alleles']). This keys the dataset by the newly defined fields, 'contig' and 'position',; and the 'alleles' field. The old row key field, 'locus', is preserved as; a non-key field. Notes; -----; See :meth:`.Table.key_by` fo",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108218,Safety,avoid,avoid,108218,"with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111086,Safety,avoid,avoid,111086,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111092,Safety,redund,redundant,111092,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:122398,Safety,avoid,avoid,122398,"eft: {self.entry.dtype}\n' f' right: {other.entry.dtype}'; ); if self.col.dtype != other.col.dtype:; raise ValueError(f'column types differ:\n' f' left: {self.col.dtype}\n' f' right: {other.col.dtype}'); if self.col_key.keys() != other.col_key.keys():; raise ValueError(; f'column key fields differ:\n'; f' left: {"", "".join(self.col_key.keys())}\n'; f' right: {"", "".join(other.col_key.keys())}'; ); if list(self.row_key.dtype.values()) != list(other.row_key.dtype.values()):; raise ValueError(; f'row key types differ:\n'; f' left: {"", "".join(self.row_key.dtype.values())}\n'; f' right: {"", "".join(other.row_key.dtype.values())}'; ). if drop_right_row_fields:; other = other.select_rows(); else:; left_fields = set(self.row_value); other_fields = set(other.row_value) - set(other.row_key); renames, _ = deduplicate(other_fields, max_attempts=100, already_used=left_fields). if renames:; renames = dict(renames); other = other.rename(renames); info(; 'Table.union_cols: renamed the following fields on the right to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renames.items()); ). return MatrixTable(ir.MatrixUnionCols(self._mir, other._mir, row_join_type)). [docs] @typecheck_method(n_rows=nullable(int), n_cols=nullable(int), n=nullable(int)); def head(self, n_rows: Optional[int], n_cols: Optional[int] = None, *, n: Optional[int] = None) -> 'MatrixTable':; """"""Subset matrix to first `n_rows` rows and `n_cols` cols. Examples; --------; >>> mt_range = hl.utils.range_matrix_table(100, 100). Passing only one argument will take the first `n_rows` rows:. >>> mt_range.head(10).count(); (10, 100). Passing two arguments refers to rows and columns, respectively:. >>> mt_range.head(10, 20).count(); (10, 20). Either argument may be ``None`` to indicate no filter. First 10 rows, all columns:. >>> mt_range.head(10, None).count(); (10, 100). All rows, first 10 columns:. >>> mt_range.head(None, 10).count(); (100, 10). Notes; -----; The number of partitions in the new matrix is eq",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:7180,Testability,assert,assert,7180," expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix, can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; if self._row_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by rows; cannot also group by cols.""); if self._col_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by cols.""). caller = 'group_cols_by'; col_key, computed_key = get_key_by_exprs(; caller,; exprs,; named_exprs,; self._parent._col_indices,; override_protected_indices={self._parent._global_indices, self._parent._row_indices},; ). self._check_bindings(caller, computed_key, self._parent._col_indices); return self._copy(col_keys=col_key, computed_col_key=computed_key). def _check_bindings(self, caller, new_bindings, indices):; empty = []. def iter_option(o):; return o if o is not None else empty. if indices == self._parent._row_indices:; fixed_fields = [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some place",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:9521,Testability,assert,assert,9521," n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by rows. Cannot aggregate over cols.""); assert self._col_keys is not None. base = self._col_fields if self._col_fields is not None else hl.struct(); for k, e in named_exprs.items():; analyze('GroupedMatrixTable.aggregate_cols', e, self._parent._global_indices, {self._parent._col_axis}). self._check_bindings('aggregate_cols', named_exprs, self._parent._col_indices); return self._copy(col_fields=base.annotate(**named_exprs)). [docs] @typecheck_method(named_exprs=expr_any); def aggregate_rows(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate rows by group. Examples; --------; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a set as a new row field:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .result()). Notes; -----; The aggregation scope includes all row fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:10753,Testability,assert,assert,10753,"col_axis}). self._check_bindings('aggregate_cols', named_exprs, self._parent._col_indices); return self._copy(col_fields=base.annotate(**named_exprs)). [docs] @typecheck_method(named_exprs=expr_any); def aggregate_rows(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate rows by group. Examples; --------; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a set as a new row field:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .result()). Notes; -----; The aggregation scope includes all row fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._col_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by cols. Cannot aggregate over rows.""); assert self._row_keys is not None. base = self._row_fields if self._row_fields is not None else hl.struct(); for k, e in named_exprs.items():; analyze('GroupedMatrixTable.aggregate_rows', e, self._parent._global_indices, {self._parent._row_axis}). self._check_bindings('aggregate_rows', named_exprs, self._parent._row_indices); return self._copy(row_fields=base.annotate(**named_exprs)). [docs] @typecheck_method(named_exprs=expr_any); def aggregate_entries(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate entries by group. Examples; --------; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See Also; --------; :meth:`.aggregate`, :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.G",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:11782,Testability,assert,assert,11782,"is not None. base = self._row_fields if self._row_fields is not None else hl.struct(); for k, e in named_exprs.items():; analyze('GroupedMatrixTable.aggregate_rows', e, self._parent._global_indices, {self._parent._row_axis}). self._check_bindings('aggregate_rows', named_exprs, self._parent._row_indices); return self._copy(row_fields=base.annotate(**named_exprs)). [docs] @typecheck_method(named_exprs=expr_any); def aggregate_entries(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate entries by group. Examples; --------; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See Also; --------; :meth:`.aggregate`, :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; assert self._row_keys is not None or self._col_keys is not None. base = self._entry_fields if self._entry_fields is not None else hl.struct(); for k, e in named_exprs.items():; analyze(; 'GroupedMatrixTable.aggregate_entries',; e,; self._fixed_indices(),; {self._parent._row_axis, self._parent._col_axis},; ). self._check_bindings(; 'aggregate_entries',; named_exprs,; self._parent._col_indices if self._col_keys is not None else self._parent._row_indices,; ); return self._copy(entry_fields=base.annotate(**named_exprs)). [docs] def result(self) -> 'MatrixTable':; """"""Return the result of aggregating by group. Examples; --------; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.c",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:13350,Testability,assert,assert,13350,"eturn the result of aggregating by group. Examples; --------; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See Also; --------; :meth:`.aggregate`. Returns; -------; :class:`.MatrixTable`; Aggregated matrix table.; """"""; assert self._row_keys is not None or self._col_keys is not None. defined_exprs = []; for e in [self._row_fields, self._col_fields, self._entry_fields]:; if e is not None:; defined_exprs.append(e); for e in [self._computed_row_key, self._computed_col_key]:; if e is not None:; defined_exprs.extend(e.values()). def promote_none(e):; return hl.struct() if e is None else e. entry_exprs = promote_none(self._entry_fields); if len(entry_exprs) == 0:; warning(""'GroupedMatrixTable.result': No entry fields were defined.""). base, cleanup = self._parent._process_joins(*defined_exprs). if self._col_keys is not None:; cck = self._computed_col_key or {}; computed_key_uids = {k: Env.get_uid() for k in cck}; modified_keys = [computed_key_uids.get(k, k) for k in self._col_keys]; mt = MatrixTable(; ir.MatrixAggregateColsByKey(; ir.MatrixMapCols(; base._mir,; self._parent.col.annotate(**{computed_key_uids[k]: v for k, v in cck.items()})._ir,; modified_keys,; ),; entry_exprs._ir,; promote_none(self._col_fields",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:21157,Testability,assert,assert,21157,"`_unlocalize_entries`. In; # this form, the column table is bundled with the globals and the entries; # for each row is stored on the row.; def raise_when_mismatched_property_dimensions(kvs: Dict[str, Iterable[Any]]):; def value_len(entry):; return len(entry[1]). kvs = sorted(kvs.items(), key=value_len); dims = itertools.groupby(kvs, value_len); dims = {size: [k for k, _ in group] for size, group in dims}; if len(dims) > 1:; raise ValueError(f""property matrix dimensions do not match: {dims}.""). def transpose(kvs: Dict[str, Iterable[Any]]) -> List[Dict[str, Any]]:; raise_when_mismatched_property_dimensions(kvs); return [dict(zip(kvs, vs)) for vs in zip(*kvs.values())]. def anyval(kvs):; return next(iter(kvs.values())). # In the case rows or cols aren't specified, we need to infer the; # matrix dimensions from *an* entry. Which one isn't important as we; # enforce congruence among input dimensions.; assert not ((rows is None or cols is None) and (entries is None)); cols = transpose(cols) if cols else [{} for _ in anyval(entries)[0]]; for i, _ in enumerate(cols):; cols[i] = hl.struct(col_idx=i, **cols[i]). if globals is None:; globals = {}. cols_field_name = Env.get_uid(); globals[cols_field_name] = cols. rows = transpose(rows) if rows else [{} for _ in anyval(entries)]; entries = [transpose(e) for e in transpose(entries)] if entries else [[{} for _ in cols] for _ in rows]. if len(rows) != len(entries) or len(cols) != len(entries[0]):; raise ValueError(; (""mismatched matrix dimensions: "" ""number of rows and cols does not match entry dimensions.""); ). entries_field_name = Env.get_uid(); for i, (row_props, entry_props) in enumerate(zip(rows, entries)):; row_entries = [hl.struct(**kvs) for kvs in entry_props]; rows[i] = hl.Struct(row_idx=i, **row_props, **{entries_field_name: row_entries}). ht = Table.parallelize(rows, key='row_idx', globals=hl.struct(**globals)); return ht._unlocalize_entries(entries_field_name, cols_field_name, col_key=['col_idx']). def __init__(self, m",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:44107,Testability,assert,assert,44107," :meth:`.key_rows_by`; or :meth:`.key_cols_by` to remove the field from the key before dropping. While many operations exist independently for rows, columns, entries, and; globals, only one is needed for dropping due to the lack of any necessary; contextual information. Parameters; ----------; exprs : varargs of :class:`str` or :class:`.Expression`; Names of fields to drop or field reference expressions. Returns; -------; :class:`.MatrixTable`; Matrix table without specified fields.; """""". def check_key(name, keys):; if name in keys:; raise ValueError(""MatrixTable.drop: cannot drop key field '{}'"".format(name)); return name. all_field_exprs = {e: k for k, e in self._fields.items()}; fields_to_drop = set(); for e in exprs:; if isinstance(e, Expression):; if e in all_field_exprs:; fields_to_drop.add(all_field_exprs[e]); else:; raise ExpressionException(; ""Method 'drop' expects string field names or top-level field expressions""; "" (e.g. 'foo', matrix.foo, or matrix['foo'])""; ); else:; assert isinstance(e, str); if e not in self._fields:; raise IndexError(""MatrixTable has no field '{}'"".format(e)); fields_to_drop.add(e). m = self; global_fields = [field for field in fields_to_drop if self._fields[field]._indices == self._global_indices]; if global_fields:; m = m._select_globals(""MatrixTable.drop"", m.globals.drop(*global_fields)). row_fields = [; check_key(field, list(self.row_key)); for field in fields_to_drop; if self._fields[field]._indices == self._row_indices; ]; if row_fields:; m = m._select_rows(""MatrixTable.drop"", row=m.row.drop(*row_fields)). col_fields = [; check_key(field, list(self.col_key)); for field in fields_to_drop; if self._fields[field]._indices == self._col_indices; ]; if col_fields:; m = m._select_cols(""MatrixTable.drop"", m.col.drop(*col_fields)). entry_fields = [field for field in fields_to_drop if self._fields[field]._indices == self._entry_indices]; if entry_fields:; m = m._select_entries(""MatrixTable.drop"", m.entry.drop(*entry_fields)). return m. [",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:84196,Testability,log,logging,84196,"isplayed_n_cols != self.actual_n_cols:; s += f""showing the first { self.displayed_n_cols } of { self.actual_n_cols } columns""; return s. def __repr__(self):; return self.__str__(). def _repr_html_(self):; s = self.table_show._repr_html_(); if self.displayed_n_cols != self.actual_n_cols:; s += '<p style=""background: #fdd; padding: 0.4em;"">'; s += f""showing the first { self.displayed_n_cols } of { self.actual_n_cols } columns""; s += '</p>\n'; return s. [docs] @typecheck_method(; n_rows=nullable(int),; n_cols=nullable(int),; include_row_fields=bool,; width=nullable(int),; truncate=nullable(int),; types=bool,; handler=nullable(anyfunc),; ); def show(; self, n_rows=None, n_cols=None, include_row_fields=False, width=None, truncate=None, types=True, handler=None; ):; """"""Print the first few rows of the matrix table to the console. .. include:: _templates/experimental.rst. Notes; -----; The output can be passed piped to another output source using the `handler` argument:. >>> mt.show(handler=lambda x: logging.info(x)) # doctest: +SKIP. Parameters; ----------; n_rows : :obj:`int`; Maximum number of rows to show.; n_cols : :obj:`int`; Maximum number of columns to show.; width : :obj:`int`; Horizontal width at which to break fields.; truncate : :obj:`int`, optional; Truncate each field to the given number of characters. If; ``None``, truncate fields to the given `width`.; types : :obj:`bool`; Print an extra header line with the type of each field.; handler : Callable[[str], Any]; Handler function for data string.; """""". def estimate_size(struct_expression):; return sum(max(len(f), len(str(x.dtype))) + 3 for f, x in struct_expression.flatten().items()). if n_cols is None:; import shutil. (characters, _) = shutil.get_terminal_size((80, 10)); characters -= 6 # borders; key_characters = estimate_size(self.row_key); characters -= key_characters; if include_row_fields:; characters -= estimate_size(self.row_value); characters = max(characters, 0); n_cols = characters // (estimate_size(s",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:96923,Testability,assert,assert,96923,"exprs) == 1; and isinstance(col_exprs[0], StructExpression); and types_match(self.col_key.values(), col_exprs[0].values()); ):; return self.index_entries(row_exprs, tuple(col_exprs[0].values())); elif len(col_exprs) != len(self.col_key):; raise ExpressionException(; f'Key mismatch: matrix table has {len(self.col_key)} col key fields, '; f'found {len(col_exprs)} index expressions.'; ); else:; raise ExpressionException(; f""Key type mismatch: cannot index matrix table with given expressions:\n""; f"" MatrixTable col key: {', '.join(str(t) for t in self.col_key.dtype.values())}\n""; f"" Col index expressions: {', '.join(str(e.dtype) for e in col_exprs)}""; ). indices, aggregations = unify_all(*(row_exprs + col_exprs)); src = indices.source; if aggregations:; raise ExpressionException('Cannot join using an aggregated field'). uid = Env.get_uid(); uids = [uid]. if isinstance(src, Table):; # join table with matrix.entries_table(); return self.entries().index(*(row_exprs + col_exprs)); else:; assert isinstance(src, MatrixTable); row_uid = Env.get_uid(); uids.append(row_uid); col_uid = Env.get_uid(); uids.append(col_uid). def joiner(left: MatrixTable):; localized = self._localize_entries(row_uid, col_uid); src_cols_indexed = self.add_col_index(col_uid).cols(); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(; row_exprs={row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs={col_uid: src_cols_indexed.index(*col_exprs)[col_uid]},; ); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}). join_ir = ir.Join(; ir.ProjectedTopLevelReference('g', uid, self.entry.dtype), uids, [*row_exprs, *col_exprs], joiner; ); return construct_expr(join_ir, self.entry.dtype, indices, aggregations). @typecheck_method(entries_field_name=str, cols_field_name=str); def _localize_entries(self, entries_field_name, cols_field_name) -> 'Table':; assert entries_field_name not in self.row; assert cols_field_name not in s",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:97855,Testability,assert,assert,97855,"ce(src, Table):; # join table with matrix.entries_table(); return self.entries().index(*(row_exprs + col_exprs)); else:; assert isinstance(src, MatrixTable); row_uid = Env.get_uid(); uids.append(row_uid); col_uid = Env.get_uid(); uids.append(col_uid). def joiner(left: MatrixTable):; localized = self._localize_entries(row_uid, col_uid); src_cols_indexed = self.add_col_index(col_uid).cols(); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(; row_exprs={row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs={col_uid: src_cols_indexed.index(*col_exprs)[col_uid]},; ); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}). join_ir = ir.Join(; ir.ProjectedTopLevelReference('g', uid, self.entry.dtype), uids, [*row_exprs, *col_exprs], joiner; ); return construct_expr(join_ir, self.entry.dtype, indices, aggregations). @typecheck_method(entries_field_name=str, cols_field_name=str); def _localize_entries(self, entries_field_name, cols_field_name) -> 'Table':; assert entries_field_name not in self.row; assert cols_field_name not in self.globals; return Table(ir.CastMatrixToTable(self._mir, entries_field_name, cols_field_name)). [docs] @typecheck_method(entries_array_field_name=nullable(str), columns_array_field_name=nullable(str)); def localize_entries(self, entries_array_field_name=None, columns_array_field_name=None) -> 'Table':; """"""Convert the matrix table to a table with entries localized as an array of structs. Examples; --------; Build a numpy ndarray from a small :class:`.MatrixTable`:. >>> mt = hl.utils.range_matrix_table(3,3); >>> mt = mt.select_entries(x = mt.row_idx * mt.col_idx); >>> mt.show(); +---------+-------+-------+-------+; | row_idx | 0.x | 1.x | 2.x |; +---------+-------+-------+-------+; | int32 | int32 | int32 | int32 |; +---------+-------+-------+-------+; | 0 | 0 | 0 | 0 |; | 1 | 0 | 1 | 2 |; | 2 | 0 | 2 | 4 |; +---------+-------+-------+-------+. >>> t = mt.localiz",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:97898,Testability,assert,assert,97898,"); else:; assert isinstance(src, MatrixTable); row_uid = Env.get_uid(); uids.append(row_uid); col_uid = Env.get_uid(); uids.append(col_uid). def joiner(left: MatrixTable):; localized = self._localize_entries(row_uid, col_uid); src_cols_indexed = self.add_col_index(col_uid).cols(); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(; row_exprs={row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs={col_uid: src_cols_indexed.index(*col_exprs)[col_uid]},; ); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}). join_ir = ir.Join(; ir.ProjectedTopLevelReference('g', uid, self.entry.dtype), uids, [*row_exprs, *col_exprs], joiner; ); return construct_expr(join_ir, self.entry.dtype, indices, aggregations). @typecheck_method(entries_field_name=str, cols_field_name=str); def _localize_entries(self, entries_field_name, cols_field_name) -> 'Table':; assert entries_field_name not in self.row; assert cols_field_name not in self.globals; return Table(ir.CastMatrixToTable(self._mir, entries_field_name, cols_field_name)). [docs] @typecheck_method(entries_array_field_name=nullable(str), columns_array_field_name=nullable(str)); def localize_entries(self, entries_array_field_name=None, columns_array_field_name=None) -> 'Table':; """"""Convert the matrix table to a table with entries localized as an array of structs. Examples; --------; Build a numpy ndarray from a small :class:`.MatrixTable`:. >>> mt = hl.utils.range_matrix_table(3,3); >>> mt = mt.select_entries(x = mt.row_idx * mt.col_idx); >>> mt.show(); +---------+-------+-------+-------+; | row_idx | 0.x | 1.x | 2.x |; +---------+-------+-------+-------+; | int32 | int32 | int32 | int32 |; +---------+-------+-------+-------+; | 0 | 0 | 0 | 0 |; | 1 | 0 | 1 | 2 |; | 2 | 0 | 2 | 4 |; +---------+-------+-------+-------+. >>> t = mt.localize_entries('entry_structs', 'columns'); >>> t.describe(); ----------------------------------------; Global fiel",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:116216,Testability,test,testsetup,116216,"ws(self, caller, row) -> 'MatrixTable':; analyze(caller, row, self._row_indices, {self._col_axis}); base, cleanup = self._process_joins(row); return cleanup(MatrixTable(ir.MatrixMapRows(base._mir, row._ir))). @typecheck_method(caller=str, col=expr_struct(), new_key=nullable(sequenceof(str))); def _select_cols(self, caller, col, new_key=None) -> 'MatrixTable':; analyze(caller, col, self._col_indices, {self._row_axis}); base, cleanup = self._process_joins(col); return cleanup(MatrixTable(ir.MatrixMapCols(base._mir, col._ir, new_key))). @typecheck_method(caller=str, s=expr_struct()); def _select_globals(self, caller, s) -> 'MatrixTable':; base, cleanup = self._process_joins(s); analyze(caller, s, self._global_indices); return cleanup(MatrixTable(ir.MatrixMapGlobals(base._mir, s._ir))). [docs] @typecheck(datasets=matrix_table_type, _check_cols=bool); def union_rows(*datasets: 'MatrixTable', _check_cols=True) -> 'MatrixTable':; """"""Take the union of dataset rows. Examples; --------. .. testsetup::. dataset_to_union_1 = dataset; dataset_to_union_2 = dataset. Union the rows of two datasets:. >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:. >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:. >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; -----. In order to combine two datasets, three requirements must be met:. - The column keys must be identical, both in type, value, and ordering.; - The row key schemas and row schemas must match.; - The entry schemas must match. The column fields in the resulting dataset are the column fields from; the first dataset; the column schemas do not need to match. This method does not deduplicate; if a row exists identically in two; datasets, then it will be",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:106653,Usability,guid,guide,106653,"column order:. >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:. >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters; ----------; indices : :obj:`list` of :obj:`int`; List of old column indices. Returns; -------; :class:`.MatrixTable`; """"""; n_cols = self.count_cols(); for i in indices:; if not 0 <= i < n_cols:; raise ValueError(f""'choose_cols': expect indices between 0 and {n_cols}, found {i}""); return MatrixTable(ir.MatrixChooseCols(self._mir, indices)). [docs] def n_partitions(self) -> int:; """"""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitio",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:107994,Usability,guid,guide,107994," True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:109591,Usability,simpl,simply,109591,"`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return se",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111393,Usability,guid,guide,111393,"t in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.; """"""; return Env.backend().unpersist(self). [docs] @typecheck_method(name=str); def add_row_index(self, name: str = 'row_idx') -> 'MatrixTable':; """"""Add the integer index of each row as a new row field. Examples; --------. >>> dataset_result = dataset.add_row_index(). Notes; -----; The field adde",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
https://hail.is/docs/0.2/_modules/hail/table.html:5682,Availability,down,downstream,5682,"dTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_res",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:12746,Availability,down,down,12746,"e 2: Left distinct join: ht[ht2.key] or ht[ht2.field1, ht2.field2]""; ) from e. @property; def key(self) -> StructExpression:; """"""Row key struct. Examples; --------. List of key field names:. >>> list(table1.key); ['ID']. Number of key fields:. >>> len(table1.key); 1. Returns; -------; :class:`.StructExpression`; """"""; return self._key. @property; def _value(self) -> 'StructExpression':; return self.row.drop(*self.key). [docs] def n_partitions(self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self)",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:58731,Availability,checkpoint,checkpoint,58731,"gate over rows into a local value. Examples; --------; Aggregate over rows:. >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; expr = to_expr(expr); base, _ = self._process_joins(expr); analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}). agg_ir = ir.TableAggregate(base._tir, expr._ir). if _localize:; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]. return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`Table`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_table`. It is; possible to read the file at this path later with :func:`.read_table`. Examples; --------; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:59540,Availability,checkpoint,checkpoint,59540,"ut=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`Table`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_table`. It is; possible to read the file at this path later with :func:`.read_table`. Examples; --------; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str)); def write(self, output: str, overwrite=False, stage_locally: bool = False, _codec_spec: Optional[str] = None):; """"""Write to disk. Examples; --------. >>> table1.write('output/table1.ht', overwrite=True). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_table`. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major out",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:71580,Availability,avail,available,71580,""" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index table with given expressions:\n""; f"" Table key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index Expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). @staticmethod; def _maybe_truncate_for_flexindex(indexer, indexee_dtype):; if not len(indexee_dtype) > 0:; raise ValueError('Must have non-e",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:80110,Availability,redundant,redundant,80110,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:81825,Availability,error,errors,81825,"e.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().unpersist(self). @overload; def collect(self) -> List[hl.Struct]: ... @overload; def collect(self, _localize=False) -> ArrayExpression: ... [docs] @typecheck_method(_localize=bool, _timed=bool); def collect(self, _localize=True, *, _timed=False):; """"""Collect the rows of the table into a local list. Examples; --------; Collect a list of all `X` records:. >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; -----; This method returns a list whose elements are of type :class:`.Struct`. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (``struct.foo``) or string indexing (``struct['foo']``). Warning; -------; Using this method can cause out of memory errors. Only collect small tables. Returns; -------; :obj:`list` of :class:`.Struct`; List of rows.; """"""; if len(self.key) > 0:; t = self.order_by(*self.key); else:; t = self; rows_ir = ir.GetField(ir.TableCollect(t._tir), 'rows'); e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); if _localize:; return Env.backend().execute(e._ir, timed=_timed); else:; return e. [docs] def describe(self, handler=print, *, widget=False):; """"""Print information about the fields in the table. Note; ----; The `widget` argument is **experimental**. Parameters; ----------; handler : Callable[[str], None]; Handler function for returned string.; widget : bool; Create an interactive IPython widget.; """"""; if widget:; from hail.experimental.interact import interact. return interact(self). def format_type(typ):; return typ.pretty(indent=4).lstrip(). if len(self.globals) == 0:; global_fields = '\n None'; else:; global_fields = ''.join(; ""\n '{name}': {type} "".format(name=f, type=format_",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:85654,Availability,error,error,85654,"unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """""". return self.annotate(**{name: hl.scan.count()}). [docs] @typecheck_method(tables=table_type, unify=bool); def union(self, *tables, unify: bool = False) -> 'Table':; """"""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters; ----------; tables : varargs of :class:`.Table`; Tables to union.; unify : :obj:`bool`; Attempt to unify table field. Returns; -------; :class:`.Table`; Table with all rows from each component table.; """"""; left_key = self.key.dtype; for (; i,; ht,; ) in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(; f""'union': table {i} has a different key.""; f"" Expected: {left_key}\n""; f"" Table {i}: {ht.key.dtype}""; ). if not (unify or ht.row.dtype == self.row.dtype):; raise ValueError(; f""'union': table {i} has a different row type.\n""; f"" Expected: {self.row.dtype}\n""; f"" Table {i}: {ht.row.dtype}\n""; f"" If the tables have the same fields in different orders, or some\n""; f"" common and some unique fields, then the 'unify' parameter may be\n""; f"" able to coerce the tables to a common type.""; ); all_tables = [self]; all_tables.extend(tables). if unify and not len(set(ht.row_value.dtype for ht in all_tables)) =",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:92445,Availability,avail,available,92445,"------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Parameters; ----------; p : :obj:`float`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.Table`; Table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter(hl.rand_bool(p, seed)). [docs] @typecheck_method(n=int, shuffle=bool); def repartition(self, n, shuffle=True) -> 'Table':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark,",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:93044,Availability,resilien,resilient-distributed-datasets-rdds,93044,"mber of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return h",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:93827,Availability,checkpoint,checkpoint,93827,"s; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Para",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:93901,Availability,checkpoint,checkpoint,93901,"oncept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:93990,Availability,checkpoint,checkpoint,93990,"rg/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Ret",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:94043,Availability,checkpoint,checkpoint,94043,"stributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.Table`; Table with at most",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:127809,Availability,toler,tolerance,127809,"columns[fields[0]])):; for field in fields:; cur_val = columns[field][data_idx]. # Can't call isna on a collection or it will implicitly broadcast; if pandas.api.types.is_numeric_dtype(df[field].dtype) and pandas.isna(cur_val):; if isinstance(cur_val, float):; fixed_val = cur_val; elif isinstance(cur_val, np.floating):; fixed_val = cur_val.item(); else:; fixed_val = None; elif isinstance(df[field].dtype, pandas.StringDtype) and pandas.isna(cur_val): # No NaN to worry about; fixed_val = None; elif isinstance(cur_val, np.number):; fixed_val = cur_val.item(); else:; fixed_val = cur_val; data[data_idx][field] = fixed_val. for data_idx, field in enumerate(fields):; type_hint = dtypes_from_pandas(pd_dtypes[field]); if type_hint is not None:; hl_type_hints[field] = type_hint. new_table = hl.Table.parallelize(data, partial_type=hl_type_hints); return new_table if not key else new_table.key_by(*key). @typecheck_method(other=table_type, tolerance=nullable(numeric), absolute=bool, reorder_fields=bool); def _same(self, other, tolerance=1e-6, absolute=False, reorder_fields=False):; from hail.expr.functions import _values_similar. fd_f = set if reorder_fields else list. if fd_f(self.row) != fd_f(other.row):; print(f'Different row fields: \n {list(self.row)}\n {list(other.row)}'); return False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False. if reorder_fields:; globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select(*row_order). if self._type != other._type:; print(f'Table._same: types differ:\n {self._type}\n {other._type}'); return False. left = self; left = left.select_globals(left_globals=left.globals); left = left.group_by(key=left.key).aggregate(left_row=hl.agg.collect(left.row_value)). right = other; right = right.select_globals(",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:127898,Availability,toler,tolerance,127898,"columns[fields[0]])):; for field in fields:; cur_val = columns[field][data_idx]. # Can't call isna on a collection or it will implicitly broadcast; if pandas.api.types.is_numeric_dtype(df[field].dtype) and pandas.isna(cur_val):; if isinstance(cur_val, float):; fixed_val = cur_val; elif isinstance(cur_val, np.floating):; fixed_val = cur_val.item(); else:; fixed_val = None; elif isinstance(df[field].dtype, pandas.StringDtype) and pandas.isna(cur_val): # No NaN to worry about; fixed_val = None; elif isinstance(cur_val, np.number):; fixed_val = cur_val.item(); else:; fixed_val = cur_val; data[data_idx][field] = fixed_val. for data_idx, field in enumerate(fields):; type_hint = dtypes_from_pandas(pd_dtypes[field]); if type_hint is not None:; hl_type_hints[field] = type_hint. new_table = hl.Table.parallelize(data, partial_type=hl_type_hints); return new_table if not key else new_table.key_by(*key). @typecheck_method(other=table_type, tolerance=nullable(numeric), absolute=bool, reorder_fields=bool); def _same(self, other, tolerance=1e-6, absolute=False, reorder_fields=False):; from hail.expr.functions import _values_similar. fd_f = set if reorder_fields else list. if fd_f(self.row) != fd_f(other.row):; print(f'Different row fields: \n {list(self.row)}\n {list(other.row)}'); return False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False. if reorder_fields:; globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select(*row_order). if self._type != other._type:; print(f'Table._same: types differ:\n {self._type}\n {other._type}'); return False. left = self; left = left.select_globals(left_globals=left.globals); left = left.group_by(key=left.key).aggregate(left_row=hl.agg.collect(left.row_value)). right = other; right = right.select_globals(",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:129153,Availability,toler,tolerance,129153,"turn False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False. if reorder_fields:; globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select(*row_order). if self._type != other._type:; print(f'Table._same: types differ:\n {self._type}\n {other._type}'); return False. left = self; left = left.select_globals(left_globals=left.globals); left = left.group_by(key=left.key).aggregate(left_row=hl.agg.collect(left.row_value)). right = other; right = right.select_globals(right_globals=right.globals); right = right.group_by(key=right.key).aggregate(right_row=hl.agg.collect(right.row_value)). t = left.join(right, how='outer'). mismatched_globals, mismatched_rows = t.aggregate(; hl.tuple((; hl.or_missing(~_values_similar(t.left_globals, t.right_globals, tolerance, absolute), t.globals),; hl.agg.filter(; ~hl.all(; hl.is_defined(t.left_row),; hl.is_defined(t.right_row),; _values_similar(t.left_row, t.right_row, tolerance, absolute),; ),; hl.agg.take(t.row, 10),; ),; )); ). columns, _ = shutil.get_terminal_size((80, 10)). def pretty(obj):; pretty_str = pprint.pformat(obj, width=columns); return ''.join(' ' + line for line in pretty_str.splitlines(keepends=True)). is_same = True; if mismatched_globals is not None:; print(f""""""Table._same: globals differ:; Left:; {pretty(mismatched_globals.left_globals)}; Right:; {pretty(mismatched_globals.right_globals)}""""""); is_same = False. if len(mismatched_rows) > 0:; print('Table._same: rows differ:'); for r in mismatched_rows:; print(f"""""" Row mismatch at key={r.key}:; Left:; {pretty(r.left_row)}; Right:; {pretty(r.right_row)}""""""); is_same = False. return is_same. [docs] def collect_by_key(self, name: str = 'values') -> 'Table':; """"""Collect values for each unique key into an array. .. include:: _templates/",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:129312,Availability,toler,tolerance,129312,"rder_fields:; globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select(*row_order). if self._type != other._type:; print(f'Table._same: types differ:\n {self._type}\n {other._type}'); return False. left = self; left = left.select_globals(left_globals=left.globals); left = left.group_by(key=left.key).aggregate(left_row=hl.agg.collect(left.row_value)). right = other; right = right.select_globals(right_globals=right.globals); right = right.group_by(key=right.key).aggregate(right_row=hl.agg.collect(right.row_value)). t = left.join(right, how='outer'). mismatched_globals, mismatched_rows = t.aggregate(; hl.tuple((; hl.or_missing(~_values_similar(t.left_globals, t.right_globals, tolerance, absolute), t.globals),; hl.agg.filter(; ~hl.all(; hl.is_defined(t.left_row),; hl.is_defined(t.right_row),; _values_similar(t.left_row, t.right_row, tolerance, absolute),; ),; hl.agg.take(t.row, 10),; ),; )); ). columns, _ = shutil.get_terminal_size((80, 10)). def pretty(obj):; pretty_str = pprint.pformat(obj, width=columns); return ''.join(' ' + line for line in pretty_str.splitlines(keepends=True)). is_same = True; if mismatched_globals is not None:; print(f""""""Table._same: globals differ:; Left:; {pretty(mismatched_globals.left_globals)}; Right:; {pretty(mismatched_globals.right_globals)}""""""); is_same = False. if len(mismatched_rows) > 0:; print('Table._same: rows differ:'); for r in mismatched_rows:; print(f"""""" Row mismatch at key={r.key}:; Left:; {pretty(r.left_row)}; Right:; {pretty(r.right_row)}""""""); is_same = False. return is_same. [docs] def collect_by_key(self, name: str = 'values') -> 'Table':; """"""Collect values for each unique key into an array. .. include:: _templates/req_keyed_table.rst. Examples; --------; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar',",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:5092,Deployability,pipeline,pipeline,5092," object is not mutable""); self.__dict__[key] = value. def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]. raise AttributeError(get_nice_attr_error(self, item)). def _copy_fields_from(self, other: 'ExprContainer'):; self._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same group",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:5355,Deployability,pipeline,pipeline,5355,"lf._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.gro",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:12755,Deployability,pipeline,pipeline,12755,"e 2: Left distinct join: ht[ht2.key] or ht[ht2.field1, ht2.field2]""; ) from e. @property; def key(self) -> StructExpression:; """"""Row key struct. Examples; --------. List of key field names:. >>> list(table1.key); ['ID']. Number of key fields:. >>> len(table1.key); 1. Returns; -------; :class:`.StructExpression`; """"""; return self._key. @property; def _value(self) -> 'StructExpression':; return self.row.drop(*self.key). [docs] def n_partitions(self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self)",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:42409,Deployability,pipeline,pipeline,42409," +-------+-------+-----+-------+-------+----------+-------+----------+; +----------------+; | HT_DESCRIPTION |; +----------------+; | str |; +----------------+; | ""sixty-five"" |; | ""seventy-two"" |; | ""seventy"" |; | ""sixty"" |; +----------------+. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Expressions for new fields. Returns; -------; :class:`.Table`; Table with new fields. """"""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:42795,Deployability,pipeline,pipeline,42795,"""""""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:53565,Deployability,pipeline,pipeline,53565,"check_keys('drop', f, protected_key); row_fields = set(table.row); to_drop = [f for f in fields_to_drop if f in row_fields]; table = table._select('drop', table.row.drop(*to_drop)). return table. [docs] @typecheck_method(; output=str, types_file=nullable(str), header=bool, parallel=nullable(ir.ExportType.checker), delimiter=str; ); def export(self, output, types_file=None, header=True, parallel=None, delimiter='\t'):; """"""Export to a text file. Examples; --------; Export to a tab-separated file:. >>> table1.export('output/table1.tsv.bgz'). Note; ----; It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python's ``gzip.open``; and R's ``read.table``. Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use :meth:`flatten` first. Warning; -------; Do not export to a path that is being read from in the same pipeline. See Also; --------; :meth:`flatten`, :meth:`write`. Parameters; ----------; output : :class:`str`; URI at which to write exported file.; types_file : :class:`str`, optional; URI at which to write file containing field type information.; header : :obj:`bool`; Include a header in the file.; parallel : :class:`str`, optional; If None, a single file is produced, otherwise a; folder of file shards is produced. If 'separate_header',; the header file is output separately from the file shards. If; 'header_per_shard', each file shard has a header. If set to None; the export will be slower.; delimiter : :class:`str`; Field delimiter.; """"""; hl.current_backend().validate_file(output). parallel = ir.ExportType.default(parallel); Env.backend().execute(; ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter)); ). [docs] def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':; """"""Group by a new key for use with :me",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:80169,Deployability,pipeline,pipelines,80169,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:136644,Deployability,update,updated,136644,"bles[0]; if any(head.key.dtype != t.key.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same key type:\n '; + '\n '.join(str(t.key.dtype) for t in tables); ); if any(head.row.dtype != t.row.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same row type\n '; + '\n '.join(str(t.row.dtype) for t in tables); ); if any(head.globals.dtype != t.globals.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same global type\n '; + '\n '.join(str(t.globals.dtype) for t in tables); ); return Table(ir.TableMultiWayZipJoin([t._tir for t in tables], data_field_name, global_field_name)). def _group_within_partitions(self, name, n):; def grouping_func(part):; groups = part.grouped(n); key_names = list(self.key); return groups.map(lambda group: group[0].select(*key_names, **{name: group})). return self._map_partitions(grouping_func). @typecheck_method(f=func_spec(1, expr_stream(expr_struct()))); def _map_partitions(self, f):; rows_uid = 'tmp_rows_' + Env.get_uid(); globals_uid = 'tmp_globals_' + Env.get_uid(); expr = construct_expr(; ir.Ref(rows_uid, hl.tstream(self.row.dtype)), hl.tstream(self.row.dtype), self._row_indices; ); body = f(expr); result_t = body.dtype; if any(k not in result_t.element_type for k in self.key):; raise ValueError('Table._map_partitions must preserve key fields'). body_ir = ir.Let('global', ir.Ref(globals_uid, self._global_type), body._ir); return Table(ir.TableMapPartitions(self._tir, globals_uid, rows_uid, body_ir, len(self.key), len(self.key))). def _calculate_new_partitions(self, n_partitions):; """"""returns a set of range bounds that can be passed to write""""""; return Env.backend().execute(; ir.TableToValueApply(; self.select().select_globals()._tir,; {'name': 'TableCalculateNewPartitions', 'nPartitions': n_partitions},; ); ). table_type.set(Table). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:5582,Energy Efficiency,reduce,reduces,5582,"dTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_res",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:58186,Integrability,depend,dependent,58186," reference expressions.; named_exprs : keyword args of type :class:`.Expression`; Field names and expressions to compute them. Returns; -------; :class:`.GroupedTable`; Grouped table; use :meth:`.GroupedTable.aggregate` to complete the aggregation.; """"""; key, computed_key = get_key_by_exprs(; 'Table.group_by', exprs, named_exprs, self._row_indices, override_protected_indices={self._global_indices}; ); return GroupedTable(self, self.row.annotate(**computed_key).select(*key)). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate(self, expr, _localize=True):; """"""Aggregate over rows into a local value. Examples; --------; Aggregate over rows:. >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; expr = to_expr(expr); base, _ = self._process_joins(expr); analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}). agg_ir = ir.TableAggregate(base._tir, expr._ir). if _localize:; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]. return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copie",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:100474,Integrability,depend,depends,100474,"ey.values()); ):; raise ValueError(; 'anti_join: cannot join: table must have a key of the same type(s) and be the same length or shorter:'; f'\n Left key: {"", "".join(str(x.dtype) for x in self.key.values())}'; f'\n Right key: {"", "".join(str(x.dtype) for x in other.key.values())}'; ). return self.filter(hl.is_missing(other.index(*(self.key[i] for i in range(len(other.key)))))). [docs] @typecheck_method(; right=table_type, how=enumeration('inner', 'outer', 'left', 'right'), _mangle=anyfunc, _join_key=nullable(int); ); def join(; self,; right: 'Table',; how='inner',; _mangle: Callable[[str, int], str] = lambda s, i: f'{s}_{i}',; _join_key: Optional[int] = None,; ) -> 'Table':; """"""Join two tables together. Examples; --------; Join `table1` to `table2` to produce `table_joined`:. >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; -----; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing. Both tables must have the same number of keys and the corresponding; types of each key must be the same (order matters), but the key",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:31306,Modifiability,variab,variable-length,31306,"---; Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'target_date': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------. Fields may also be selected by their name:. >>> ht = ht.select_globals('target_date'); >>> ht.globals.show(); +--------------------+; | <expr>.target_date |; +--------------------+; | str |; +--------------------+; | ""2025-01-01"" |; +--------------------+. Notes; -----; This method creates new global fields. If a created field shares its name; with a row-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified global fields. """"""; caller = 'Table.select_globals'; new_globals = get_select_exprs(caller, exprs, named_exprs, self._global_indices, self._globals). return self._select_globals(caller, new_globals). [docs] @typecheck_method(named_exprs=expr_any); def transmute_globals(self, **named_exprs) -> 'Table':; """"""Similar to :meth:`.Table.annotate_globals`, but drops referenced fields. Notes; -----; Consider a table with global fields `population`, `area`, and `year`:. >>> ht = hl.utils.range_table(1); >>> ht = ht.annotate_globals(population=1000000, area=500, year=2020). Compute a new field, `density` from `population` and `area` and also drop the latter two; fields:. >>> ht = ht.transmute_globals(density=ht.popu",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:48670,Modifiability,variab,variable-length,48670,"-; :class:`.Table`; Filtered table. """"""; analyze('Table.filter', expr, self._row_indices); base, cleanup = self._process_joins(expr). return cleanup(Table(ir.TableFilter(base._tir, ir.filter_predicate_with_keep(expr._ir, keep)))). [docs] @typecheck_method(exprs=oneof(Expression, str), named_exprs=anytype); def select(self, *exprs, **named_exprs) -> 'Table':; """"""Select existing fields or create new fields by name, dropping the rest. Examples; --------; Select a few old fields and compute a new one:. >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; -----; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:49231,Modifiability,variab,variable-length,49231,"table1.Z - table1.X). Notes; -----; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Not",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:49336,Modifiability,variab,variable-length,49336,"field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-l",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:49541,Modifiability,variab,variable-length,49541,".select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Re",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:50327,Modifiability,variab,variable-length,50327," variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified fields.; """"""; row = get_select_exprs('Table.select', exprs, named_exprs, self._row_indices, self._row). return self._select('Table.select', row). [docs] @typecheck_method(exprs=oneof(str, Expression)); def drop(self, *exprs) -> 'Table':; """"""Drop fields from the table. Examples; --------. Drop fields `C1` and `C2` using strings:. >>> table_result = table1.drop('C1', 'C2'). Drop fields `C1` and `C2` using field references:. >>> table_result = table1.drop(table1.C1, table1.C2). Drop a list of fields:. >>> fields_to_drop = ['C1', 'C2']; >>> table_result = table1.drop(*fields_to_drop). Notes; -----. This method can be used to drop global or row-indexed fields. The arguments; can be either strings (``'field'``)",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:55265,Modifiability,variab,variable-length,55265,"parallel); Env.backend().execute(; ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter)); ). [docs] def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':; """"""Group by a new key for use with :meth:`.GroupedTable.aggregate`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:55652,Modifiability,variab,variable-length,55652," (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=tab",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:55808,Modifiability,variab,variable-length,55808," ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arg",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:56115,Modifiability,variab,variable-length,56115,"*group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = (table1.group_by(table1.C1, 'C2', height_bin = table1.HT // 20); ... .aggregate(meanX = hl.agg.mean(table1.X))). Note; ----; This method does not support aggregation in key expressions. Arguments; ---------; exprs : varargs of type str or :class:`",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:71872,Modifiability,variab,variable-length,71872,"table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index table with given expressions:\n""; f"" Table key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index Expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). @staticmethod; def _maybe_truncate_for_flexindex(indexer, indexee_dtype):; if not len(indexee_dtype) > 0:; raise ValueError('Must have non-empty key to index'). if not isinstance(indexer.dtype, (hl.tstruct, hl.ttuple)):; indexer = hl.tuple([indexer]). matching_prefix = 0; for x, y in zip(indexer.dtype.types, indexee_dtype.types):; if x != y:; break; matching_prefix += 1; prefix_match = matching_prefix == len(indexee_dtype); direc",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:78155,Modifiability,extend,extend,78155,"e(; ir.MatrixMapRows(; mart,; ir.InsertFields(; ir.Ref('va', mart.typ.row_type),; [; (; uid,; ir.Apply(; 'get',; join_table._row_type[uid].value_type,; ir.GetField(ir.GetField(ir.Ref('va', mart.typ.row_type), uid), uid),; ir.MakeTuple([e._ir for e in exprs]),; ),; ); ],; None,; ),; ); ). else:. def joiner(left: MatrixTable):; return MatrixTable(ir.MatrixAnnotateRowsTable(left._mir, right._tir, uid, all_matches)). ast = ir.Join(ir.ProjectedTopLevelReference('va', uid, new_schema), [uid], exprs, joiner); return construct_expr(ast, new_schema, indices, aggregations); elif indices == src._col_indices and not (is_interval and all_matches):; all_uids = [uid]; if len(exprs) == len(src.col_key) and all([exprs[i] is src.col_key[i] for i in range(len(exprs))]):; # key is already correct; def joiner(left):; return MatrixTable(ir.MatrixAnnotateColsTable(left._mir, right._tir, uid)). else:; index_uid = Env.get_uid(); uids = [Env.get_uid() for _ in exprs]. all_uids.append(index_uid); all_uids.extend(uids). def joiner(left: MatrixTable):; prev_key = list(src.col_key); joined = (; src.annotate_cols(**dict(zip(uids, exprs))); .add_col_index(index_uid); .key_cols_by(*uids); .cols(); .select(index_uid); .join(self, 'inner'); .key_by(index_uid); .drop(*uids); ); result = MatrixTable(; ir.MatrixAnnotateColsTable(; (left.add_col_index(index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :cl",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:78981,Modifiability,variab,variables,78981,"eft):; return MatrixTable(ir.MatrixAnnotateColsTable(left._mir, right._tir, uid)). else:; index_uid = Env.get_uid(); uids = [Env.get_uid() for _ in exprs]. all_uids.append(index_uid); all_uids.extend(uids). def joiner(left: MatrixTable):; prev_key = list(src.col_key); joined = (; src.annotate_cols(**dict(zip(uids, exprs))); .add_col_index(index_uid); .key_cols_by(*uids); .cols(); .select(index_uid); .join(self, 'inner'); .key_by(index_uid); .drop(*uids); ); result = MatrixTable(; ir.MatrixAnnotateColsTable(; (left.add_col_index(index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:86532,Modifiability,extend,extend,86532,"ntain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters; ----------; tables : varargs of :class:`.Table`; Tables to union.; unify : :obj:`bool`; Attempt to unify table field. Returns; -------; :class:`.Table`; Table with all rows from each component table.; """"""; left_key = self.key.dtype; for (; i,; ht,; ) in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(; f""'union': table {i} has a different key.""; f"" Expected: {left_key}\n""; f"" Table {i}: {ht.key.dtype}""; ). if not (unify or ht.row.dtype == self.row.dtype):; raise ValueError(; f""'union': table {i} has a different row type.\n""; f"" Expected: {self.row.dtype}\n""; f"" Table {i}: {ht.row.dtype}\n""; f"" If the tables have the same fields in different orders, or some\n""; f"" common and some unique fields, then the 'unify' parameter may be\n""; f"" able to coerce the tables to a common type.""; ); all_tables = [self]; all_tables.extend(tables). if unify and not len(set(ht.row_value.dtype for ht in all_tables)) == 1:; discovered = collections.defaultdict(dict); for i, ht in enumerate(all_tables):; for field_name in ht.row_value:; discovered[field_name][i] = ht[field_name]; all_fields = [{} for _ in all_tables]; for field_name, expr_dict in discovered.items():; *unified, can_unify = hl.expr.expressions.unify_exprs(*expr_dict.values()); if not can_unify:; raise ValueError(; f""cannot unify field {field_name!r}: found fields of types ""; f""{[str(t) for t in {e.dtype for e in expr_dict.values()}]}""; ); unified_map = dict(zip(expr_dict.keys(), unified)); default = hl.missing(unified[0].dtype); for i in range(len(all_tables)):; all_fields[i][field_name] = unified_map.get(i, default). for i, t in enumerate(all_tables):; all_tables[i] = t.select(**all_fields[i]). return Table(ir.TableUnion([table._tir for table in all_tables])). [docs] @typecheck_method(n=int, _localize=bool); def take(self, n, _localize=True):; """"""Collect the f",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:5285,Performance,optimiz,optimizer,5285,"lf._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.gro",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:13155,Performance,load,loaded,13155,"self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self):; return await Env.backend()._async_execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). @typecheck_method(caller=str, row=expr_struct()); def _select(self, caller, row) -> 'Table':; analyze(caller, row, self._row_indices); base, cleanup = self._process_joins(row); return cleanup(Table(ir.TableMapRows(base._tir, row._ir))). @typecheck_method(caller=str, s=expr_struct()); def _select_globals(self, caller, s) -> 'Table':; base",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:42454,Performance,optimiz,optimization,42454,"---+; | HT_DESCRIPTION |; +----------------+; | str |; +----------------+; | ""sixty-five"" |; | ""seventy-two"" |; | ""seventy"" |; | ""sixty"" |; +----------------+. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Expressions for new fields. Returns; -------; :class:`.Table`; Table with new fields. """"""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:79358,Performance,cache,cache,79358,"ner'); .key_by(index_uid); .drop(*uids); ); result = MatrixTable(; ir.MatrixAnnotateColsTable(; (left.add_col_index(index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apach",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:79484,Performance,cache,cache,79484,"index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Par",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:80028,Performance,cache,cache,80028,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:80149,Performance,perform,performance,80149,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:84723,Performance,perform,performance,84723,"PUT_CHECK; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; -----. This method returns a table with a new field whose name is given by; the `name` parameter, with type :py:data:`.tint64`. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like :meth:`.Table.take` or :meth:`.Table.export`) will; return rows in order. This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """""". return self.annotate(**{name: hl.scan.count()}). [docs] @typecheck_method(tables=table_type, unify=bool); def union(self, *tables, unify: bool = False) -> 'Table':; """"""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an err",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:92697,Performance,perform,performance,92697,"loat`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.Table`; Table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter(hl.rand_bool(p, seed)). [docs] @typecheck_method(n=int, shuffle=bool); def repartition(self, n, shuffle=True) -> 'Table':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:80104,Safety,avoid,avoid,80104,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:80110,Safety,redund,redundant,80110,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:93257,Safety,avoid,avoid,93257," with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; )",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:103081,Safety,avoid,avoid,103081,"esulting table can be larger than; the number of records on the left or right if duplicate keys are; present. Parameters; ----------; right : :class:`.Table`; Table to join.; how : :class:`str`; Join type. One of ""inner"", ""left"", ""right"", ""outer"". Returns; -------; :class:`.Table`; Joined table. """"""; if _join_key is None:; _join_key = max(len(self.key), len(right.key)). left_key_types = list(self.key.dtype.values())[:_join_key]; right_key_types = list(right.key.dtype.values())[:_join_key]; if not left_key_types == right_key_types:; raise ValueError(; f""'join': key mismatch:\n ""; f"" left: [{', '.join(str(t) for t in left_key_types)}]\n ""; f"" right: [{', '.join(str(t) for t in right_key_types)}]""; ); left_fields = set(self._fields); right_fields = set(right._fields) - set(right.key). renames, _ = deduplicate(right_fields, max_attempts=100, already_used=left_fields). if renames:; renames = dict(renames); right = right.rename(renames); info(; 'Table.join: renamed the following fields on the right to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renames.items()); ). return Table(ir.TableJoin(self._tir, right._tir, how, _join_key)). [docs] @typecheck_method(expr=BooleanExpression); def all(self, expr):; """"""Evaluate whether a boolean expression is true for all rows. Examples; --------; Test whether `C1` is greater than 5 in all rows of the table:. >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Expression to test. Returns; -------; :obj:`bool`; """"""; return self.aggregate(hl.agg.all(expr)). [docs] @typecheck_method(expr=BooleanExpression); def any(self, expr):; """"""Evaluate whether a Boolean expression is true for at least one row. Examples; --------. Test whether `C1` is equal to 5 any row in any row of the table:. >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Boolean exp",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:71168,Security,expose,expose,71168,"prs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mis",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:81648,Security,access,accessed,81648,"ND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().unpersist(self). @overload; def collect(self) -> List[hl.Struct]: ... @overload; def collect(self, _localize=False) -> ArrayExpression: ... [docs] @typecheck_method(_localize=bool, _timed=bool); def collect(self, _localize=True, *, _timed=False):; """"""Collect the rows of the table into a local list. Examples; --------; Collect a list of all `X` records:. >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; -----; This method returns a list whose elements are of type :class:`.Struct`. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (``struct.foo``) or string indexing (``struct['foo']``). Warning; -------; Using this method can cause out of memory errors. Only collect small tables. Returns; -------; :obj:`list` of :class:`.Struct`; List of rows.; """"""; if len(self.key) > 0:; t = self.order_by(*self.key); else:; t = self; rows_ir = ir.GetField(ir.TableCollect(t._tir), 'rows'); e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); if _localize:; return Env.backend().execute(e._ir, timed=_timed); else:; return e. [docs] def describe(self, handler=print, *, widget=False):; """"""Print information about the fields in the table. Note; ----; The `widget` argument is **experimental**. Parameters; ----------; handler : Callable[[str], None]; Handler function for returned string.; widget : bool; Create an interactive IPython widget.; """"""; if widget:; from hail.experimental.interact import interact. return interact(self). def format_type(typ):; return typ.pretty(indent=4).lstrip(). if len(",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:3105,Testability,assert,assert,3105,"xpressions. class Ascending:; def __init__(self, col):; self.col = col. def __eq__(self, other):; return isinstance(other, Ascending) and self.col == other.col. def __ne__(self, other):; return not self == other. class Descending:; def __init__(self, col):; self.col = col. def __eq__(self, other):; return isinstance(other, Descending) and self.col == other.col. def __ne__(self, other):; return not self == other. [docs]@typecheck(col=oneof(Expression, str)); def asc(col):; """"""Sort by `col` ascending."""""". return Ascending(col). [docs]@typecheck(col=oneof(Expression, str)); def desc(col):; """"""Sort by `col` descending."""""". return Descending(col). class ExprContainer:; # this can only grow as big as the object dir, so no need to worry about memory leak; _warned_about: ClassVar = set(). def __init__(self):; self._fields: Dict[str, Expression] = {}; self._fields_inverse: Dict[Expression, str] = {}; self._dir = set(dir(self)); super(ExprContainer, self).__init__(). def _set_field(self, key, value):; assert key not in self._fields_inverse, key; self._fields[key] = value; self._fields_inverse[value] = key. # key is in __dir for methods; # key is in __dict__ for private class fields; if key in self._dir or key in self.__dict__:; if key not in ExprContainer._warned_about:; ExprContainer._warned_about.add(key); warning(; f""Name collision: field {key!r} already in object dict. ""; f""\n This field must be referenced with __getitem__ syntax: obj[{key!r}]""; ); else:; self.__dict__[key] = value. def _get_field(self, item) -> Expression:; if item in self._fields:; return self._fields[item]. raise LookupError(get_nice_field_error(self, item)). def __iter__(self):; raise TypeError(f""'{self.__class__.__name__}' object is not iterable""). def __delattr__(self, item):; if not item[0] == '_':; raise NotImplementedError(f""'{self.__class__.__name__}' object is not mutable""). def __setattr__(self, key, value):; if not key[0] == '_':; raise NotImplementedError(f""'{self.__class__.__name__}' object ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:16377,Testability,log,login,16377,"5, 'b': 10}, {'a': 0, 'b': 200}],; ... schema=hl.tstruct(a=hl.tint, b=hl.tint),; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+------------",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:16506,Testability,log,login,16506," | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:16642,Testability,log,login,16642,"e schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:16767,Testability,log,login,16767," t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:16895,Testability,log,login,16895," a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:17223,Testability,log,login,17223," (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; +-----------+------------+. Parallelizing with a specified number of partitions:. >>> rows = [ {'a': i} for i in range(100) ]; >>> ht = hl.Table.parallelize(rows, n_partitions=10); >>> ht.n_partitions(); 10; >>> ht.count(); 100. Parallelizing with some global",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:51959,Testability,assert,assert,51959,"t = table1.drop('C1', 'C2'). Drop fields `C1` and `C2` using field references:. >>> table_result = table1.drop(table1.C1, table1.C2). Drop a list of fields:. >>> fields_to_drop = ['C1', 'C2']; >>> table_result = table1.drop(*fields_to_drop). Notes; -----. This method can be used to drop global or row-indexed fields. The arguments; can be either strings (``'field'``), or top-level field references; (``table.field`` or ``table['field']``). Parameters; ----------; exprs : varargs of :class:`str` or :class:`.Expression`; Names of fields to drop or field reference expressions. Returns; -------; :class:`.Table`; Table without specified fields.; """"""; all_field_exprs = {e: k for k, e in self._fields.items()}; fields_to_drop = set(); for e in exprs:; if isinstance(e, Expression):; if e in all_field_exprs:; fields_to_drop.add(all_field_exprs[e]); else:; raise ExpressionException(; ""method 'drop' expects string field names or top-level field expressions"" "" (e.g. table['foo'])""; ); else:; assert isinstance(e, str); if e not in self._fields:; raise IndexError(""table has no field '{}'"".format(e)); fields_to_drop.add(e). table = self; if any(self._fields[field]._indices == self._global_indices for field in fields_to_drop):; # need to drop globals; table = table._select_globals(; 'drop', self._globals.drop(*[f for f in table.globals if f in fields_to_drop]); ). if any(self._fields[field]._indices == self._row_indices for field in fields_to_drop):; # need to drop row fields; protected_key = set(self._row_indices.protected_key); for f in fields_to_drop:; check_keys('drop', f, protected_key); row_fields = set(table.row); to_drop = [f for f in fields_to_drop if f in row_fields]; table = table._select('drop', table.row.drop(*to_drop)). return table. [docs] @typecheck_method(; output=str, types_file=nullable(str), header=bool, parallel=nullable(ir.ExportType.checker), delimiter=str; ); def export(self, output, types_file=None, header=True, parallel=None, delimiter='\t'):; """"""Export to a t",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:69283,Testability,log,logging,69283,"able(int),; width=nullable(int),; truncate=nullable(int),; types=bool,; handler=nullable(anyfunc),; n_rows=nullable(int),; ); def show(self, n=None, width=None, truncate=None, types=True, handler=None, n_rows=None):; """"""Print the first few rows of the table to the console. Examples; --------; Show the first lines of the table:. >>> table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; -----; The output can be passed piped to another output source using the `handler` argument:. >>> ht.show(handler=lambda x: logging.info(x)) # doctest: +SKIP. Parameters; ----------; n or n_rows : :obj:`int`; Maximum number of rows to show, or negative to show all rows.; width : :obj:`int`; Horizontal width at which to break fields.; truncate : :obj:`int`, optional; Truncate each field to the given number of characters. If; ``None``, truncate fields to the given `width`.; types : :obj:`bool`; Print an extra header line with the type of each field.; handler : Callable[[str], Any]; Handler function for data string.; """"""; if n_rows is not None and n is not None:; raise ValueError(f'specify one of n_rows or n, received {n_rows} and {n}'); if n_rows is not None:; n = n_rows; del n_rows; if handler is None:; handler = hl.utils.default_handler(); return handler(self._show(n, width, truncate, types)). [docs] def index(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; In the example below, both `table1` and ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:103604,Testability,test,test,103604," right_key_types:; raise ValueError(; f""'join': key mismatch:\n ""; f"" left: [{', '.join(str(t) for t in left_key_types)}]\n ""; f"" right: [{', '.join(str(t) for t in right_key_types)}]""; ); left_fields = set(self._fields); right_fields = set(right._fields) - set(right.key). renames, _ = deduplicate(right_fields, max_attempts=100, already_used=left_fields). if renames:; renames = dict(renames); right = right.rename(renames); info(; 'Table.join: renamed the following fields on the right to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renames.items()); ). return Table(ir.TableJoin(self._tir, right._tir, how, _join_key)). [docs] @typecheck_method(expr=BooleanExpression); def all(self, expr):; """"""Evaluate whether a boolean expression is true for all rows. Examples; --------; Test whether `C1` is greater than 5 in all rows of the table:. >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Expression to test. Returns; -------; :obj:`bool`; """"""; return self.aggregate(hl.agg.all(expr)). [docs] @typecheck_method(expr=BooleanExpression); def any(self, expr):; """"""Evaluate whether a Boolean expression is true for at least one row. Examples; --------. Test whether `C1` is equal to 5 any row in any row of the table:. >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Boolean expression. Returns; -------; :obj:`bool`; ``True`` if the predicate evaluated for ``True`` for any row, otherwise ``False``.; """"""; return self.aggregate(hl.agg.any(expr)). [docs] @typecheck_method(mapping=dictof(str, str)); def rename(self, mapping) -> 'Table':; """"""Rename fields of the table. Examples; --------; Rename `C1` to `col1` and `C2` to `col2`:. >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters; ----------; mapping : :obj:`dict` of :class:`str`, :obj:`str`; Mapping from old field n",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:105227,Testability,assert,assert,105227,"egate(hl.agg.any(expr)). [docs] @typecheck_method(mapping=dictof(str, str)); def rename(self, mapping) -> 'Table':; """"""Rename fields of the table. Examples; --------; Rename `C1` to `col1` and `C2` to `col2`:. >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters; ----------; mapping : :obj:`dict` of :class:`str`, :obj:`str`; Mapping from old field names to new field names. Notes; -----; Any field that does not appear as a key in `mapping` will not be; renamed. Returns; -------; :class:`.Table`; Table with renamed fields.; """"""; seen = {}. row_map = {}; global_map = {}. for k, v in mapping.items():; if v in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[v]), repr(k), repr(v); ); ); if v in self._fields and v not in mapping:; raise ValueError(""Cannot rename {} to {}: field already exists."".format(repr(k), repr(v))); seen[v] = k; if self[k]._indices == self._row_indices:; row_map[k] = v; else:; assert self[k]._indices == self._global_indices; global_map[k] = v. stray = set(mapping.keys()) - set(seen.values()); if stray:; raise ValueError(f""found rename rules for fields not present in table: {list(stray)}""). return Table(ir.TableRename(self._tir, row_map, global_map)). [docs] def expand_types(self) -> 'Table':; """"""Expand complex types into structs and arrays. Examples; --------. >>> table_result = table1.expand_types(). Notes; -----; Expands the following types: :class:`.tlocus`, :class:`.tinterval`,; :class:`.tset`, :class:`.tdict`, :class:`.ttuple`. The only types that will remain after this method are:; :py:data:`.tbool`, :py:data:`.tint32`, :py:data:`.tint64`,; :py:data:`.tfloat64`, :py:data:`.tfloat32`, :class:`.tarray`,; :class:`.tstruct`. Note, expand_types always returns an unkeyed table. Returns; -------; :class:`.Table`; Expanded table.; """""". t = self; if len(t.key) > 0:; t = t.order_by(*t.key). def _expand(e):; if isinstance(e, (CollectionExpression, Dict",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:106957,Testability,assert,assert,106957,"`. Note, expand_types always returns an unkeyed table. Returns; -------; :class:`.Table`; Expanded table.; """""". t = self; if len(t.key) > 0:; t = t.order_by(*t.key). def _expand(e):; if isinstance(e, (CollectionExpression, DictExpression)):; return hl.map(lambda x: _expand(x), hl.array(e)); elif isinstance(e, StructExpression):; return hl.struct(**{k: _expand(v) for (k, v) in e.items()}); elif isinstance(e, TupleExpression):; return hl.struct(**{f'_{i}': x for (i, x) in enumerate(e)}); elif isinstance(e, IntervalExpression):; return hl.struct(start=e.start, end=e.end, includesStart=e.includes_start, includesEnd=e.includes_end); elif isinstance(e, LocusExpression):; return hl.struct(contig=e.contig, position=e.position); elif isinstance(e, CallExpression):; return hl.struct(alleles=hl.map(lambda i: e[i], hl.range(0, e.ploidy)), phased=e.phased); elif isinstance(e, NDArrayExpression):; return hl.struct(shape=e.shape, data=_expand(e._data_array())); else:; assert isinstance(e, (NumericExpression, BooleanExpression, StringExpression)); return e. t = t.select(**_expand(t.row)); t = t.select_globals(**_expand(t.globals)); return t. [docs] def flatten(self) -> 'Table':; """"""Flatten nested structs. Examples; --------; Flatten table:. >>> table_result = table1.flatten(). Notes; -----; Consider a table with signature. .. code-block:: text. a: struct{; p: int32,; q: str; },; b: int32,; c: struct{; x: str,; y: array<struct{; y: str,; z: str; }>; }. and key ``a``. The result of flatten is. .. code-block:: text. a.p: int32; a.q: str; b: int32; c.x: str; c.y: array<struct{; y: str,; z: str; }>. with key ``a.p, a.q``. Note, structures inside collections like arrays or sets will not be; flattened. Note, the result of flatten is always unkeyed. Warning; -------; Flattening a table will produces fields that cannot be referenced using; the ``table.<field>`` syntax, e.g. ""a.b"". Reference these fields using; square bracket lookups: ``table['a.b']``. Returns; -------; :class:`.Table`; Table",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:8171,Usability,guid,guide,8171,"amed_exprs.items():; analyze(f'{caller}: ({name!r})', expr, self._parent._global_indices, {self._parent._row_axis}); check_collisions(caller, list(named_exprs), self._parent._row_indices); if not named_exprs.keys().isdisjoint(set(self._key_expr)):; intersection = set(named_exprs.keys()) & set(self._key_expr); raise ValueError(; f'GroupedTable.aggregate: Group names and aggregration expression names overlap: {intersection}'; ). base, _ = self._parent._process_joins(self._key_expr, *named_exprs.values()). key_struct = self._key_expr; return Table(; ir.TableKeyByAndAggregate(; base._tir, hl.struct(**named_exprs)._ir, key_struct._ir, self._npartitions, self._buffer_size; ); ). [docs]class Table(ExprContainer):; """"""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:8200,Usability,learn,learning,8200,"{name!r})', expr, self._parent._global_indices, {self._parent._row_axis}); check_collisions(caller, list(named_exprs), self._parent._row_indices); if not named_exprs.keys().isdisjoint(set(self._key_expr)):; intersection = set(named_exprs.keys()) & set(self._key_expr); raise ValueError(; f'GroupedTable.aggregate: Group names and aggregration expression names overlap: {intersection}'; ). base, _ = self._parent._process_joins(self._key_expr, *named_exprs.values()). key_struct = self._key_expr; return Table(; ir.TableKeyByAndAggregate(; base._tir, hl.struct(**named_exprs)._ir, key_struct._ir, self._npartitions, self._buffer_size; ); ). [docs]class Table(ExprContainer):; """"""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-block:: text. +-------+-------+--------+; |",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:8250,Usability,guid,guide,8250,"ces, {self._parent._row_axis}); check_collisions(caller, list(named_exprs), self._parent._row_indices); if not named_exprs.keys().isdisjoint(set(self._key_expr)):; intersection = set(named_exprs.keys()) & set(self._key_expr); raise ValueError(; f'GroupedTable.aggregate: Group names and aggregration expression names overlap: {intersection}'; ). base, _ = self._parent._process_joins(self._key_expr, *named_exprs.values()). key_struct = self._key_expr; return Table(; ir.TableKeyByAndAggregate(; base._tir, hl.struct(**named_exprs)._ir, key_struct._ir, self._npartitions, self._buffer_size; ); ). [docs]class Table(ExprContainer):; """"""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-block:: text. +-------+-------+--------+; | ID | A | B |; +-------+-------+--------+; ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:21498,Usability,simpl,simple,21498,"ou mind.; """"""; context_name = f""context_{Env.get_uid()}""; ctype = contexts.dtype.element_type; cexpr = construct_expr(ir.Ref(context_name, ctype), ctype). globals_name = f""globals_{Env.get_uid()}""; globals = globals or hl.struct(); gexpr = construct_expr(ir.Ref(globals_name, globals.dtype), globals.dtype). body = ir.toStream(rowfn(cexpr, gexpr)._ir). if isinstance(partitions, int):; partitions = [Interval(hl.Struct(), hl.Struct(), True, True) for _ in range(partitions)]. partitioner = ir.Partitioner(partitions[0].point_type, partitions). return Table(ir.TableGen(ir.toStream(contexts._ir), globals._ir, context_name, globals_name, body, partitioner)). [docs] @typecheck_method(keys=oneof(str, expr_any), named_keys=expr_any); def key_by(self, *keys, **named_keys) -> 'Table':; """"""Key table by a new set of fields. Table keys control both the order of the rows in the table and the ability to join or; annotate one table with the information in another table. Examples; --------. Consider a simple unkeyed table. Its rows appear are guaranteed to appear in the same order; as they were in the source text file. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Changing the key forces the rows to appear in ascending order. For this reason,; :meth:`.key_by` is a relatively expensive operation. It must sort the entire dataset. >>> ht = ht.key_by('HT'); >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID |",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:71334,Usability,usab,usable,71334,"ect(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index table with given expressions:\n""; f"" Table key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" In",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:80411,Usability,guid,guide,80411,"able in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().unpersist(self). @overload; def collect(self) -> List[hl.Struct]: ... @overload; def collect(self, _localize=False) -> ArrayExpression: ... [docs] @typecheck_method(_localize=bool, _timed=bool); def collect(self, _localize=True, *, _timed=False):; """"""Collect the rows of the table into a local li",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:84704,Usability,simpl,simple,84704,"PUT_CHECK; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; -----. This method returns a table with a new field whose name is given by; the `name` parameter, with type :py:data:`.tint64`. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like :meth:`.Table.take` or :meth:`.Table.export`) will; return rows in order. This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """""". return self.annotate(**{name: hl.scan.count()}). [docs] @typecheck_method(tables=table_type, unify=bool); def union(self, *tables, unify: bool = False) -> 'Table':; """"""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an err",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:93033,Usability,guid,guide,93033,"n, shuffle=True) -> 'Table':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hail/table.html:94537,Usability,simpl,simply,94537," ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.Table`; Table with at most `max_partitions` partitions.; """"""; return Table(ir.TableRepartition(self._tir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] @typecheck_method(other=table_type); def semi_join(self, other: 'Table') -> 'Table':; """"""Filters the table to rows whose key appears in `other`. Parameters; ----------; other : :class:`.Table`; Table with compatible key field(s). Returns; -------; :class:`.Table`. Notes; -----; The key type of the table must match the key type of `other`. This method does not ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
https://hail.is/docs/0.2/_modules/hailtop/frozendict.html:1631,Deployability,update,updated,1631,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hailtop/frozendict.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html
https://hail.is/docs/0.2/_modules/hailtop/frozendict.html:1416,Security,hash,hash,1416,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hailtop/frozendict.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html
https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1738,Availability,avail,available,1738,"> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:2281,Availability,avail,available,2281,"netic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes =",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:2984,Availability,avail,available,2984,"one``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; r",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:3172,Availability,avail,available,3172,"ify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region];",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
