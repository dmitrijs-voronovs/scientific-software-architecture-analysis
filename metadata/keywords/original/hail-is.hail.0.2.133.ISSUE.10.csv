id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/10197:933,Modifiability,sandbox,sandbox,933,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ‚ÄãI get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:9445,Modifiability,sandbox,sandbox,9445,"hetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:9622,Modifiability,enhance,enhanced,9622,".3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:9880,Modifiability,sandbox,sandbox,9880,"numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.acc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10246,Modifiability,sandbox,sandbox,10246,"thlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10437,Modifiability,sandbox,sandbox,10437,"sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10701,Modifiability,sandbox,sandbox,10701,"---------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10990,Modifiability,sandbox,sandbox,10990,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:11218,Modifiability,sandbox,sandbox,11218,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:11424,Modifiability,sandbox,sandbox,11424,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:11602,Modifiability,sandbox,sandbox,11602,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:413,Performance,cache,cached,413,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ‚ÄãI get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:487,Performance,cache,cached,487,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ‚ÄãI get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1017,Performance,cache,cached,1017,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ‚ÄãI get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1090,Performance,cache,cached,1090,"et `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1178,Performance,cache,cached,1178,"nscript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1285,Performance,cache,cached,1285,"hon3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1371,Performance,cache,cached,1371,"ools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1455,Performance,cache,cached,1455," kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1542,Performance,cache,cached,1542,"lected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1613,Performance,cache,cached,1613,"ptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1701,Performance,cache,cached,1701," uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1783,Performance,cache,cached,1783,"lation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting deco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1857,Performance,cache,cached,1857,"ip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1939,Performance,cache,cached,1939,"ail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2018,Performance,cache,cached,2018," hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2098,Performance,cache,cached,2098,"cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2169,Performance,cache,cached,2169,"1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2279,Performance,cache,cached,2279,"ched python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2371,Performance,cache,cached,2371,"ched gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2453,Performance,cache,cached,2453,"ched requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2543,Performance,cache,cached,2543,"hed tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting mul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2620,Performance,cache,cached,2620,"t_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2722,Performance,cache,cached,2722,"py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2802,Performance,cache,cached,2802,"park-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:2889,Performance,cache,cached,2889,"whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3002,Performance,cache,cached,3002,"Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3100,Performance,cache,cached,3100,"ed PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3206,Performance,cache,cached,3206,"6_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3297,Performance,cache,cached,3297,"n-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3383,Performance,cache,cached,3383,"3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3477,Performance,cache,cached,3477,"one-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3569,Performance,cache,cached,3569,"one-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3672,Performance,cache,cached,3672,"B); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Usin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3768,Performance,cache,cached,3768,"y.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; U",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3864,Performance,cache,cached,3864,"aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3962,Performance,cache,cached,3962,"e-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4079,Performance,cache,cached,4079,".0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (66",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4330,Performance,cache,cached,4330,"whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4417,Performance,cache,cached,4417,"y.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4500,Performance,cache,cached,4500,"y3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4587,Performance,cache,cached,4587,"-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4680,Performance,cache,cached,4680,"gle_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4762,Performance,cache,cached,4762," cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4844,Performance,cache,cached,4844,"ec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collectin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:4945,Performance,cache,cached,4945,"0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting Mar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5036,Performance,cache,cached,5036,"e-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5128,Performance,cache,cached,5128,".whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5213,Performance,cache,cached,5213,"site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5301,Performance,cache,cached,5301,"<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collectin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5392,Performance,cache,cached,5392," idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5446,Performance,cache,cachetools,5446,"-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5476,Performance,cache,cached,5476,">=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5483,Performance,cache,cachetools-,5483,">=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5578,Performance,cache,cached,5578," cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5661,Performance,cache,cached,5661,"Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5769,Performance,cache,cached,5769,".9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5874,Performance,cache,cached,5874,"linux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:5974,Performance,cache,cached,5974,"y3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6077,Performance,cache,cached,6077,"4.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6169,Performance,cache,cached,6169,"25 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6250,Performance,cache,cached,6250,"_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6363,Performance,cache,cached,6363," rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6449,Performance,cache,cached,6449,"llecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6538,Performance,cache,cached,6538,"-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6618,Performance,cache,cached,6618,"ny.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6698,Performance,cache,cached,6698," (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthli",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6775,Performance,cache,cached,6775,"g cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6851,Performance,cache,cached,6851,"Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, num",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:6941,Performance,cache,cached,6941,"cting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:7024,Performance,cache,cached,7024,"whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJW",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:7104,Performance,cache,cached,7104,"-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:7190,Performance,cache,cached,7190,"sn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:7293,Performance,cache,cached,7293,"llecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:7388,Performance,cache,cached,7388,"py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 asyn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:7560,Performance,cache,cachetools,7560,"hed Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:8465,Performance,cache,cachetools-,8465,": six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:3452,Safety,timeout,timeout,3452,".0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:7745,Safety,timeout,timeout,7745,"hed Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:8395,Safety,timeout,timeout-,8395,"authlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:933,Testability,sandbox,sandbox,933,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ‚ÄãI get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:1263,Testability,log,logger,1263,"ctivate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:8009,Testability,log,logger,8009,"hed Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:9200,Testability,log,logger-,9200,"okeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:9445,Testability,sandbox,sandbox,9445,"hetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:9880,Testability,sandbox,sandbox,9880,"numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.acc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10246,Testability,sandbox,sandbox,10246,"thlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ‚úî ~/sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10437,Testability,sandbox,sandbox,10437,"sandbox/hail [master|ùö´8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10701,Testability,sandbox,sandbox,10701,"---------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:10990,Testability,sandbox,sandbox,10990,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:11218,Testability,sandbox,sandbox,11218,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:11424,Testability,sandbox,sandbox,11424,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/issues/10197:11602,Testability,sandbox,sandbox,11602,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197
https://github.com/hail-is/hail/pull/10200:114,Energy Efficiency,allocate,allocate,114,"And lots of other things along the way. Summary of changes:; - Add RawArenaAllocator; - Add runtime to call arena allocate from generated code; - Support MakeTuple, GetTupleElement; - Intern STypes, add STypeContext; - Add EmitValue.cast_to, SType.{load_from_address, construct_from_value, construct_at_address_from_value}; - Add SType.get_memory_size, only works for types for which is_heap_stype<T>() is true; - Use phi nodes to implement EmitValue.as_data(); - isa now calls T::is_instance_tag rather than checking tag directly",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10200
https://github.com/hail-is/hail/pull/10201:64,Availability,checkpoint,checkpointed,64,"This should make computing the loadings better, since it uses a checkpointed variants table instead of accidentally recomputing the incoming MatrixTable. Also made a change to avoid accidentally clobbering a field name if someone had a field named `idx`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10201
https://github.com/hail-is/hail/pull/10201:31,Performance,load,loadings,31,"This should make computing the loadings better, since it uses a checkpointed variants table instead of accidentally recomputing the incoming MatrixTable. Also made a change to avoid accidentally clobbering a field name if someone had a field named `idx`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10201
https://github.com/hail-is/hail/pull/10201:176,Safety,avoid,avoid,176,"This should make computing the loadings better, since it uses a checkpointed variants table instead of accidentally recomputing the incoming MatrixTable. Also made a change to avoid accidentally clobbering a field name if someone had a field named `idx`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10201
https://github.com/hail-is/hail/pull/10203:707,Deployability,deploy,deployments,707,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10203:44,Integrability,rout,router-resolver,44,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10203:90,Integrability,rout,router-resolver,90,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10203:265,Integrability,rout,router,265,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10203:317,Integrability,rout,router-resolver,317,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10203:441,Integrability,rout,router-resolver,441,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10203:772,Integrability,rout,router-resolver,772,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10203:830,Integrability,rout,router-resolver,830,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10203
https://github.com/hail-is/hail/pull/10204:27,Deployability,configurat,configuration,27,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:48,Integrability,rout,router,48,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:138,Integrability,rout,router,138,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:195,Integrability,rout,routing,195,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:27,Modifiability,config,configuration,27,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:109,Modifiability,extend,extends,109,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:203,Testability,log,logic,203,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:231,Testability,test,test,231,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:285,Testability,test,tested,285,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10204:312,Testability,log,log,312,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10204
https://github.com/hail-is/hail/pull/10207:165,Integrability,rout,router-resolver,165,"This PR only impacts `internal.hail.is`. The current internal flow is:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. `auth_request` to router-resolver for auth & router ip, and in router-resolver; a. request to auth that the user is a signed in developer; b. retrieve router ip from k8s; 3. `proxy_pass` to router IP returned from router-resolver in $namespace which proxies to $service in $namespace. This changes the flow to:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. request to auth that the user is a signed in developer; 3. `proxy_pass` to $service in $namespace. This does impose the restriction that there is an equivalence between subdomains and k8s Services. For the most part we enforce this just by convention, with the exception of `www` -> `website`. `router` currently does this mapping and is the only capability that this version drops. However, `www` in internal is already broken, I think because `website` does not consider `www` as potentially part of its base path, so that might need to be considered on its own. Aside from the `base_path` problem, I think it would make sense to create alias k8s Services like we do with notebook/workshop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10207
https://github.com/hail-is/hail/pull/10207:192,Integrability,rout,router,192,"This PR only impacts `internal.hail.is`. The current internal flow is:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. `auth_request` to router-resolver for auth & router ip, and in router-resolver; a. request to auth that the user is a signed in developer; b. retrieve router ip from k8s; 3. `proxy_pass` to router IP returned from router-resolver in $namespace which proxies to $service in $namespace. This changes the flow to:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. request to auth that the user is a signed in developer; 3. `proxy_pass` to $service in $namespace. This does impose the restriction that there is an equivalence between subdomains and k8s Services. For the most part we enforce this just by convention, with the exception of `www` -> `website`. `router` currently does this mapping and is the only capability that this version drops. However, `www` in internal is already broken, I think because `website` does not consider `www` as potentially part of its base path, so that might need to be considered on its own. Aside from the `base_path` problem, I think it would make sense to create alias k8s Services like we do with notebook/workshop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10207
https://github.com/hail-is/hail/pull/10207:210,Integrability,rout,router-resolver,210,"This PR only impacts `internal.hail.is`. The current internal flow is:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. `auth_request` to router-resolver for auth & router ip, and in router-resolver; a. request to auth that the user is a signed in developer; b. retrieve router ip from k8s; 3. `proxy_pass` to router IP returned from router-resolver in $namespace which proxies to $service in $namespace. This changes the flow to:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. request to auth that the user is a signed in developer; 3. `proxy_pass` to $service in $namespace. This does impose the restriction that there is an equivalence between subdomains and k8s Services. For the most part we enforce this just by convention, with the exception of `www` -> `website`. `router` currently does this mapping and is the only capability that this version drops. However, `www` in internal is already broken, I think because `website` does not consider `www` as potentially part of its base path, so that might need to be considered on its own. Aside from the `base_path` problem, I think it would make sense to create alias k8s Services like we do with notebook/workshop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10207
https://github.com/hail-is/hail/pull/10207:298,Integrability,rout,router,298,"This PR only impacts `internal.hail.is`. The current internal flow is:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. `auth_request` to router-resolver for auth & router ip, and in router-resolver; a. request to auth that the user is a signed in developer; b. retrieve router ip from k8s; 3. `proxy_pass` to router IP returned from router-resolver in $namespace which proxies to $service in $namespace. This changes the flow to:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. request to auth that the user is a signed in developer; 3. `proxy_pass` to $service in $namespace. This does impose the restriction that there is an equivalence between subdomains and k8s Services. For the most part we enforce this just by convention, with the exception of `www` -> `website`. `router` currently does this mapping and is the only capability that this version drops. However, `www` in internal is already broken, I think because `website` does not consider `www` as potentially part of its base path, so that might need to be considered on its own. Aside from the `base_path` problem, I think it would make sense to create alias k8s Services like we do with notebook/workshop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10207
https://github.com/hail-is/hail/pull/10207:337,Integrability,rout,router,337,"This PR only impacts `internal.hail.is`. The current internal flow is:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. `auth_request` to router-resolver for auth & router ip, and in router-resolver; a. request to auth that the user is a signed in developer; b. retrieve router ip from k8s; 3. `proxy_pass` to router IP returned from router-resolver in $namespace which proxies to $service in $namespace. This changes the flow to:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. request to auth that the user is a signed in developer; 3. `proxy_pass` to $service in $namespace. This does impose the restriction that there is an equivalence between subdomains and k8s Services. For the most part we enforce this just by convention, with the exception of `www` -> `website`. `router` currently does this mapping and is the only capability that this version drops. However, `www` in internal is already broken, I think because `website` does not consider `www` as potentially part of its base path, so that might need to be considered on its own. Aside from the `base_path` problem, I think it would make sense to create alias k8s Services like we do with notebook/workshop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10207
https://github.com/hail-is/hail/pull/10207:361,Integrability,rout,router-resolver,361,"This PR only impacts `internal.hail.is`. The current internal flow is:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. `auth_request` to router-resolver for auth & router ip, and in router-resolver; a. request to auth that the user is a signed in developer; b. retrieve router ip from k8s; 3. `proxy_pass` to router IP returned from router-resolver in $namespace which proxies to $service in $namespace. This changes the flow to:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. request to auth that the user is a signed in developer; 3. `proxy_pass` to $service in $namespace. This does impose the restriction that there is an equivalence between subdomains and k8s Services. For the most part we enforce this just by convention, with the exception of `www` -> `website`. `router` currently does this mapping and is the only capability that this version drops. However, `www` in internal is already broken, I think because `website` does not consider `www` as potentially part of its base path, so that might need to be considered on its own. Aside from the `base_path` problem, I think it would make sense to create alias k8s Services like we do with notebook/workshop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10207
https://github.com/hail-is/hail/pull/10207:829,Integrability,rout,router,829,"This PR only impacts `internal.hail.is`. The current internal flow is:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. `auth_request` to router-resolver for auth & router ip, and in router-resolver; a. request to auth that the user is a signed in developer; b. retrieve router ip from k8s; 3. `proxy_pass` to router IP returned from router-resolver in $namespace which proxies to $service in $namespace. This changes the flow to:; 1. gateway receives request for `internal.hail.is/$namespace/$service`; 2. request to auth that the user is a signed in developer; 3. `proxy_pass` to $service in $namespace. This does impose the restriction that there is an equivalence between subdomains and k8s Services. For the most part we enforce this just by convention, with the exception of `www` -> `website`. `router` currently does this mapping and is the only capability that this version drops. However, `www` in internal is already broken, I think because `website` does not consider `www` as potentially part of its base path, so that might need to be considered on its own. Aside from the `base_path` problem, I think it would make sense to create alias k8s Services like we do with notebook/workshop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10207
https://github.com/hail-is/hail/pull/10209:3622,Availability,error,error,3622,"ourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3719,Availability,error,error,3719,"ourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:4170,Availability,error,error,4170,"cts with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.11.1</h2>; <p>Released 2020-01-30</p>; <ul>; <li>Fix a bug that prevented looking up a key after an attribute; (<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:93,Deployability,Release,Release,93,"Bumps [jinja2](https://github.com/pallets/jinja) from 2.10.1 to 2.11.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/releases"">jinja2's releases</a>.</em></p>; <blockquote>; <h2>2.11.3</h2>; <p>This contains a fix for a speed issue with the <code>urlize</code> filter. <code>urlize</code> is likely to be called on untrusted user input. For certain inputs some of the regular expressions used to parse the text could take a very long time due to backtracking. As part of the fix, the email matching became slightly stricter. The various speedups apply to <code>urlize</code> in general, not just the specific input cases.</p>; <ul>; <li>PyPI: <a href=""https://pypi.org/project/Jinja2/2.11.3/"">https://pypi.org/project/Jinja2/2.11.3/</a></li>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3</a></li>; </ul>; <h2>2.11.2</h2>; <ul>; <li>Changelog: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2</a></li>; </ul>; <h2>2.11.1</h2>; <p>This fixes an issue in async environment when indexing the result of an attribute lookup, like <code>{{ data.items[1:] }}</code>.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1</a></li>; </ul>; <h2>2.11.0</h2>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0</a></li>; <li>Blog: <a href=""https://palletsprojects.com/blog/jinja-2-11-0-released/"">https://palletsprojects.com/blog/jinja-2-11-0-released/</a></li>; <li>Twitter: <a href=""https://twitter.com/PalletsTeam/status/1221883554537230336"">https://twitter.com/PalletsTeam/status/1221883554",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:180,Deployability,release,releases,180,"Bumps [jinja2](https://github.com/pallets/jinja) from 2.10.1 to 2.11.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/releases"">jinja2's releases</a>.</em></p>; <blockquote>; <h2>2.11.3</h2>; <p>This contains a fix for a speed issue with the <code>urlize</code> filter. <code>urlize</code> is likely to be called on untrusted user input. For certain inputs some of the regular expressions used to parse the text could take a very long time due to backtracking. As part of the fix, the email matching became slightly stricter. The various speedups apply to <code>urlize</code> in general, not just the specific input cases.</p>; <ul>; <li>PyPI: <a href=""https://pypi.org/project/Jinja2/2.11.3/"">https://pypi.org/project/Jinja2/2.11.3/</a></li>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3</a></li>; </ul>; <h2>2.11.2</h2>; <ul>; <li>Changelog: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2</a></li>; </ul>; <h2>2.11.1</h2>; <p>This fixes an issue in async environment when indexing the result of an attribute lookup, like <code>{{ data.items[1:] }}</code>.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1</a></li>; </ul>; <h2>2.11.0</h2>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0</a></li>; <li>Blog: <a href=""https://palletsprojects.com/blog/jinja-2-11-0-released/"">https://palletsprojects.com/blog/jinja-2-11-0-released/</a></li>; <li>Twitter: <a href=""https://twitter.com/PalletsTeam/status/1221883554537230336"">https://twitter.com/PalletsTeam/status/1221883554",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:199,Deployability,release,releases,199,"Bumps [jinja2](https://github.com/pallets/jinja) from 2.10.1 to 2.11.3.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/releases"">jinja2's releases</a>.</em></p>; <blockquote>; <h2>2.11.3</h2>; <p>This contains a fix for a speed issue with the <code>urlize</code> filter. <code>urlize</code> is likely to be called on untrusted user input. For certain inputs some of the regular expressions used to parse the text could take a very long time due to backtracking. As part of the fix, the email matching became slightly stricter. The various speedups apply to <code>urlize</code> in general, not just the specific input cases.</p>; <ul>; <li>PyPI: <a href=""https://pypi.org/project/Jinja2/2.11.3/"">https://pypi.org/project/Jinja2/2.11.3/</a></li>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3</a></li>; </ul>; <h2>2.11.2</h2>; <ul>; <li>Changelog: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2</a></li>; </ul>; <h2>2.11.1</h2>; <p>This fixes an issue in async environment when indexing the result of an attribute lookup, like <code>{{ data.items[1:] }}</code>.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1</a></li>; </ul>; <h2>2.11.0</h2>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0</a></li>; <li>Blog: <a href=""https://palletsprojects.com/blog/jinja-2-11-0-released/"">https://palletsprojects.com/blog/jinja-2-11-0-released/</a></li>; <li>Twitter: <a href=""https://twitter.com/PalletsTeam/status/1221883554537230336"">https://twitter.com/PalletsTeam/status/1221883554",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:1793,Deployability,release,released,1793,"</li>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3</a></li>; </ul>; <h2>2.11.2</h2>; <ul>; <li>Changelog: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2</a></li>; </ul>; <h2>2.11.1</h2>; <p>This fixes an issue in async environment when indexing the result of an attribute lookup, like <code>{{ data.items[1:] }}</code>.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1</a></li>; </ul>; <h2>2.11.0</h2>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0</a></li>; <li>Blog: <a href=""https://palletsprojects.com/blog/jinja-2-11-0-released/"">https://palletsprojects.com/blog/jinja-2-11-0-released/</a></li>; <li>Twitter: <a href=""https://twitter.com/PalletsTeam/status/1221883554537230336"">https://twitter.com/PalletsTeam/status/1221883554537230336</a></li>; </ul>; <p>This is the last version to support Python 2.7 and 3.5. The next version will be Jinja 3.0 and will support Python 3.6 and newer.</p>; <h2>2.10.3</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3</a></li>; </ul>; <h2>2.10.2</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:1850,Deployability,release,released,1850,"1.x/changelog/#version-2-11-3"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-3</a></li>; </ul>; <h2>2.11.2</h2>; <ul>; <li>Changelog: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-2</a></li>; </ul>; <h2>2.11.1</h2>; <p>This fixes an issue in async environment when indexing the result of an attribute lookup, like <code>{{ data.items[1:] }}</code>.</p>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-1</a></li>; </ul>; <h2>2.11.0</h2>; <ul>; <li>Changes: <a href=""https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0"">https://jinja.palletsprojects.com/en/2.11.x/changelog/#version-2-11-0</a></li>; <li>Blog: <a href=""https://palletsprojects.com/blog/jinja-2-11-0-released/"">https://palletsprojects.com/blog/jinja-2-11-0-released/</a></li>; <li>Twitter: <a href=""https://twitter.com/PalletsTeam/status/1221883554537230336"">https://twitter.com/PalletsTeam/status/1221883554537230336</a></li>; </ul>; <p>This is the last version to support Python 2.7 and 3.5. The next version will be Jinja 3.0 and will support Python 3.6 and newer.</p>; <h2>2.10.3</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3</a></li>; </ul>; <h2>2.10.2</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>url",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:2799,Deployability,Release,Released,2799,"eased/</a></li>; <li>Twitter: <a href=""https://twitter.com/PalletsTeam/status/1221883554537230336"">https://twitter.com/PalletsTeam/status/1221883554537230336</a></li>; </ul>; <p>This is the last version to support Python 2.7 and 3.5. The next version will be Jinja 3.0 and will support Python 3.6 and newer.</p>; <h2>2.10.3</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3</a></li>; </ul>; <h2>2.10.2</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3094,Deployability,Release,Released,3094,"ewer.</p>; <h2>2.10.3</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3</a></li>; </ul>; <h2>2.10.2</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3336,Deployability,Update,Update,3336,"; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix white",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:4777,Deployability,Release,Released,4777," 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.11.1</h2>; <p>Released 2020-01-30</p>; <ul>; <li>Fix a bug that prevented looking up a key after an attribute; (<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.depen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:5213,Deployability,release,release,5213,"messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.11.1</h2>; <p>Released 2020-01-30</p>; <ul>; <li>Fix a bug that prevented looking up a key after an attribute; (<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:5888,Deployability,patch,patch-,5888,"{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:6139,Deployability,patch,patch-,6139,"jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:6286,Deployability,update,update,6286,"b.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:6766,Deployability,release,release,6766,"d61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7486,Deployability,update,updates,7486,"ocs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8638,Deployability,upgrade,upgrade,8638,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8814,Deployability,upgrade,upgrade,8814,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8984,Deployability,upgrade,upgrade,8984,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3426,Integrability,wrap,wrapping,3426,"; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix white",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3852,Integrability,depend,dependency,3852," reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.11.1</h2>; <p>Released 2020-01-30</p>; <ul>; <li>Fix a bug that prevented looking up a key after an attribute; (<code>{{ data",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:4176,Integrability,message,messages,4176,"cts with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.11.1</h2>; <p>Released 2020-01-30</p>; <ul>; <li>Fix a bug that prevented looking up a key after an attribute; (<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:5414,Integrability,depend,dependabot,5414," in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.11.1</h2>; <p>Released 2020-01-30</p>; <ul>; <li>Fix a bug that prevented looking up a key after an attribute; (<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:5823,Integrability,depend,dependabot,5823,"revented looking up a key after an attribute; (<code>{{ data.items[1:] }}</code>) in an async template. :issue:<code>1141</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:6074,Integrability,depend,dependabot,6074,"mmary>; <ul>; <li><a href=""https://github.com/pallets/jinja/commit/cf215390d4a4d6f0a4de27e2687eed176878f13d""><code>cf21539</code></a> release version 2.11.3</li>; <li><a href=""https://github.com/pallets/jinja/commit/15ef8f09b659f9100610583938005a7a10472d4d""><code>15ef8f0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1343"">#1343</a> from pallets/urlize-speedup</li>; <li><a href=""https://github.com/pallets/jinja/commit/ef658dc3b6389b091d608e710a810ce8b87995b3""><code>ef658dc</code></a> speed up urlize matching</li>; <li><a href=""https://github.com/pallets/jinja/commit/eeca0fecc3318d43f61bc340ad61db641b861ade""><code>eeca0fe</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1207"">#1207</a> from mhansen/patch-1</li>; <li><a href=""https://github.com/pallets/jinja/commit/2dd769111cbb1a2637f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:6967,Integrability,depend,dependabot,6967,"7f805b3b4c652ec8096d371""><code>2dd7691</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1209"">#1209</a> from mhansen/patch-3</li>; <li><a href=""https://github.com/pallets/jinja/commit/48929401db7228db04dfd8e88115dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` wil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7205,Integrability,Depend,Dependabot,7205,"15dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7245,Integrability,depend,dependabot-badges,7245,"15dd5c30dc2d86""><code>4892940</code></a> do_dictsort: update example ready to copy/paste</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7304,Integrability,depend,dependency-name,7304,"e</li>; <li><a href=""https://github.com/pallets/jinja/commit/7db7d336ba12574e6205fdd929386fd529e3fad4""><code>7db7d33</code></a> api.rst: bugfix in docs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7466,Integrability,depend,dependabot-security-updates,7466,"ocs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7523,Integrability,Depend,Dependabot,7523,"05bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7669,Integrability,depend,dependabot,7669,"<a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7698,Integrability,depend,dependabot-automerge-start,7698,"62f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7736,Integrability,depend,dependabot-automerge-end,7736,"62f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7788,Integrability,Depend,Dependabot,7788,"href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7855,Integrability,Depend,Dependabot,7855,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7905,Integrability,depend,dependabot,7905,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7949,Integrability,depend,dependabot,7949,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8046,Integrability,depend,dependabot,8046,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8115,Integrability,depend,dependabot,8115,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8206,Integrability,depend,dependabot,8206,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8299,Integrability,depend,dependabot,8299,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8359,Integrability,depend,dependabot,8359,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8405,Integrability,Depend,Dependabot,8405,"/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8491,Integrability,depend,dependabot,8491,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8557,Integrability,Depend,Dependabot,8557,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8667,Integrability,depend,dependabot,8667,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8733,Integrability,Depend,Dependabot,8733,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8843,Integrability,depend,dependabot,8843,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8866,Integrability,depend,dependency,8866,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8906,Integrability,Depend,Dependabot,8906,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:8944,Integrability,depend,dependency,8944,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:9013,Integrability,depend,dependabot,9013,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:9132,Integrability,depend,dependabot,9132,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:9257,Integrability,depend,dependabot,9257,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:9382,Integrability,depend,dependabot,9382,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3794,Performance,load,loaders,3794,"quote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents early; evaluation which could change the value of an expression.; :issue:<code>1186</code></li>; </ul>; <h2>Version 2.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3686,Security,access,access,3686,"ourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>namespace()</code> assignment object in templates works in; async environments. :issue:<code>1180</code></li>; <li>Fix whitespace being removed before tags in the middle of lines when; <code>lstrip_blocks</code> is enabled. :issue:<code>1138</code></li>; <li>:class:<code>~nativetypes.NativeEnvironment</code> doesn't evaluate; intermediate strings during rendering. This prevents ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7435,Security,secur,security-vulnerabilities,7435,"ocs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:7477,Security,secur,security-updates,7477,"ocs, import PackageLoader</li>; <li><a href=""https://github.com/pallets/jinja/commit/9ec465baefe32e305bd4e61da49e6c39360c194e""><code>9ec465b</code></a> fix changelog header</li>; <li><a href=""https://github.com/pallets/jinja/commit/737a4cd41d09878e7e6c584a2062f5853dc30150""><code>737a4cd</code></a> release version 2.11.2</li>; <li><a href=""https://github.com/pallets/jinja/commit/179df6b54e87b3d420cabf65fc07b2605ffc05f8""><code>179df6b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/jinja/issues/1190"">#1190</a> from pallets/native-eval</li>; <li>Additional commits viewable in <a href=""https://github.com/pallets/jinja/compare/2.10.1...2.11.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jinja2&package-manager=pip&previous-version=2.10.1&new-version=2.11.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:9528,Security,secur,security,9528,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:9569,Security,Secur,Security,9569,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3228,Testability,mock,mock,3228,"sprojects.com/en/2.10.x/changelog/#version-2-10-3"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3</a></li>; </ul>; <h2>2.10.2</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10209:3233,Testability,Mock,Mock,3233,"ersion-2-10-3"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-3</a></li>; </ul>; <h2>2.10.2</h2>; <ul>; <li>Changes: <a href=""http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2"">http://jinja.palletsprojects.com/en/2.10.x/changelog/#version-2-10-2</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pallets/jinja/blob/master/CHANGES.rst"">jinja2's changelog</a>.</em></p>; <blockquote>; <h2>Version 2.11.3</h2>; <p>Released 2021-01-31</p>; <ul>; <li>Improve the speed of the <code>urlize</code> filter by reducing regex; backtracking. Email matching requires a word character at the start; of the domain part, and only word characters in the TLD. :pr:<code>1343</code></li>; </ul>; <h2>Version 2.11.2</h2>; <p>Released 2020-04-13</p>; <ul>; <li>Fix a bug that caused callable objects with <code>__getattr__</code>, like; :class:<code>~unittest.mock.Mock</code> to be treated as a; :func:<code>contextfunction</code>. :issue:<code>1145</code></li>; <li>Update <code>wordcount</code> filter to trigger :class:<code>Undefined</code> methods; by wrapping the input in :func:<code>soft_str</code>. :pr:<code>1160</code></li>; <li>Fix a hang when displaying tracebacks on Python 32-bit.; :issue:<code>1162</code></li>; <li>Showing an undefined error for an object that raises; <code>AttributeError</code> on access doesn't cause a recursion error.; :issue:<code>1177</code></li>; <li>Revert changes to :class:<code>~loaders.PackageLoader</code> from 2.10 which; removed the dependency on setuptools and pkg_resources, and added; limited support for namespace packages. The changes caused issues; when using Pytest. Due to the difficulty in supporting Python 2 and; :pep:<code>451</code> simultaneously, the changes are reverted until 3.0.; :pr:<code>1182</code></li>; <li>Fix line numbers in error messages when newlines are stripped.; :pr:<code>1178</code></li>; <li>The special <code>nam",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10209
https://github.com/hail-is/hail/pull/10210:15,Energy Efficiency,monitor,monitoring,15,"Added endpoint monitoring to auth, query, memory, atgu, workshop and website, as well as removed an unused label on address.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10210
https://github.com/hail-is/hail/pull/10212:711,Deployability,deploy,deployment,711,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:50,Integrability,rout,router-resolver,50,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:72,Integrability,rout,router,72,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:101,Integrability,rout,route,101,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:243,Integrability,rout,router-resolver,243,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:291,Integrability,rout,router,291,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:428,Integrability,rout,router,428,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:495,Integrability,rout,router-resolver,495,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10212:694,Integrability,rout,router-resolver,694,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10212
https://github.com/hail-is/hail/pull/10215:575,Availability,checkpoint,checkpoint,575,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10215:347,Integrability,message,messages,347,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10215:2257,Integrability,depend,dependencies,2257,"le(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_readback = hl.read_matrix_table('/tmp/mt_temp4.mt'); ```. I think it would be cool to make this an automatic default feature. The version, a digest of the IR, and the global random seed; seem necessary, but we would really also need some digest of all file dependencies to ensure we're really resuming the same; computation, and that's hard. I also don't have a plan for supporting this in the lowered execution path at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10215:217,Performance,perform,performance,217,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10215:297,Performance,perform,performance,297,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10215:91,Testability,test,tested,91,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10215:113,Testability,test,test,113,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10215:339,Testability,Log,Logging,339,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10215
https://github.com/hail-is/hail/pull/10217:162,Deployability,deploy,deploy,162,"For some reason we delete secrets when re-creating them. This creates a race condition which Batch; appears to encounter whenever it is under heavy load during a deploy. @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I am using the command sequence suggested here: https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10217
https://github.com/hail-is/hail/pull/10217:365,Deployability,update,update-a-secret-on-kubernetes-when-it-is-generated-from-a-file,365,"For some reason we delete secrets when re-creating them. This creates a race condition which Batch; appears to encounter whenever it is under heavy load during a deploy. @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I am using the command sequence suggested here: https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10217
https://github.com/hail-is/hail/pull/10217:72,Performance,race condition,race condition,72,"For some reason we delete secrets when re-creating them. This creates a race condition which Batch; appears to encounter whenever it is under heavy load during a deploy. @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I am using the command sequence suggested here: https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10217
https://github.com/hail-is/hail/pull/10217:148,Performance,load,load,148,"For some reason we delete secrets when re-creating them. This creates a race condition which Batch; appears to encounter whenever it is under heavy load during a deploy. @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I am using the command sequence suggested here: https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10217
https://github.com/hail-is/hail/pull/10223:47,Performance,load,loadings,47,"We use Q twice if someone wants to compute the loadings, so we want to save it to avoid redoing full QR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10223
https://github.com/hail-is/hail/pull/10223:82,Safety,avoid,avoid,82,"We use Q twice if someone wants to compute the loadings, so we want to save it to avoid redoing full QR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10223
https://github.com/hail-is/hail/pull/10224:4,Testability,test,testing,4,"Was testing that we could run with newer pythons, encountered some warnings to clean up. - We aren't supposed to use `collections.Sequence` anymore. It's `collections.abc.Sequence`. They said this wouldn't work in 3.9, but they decided to delay removal again to 3.10.; - `np.bool` is just the same as `bool`, so numpy has deprecated `np.bool`. ; - We deprecatd `hl.null` in favor of `hl.missing` a little while ago.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10224
https://github.com/hail-is/hail/pull/10225:7,Performance,latency,latency,7,Tracks latency and status code metrics on endpoints so we can see those too in the batch grafana dashboard.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10225
https://github.com/hail-is/hail/pull/10227:241,Energy Efficiency,allocate,allocate,241,"I'm not sure this is the right fix. The segfault was coming from dgeqrf itself, and it was happening only when `N` is 0 (`M` being 0 was fine). The real issue is that when `N` is 0, LAPACK computes `LWORK` to be 0, meaning we don't actually allocate any memory for the `WORK` array and we segfault. To contrast, if I do a workspace size query for shape `(0, 10)`, I get `320` for `LWORK`. This also seems wrong though, as you shouldn't really need any work space for an empty matrix? . Anyway, the segfault preventing fix was just to make sure LWORK was at least 1. I was hoping you (Patrick) could use Julia interface to play with the LAPACK functions and see what behavior you notice, and if it's consistent with the above.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10227
https://github.com/hail-is/hail/pull/10227:609,Integrability,interface,interface,609,"I'm not sure this is the right fix. The segfault was coming from dgeqrf itself, and it was happening only when `N` is 0 (`M` being 0 was fine). The real issue is that when `N` is 0, LAPACK computes `LWORK` to be 0, meaning we don't actually allocate any memory for the `WORK` array and we segfault. To contrast, if I do a workspace size query for shape `(0, 10)`, I get `320` for `LWORK`. This also seems wrong though, as you shouldn't really need any work space for an empty matrix? . Anyway, the segfault preventing fix was just to make sure LWORK was at least 1. I was hoping you (Patrick) could use Julia interface to play with the LAPACK functions and see what behavior you notice, and if it's consistent with the above.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10227
https://github.com/hail-is/hail/pull/10228:112,Security,password,password,112,"I also changed the highlight color for the GitHub icon because it was too dark to clearly see. The username and password for our Font Awesome account are in the usual place. I also had a bit too much fun using ""GIPHY CAPTURE"", Giphy's Mac video capture app to create this gif of my change:; ![highlighticon](https://user-images.githubusercontent.com/106194/112545998-4147db00-8d8f-11eb-893e-6f01ea76b79b.gif)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10228
https://github.com/hail-is/hail/pull/10228:82,Usability,clear,clearly,82,"I also changed the highlight color for the GitHub icon because it was too dark to clearly see. The username and password for our Font Awesome account are in the usual place. I also had a bit too much fun using ""GIPHY CAPTURE"", Giphy's Mac video capture app to create this gif of my change:; ![highlighticon](https://user-images.githubusercontent.com/106194/112545998-4147db00-8d8f-11eb-893e-6f01ea76b79b.gif)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10228
https://github.com/hail-is/hail/pull/10230:247,Availability,down,downscale,247,This must have been skipped or lost in a rebase but keeping the resources at minimum is keeping the notebook deployment at max replicas after the workshop. Giving a bit higher request (what I set as the baseline in #10117) should encourage k8s to downscale back to 3 replicas.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10230
https://github.com/hail-is/hail/pull/10230:109,Deployability,deploy,deployment,109,This must have been skipped or lost in a rebase but keeping the resources at minimum is keeping the notebook deployment at max replicas after the workshop. Giving a bit higher request (what I set as the baseline in #10117) should encourage k8s to downscale back to 3 replicas.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10230
https://github.com/hail-is/hail/pull/10231:392,Availability,failure,failure,392,"Very preliminary. Builds test instead of main by default, output looks like this:. ```; ~/hail/libhail/build$ ./test; RUN test_int64_value; RUN test_int64_value OK; RUN test_int32_value; RUN test_int32_value OK; RUN test_bool_value; RUN test_bool_value OK; ...; ```. Added `std::string render(...)` to format module. Added some format tests. Added `CHECK_EQ` macro that prints the details on failure:. ```; ../src/hail/test.hpp:42: assert failed:; CHECK_EQ(render(FormatAddress(nullptr)), ""0x0000000000000000""); with values:; CHECK_EQ(0000000000000000, 0x0000000000000000); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10231
https://github.com/hail-is/hail/pull/10231:25,Testability,test,test,25,"Very preliminary. Builds test instead of main by default, output looks like this:. ```; ~/hail/libhail/build$ ./test; RUN test_int64_value; RUN test_int64_value OK; RUN test_int32_value; RUN test_int32_value OK; RUN test_bool_value; RUN test_bool_value OK; ...; ```. Added `std::string render(...)` to format module. Added some format tests. Added `CHECK_EQ` macro that prints the details on failure:. ```; ../src/hail/test.hpp:42: assert failed:; CHECK_EQ(render(FormatAddress(nullptr)), ""0x0000000000000000""); with values:; CHECK_EQ(0000000000000000, 0x0000000000000000); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10231
https://github.com/hail-is/hail/pull/10231:112,Testability,test,test,112,"Very preliminary. Builds test instead of main by default, output looks like this:. ```; ~/hail/libhail/build$ ./test; RUN test_int64_value; RUN test_int64_value OK; RUN test_int32_value; RUN test_int32_value OK; RUN test_bool_value; RUN test_bool_value OK; ...; ```. Added `std::string render(...)` to format module. Added some format tests. Added `CHECK_EQ` macro that prints the details on failure:. ```; ../src/hail/test.hpp:42: assert failed:; CHECK_EQ(render(FormatAddress(nullptr)), ""0x0000000000000000""); with values:; CHECK_EQ(0000000000000000, 0x0000000000000000); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10231
https://github.com/hail-is/hail/pull/10231:335,Testability,test,tests,335,"Very preliminary. Builds test instead of main by default, output looks like this:. ```; ~/hail/libhail/build$ ./test; RUN test_int64_value; RUN test_int64_value OK; RUN test_int32_value; RUN test_int32_value OK; RUN test_bool_value; RUN test_bool_value OK; ...; ```. Added `std::string render(...)` to format module. Added some format tests. Added `CHECK_EQ` macro that prints the details on failure:. ```; ../src/hail/test.hpp:42: assert failed:; CHECK_EQ(render(FormatAddress(nullptr)), ""0x0000000000000000""); with values:; CHECK_EQ(0000000000000000, 0x0000000000000000); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10231
https://github.com/hail-is/hail/pull/10231:419,Testability,test,test,419,"Very preliminary. Builds test instead of main by default, output looks like this:. ```; ~/hail/libhail/build$ ./test; RUN test_int64_value; RUN test_int64_value OK; RUN test_int32_value; RUN test_int32_value OK; RUN test_bool_value; RUN test_bool_value OK; ...; ```. Added `std::string render(...)` to format module. Added some format tests. Added `CHECK_EQ` macro that prints the details on failure:. ```; ../src/hail/test.hpp:42: assert failed:; CHECK_EQ(render(FormatAddress(nullptr)), ""0x0000000000000000""); with values:; CHECK_EQ(0000000000000000, 0x0000000000000000); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10231
https://github.com/hail-is/hail/pull/10231:432,Testability,assert,assert,432,"Very preliminary. Builds test instead of main by default, output looks like this:. ```; ~/hail/libhail/build$ ./test; RUN test_int64_value; RUN test_int64_value OK; RUN test_int32_value; RUN test_int32_value OK; RUN test_bool_value; RUN test_bool_value OK; ...; ```. Added `std::string render(...)` to format module. Added some format tests. Added `CHECK_EQ` macro that prints the details on failure:. ```; ../src/hail/test.hpp:42: assert failed:; CHECK_EQ(render(FormatAddress(nullptr)), ""0x0000000000000000""); with values:; CHECK_EQ(0000000000000000, 0x0000000000000000); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10231
https://github.com/hail-is/hail/pull/10232:470,Testability,test,tests,470,"`json4s` was the main problem with supporting Spark 2.4.x and 3.1.1 simultaneously. I think if I remove our use of `typeHintFieldName`, we might be able to use both versions of `json4s` interchangeably, and thus use both Sparks. . As it turns out, AWS EMR only supports Spark 3.0.1 one right now, which also uses the older version of `json4s`. It seems therefore like supporting this is going to be a necessity. . Right now, just PRing to see if my changes pass current tests with Spark 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10232
https://github.com/hail-is/hail/pull/10233:102,Energy Efficiency,allocate,allocated,102,"This PR changes the way loop memory management works. . Previously, memory used while in the loop was allocated in one region, with no cleanup happening between loops. A long running loop that did a lot of allocation could therefore easily cause hail to run out of memory. This PR fixes that by creating two regions for each loop. Every iteration, we emit everything in region 1, copy the state that is necessary to region 2, then clear region 1. Then we swap the regions and repeat this until the loop terminates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10233
https://github.com/hail-is/hail/pull/10233:431,Usability,clear,clear,431,"This PR changes the way loop memory management works. . Previously, memory used while in the loop was allocated in one region, with no cleanup happening between loops. A long running loop that did a lot of allocation could therefore easily cause hail to run out of memory. This PR fixes that by creating two regions for each loop. Every iteration, we emit everything in region 1, copy the state that is necessary to region 2, then clear region 1. Then we swap the regions and repeat this until the loop terminates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10233
https://github.com/hail-is/hail/pull/10235:9,Availability,avail,available,9,"Adds the available [GIANT 2018 Exome Array Summary Statistics](https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files#2018_Exome_Array_Summary_Statistics) datasets for WHR, BMI, and height as Hail Tables. For reproducibility, I added the notebook I used to generate the tables and schemas. The datasets were small in this case, and I ended up doing things locally on my machine. It didn't seem to make sense to try to redo things to fit into the older extract/load workflow once everything had already been generated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10235
https://github.com/hail-is/hail/pull/10235:498,Performance,load,load,498,"Adds the available [GIANT 2018 Exome Array Summary Statistics](https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files#2018_Exome_Array_Summary_Statistics) datasets for WHR, BMI, and height as Hail Tables. For reproducibility, I added the notebook I used to generate the tables and schemas. The datasets were small in this case, and I ended up doing things locally on my machine. It didn't seem to make sense to try to redo things to fit into the older extract/load workflow once everything had already been generated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10235
https://github.com/hail-is/hail/pull/10237:40,Energy Efficiency,power,powers,40,"Outside collaborators have virtually no powers on the right-hand side of the PR page w.r.t. reviewers/assignees/labels, so this is my best shot at letting them get someone assigned on their PRs. If a PR does not have any assignees or reviewers, but has #assign services or #assign compiler in the PR body, CI will randomly select a collaborator from those teams and assign them (including both will assign one person from each). Outside collaborators can re-request reviews, so I think this solves the PR review problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10237
https://github.com/hail-is/hail/pull/10238:664,Deployability,deploy,deployed,664,"When I unified the CSS for the docs and the website, every block quote in; the docs (including all bulleted lists) received a border. `style.css` originally; came from the main Hail website (https://hail.is, https://hail.is/references.html, etc.).; I cannot find any place where we needed a border around a blockquote, so I; have removed the blockquote border rules and consolidated the pre rules into; one block. @CDiaz96, can you check out this branch, run the website locally and poke around and make sure everything looks OK to you? The main page, the references.html page, etc. should all look these same as https://hail.is. The docs for PC Relate (currently deployed: https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate; URL for your laptop http://localhost:5000/docs/0.2/methods/relatedness.html#hail.methods.pc_relate) should not have a box around the unordered lists.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10238
https://github.com/hail-is/hail/pull/10240:13,Modifiability,config,config,13,`hailctl dev config show` is currently inconsistent with the verbiage used; to change those variables. This change ensures `hailctl dev config set k v`; is displayed as `k: v`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10240
https://github.com/hail-is/hail/pull/10240:92,Modifiability,variab,variables,92,`hailctl dev config show` is currently inconsistent with the verbiage used; to change those variables. This change ensures `hailctl dev config set k v`; is displayed as `k: v`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10240
https://github.com/hail-is/hail/pull/10240:136,Modifiability,config,config,136,`hailctl dev config show` is currently inconsistent with the verbiage used; to change those variables. This change ensures `hailctl dev config set k v`; is displayed as `k: v`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10240
https://github.com/hail-is/hail/pull/10243:792,Energy Efficiency,schedul,scheduled,792,"This test fails often ([for example](https://ci.hail.is/batches/234836/jobs/73)). I do not believe the test itself is being cancelled; by pytest-asyncio. Instead, I believe the aiotools code inadvertantly allows a; `CancelledError` to bubble to the top-level. Unfortunately, `asyncio` appears; to treat that as if the *task itself was cancelled*. Because it treats the; task as cancelled, we lose the stack trace (which would tell us where; we were just before the cancellation occurred). You can see for yourself this unfortunate behavior here:; ```; In [1]: import asyncio ; ...: ; ...: async def foo(): ; ...: raise asyncio.CancelledError() ; ...: ; ...: loop = asyncio.get_event_loop() ; ...: future = asyncio.ensure_future(foo()) ; ...: # allow the loop to run long enough that foo gets scheduled ; ...: loop.run_until_complete(asyncio.sleep(1)) ; ...: print(f'is cancelled? {future.cancelled()}') ; ...: future.result() ; is cancelled? True; Traceback (most recent call last):; File ""<ipython-input-1-14d822ad2a93>"", line 11, in <module>; future.result(); CancelledError. In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10243
https://github.com/hail-is/hail/pull/10243:5,Testability,test,test,5,"This test fails often ([for example](https://ci.hail.is/batches/234836/jobs/73)). I do not believe the test itself is being cancelled; by pytest-asyncio. Instead, I believe the aiotools code inadvertantly allows a; `CancelledError` to bubble to the top-level. Unfortunately, `asyncio` appears; to treat that as if the *task itself was cancelled*. Because it treats the; task as cancelled, we lose the stack trace (which would tell us where; we were just before the cancellation occurred). You can see for yourself this unfortunate behavior here:; ```; In [1]: import asyncio ; ...: ; ...: async def foo(): ; ...: raise asyncio.CancelledError() ; ...: ; ...: loop = asyncio.get_event_loop() ; ...: future = asyncio.ensure_future(foo()) ; ...: # allow the loop to run long enough that foo gets scheduled ; ...: loop.run_until_complete(asyncio.sleep(1)) ; ...: print(f'is cancelled? {future.cancelled()}') ; ...: future.result() ; is cancelled? True; Traceback (most recent call last):; File ""<ipython-input-1-14d822ad2a93>"", line 11, in <module>; future.result(); CancelledError. In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10243
https://github.com/hail-is/hail/pull/10243:103,Testability,test,test,103,"This test fails often ([for example](https://ci.hail.is/batches/234836/jobs/73)). I do not believe the test itself is being cancelled; by pytest-asyncio. Instead, I believe the aiotools code inadvertantly allows a; `CancelledError` to bubble to the top-level. Unfortunately, `asyncio` appears; to treat that as if the *task itself was cancelled*. Because it treats the; task as cancelled, we lose the stack trace (which would tell us where; we were just before the cancellation occurred). You can see for yourself this unfortunate behavior here:; ```; In [1]: import asyncio ; ...: ; ...: async def foo(): ; ...: raise asyncio.CancelledError() ; ...: ; ...: loop = asyncio.get_event_loop() ; ...: future = asyncio.ensure_future(foo()) ; ...: # allow the loop to run long enough that foo gets scheduled ; ...: loop.run_until_complete(asyncio.sleep(1)) ; ...: print(f'is cancelled? {future.cancelled()}') ; ...: future.result() ; is cancelled? True; Traceback (most recent call last):; File ""<ipython-input-1-14d822ad2a93>"", line 11, in <module>; future.result(); CancelledError. In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10243
https://github.com/hail-is/hail/pull/10245:68,Availability,down,downloaded,68,After a clean `make shadowJar` takes 2m33s on my laptop now. I just downloaded the latest Gradle from home-brew and ran `gradle wrapper` in the hail directory.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10245
https://github.com/hail-is/hail/pull/10245:128,Integrability,wrap,wrapper,128,After a clean `make shadowJar` takes 2m33s on my laptop now. I just downloaded the latest Gradle from home-brew and ran `gradle wrapper` in the hail directory.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10245
https://github.com/hail-is/hail/pull/10247:129,Integrability,rout,router,129,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10247
https://github.com/hail-is/hail/pull/10247:136,Integrability,rout,router-resolver,136,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10247
https://github.com/hail-is/hail/pull/10247:491,Modifiability,config,config,491,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10247
https://github.com/hail-is/hail/pull/10247:298,Security,authenticat,authenticated,298,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10247
https://github.com/hail-is/hail/pull/10247:327,Security,expose,expose,327,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10247
https://github.com/hail-is/hail/pull/10247:603,Security,expose,expose,603,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10247
https://github.com/hail-is/hail/pull/10247:1168,Security,certificate,certificates,1168,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10247
https://github.com/hail-is/hail/pull/10250:272,Availability,error,errors,272,"Realized that the notebook python app should in fact speak https because it is exposed on the pod even though it does not have a service in front of it. For example, prometheus scrapes all visible ports on a pod and it anticipates https. This was triggering the deluge of errors from notebook and deploying this into default seems to have stopped them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10250
https://github.com/hail-is/hail/pull/10250:297,Deployability,deploy,deploying,297,"Realized that the notebook python app should in fact speak https because it is exposed on the pod even though it does not have a service in front of it. For example, prometheus scrapes all visible ports on a pod and it anticipates https. This was triggering the deluge of errors from notebook and deploying this into default seems to have stopped them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10250
https://github.com/hail-is/hail/pull/10250:79,Security,expose,exposed,79,"Realized that the notebook python app should in fact speak https because it is exposed on the pod even though it does not have a service in front of it. For example, prometheus scrapes all visible ports on a pod and it anticipates https. This was triggering the deluge of errors from notebook and deploying this into default seems to have stopped them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10250
https://github.com/hail-is/hail/pull/10253:193,Security,access,access,193,"`IOUtils.toString(InputStream, Charset)` was only introduced in a later version of `commons-io` than we're stuck with because of Indeed's lsmtree. This workaround uses a method that we do have access to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10253
https://github.com/hail-is/hail/pull/10255:123,Availability,down,download,123,"Naturally, every possible Spark version uses a different elasticsearch library. . This also uses curl instead of gsutil to download the jar, so we don't require people to have gsutil to make.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10255
https://github.com/hail-is/hail/pull/10257:5,Testability,test,testing,5,"From testing in Julia, I think this is critical for accuracy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10257
https://github.com/hail-is/hail/pull/10259:298,Availability,avail,available,298,"This is a simplified implementation of `Process.communicate`. We feed lines into the log one-by-one; until we reach the end of both stdout and stderr. When both stdout and stderr have been closed by; the child process, we wait for the process to exit. At any point in time, the most recent log is; available to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10259
https://github.com/hail-is/hail/pull/10259:85,Testability,log,log,85,"This is a simplified implementation of `Process.communicate`. We feed lines into the log one-by-one; until we reach the end of both stdout and stderr. When both stdout and stderr have been closed by; the child process, we wait for the process to exit. At any point in time, the most recent log is; available to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10259
https://github.com/hail-is/hail/pull/10259:290,Testability,log,log,290,"This is a simplified implementation of `Process.communicate`. We feed lines into the log one-by-one; until we reach the end of both stdout and stderr. When both stdout and stderr have been closed by; the child process, we wait for the process to exit. At any point in time, the most recent log is; available to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10259
https://github.com/hail-is/hail/pull/10259:10,Usability,simpl,simplified,10,"This is a simplified implementation of `Process.communicate`. We feed lines into the log one-by-one; until we reach the end of both stdout and stderr. When both stdout and stderr have been closed by; the child process, we wait for the process to exit. At any point in time, the most recent log is; available to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10259
https://github.com/hail-is/hail/pull/10260:46,Availability,failure,failures,46,"This is a temporary fix for the sporadic copy failures. The problem is that this code cancels a task managed by the online bounded pool, and the pool treats that cancellation as an exception that it propagates up. I need to think through the details of the bounded gather with respect to cancellation, and that's going to take a few days. We could put this back when that's done, but honestly, it doesn't seem like an important optimization (given how rarely this failure comes up), so I'll probably just leave it out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10260
https://github.com/hail-is/hail/pull/10260:464,Availability,failure,failure,464,"This is a temporary fix for the sporadic copy failures. The problem is that this code cancels a task managed by the online bounded pool, and the pool treats that cancellation as an exception that it propagates up. I need to think through the details of the bounded gather with respect to cancellation, and that's going to take a few days. We could put this back when that's done, but honestly, it doesn't seem like an important optimization (given how rarely this failure comes up), so I'll probably just leave it out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10260
https://github.com/hail-is/hail/pull/10260:428,Performance,optimiz,optimization,428,"This is a temporary fix for the sporadic copy failures. The problem is that this code cancels a task managed by the online bounded pool, and the pool treats that cancellation as an exception that it propagates up. I need to think through the details of the bounded gather with respect to cancellation, and that's going to take a few days. We could put this back when that's done, but honestly, it doesn't seem like an important optimization (given how rarely this failure comes up), so I'll probably just leave it out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10260
https://github.com/hail-is/hail/pull/10263:66,Deployability,release,releases,66,"The changelog for [6.0.0](https://github.com/johnrengelman/shadow/releases/tag/6.0.0) claims performance improvements. In practice,; I save maybe a few second on the `shadowJar` step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10263
https://github.com/hail-is/hail/pull/10263:93,Performance,perform,performance,93,"The changelog for [6.0.0](https://github.com/johnrengelman/shadow/releases/tag/6.0.0) claims performance improvements. In practice,; I save maybe a few second on the `shadowJar` step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10263
https://github.com/hail-is/hail/pull/10264:189,Performance,optimiz,optimization,189,"This flips the pca algorithm, so that we're locally collecting a Krylov space basis on the column side (presumably the smaller side). This leads to a few simplifications, and allows for an optimization in the `compute_loadings=False` case. Once we have a complete TSQR implementation, we can optimize the `compute_loadings=True` case as well. With that, there should be no memory limitation to the number of rows (no local value is O(rows)).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10264
https://github.com/hail-is/hail/pull/10264:292,Performance,optimiz,optimize,292,"This flips the pca algorithm, so that we're locally collecting a Krylov space basis on the column side (presumably the smaller side). This leads to a few simplifications, and allows for an optimization in the `compute_loadings=False` case. Once we have a complete TSQR implementation, we can optimize the `compute_loadings=True` case as well. With that, there should be no memory limitation to the number of rows (no local value is O(rows)).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10264
https://github.com/hail-is/hail/pull/10264:154,Usability,simpl,simplifications,154,"This flips the pca algorithm, so that we're locally collecting a Krylov space basis on the column side (presumably the smaller side). This leads to a few simplifications, and allows for an optimization in the `compute_loadings=False` case. Once we have a complete TSQR implementation, we can optimize the `compute_loadings=True` case as well. With that, there should be no memory limitation to the number of rows (no local value is O(rows)).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10264
https://github.com/hail-is/hail/pull/10266:38,Integrability,depend,dependency,38,"Spark imports json4s but excludes its dependency, com.fasterxml.jackson.core:jackson-databind. We should do the same when shadowing json4s.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10266
https://github.com/hail-is/hail/pull/10271:382,Modifiability,extend,extend,382,"I think prometheus got forcibly shutdown and its volume got corrupted, which caused it to fail on startup. I had to wipe the volume. Starting with a new volume introduced permissions issues, I think because that volume claim might still have been there from the original time we were running prometheus.. Either way I was able to get this up and running in default. I had wanted to extend the retention time so we can see a full month of metrics instead of just two weeks, so I'm being extra nice with the volume and moving prometheus off of preemptibles in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10271
https://github.com/hail-is/hail/pull/10272:328,Availability,redundant,redundant,328,"Now that we are running with very small nodes, image-fetcher doesn't seem to provide all that much benefit. The intention is that caching through the memory service will ultimately prove better without having to run a daemonset. This also was running a whole daemon set for every PR namespace which meant a lot of our pods were redundant image-fetchers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10272
https://github.com/hail-is/hail/pull/10272:328,Safety,redund,redundant,328,"Now that we are running with very small nodes, image-fetcher doesn't seem to provide all that much benefit. The intention is that caching through the memory service will ultimately prove better without having to run a daemonset. This also was running a whole daemon set for every PR namespace which meant a lot of our pods were redundant image-fetchers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10272
https://github.com/hail-is/hail/pull/10275:42,Modifiability,variab,variable,42,"Now the `HAIL_SSL_CONFIG_DIR` environment variable can point to any absolute path. It must contain a `ssl-config.json` file that contains relative paths to `HAIL_SSL_CONFIG_DIR` for the rest of the ssl config files. Also allows `HAIL_TOKENS_FILE` in python, which was already implemented in Scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10275
https://github.com/hail-is/hail/pull/10275:106,Modifiability,config,config,106,"Now the `HAIL_SSL_CONFIG_DIR` environment variable can point to any absolute path. It must contain a `ssl-config.json` file that contains relative paths to `HAIL_SSL_CONFIG_DIR` for the rest of the ssl config files. Also allows `HAIL_TOKENS_FILE` in python, which was already implemented in Scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10275
https://github.com/hail-is/hail/pull/10275:202,Modifiability,config,config,202,"Now the `HAIL_SSL_CONFIG_DIR` environment variable can point to any absolute path. It must contain a `ssl-config.json` file that contains relative paths to `HAIL_SSL_CONFIG_DIR` for the rest of the ssl config files. Also allows `HAIL_TOKENS_FILE` in python, which was already implemented in Scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10275
https://github.com/hail-is/hail/pull/10276:50,Deployability,deploy,deploys,50,"Potential footgun but specifically limited to dev deploys. This allows you to explicitly exclude build steps when dev deploying. For example, `hailctl dev deploy -b ‚Ä¶ -s deploy_query --excluded_steps deploy_batch` would make sure that `deploy_batch` and any jobs only in the `deploy_batch` subtree are not run. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10276
https://github.com/hail-is/hail/pull/10276:118,Deployability,deploy,deploying,118,"Potential footgun but specifically limited to dev deploys. This allows you to explicitly exclude build steps when dev deploying. For example, `hailctl dev deploy -b ‚Ä¶ -s deploy_query --excluded_steps deploy_batch` would make sure that `deploy_batch` and any jobs only in the `deploy_batch` subtree are not run. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10276
https://github.com/hail-is/hail/pull/10276:155,Deployability,deploy,deploy,155,"Potential footgun but specifically limited to dev deploys. This allows you to explicitly exclude build steps when dev deploying. For example, `hailctl dev deploy -b ‚Ä¶ -s deploy_query --excluded_steps deploy_batch` would make sure that `deploy_batch` and any jobs only in the `deploy_batch` subtree are not run. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10276
https://github.com/hail-is/hail/pull/10278:268,Deployability,release,release,268,"@nawatts points out that the docs are missing the font-awesome icons. This new website; stuff is confusing! Here is how to think about it going forward:. This PR is the *long-term fix*. It will not resolve the current production issue. The easiest fix; for that is to release a new version of Hail, which will generate a new version of the docs. That; new version will be compatible with the currently released `website`. We have two phases: docs-generation-time and serve-time. Importantly, we always need to support the; *previously released documentation* with the current `main` `website`. This PR makes things work like this:. At docs-generation-time (which happens every time we release a new version of the Hail package to; PyPI), Sphinx uses `dynamic-base.html` to build the docs website. Those HTML files will look like; this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. {% include ""base-head.html"" %}. <!-- Sphinx will insert some header stuff here -->; </head>; <body>; {% include ""nav-top.html"" %}; <div id=""main"">; {% raw %}; <!-- Sphinx will insert the actual documentation here -->; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>; ```. Note that this is *still a Jinja2 template!*. At serve-time, `website` will run this through Jinja2 templating *a second time*. At this time,; we'll use the latest `base-head.html`. In particular, suppose I need access to a new CSS file in `nav-top.html`. I can modify; `base-head.html`. Since `base-head.html` is added to the web page *at serve-time*, it will include; all the latest changes. In contrast, everything inside the `#main` `div` *only changes when Hail is released* because it is; only updated at docs-generation-time. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10278
https://github.com/hail-is/hail/pull/10278:402,Deployability,release,released,402,"@nawatts points out that the docs are missing the font-awesome icons. This new website; stuff is confusing! Here is how to think about it going forward:. This PR is the *long-term fix*. It will not resolve the current production issue. The easiest fix; for that is to release a new version of Hail, which will generate a new version of the docs. That; new version will be compatible with the currently released `website`. We have two phases: docs-generation-time and serve-time. Importantly, we always need to support the; *previously released documentation* with the current `main` `website`. This PR makes things work like this:. At docs-generation-time (which happens every time we release a new version of the Hail package to; PyPI), Sphinx uses `dynamic-base.html` to build the docs website. Those HTML files will look like; this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. {% include ""base-head.html"" %}. <!-- Sphinx will insert some header stuff here -->; </head>; <body>; {% include ""nav-top.html"" %}; <div id=""main"">; {% raw %}; <!-- Sphinx will insert the actual documentation here -->; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>; ```. Note that this is *still a Jinja2 template!*. At serve-time, `website` will run this through Jinja2 templating *a second time*. At this time,; we'll use the latest `base-head.html`. In particular, suppose I need access to a new CSS file in `nav-top.html`. I can modify; `base-head.html`. Since `base-head.html` is added to the web page *at serve-time*, it will include; all the latest changes. In contrast, everything inside the `#main` `div` *only changes when Hail is released* because it is; only updated at docs-generation-time. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10278
https://github.com/hail-is/hail/pull/10278:535,Deployability,release,released,535,"@nawatts points out that the docs are missing the font-awesome icons. This new website; stuff is confusing! Here is how to think about it going forward:. This PR is the *long-term fix*. It will not resolve the current production issue. The easiest fix; for that is to release a new version of Hail, which will generate a new version of the docs. That; new version will be compatible with the currently released `website`. We have two phases: docs-generation-time and serve-time. Importantly, we always need to support the; *previously released documentation* with the current `main` `website`. This PR makes things work like this:. At docs-generation-time (which happens every time we release a new version of the Hail package to; PyPI), Sphinx uses `dynamic-base.html` to build the docs website. Those HTML files will look like; this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. {% include ""base-head.html"" %}. <!-- Sphinx will insert some header stuff here -->; </head>; <body>; {% include ""nav-top.html"" %}; <div id=""main"">; {% raw %}; <!-- Sphinx will insert the actual documentation here -->; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>; ```. Note that this is *still a Jinja2 template!*. At serve-time, `website` will run this through Jinja2 templating *a second time*. At this time,; we'll use the latest `base-head.html`. In particular, suppose I need access to a new CSS file in `nav-top.html`. I can modify; `base-head.html`. Since `base-head.html` is added to the web page *at serve-time*, it will include; all the latest changes. In contrast, everything inside the `#main` `div` *only changes when Hail is released* because it is; only updated at docs-generation-time. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10278
https://github.com/hail-is/hail/pull/10278:685,Deployability,release,release,685,"@nawatts points out that the docs are missing the font-awesome icons. This new website; stuff is confusing! Here is how to think about it going forward:. This PR is the *long-term fix*. It will not resolve the current production issue. The easiest fix; for that is to release a new version of Hail, which will generate a new version of the docs. That; new version will be compatible with the currently released `website`. We have two phases: docs-generation-time and serve-time. Importantly, we always need to support the; *previously released documentation* with the current `main` `website`. This PR makes things work like this:. At docs-generation-time (which happens every time we release a new version of the Hail package to; PyPI), Sphinx uses `dynamic-base.html` to build the docs website. Those HTML files will look like; this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. {% include ""base-head.html"" %}. <!-- Sphinx will insert some header stuff here -->; </head>; <body>; {% include ""nav-top.html"" %}; <div id=""main"">; {% raw %}; <!-- Sphinx will insert the actual documentation here -->; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>; ```. Note that this is *still a Jinja2 template!*. At serve-time, `website` will run this through Jinja2 templating *a second time*. At this time,; we'll use the latest `base-head.html`. In particular, suppose I need access to a new CSS file in `nav-top.html`. I can modify; `base-head.html`. Since `base-head.html` is added to the web page *at serve-time*, it will include; all the latest changes. In contrast, everything inside the `#main` `div` *only changes when Hail is released* because it is; only updated at docs-generation-time. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10278
https://github.com/hail-is/hail/pull/10278:1738,Deployability,release,released,1738,"@nawatts points out that the docs are missing the font-awesome icons. This new website; stuff is confusing! Here is how to think about it going forward:. This PR is the *long-term fix*. It will not resolve the current production issue. The easiest fix; for that is to release a new version of Hail, which will generate a new version of the docs. That; new version will be compatible with the currently released `website`. We have two phases: docs-generation-time and serve-time. Importantly, we always need to support the; *previously released documentation* with the current `main` `website`. This PR makes things work like this:. At docs-generation-time (which happens every time we release a new version of the Hail package to; PyPI), Sphinx uses `dynamic-base.html` to build the docs website. Those HTML files will look like; this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. {% include ""base-head.html"" %}. <!-- Sphinx will insert some header stuff here -->; </head>; <body>; {% include ""nav-top.html"" %}; <div id=""main"">; {% raw %}; <!-- Sphinx will insert the actual documentation here -->; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>; ```. Note that this is *still a Jinja2 template!*. At serve-time, `website` will run this through Jinja2 templating *a second time*. At this time,; we'll use the latest `base-head.html`. In particular, suppose I need access to a new CSS file in `nav-top.html`. I can modify; `base-head.html`. Since `base-head.html` is added to the web page *at serve-time*, it will include; all the latest changes. In contrast, everything inside the `#main` `div` *only changes when Hail is released* because it is; only updated at docs-generation-time. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10278
https://github.com/hail-is/hail/pull/10278:1768,Deployability,update,updated,1768,"@nawatts points out that the docs are missing the font-awesome icons. This new website; stuff is confusing! Here is how to think about it going forward:. This PR is the *long-term fix*. It will not resolve the current production issue. The easiest fix; for that is to release a new version of Hail, which will generate a new version of the docs. That; new version will be compatible with the currently released `website`. We have two phases: docs-generation-time and serve-time. Importantly, we always need to support the; *previously released documentation* with the current `main` `website`. This PR makes things work like this:. At docs-generation-time (which happens every time we release a new version of the Hail package to; PyPI), Sphinx uses `dynamic-base.html` to build the docs website. Those HTML files will look like; this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. {% include ""base-head.html"" %}. <!-- Sphinx will insert some header stuff here -->; </head>; <body>; {% include ""nav-top.html"" %}; <div id=""main"">; {% raw %}; <!-- Sphinx will insert the actual documentation here -->; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>; ```. Note that this is *still a Jinja2 template!*. At serve-time, `website` will run this through Jinja2 templating *a second time*. At this time,; we'll use the latest `base-head.html`. In particular, suppose I need access to a new CSS file in `nav-top.html`. I can modify; `base-head.html`. Since `base-head.html` is added to the web page *at serve-time*, it will include; all the latest changes. In contrast, everything inside the `#main` `div` *only changes when Hail is released* because it is; only updated at docs-generation-time. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10278
https://github.com/hail-is/hail/pull/10278:1480,Security,access,access,1480,"@nawatts points out that the docs are missing the font-awesome icons. This new website; stuff is confusing! Here is how to think about it going forward:. This PR is the *long-term fix*. It will not resolve the current production issue. The easiest fix; for that is to release a new version of Hail, which will generate a new version of the docs. That; new version will be compatible with the currently released `website`. We have two phases: docs-generation-time and serve-time. Importantly, we always need to support the; *previously released documentation* with the current `main` `website`. This PR makes things work like this:. At docs-generation-time (which happens every time we release a new version of the Hail package to; PyPI), Sphinx uses `dynamic-base.html` to build the docs website. Those HTML files will look like; this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. {% include ""base-head.html"" %}. <!-- Sphinx will insert some header stuff here -->; </head>; <body>; {% include ""nav-top.html"" %}; <div id=""main"">; {% raw %}; <!-- Sphinx will insert the actual documentation here -->; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>; ```. Note that this is *still a Jinja2 template!*. At serve-time, `website` will run this through Jinja2 templating *a second time*. At this time,; we'll use the latest `base-head.html`. In particular, suppose I need access to a new CSS file in `nav-top.html`. I can modify; `base-head.html`. Since `base-head.html` is added to the web page *at serve-time*, it will include; all the latest changes. In contrast, everything inside the `#main` `div` *only changes when Hail is released* because it is; only updated at docs-generation-time. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10278
https://github.com/hail-is/hail/pull/10279:2980,Availability,error,error,2980,"e should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; methods anyway, that would amount to Hail Queries inside of Hail Queries, madness!; - New transient error in Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:95,Deployability,deploy,deployment,95,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:2085,Deployability,update,updated,2085,"e should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; methods anyway, that would amount to Hail Queries inside of Hail Queries, madness!; - New transient error in Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:2170,Modifiability,config,config,2170,"e should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; methods anyway, that would amount to Hail Queries inside of Hail Queries, madness!; - New transient error in Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:212,Performance,load,load,212,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:275,Performance,load,loader,275,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:356,Performance,cache,cache,356,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:838,Performance,load,loadClass,838,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1003,Performance,load,load,1003,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1103,Performance,load,loadClass,1103," backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1141,Performance,load,loading,1141," backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1222,Performance,load,loading,1222,"demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Sm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1335,Performance,Load,LoadSelfFirstURLClassLoader,1335,"e cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's ow",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1417,Performance,Load,LoadSelfFirstURLClassLoader,1417,"ode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add ter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1533,Performance,load,load,1533," a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1587,Performance,load,load,1587,"the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1783,Performance,load,loader,1783,"rbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1799,Performance,load,load,1799,"rbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1909,Performance,load,loads,1909,"ch to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; meth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:1902,Testability,Test,TestNG,1902,"ch to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; meth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:2395,Testability,test,test,2395,"e should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; methods anyway, that would amount to Hail Queries inside of Hail Queries, madness!; - New transient error in Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10279:2417,Testability,test,test,2417,"e should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; methods anyway, that would amount to Hail Queries inside of Hail Queries, madness!; - New transient error in Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10279
https://github.com/hail-is/hail/pull/10282:29,Performance,cache,cache,29,This avoids confusing Docker cache behavior by baking the verison number into; the RUN command string.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10282
https://github.com/hail-is/hail/pull/10282:5,Safety,avoid,avoids,5,This avoids confusing Docker cache behavior by baking the verison number into; the RUN command string.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10282
https://github.com/hail-is/hail/pull/10283:21,Security,Expose,Expose,21,Stacked on #10097. - Expose `machine_type` and `preemptible` in hailtop.batch; - Sets default storage when specifying the machine type to 100Gi; - Selects the cheapest machine type automatically for the user when they want a nonpreemptible worker,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10283
https://github.com/hail-is/hail/pull/10286:395,Availability,error,error,395,"Rahul reported this failing on the following gnomad pipeline:. ```; import hail as hl; from gnomad.utils.vep import process_consequences; from gnomad.resources.grch37 import gnomad. gnomad_v2_exomes = gnomad.public_release(""exomes""); ht_exomes = gnomad_v2_exomes.ht(); ht_exomes_proc = process_consequences(ht_exomes); ht_exomes_proc._force_count(); ```. No test case included, but this kind of error will be impossible soon; (when requiredness exists on EmitType, not SType/PType).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10286
https://github.com/hail-is/hail/pull/10286:52,Deployability,pipeline,pipeline,52,"Rahul reported this failing on the following gnomad pipeline:. ```; import hail as hl; from gnomad.utils.vep import process_consequences; from gnomad.resources.grch37 import gnomad. gnomad_v2_exomes = gnomad.public_release(""exomes""); ht_exomes = gnomad_v2_exomes.ht(); ht_exomes_proc = process_consequences(ht_exomes); ht_exomes_proc._force_count(); ```. No test case included, but this kind of error will be impossible soon; (when requiredness exists on EmitType, not SType/PType).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10286
https://github.com/hail-is/hail/pull/10286:358,Testability,test,test,358,"Rahul reported this failing on the following gnomad pipeline:. ```; import hail as hl; from gnomad.utils.vep import process_consequences; from gnomad.resources.grch37 import gnomad. gnomad_v2_exomes = gnomad.public_release(""exomes""); ht_exomes = gnomad_v2_exomes.ht(); ht_exomes_proc = process_consequences(ht_exomes); ht_exomes_proc._force_count(); ```. No test case included, but this kind of error will be impossible soon; (when requiredness exists on EmitType, not SType/PType).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10286
https://github.com/hail-is/hail/pull/10292:396,Deployability,deploy,deployed,396,"The `notebook` and `workshop` services were incorrectly pointing to the python notebook app instead of its nginx proxy, which handles proxying to notebook workers. In #10250 I added back https to the notebook python app and accidentally changed the nginx -> notebook worker connection `https`, where it should not be. These combined meant that notebook was unable to proxy to notebook workers. I deployed this into default and verified that I can get to jupyter, and also run scale tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10292
https://github.com/hail-is/hail/pull/10292:482,Testability,test,tests,482,"The `notebook` and `workshop` services were incorrectly pointing to the python notebook app instead of its nginx proxy, which handles proxying to notebook workers. In #10250 I added back https to the notebook python app and accidentally changed the nginx -> notebook worker connection `https`, where it should not be. These combined meant that notebook was unable to proxy to notebook workers. I deployed this into default and verified that I can get to jupyter, and also run scale tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10292
https://github.com/hail-is/hail/pull/10293:5,Deployability,pipeline,pipelines,5,Most pipelines will take the old path.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10293
https://github.com/hail-is/hail/pull/10295:122,Performance,load,loading,122,"Batches that have a `pr` attribute now link back to the corresponding PR page. Also fixed a latent bug where we were only loading batches for a PR based on the PR number, so if we had multiple watched branches you could display batches for PRs of the same number across branches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10295
https://github.com/hail-is/hail/pull/10298:8,Availability,error,error,8,The old error message for matmul was impossible to read. This is much clearer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10298
https://github.com/hail-is/hail/pull/10298:14,Integrability,message,message,14,The old error message for matmul was impossible to read. This is much clearer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10298
https://github.com/hail-is/hail/pull/10298:70,Usability,clear,clearer,70,The old error message for matmul was impossible to read. This is much clearer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10298
https://github.com/hail-is/hail/pull/10299:19,Testability,test,tests,19,- add 1st round of tests for stream memory usage; - clarify streamfilter implementation (no bug here though),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10299
https://github.com/hail-is/hail/pull/10302:111,Availability,down,downloaded,111,"The fix for Safari will take effect the next time the docs are deployed. In the meantime,; the docs are indeed downloaded, but Safari tells you there was a problem. If users navigate; to the Downloads directory, the file should indeed be present.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10302
https://github.com/hail-is/hail/pull/10302:191,Availability,Down,Downloads,191,"The fix for Safari will take effect the next time the docs are deployed. In the meantime,; the docs are indeed downloaded, but Safari tells you there was a problem. If users navigate; to the Downloads directory, the file should indeed be present.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10302
https://github.com/hail-is/hail/pull/10302:63,Deployability,deploy,deployed,63,"The fix for Safari will take effect the next time the docs are deployed. In the meantime,; the docs are indeed downloaded, but Safari tells you there was a problem. If users navigate; to the Downloads directory, the file should indeed be present.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10302
https://github.com/hail-is/hail/pull/10309:46,Performance,cache,cache,46,Avoid query-service races when looking at the cache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10309
https://github.com/hail-is/hail/pull/10309:0,Safety,Avoid,Avoid,0,Avoid query-service races when looking at the cache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10309
https://github.com/hail-is/hail/pull/10310:97,Deployability,Deploy,DeployConfig,97,- google.api_core.exceptions.ServiceUnavailable is just 503 in an unnecessary new class; - Teach DeployConfig.socket to use retryTransientErrors.; - Also retry NoRouteToHostException (the host should *eventually* come into existence).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10310
https://github.com/hail-is/hail/pull/10314:540,Availability,down,download,540,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:653,Availability,down,downloading,653,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1842,Availability,error,error,1842,"VMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:96,Deployability,deploy,deployment,96,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:3448,Deployability,update,updated,3448,"e key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule. - Only apply nest_asyncio when an event loop is already started.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1848,Integrability,message,message,1848,"VMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1922,Integrability,message,message,1922,"VMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:2729,Integrability,contract,contract,2729,"that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:3082,Integrability,protocol,protocol,3082,"ta out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:3139,Integrability,protocol,protocols,3139,"bjects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:3533,Modifiability,config,config,3533,"e key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule. - Only apply nest_asyncio when an event loop is already started.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:213,Performance,load,load,213,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:427,Performance,cache,cache,427,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:858,Performance,load,load,858,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:957,Performance,multi-thread,multi-threaded,957,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:2057,Safety,Safe,SafeRow,2057,"ach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol whe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1409,Testability,test,tests,1409,"he correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM fiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1421,Testability,test,tests,1421,"ache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1474,Testability,test,tests,1474,"bandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1563,Testability,test,tests,1563,"to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` fr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1574,Testability,test,test,1574,"ck to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1614,Testability,test,tests,1614,"ck to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:1868,Testability,test,tests,1868,"VMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:3757,Testability,test,test,3757,"e key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule. - Only apply nest_asyncio when an event loop is already started.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:3779,Testability,test,test,3779,"e key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule. - Only apply nest_asyncio when an event loop is already started.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10314:363,Usability,simpl,simply,363,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10314
https://github.com/hail-is/hail/pull/10315:686,Availability,down,down,686,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10315
https://github.com/hail-is/hail/pull/10315:239,Integrability,depend,depending,239,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10315
https://github.com/hail-is/hail/pull/10315:256,Performance,latency,latency,256,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10315
https://github.com/hail-is/hail/pull/10315:371,Performance,cache,cacheable,371,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10315
https://github.com/hail-is/hail/pull/10315:657,Testability,benchmark,benchmarks,657,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10315
https://github.com/hail-is/hail/pull/10315:126,Usability,simpl,simple,126,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10315
https://github.com/hail-is/hail/pull/10317:71,Testability,test,test,71,It seems that referencing a global function serializes a module named `test` which; does not exist on the worker.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10317
https://github.com/hail-is/hail/pull/10319:33,Testability,test,tests,33,Still need to change the cluster tests before this goes in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10319
https://github.com/hail-is/hail/pull/10322:31,Availability,error,errors,31,"While trying to fix some weird errors, I realized this was more complicated than necessary and, I; think, broken. In the new implementation, a BatchPoolFuture is a thin wrapper around an asyncio.Future. Instead of; tracking the value and any exceptions manually, the BatchPoolFuture relies on asyncio.Future. I think the diff is not very helpful, just look at the new, simpler implementation. I also forgot to close the ServiceBackend, I now do that in the cleanup method that happens after all future complete.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10322
https://github.com/hail-is/hail/pull/10322:169,Integrability,wrap,wrapper,169,"While trying to fix some weird errors, I realized this was more complicated than necessary and, I; think, broken. In the new implementation, a BatchPoolFuture is a thin wrapper around an asyncio.Future. Instead of; tracking the value and any exceptions manually, the BatchPoolFuture relies on asyncio.Future. I think the diff is not very helpful, just look at the new, simpler implementation. I also forgot to close the ServiceBackend, I now do that in the cleanup method that happens after all future complete.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10322
https://github.com/hail-is/hail/pull/10322:369,Usability,simpl,simpler,369,"While trying to fix some weird errors, I realized this was more complicated than necessary and, I; think, broken. In the new implementation, a BatchPoolFuture is a thin wrapper around an asyncio.Future. Instead of; tracking the value and any exceptions manually, the BatchPoolFuture relies on asyncio.Future. I think the diff is not very helpful, just look at the new, simpler implementation. I also forgot to close the ServiceBackend, I now do that in the cleanup method that happens after all future complete.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10322
https://github.com/hail-is/hail/pull/10325:97,Availability,error,error,97,"If instance name isn't active, look to see whether it existed in the database before printing an error message about an unknown instance.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10325
https://github.com/hail-is/hail/pull/10325:103,Integrability,message,message,103,"If instance name isn't active, look to see whether it existed in the database before printing an error message about an unknown instance.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10325
https://github.com/hail-is/hail/pull/10327:269,Availability,robust,robust,269,"Tags and digests have no affect on whether an image is one of the hailgenetics; images that are stored in both GCR and DockerHub. This change ignores the tag and; digest when checking if we should warn about using DockerHub. In doing this, I consolidated and made more robust our Docker-image-reference parsing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10327
https://github.com/hail-is/hail/pull/10330:12,Availability,error,errors,12,"We will get errors about methods not being well formed if we don't define every label on an IEmitCode, so we have to consume each one even if we don't do anything with it. . I also moved `StreamLen` to `emitI`, because we were pointlessly switching back and forth from `EmitCode` to `IEmitCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10330
https://github.com/hail-is/hail/pull/10337:42,Performance,perform,performance-sensitive,42,Generates a lot of bytecode and called in performance-sensitive places. Helps with profiling a bit.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10337
https://github.com/hail-is/hail/pull/10339:221,Deployability,install,install,221,"setup.py opens the requirements.txt file, because of this it must be in any source distribution. This will become more relevant as we start distributing more (any) native code, reducing the number of platforms where `pip install hail` installs a binary distribution. tested manually with:; ```; make copy-py-files; cd build/deploy; python3 setup.py sdist; cd dist; tar xf hail-0.2.64.tar.gz; cd hail-0.2.64; python setup.py bdist_wheel; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10339
https://github.com/hail-is/hail/pull/10339:235,Deployability,install,installs,235,"setup.py opens the requirements.txt file, because of this it must be in any source distribution. This will become more relevant as we start distributing more (any) native code, reducing the number of platforms where `pip install hail` installs a binary distribution. tested manually with:; ```; make copy-py-files; cd build/deploy; python3 setup.py sdist; cd dist; tar xf hail-0.2.64.tar.gz; cd hail-0.2.64; python setup.py bdist_wheel; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10339
https://github.com/hail-is/hail/pull/10339:324,Deployability,deploy,deploy,324,"setup.py opens the requirements.txt file, because of this it must be in any source distribution. This will become more relevant as we start distributing more (any) native code, reducing the number of platforms where `pip install hail` installs a binary distribution. tested manually with:; ```; make copy-py-files; cd build/deploy; python3 setup.py sdist; cd dist; tar xf hail-0.2.64.tar.gz; cd hail-0.2.64; python setup.py bdist_wheel; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10339
https://github.com/hail-is/hail/pull/10339:267,Testability,test,tested,267,"setup.py opens the requirements.txt file, because of this it must be in any source distribution. This will become more relevant as we start distributing more (any) native code, reducing the number of platforms where `pip install hail` installs a binary distribution. tested manually with:; ```; make copy-py-files; cd build/deploy; python3 setup.py sdist; cd dist; tar xf hail-0.2.64.tar.gz; cd hail-0.2.64; python setup.py bdist_wheel; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10339
https://github.com/hail-is/hail/pull/10340:43,Modifiability,config,config,43,Adds the `docker_root_image` to the global config in `build.py` so that #10107 can pass CI tests without manually redeploying CI.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10340
https://github.com/hail-is/hail/pull/10340:91,Testability,test,tests,91,Adds the `docker_root_image` to the global config in `build.py` so that #10107 can pass CI tests without manually redeploying CI.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10340
https://github.com/hail-is/hail/pull/10342:549,Safety,safe,safer,549,"I think this should resolve the issues we were seeing with OnlineBoundedGather2. Changes:; - cancelled tasks (those that raise CancelledError) are ignored (we don't propagate cancelled out of background tasks); - Make sure all exceptions are either reraised or logged; - The first exception is raised out of exit, not call; - call raises PoolShutdownError if the pool is shutdown; - _shutdown doesn't signal _done_event until all cancelled tasks are complete; - call clears _done_event (not strictly necessary because exit checks pending, but seems safer); - added copious docstrings",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10342
https://github.com/hail-is/hail/pull/10342:261,Testability,log,logged,261,"I think this should resolve the issues we were seeing with OnlineBoundedGather2. Changes:; - cancelled tasks (those that raise CancelledError) are ignored (we don't propagate cancelled out of background tasks); - Make sure all exceptions are either reraised or logged; - The first exception is raised out of exit, not call; - call raises PoolShutdownError if the pool is shutdown; - _shutdown doesn't signal _done_event until all cancelled tasks are complete; - call clears _done_event (not strictly necessary because exit checks pending, but seems safer); - added copious docstrings",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10342
https://github.com/hail-is/hail/pull/10342:467,Usability,clear,clears,467,"I think this should resolve the issues we were seeing with OnlineBoundedGather2. Changes:; - cancelled tasks (those that raise CancelledError) are ignored (we don't propagate cancelled out of background tasks); - Make sure all exceptions are either reraised or logged; - The first exception is raised out of exit, not call; - call raises PoolShutdownError if the pool is shutdown; - _shutdown doesn't signal _done_event until all cancelled tasks are complete; - call clears _done_event (not strictly necessary because exit checks pending, but seems safer); - added copious docstrings",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10342
https://github.com/hail-is/hail/pull/10344:105,Integrability,depend,dependencies,105,"Includes are grouped by source (system, LLVM, internal), the groups; themselves are then ordered stdlib, dependencies, internal. Ordering; within groups is alphabetical. Also switch internal include style from #include <> to #include """"",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10344
https://github.com/hail-is/hail/pull/10349:163,Security,expose,exposed,163,"boto3 is the AWS Python client library. This is for the upcoming S3AsyncFS. FYI @tpoterba I also added it to the hail package requirements file because it will be exposed via `hailctl cp ...`, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10349
https://github.com/hail-is/hail/issues/10351:180,Deployability,install,install,180,"On the docs for 'Other Spark Clusters', it states that Hail is built on Spark 2.4; `Hail should work with any Spark 2.4.x cluster built with Scala 2.11.`; https://hail.is/docs/0.2/install/other-cluster.html. My understanding is that this is now Spark 3.1? Just want to check this. Cheers!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10351
https://github.com/hail-is/hail/issues/10352:328,Availability,Error,Error,328,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hail commit version: 5852a71e9e23. Error received:; ```; amazon-ebs: cd build/deploy; python3 setup.py -q sdist bdist_wheel; amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ==> amazon-ebs: ERROR: Invalid requirement: '#'; ==> amazon-ebs: make: *** [install-on-cluster] Error 123; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10352
https://github.com/hail-is/hail/issues/10352:524,Availability,ERROR,ERROR,524,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hail commit version: 5852a71e9e23. Error received:; ```; amazon-ebs: cd build/deploy; python3 setup.py -q sdist bdist_wheel; amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ==> amazon-ebs: ERROR: Invalid requirement: '#'; ==> amazon-ebs: make: *** [install-on-cluster] Error 123; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10352
https://github.com/hail-is/hail/issues/10352:604,Availability,Error,Error,604,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hail commit version: 5852a71e9e23. Error received:; ```; amazon-ebs: cd build/deploy; python3 setup.py -q sdist bdist_wheel; amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ==> amazon-ebs: ERROR: Invalid requirement: '#'; ==> amazon-ebs: make: *** [install-on-cluster] Error 123; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10352
https://github.com/hail-is/hail/issues/10352:371,Deployability,deploy,deploy,371,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hail commit version: 5852a71e9e23. Error received:; ```; amazon-ebs: cd build/deploy; python3 setup.py -q sdist bdist_wheel; amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ==> amazon-ebs: ERROR: Invalid requirement: '#'; ==> amazon-ebs: make: *** [install-on-cluster] Error 123; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10352
https://github.com/hail-is/hail/issues/10352:496,Deployability,install,install,496,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hail commit version: 5852a71e9e23. Error received:; ```; amazon-ebs: cd build/deploy; python3 setup.py -q sdist bdist_wheel; amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ==> amazon-ebs: ERROR: Invalid requirement: '#'; ==> amazon-ebs: make: *** [install-on-cluster] Error 123; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10352
https://github.com/hail-is/hail/issues/10352:584,Deployability,install,install-on-cluster,584,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hail commit version: 5852a71e9e23. Error received:; ```; amazon-ebs: cd build/deploy; python3 setup.py -q sdist bdist_wheel; amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ==> amazon-ebs: ERROR: Invalid requirement: '#'; ==> amazon-ebs: make: *** [install-on-cluster] Error 123; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10352
https://github.com/hail-is/hail/pull/10356:16,Availability,down,download,16,We can directly download the key and specify the repository. I used these instructions:; - https://wiki.debian.org/DebianRepository/UseThirdParty; - https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa. This builds in like 2 minutes rather than 4. The frontend part avoids issues in downstream docker files where the installer might try to interact with the user. I noticed that some package was updated and now pulls in `tzdata` which asks you to select a timezone at install-time (if Debian frontend is interactive).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10356
https://github.com/hail-is/hail/pull/10356:284,Availability,down,downstream,284,We can directly download the key and specify the repository. I used these instructions:; - https://wiki.debian.org/DebianRepository/UseThirdParty; - https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa. This builds in like 2 minutes rather than 4. The frontend part avoids issues in downstream docker files where the installer might try to interact with the user. I noticed that some package was updated and now pulls in `tzdata` which asks you to select a timezone at install-time (if Debian frontend is interactive).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10356
https://github.com/hail-is/hail/pull/10356:318,Deployability,install,installer,318,We can directly download the key and specify the repository. I used these instructions:; - https://wiki.debian.org/DebianRepository/UseThirdParty; - https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa. This builds in like 2 minutes rather than 4. The frontend part avoids issues in downstream docker files where the installer might try to interact with the user. I noticed that some package was updated and now pulls in `tzdata` which asks you to select a timezone at install-time (if Debian frontend is interactive).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10356
https://github.com/hail-is/hail/pull/10356:397,Deployability,update,updated,397,We can directly download the key and specify the repository. I used these instructions:; - https://wiki.debian.org/DebianRepository/UseThirdParty; - https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa. This builds in like 2 minutes rather than 4. The frontend part avoids issues in downstream docker files where the installer might try to interact with the user. I noticed that some package was updated and now pulls in `tzdata` which asks you to select a timezone at install-time (if Debian frontend is interactive).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10356
https://github.com/hail-is/hail/pull/10356:470,Deployability,install,install-time,470,We can directly download the key and specify the repository. I used these instructions:; - https://wiki.debian.org/DebianRepository/UseThirdParty; - https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa. This builds in like 2 minutes rather than 4. The frontend part avoids issues in downstream docker files where the installer might try to interact with the user. I noticed that some package was updated and now pulls in `tzdata` which asks you to select a timezone at install-time (if Debian frontend is interactive).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10356
https://github.com/hail-is/hail/pull/10356:267,Safety,avoid,avoids,267,We can directly download the key and specify the repository. I used these instructions:; - https://wiki.debian.org/DebianRepository/UseThirdParty; - https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa. This builds in like 2 minutes rather than 4. The frontend part avoids issues in downstream docker files where the installer might try to interact with the user. I noticed that some package was updated and now pulls in `tzdata` which asks you to select a timezone at install-time (if Debian frontend is interactive).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10356
https://github.com/hail-is/hail/pull/10357:4,Testability,test,test,4,"The test was lacking asserts, so it didn't fail even though it wasn't right. The tables were missing keys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10357
https://github.com/hail-is/hail/pull/10357:21,Testability,assert,asserts,21,"The test was lacking asserts, so it didn't fail even though it wasn't right. The tables were missing keys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10357
https://github.com/hail-is/hail/pull/10358:121,Availability,avail,available,121,"Existing tables and matrix tables that we don't own/didn't create but are accessible via the datasets API are not always available on both GCS and S3 (e.g. pan UKB datasets are only on AWS S3), or are not available in an `eu` bucket on GCS (e.g. gnomAD datasets). . This just adds a column `cloud: [regions]` to the tables on the datasets API and annotation DB docs pages to be more explicit about what datasets/versions are available on which cloud platform. . So, for example, if a version of a dataset is in `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1`, then under `cloud: [regions]` we would see `gcp: [eu,us], aws: [us]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10358
https://github.com/hail-is/hail/pull/10358:205,Availability,avail,available,205,"Existing tables and matrix tables that we don't own/didn't create but are accessible via the datasets API are not always available on both GCS and S3 (e.g. pan UKB datasets are only on AWS S3), or are not available in an `eu` bucket on GCS (e.g. gnomAD datasets). . This just adds a column `cloud: [regions]` to the tables on the datasets API and annotation DB docs pages to be more explicit about what datasets/versions are available on which cloud platform. . So, for example, if a version of a dataset is in `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1`, then under `cloud: [regions]` we would see `gcp: [eu,us], aws: [us]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10358
https://github.com/hail-is/hail/pull/10358:425,Availability,avail,available,425,"Existing tables and matrix tables that we don't own/didn't create but are accessible via the datasets API are not always available on both GCS and S3 (e.g. pan UKB datasets are only on AWS S3), or are not available in an `eu` bucket on GCS (e.g. gnomAD datasets). . This just adds a column `cloud: [regions]` to the tables on the datasets API and annotation DB docs pages to be more explicit about what datasets/versions are available on which cloud platform. . So, for example, if a version of a dataset is in `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1`, then under `cloud: [regions]` we would see `gcp: [eu,us], aws: [us]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10358
https://github.com/hail-is/hail/pull/10358:74,Security,access,accessible,74,"Existing tables and matrix tables that we don't own/didn't create but are accessible via the datasets API are not always available on both GCS and S3 (e.g. pan UKB datasets are only on AWS S3), or are not available in an `eu` bucket on GCS (e.g. gnomAD datasets). . This just adds a column `cloud: [regions]` to the tables on the datasets API and annotation DB docs pages to be more explicit about what datasets/versions are available on which cloud platform. . So, for example, if a version of a dataset is in `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1`, then under `cloud: [regions]` we would see `gcp: [eu,us], aws: [us]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10358
https://github.com/hail-is/hail/pull/10362:35,Availability,avail,available,35,`max-idle` and `max-age` have been available since 258 https://cloud.google.com/sdk/docs/release-notes#25800_2019-08-13; We already require >=285 https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/cli.py#L16,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10362
https://github.com/hail-is/hail/pull/10362:89,Deployability,release,release-notes,89,`max-idle` and `max-age` have been available since 258 https://cloud.google.com/sdk/docs/release-notes#25800_2019-08-13; We already require >=285 https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/cli.py#L16,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10362
https://github.com/hail-is/hail/pull/10364:123,Availability,error,error,123,"I do not fully understand why, but pytest sometimes imports Hail in threads; other than the main thread. Asyncio raises an error if you try to start an; event loop in a non-main thread. This PR only runs nest_asyncio if there is already a running event loop. If; there is no running event loop, we do not need to run nest_asyncio anyway! If; there is a running event loop, we can modify it to be nest-able even from; a non-main thread.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10364
https://github.com/hail-is/hail/pull/10365:245,Integrability,message,message,245,"`exc` is a pytest exception holder. `exc.value` is the actual exception. If pytest has to call `__str__(self)` on an object, pytest will automatically; truncate the result. However, pytest does not truncate `str` objects, that is; why I added a message to the assert and ensured that the message is already a; `str` before pytest sees it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10365
https://github.com/hail-is/hail/pull/10365:288,Integrability,message,message,288,"`exc` is a pytest exception holder. `exc.value` is the actual exception. If pytest has to call `__str__(self)` on an object, pytest will automatically; truncate the result. However, pytest does not truncate `str` objects, that is; why I added a message to the assert and ensured that the message is already a; `str` before pytest sees it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10365
https://github.com/hail-is/hail/pull/10365:260,Testability,assert,assert,260,"`exc` is a pytest exception holder. `exc.value` is the actual exception. If pytest has to call `__str__(self)` on an object, pytest will automatically; truncate the result. However, pytest does not truncate `str` objects, that is; why I added a message to the assert and ensured that the message is already a; `str` before pytest sees it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10365
https://github.com/hail-is/hail/pull/10367:309,Deployability,update,update,309,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10367:620,Deployability,update,update,620,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10367:829,Deployability,update,update,829,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10367:912,Deployability,update,update,912,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10367:992,Deployability,update,update,992,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10367:1073,Deployability,update,update,1073,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10367:690,Energy Efficiency,schedul,scheduling,690,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10367:50,Testability,test,tests,50,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10367
https://github.com/hail-is/hail/pull/10370:93,Modifiability,Plugin,Plugins,93,"Deleted the `gsutil cat` line, it wasn't doing anything because of that erroneous `/vep_data/Plugins.tar`. I don't think plugin needs to be included here, as it's in the docker image. . Deleted the `gsutil cp`, as the file it referenced did not exist. Grabbed the 1var.vcf from the already copied loftee_data. . It's not clear to me why we don't do the `1var.vcf` vep run stuff when using `GRCh38`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10370
https://github.com/hail-is/hail/pull/10370:121,Modifiability,plugin,plugin,121,"Deleted the `gsutil cat` line, it wasn't doing anything because of that erroneous `/vep_data/Plugins.tar`. I don't think plugin needs to be included here, as it's in the docker image. . Deleted the `gsutil cp`, as the file it referenced did not exist. Grabbed the 1var.vcf from the already copied loftee_data. . It's not clear to me why we don't do the `1var.vcf` vep run stuff when using `GRCh38`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10370
https://github.com/hail-is/hail/pull/10370:321,Usability,clear,clear,321,"Deleted the `gsutil cat` line, it wasn't doing anything because of that erroneous `/vep_data/Plugins.tar`. I don't think plugin needs to be included here, as it's in the docker image. . Deleted the `gsutil cp`, as the file it referenced did not exist. Grabbed the 1var.vcf from the already copied loftee_data. . It's not clear to me why we don't do the `1var.vcf` vep run stuff when using `GRCh38`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10370
https://github.com/hail-is/hail/pull/10376:1653,Availability,reliab,reliable,1653,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10376:1674,Availability,reliab,reliable,1674,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10376:271,Modifiability,config,configure,271,"This is the first step to removing batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10376:1591,Modifiability,config,configuring,1591,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10376:1957,Modifiability,variab,variables,1957,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10376:933,Performance,cache,cached,933,"This is the first step to removing batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10376:1635,Performance,perform,performance,1635,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10376:1698,Performance,perform,performance,1698,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10376
https://github.com/hail-is/hail/pull/10379:88,Availability,error,error,88,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:1446,Energy Efficiency,adapt,adapted,1446,.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:131); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:224,Integrability,Wrap,WrappedArray,224,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:1678,Integrability,Wrap,WrappedArray,1678,g.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:131); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); E 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:69); E 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:128); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeJSON$1(L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:1699,Integrability,Wrap,WrappedArray,1699,tedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:131); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); E 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:69); E 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:128); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeJSON$1(LocalBackend.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:1446,Modifiability,adapt,adapted,1446,.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:131); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); E 	at is.hail.utils.package$.using(package.scala:627); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:22,Testability,test,tests,22,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:134,Testability,Assert,AssertionError,134,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:150,Testability,assert,assertion,150,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10379:260,Testability,assert,assert,260,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10379
https://github.com/hail-is/hail/pull/10381:77,Availability,error,errors,77,I have no idea how this ever worked. It should have triggered UTF-8 decoding errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10381
https://github.com/hail-is/hail/pull/10390:539,Availability,down,download,539,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:652,Availability,down,downloading,652,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1701,Availability,error,error,1701,"assLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:95,Deployability,deploy,deployment,95,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:3638,Deployability,update,updated,3638,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:4472,Deployability,pipeline,pipelines,4472,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1707,Integrability,message,message,1707,"assLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1781,Integrability,message,message,1781,"assLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:2588,Integrability,contract,contract,2588," service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurren",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:2941,Integrability,protocol,protocol,2941,"ta out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:2998,Integrability,protocol,protocols,2998,"bjects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespace",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:3723,Modifiability,config,config,3723,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:212,Performance,load,load,212,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:426,Performance,cache,cache,426,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:848,Performance,load,load,848,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:947,Performance,multi-thread,multi-threaded,947,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:3590,Performance,concurren,concurrent,3590,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1916,Safety,Safe,SafeRow,1916,"threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol whe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:3510,Security,expose,expose,3510,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:3601,Security,hash,hash,3601,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1396,Testability,test,tests,1396," JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1408,Testability,test,tests,1408,"sspath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whos",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1461,Testability,test,tests,1461,". I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1550,Testability,test,tests,1550,"e JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1566,Testability,test,tests,1566,"es a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`Inpu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1608,Testability,test,tests,1608,"es a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`Inpu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:1727,Testability,test,tests,1727,"assLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3. Re-enable serialization of GoogleStorageFS (including its private key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:3947,Testability,test,test,3947,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:3969,Testability,test,test,3969,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10390:362,Usability,simpl,simply,362,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10390
https://github.com/hail-is/hail/pull/10394:44,Deployability,update,update,44,I changed this late in the PR and forgot to update the Makefile.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10394
https://github.com/hail-is/hail/pull/10396:11,Performance,cache,cache-repo,11,"Without `--cache-repo`, Kaniko uses `{image_name}/cache` (e.g. `gcr.io/hail-vdc/batch-worker/cache`). That works particularly poorly for all the test images which are `gcr.io/hail-vdc/ci-intermediate` and therefore don't share a cache with the default images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10396
https://github.com/hail-is/hail/pull/10396:50,Performance,cache,cache,50,"Without `--cache-repo`, Kaniko uses `{image_name}/cache` (e.g. `gcr.io/hail-vdc/batch-worker/cache`). That works particularly poorly for all the test images which are `gcr.io/hail-vdc/ci-intermediate` and therefore don't share a cache with the default images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10396
https://github.com/hail-is/hail/pull/10396:93,Performance,cache,cache,93,"Without `--cache-repo`, Kaniko uses `{image_name}/cache` (e.g. `gcr.io/hail-vdc/batch-worker/cache`). That works particularly poorly for all the test images which are `gcr.io/hail-vdc/ci-intermediate` and therefore don't share a cache with the default images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10396
https://github.com/hail-is/hail/pull/10396:229,Performance,cache,cache,229,"Without `--cache-repo`, Kaniko uses `{image_name}/cache` (e.g. `gcr.io/hail-vdc/batch-worker/cache`). That works particularly poorly for all the test images which are `gcr.io/hail-vdc/ci-intermediate` and therefore don't share a cache with the default images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10396
https://github.com/hail-is/hail/pull/10396:145,Testability,test,test,145,"Without `--cache-repo`, Kaniko uses `{image_name}/cache` (e.g. `gcr.io/hail-vdc/batch-worker/cache`). That works particularly poorly for all the test images which are `gcr.io/hail-vdc/ci-intermediate` and therefore don't share a cache with the default images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10396
https://github.com/hail-is/hail/pull/10398:173,Performance,perform,performant,173,"I mostly want this for debugging lowering on `LocalBackend`, but I added a SparkBackend implementation as well (goes through breeze and java literals, not going to be super performant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10398
https://github.com/hail-is/hail/pull/10399:36,Availability,error,errors,36,"I do not know why but /etc triggers errors about:; ```; archive/tar: write too long; ```; Even though /etc is not very large (1.4MB). I suspect there is some symlink; or other nonsense which is breaking Kaniko. The solution, after much trial and error, was simple: copy over directories that do not; cause issues and copy only the necessary files out of etc. A mix of speculation and; binary search lead me to the conclusion that /etc/ld.so.* are the only files necessary; from /etc for python to run correctly. These files tell the kernel how to link python3.7; to the various libraries on which it depends (which live in lib and lib64). Anyway, I've tested that this image can build itself, so it should be good enough for; our purposes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10399
https://github.com/hail-is/hail/pull/10399:246,Availability,error,error,246,"I do not know why but /etc triggers errors about:; ```; archive/tar: write too long; ```; Even though /etc is not very large (1.4MB). I suspect there is some symlink; or other nonsense which is breaking Kaniko. The solution, after much trial and error, was simple: copy over directories that do not; cause issues and copy only the necessary files out of etc. A mix of speculation and; binary search lead me to the conclusion that /etc/ld.so.* are the only files necessary; from /etc for python to run correctly. These files tell the kernel how to link python3.7; to the various libraries on which it depends (which live in lib and lib64). Anyway, I've tested that this image can build itself, so it should be good enough for; our purposes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10399
https://github.com/hail-is/hail/pull/10399:600,Integrability,depend,depends,600,"I do not know why but /etc triggers errors about:; ```; archive/tar: write too long; ```; Even though /etc is not very large (1.4MB). I suspect there is some symlink; or other nonsense which is breaking Kaniko. The solution, after much trial and error, was simple: copy over directories that do not; cause issues and copy only the necessary files out of etc. A mix of speculation and; binary search lead me to the conclusion that /etc/ld.so.* are the only files necessary; from /etc for python to run correctly. These files tell the kernel how to link python3.7; to the various libraries on which it depends (which live in lib and lib64). Anyway, I've tested that this image can build itself, so it should be good enough for; our purposes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10399
https://github.com/hail-is/hail/pull/10399:652,Testability,test,tested,652,"I do not know why but /etc triggers errors about:; ```; archive/tar: write too long; ```; Even though /etc is not very large (1.4MB). I suspect there is some symlink; or other nonsense which is breaking Kaniko. The solution, after much trial and error, was simple: copy over directories that do not; cause issues and copy only the necessary files out of etc. A mix of speculation and; binary search lead me to the conclusion that /etc/ld.so.* are the only files necessary; from /etc for python to run correctly. These files tell the kernel how to link python3.7; to the various libraries on which it depends (which live in lib and lib64). Anyway, I've tested that this image can build itself, so it should be good enough for; our purposes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10399
https://github.com/hail-is/hail/pull/10399:257,Usability,simpl,simple,257,"I do not know why but /etc triggers errors about:; ```; archive/tar: write too long; ```; Even though /etc is not very large (1.4MB). I suspect there is some symlink; or other nonsense which is breaking Kaniko. The solution, after much trial and error, was simple: copy over directories that do not; cause issues and copy only the necessary files out of etc. A mix of speculation and; binary search lead me to the conclusion that /etc/ld.so.* are the only files necessary; from /etc for python to run correctly. These files tell the kernel how to link python3.7; to the various libraries on which it depends (which live in lib and lib64). Anyway, I've tested that this image can build itself, so it should be good enough for; our purposes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10399
https://github.com/hail-is/hail/pull/10400:202,Safety,risk,risk,202,"I think this was a red-herring caused by Kaniko blowing disk; trying to copy special files. If this PR passes, then Kaniko is able to build itself with normal workers; and no extra disk, so there is no risk of another firedrill wherein; Kaniko is broken",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10400
https://github.com/hail-is/hail/pull/10407:40,Integrability,message,message,40,"This works by parsing log files for the message produced by TaskContext cleanup added in #10392. A bit hacky, but it works!. ```; Benchmark Name Ratio Time 1 Time 2 Mem Ratio Mem 1 (MB) Mem 2 (MB); -------------- ----- ------ ------ --------- ---------- ----------; union_p1000_p1000 116.0% 28.394 32.935 100.0% 1 1; union_p10_p100 114.8% 33.685 38.683 100.0% 1 1; compile_2k_merge 112.1% 253.023 283.740 100.0% 7 7; ndarray_matmul_int64_benchmark 110.1% 8.165 8.992 100.0% 1 1; shuffle_key_by_aggregate_bad_locality 108.9% 414.396 451.457 100.0% 4 4; shuffle_order_by_10m_int 108.7% 95.688 103.993 100.0% 1 1; import_and_transform_gvcf 107.0% 109.282 116.941 100.0% 1 1; matrix_table_entries_show 106.8% 1.318 1.408 100.0% 2 2; sample_qc 106.7% 31.869 34.002 100.0% 2 2; matrix_table_scan_count_rows 105.3% 129.907 136.832 100.0% 1 1; shuffle_key_rows_by_mt 104.9% 25.313 26.553 100.0% 3 3; matrix_table_entries_table_no_key 104.9% 47.893 50.225 100.0% 1 1; table_aggregate_downsample_worst_case 104.7% 33.056 34.625 100.0% 1 1; python_only_10k_transform 104.6% 69.072 72.271 100.0% 1 1; shuffle_key_by_aggregate_good_locality 104.4% 10.776 11.250 100.0% 24 24; table_aggregate_take_by_strings 104.3% 7.718 8.049 100.0% 1 1; kyle_sex_specific_qc 104.2% 9.326 9.715 100.0% 1 1; union_p100_p100 104.1% 24.184 25.174 100.0% 1 1; table_scan_prev_non_null 103.6% 111.901 115.887 100.0% 1 1; write_range_matrix_table_p100 103.3% 6.568 6.786 100.0% 1 1; read_force_count_p10 103.2% 3.644 3.760 100.0% 1 1; variant_and_sample_qc_nested_with_filters_4 103.1% 33.173 34.204 100.0% 2 2; concordance 102.8% 34.514 35.475 100.0% 3 3; table_foreign_key_join_same_cardinality 102.4% 17.859 18.279 100.0% 2 2; gnomad_coverage_stats_optimized 102.3% 27.443 28.087 100.0% 1 1; table_group_by_aggregate_sorted 101.6% 7.337 7.458 100.0% 4 4; write_range_table_p1000 101.0% 43.753 44.186 100.0% 1 1; table_range_means 100.9% 9.722 9.810 100.0% 1 1; matrix_table_decode_and_count_just_gt 100.7% 5.026 5.063 100.0% 1 1; imp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10407
https://github.com/hail-is/hail/pull/10407:22,Testability,log,log,22,"This works by parsing log files for the message produced by TaskContext cleanup added in #10392. A bit hacky, but it works!. ```; Benchmark Name Ratio Time 1 Time 2 Mem Ratio Mem 1 (MB) Mem 2 (MB); -------------- ----- ------ ------ --------- ---------- ----------; union_p1000_p1000 116.0% 28.394 32.935 100.0% 1 1; union_p10_p100 114.8% 33.685 38.683 100.0% 1 1; compile_2k_merge 112.1% 253.023 283.740 100.0% 7 7; ndarray_matmul_int64_benchmark 110.1% 8.165 8.992 100.0% 1 1; shuffle_key_by_aggregate_bad_locality 108.9% 414.396 451.457 100.0% 4 4; shuffle_order_by_10m_int 108.7% 95.688 103.993 100.0% 1 1; import_and_transform_gvcf 107.0% 109.282 116.941 100.0% 1 1; matrix_table_entries_show 106.8% 1.318 1.408 100.0% 2 2; sample_qc 106.7% 31.869 34.002 100.0% 2 2; matrix_table_scan_count_rows 105.3% 129.907 136.832 100.0% 1 1; shuffle_key_rows_by_mt 104.9% 25.313 26.553 100.0% 3 3; matrix_table_entries_table_no_key 104.9% 47.893 50.225 100.0% 1 1; table_aggregate_downsample_worst_case 104.7% 33.056 34.625 100.0% 1 1; python_only_10k_transform 104.6% 69.072 72.271 100.0% 1 1; shuffle_key_by_aggregate_good_locality 104.4% 10.776 11.250 100.0% 24 24; table_aggregate_take_by_strings 104.3% 7.718 8.049 100.0% 1 1; kyle_sex_specific_qc 104.2% 9.326 9.715 100.0% 1 1; union_p100_p100 104.1% 24.184 25.174 100.0% 1 1; table_scan_prev_non_null 103.6% 111.901 115.887 100.0% 1 1; write_range_matrix_table_p100 103.3% 6.568 6.786 100.0% 1 1; read_force_count_p10 103.2% 3.644 3.760 100.0% 1 1; variant_and_sample_qc_nested_with_filters_4 103.1% 33.173 34.204 100.0% 2 2; concordance 102.8% 34.514 35.475 100.0% 3 3; table_foreign_key_join_same_cardinality 102.4% 17.859 18.279 100.0% 2 2; gnomad_coverage_stats_optimized 102.3% 27.443 28.087 100.0% 1 1; table_group_by_aggregate_sorted 101.6% 7.337 7.458 100.0% 4 4; write_range_table_p1000 101.0% 43.753 44.186 100.0% 1 1; table_range_means 100.9% 9.722 9.810 100.0% 1 1; matrix_table_decode_and_count_just_gt 100.7% 5.026 5.063 100.0% 1 1; imp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10407
https://github.com/hail-is/hail/pull/10407:130,Testability,Benchmark,Benchmark,130,"This works by parsing log files for the message produced by TaskContext cleanup added in #10392. A bit hacky, but it works!. ```; Benchmark Name Ratio Time 1 Time 2 Mem Ratio Mem 1 (MB) Mem 2 (MB); -------------- ----- ------ ------ --------- ---------- ----------; union_p1000_p1000 116.0% 28.394 32.935 100.0% 1 1; union_p10_p100 114.8% 33.685 38.683 100.0% 1 1; compile_2k_merge 112.1% 253.023 283.740 100.0% 7 7; ndarray_matmul_int64_benchmark 110.1% 8.165 8.992 100.0% 1 1; shuffle_key_by_aggregate_bad_locality 108.9% 414.396 451.457 100.0% 4 4; shuffle_order_by_10m_int 108.7% 95.688 103.993 100.0% 1 1; import_and_transform_gvcf 107.0% 109.282 116.941 100.0% 1 1; matrix_table_entries_show 106.8% 1.318 1.408 100.0% 2 2; sample_qc 106.7% 31.869 34.002 100.0% 2 2; matrix_table_scan_count_rows 105.3% 129.907 136.832 100.0% 1 1; shuffle_key_rows_by_mt 104.9% 25.313 26.553 100.0% 3 3; matrix_table_entries_table_no_key 104.9% 47.893 50.225 100.0% 1 1; table_aggregate_downsample_worst_case 104.7% 33.056 34.625 100.0% 1 1; python_only_10k_transform 104.6% 69.072 72.271 100.0% 1 1; shuffle_key_by_aggregate_good_locality 104.4% 10.776 11.250 100.0% 24 24; table_aggregate_take_by_strings 104.3% 7.718 8.049 100.0% 1 1; kyle_sex_specific_qc 104.2% 9.326 9.715 100.0% 1 1; union_p100_p100 104.1% 24.184 25.174 100.0% 1 1; table_scan_prev_non_null 103.6% 111.901 115.887 100.0% 1 1; write_range_matrix_table_p100 103.3% 6.568 6.786 100.0% 1 1; read_force_count_p10 103.2% 3.644 3.760 100.0% 1 1; variant_and_sample_qc_nested_with_filters_4 103.1% 33.173 34.204 100.0% 2 2; concordance 102.8% 34.514 35.475 100.0% 3 3; table_foreign_key_join_same_cardinality 102.4% 17.859 18.279 100.0% 2 2; gnomad_coverage_stats_optimized 102.3% 27.443 28.087 100.0% 1 1; table_group_by_aggregate_sorted 101.6% 7.337 7.458 100.0% 4 4; write_range_table_p1000 101.0% 43.753 44.186 100.0% 1 1; table_range_means 100.9% 9.722 9.810 100.0% 1 1; matrix_table_decode_and_count_just_gt 100.7% 5.026 5.063 100.0% 1 1; imp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10407
https://github.com/hail-is/hail/pull/10408:22,Deployability,update,updated,22,It was unused / never updated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10408
https://github.com/hail-is/hail/pull/10411:105,Deployability,update,update,105,Tested this manually to make sure that the build-worker instance ran to completion (it fails on `apt-get update` currently),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10411
https://github.com/hail-is/hail/pull/10411:0,Testability,Test,Tested,0,Tested this manually to make sure that the build-worker instance ran to completion (it fails on `apt-get update` currently),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10411
https://github.com/hail-is/hail/pull/10412:269,Safety,avoid,avoid,269,"`curlylint` doesn't like logical blocks that produce incomplete html tags. This is the only instance in our codebase where we do that though. In either path of the if/else block we produce an open `a` tag and then close it out outside of the block. I reorganized it to avoid this and think it's actually more clear, and that creating open tags like this is a footgun to avoid since it's super easy not to close them (which we have had a lot).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10412
https://github.com/hail-is/hail/pull/10412:370,Safety,avoid,avoid,370,"`curlylint` doesn't like logical blocks that produce incomplete html tags. This is the only instance in our codebase where we do that though. In either path of the if/else block we produce an open `a` tag and then close it out outside of the block. I reorganized it to avoid this and think it's actually more clear, and that creating open tags like this is a footgun to avoid since it's super easy not to close them (which we have had a lot).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10412
https://github.com/hail-is/hail/pull/10412:25,Testability,log,logical,25,"`curlylint` doesn't like logical blocks that produce incomplete html tags. This is the only instance in our codebase where we do that though. In either path of the if/else block we produce an open `a` tag and then close it out outside of the block. I reorganized it to avoid this and think it's actually more clear, and that creating open tags like this is a footgun to avoid since it's super easy not to close them (which we have had a lot).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10412
https://github.com/hail-is/hail/pull/10412:309,Usability,clear,clear,309,"`curlylint` doesn't like logical blocks that produce incomplete html tags. This is the only instance in our codebase where we do that though. In either path of the if/else block we produce an open `a` tag and then close it out outside of the block. I reorganized it to avoid this and think it's actually more clear, and that creating open tags like this is a footgun to avoid since it's super easy not to close them (which we have had a lot).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10412
https://github.com/hail-is/hail/pull/10413:425,Deployability,release,release,425,"Adds the [1000 Genomes NYGC 30x on GRCh38](https://www.internationalgenome.org/data-portal/data-collection/30x-grch38) autosomes, chrX, and chrY MatrixTables as `1000_Genomes_HighCov_autosomes`, `1000_Genomes_HighCov_chrX`, and `1000_Genomes_HighCov_chrY`. . The `1000_Genomes_HighCov_autosomes` and `1000_Genomes_HighCov_chrX` datasets have phased calls. . [Due to multiple issues](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/README_GRCh38_liftover_20170504.txt), the phase 3 GRCh38 versions of the current `1000_Genomes_autosomes`, `1000_Genomes_chrX`, and `1000_Genomes_chrY` datasets have been retracted. They have been renamed to `1000_Genomes_Retracted_autosomes`, `1000_Genomes_Retracted_chrX`, and `1000_Genomes_Retracted_chrY`, respectively. The phase 3 GRCh37 versions of `1000_Genomes_autosomes`, `1000_Genomes_chrX`, and `1000_Genomes_chrY` are not affected and are still accessible as before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10413
https://github.com/hail-is/hail/pull/10413:927,Security,access,accessible,927,"Adds the [1000 Genomes NYGC 30x on GRCh38](https://www.internationalgenome.org/data-portal/data-collection/30x-grch38) autosomes, chrX, and chrY MatrixTables as `1000_Genomes_HighCov_autosomes`, `1000_Genomes_HighCov_chrX`, and `1000_Genomes_HighCov_chrY`. . The `1000_Genomes_HighCov_autosomes` and `1000_Genomes_HighCov_chrX` datasets have phased calls. . [Due to multiple issues](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/README_GRCh38_liftover_20170504.txt), the phase 3 GRCh38 versions of the current `1000_Genomes_autosomes`, `1000_Genomes_chrX`, and `1000_Genomes_chrY` datasets have been retracted. They have been renamed to `1000_Genomes_Retracted_autosomes`, `1000_Genomes_Retracted_chrX`, and `1000_Genomes_Retracted_chrY`, respectively. The phase 3 GRCh37 versions of `1000_Genomes_autosomes`, `1000_Genomes_chrX`, and `1000_Genomes_chrY` are not affected and are still accessible as before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10413
https://github.com/hail-is/hail/pull/10414:8,Availability,down,download,8,"When we download files from GCS, they lose their permissions. Git complains and then the checkout fails.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10414
https://github.com/hail-is/hail/pull/10415:30,Testability,test,tests,30,"I split it up into 5 separate tests, and I took away one of the 3 data sizes we test on, so it's faster and more easily split now. Let me know if you think there's other things we can remove/change without losing information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10415
https://github.com/hail-is/hail/pull/10415:80,Testability,test,test,80,"I split it up into 5 separate tests, and I took away one of the 3 data sizes we test on, so it's faster and more easily split now. Let me know if you think there's other things we can remove/change without losing information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10415
https://github.com/hail-is/hail/pull/10416:272,Usability,simpl,simplicity,272,* compiled iterators use the wrong region; * TableRange generates IR that does not flag itself as memory managed; * TableParallelize generates IR that does not flag itself as memory managed; * RepartitionNoShuffle generates a mixed flatMap; make it all memory managed for simplicity.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10416
https://github.com/hail-is/hail/pull/10426:355,Modifiability,refactor,refactoring,355,"This was an easy one, since we already had `BlockMatrixBroadcast` lowered. All `ValueToBlockMatrix` does is make a tiny 1x1 BlockMatrix that can later be broadcast. I kind of think this is a weird IR design and that we should actually have a `BlockMatrixBroadcast` that takes a value `IR` child instead of a `BlockMatrixIR`, but I'll experiment with that refactoring later. Trying to just get the current thing lowered.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10426
https://github.com/hail-is/hail/pull/10429:54,Availability,failure,failures,54,"Also, make sure we always delete the disk despite any failures",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10429
https://github.com/hail-is/hail/pull/10438:41,Deployability,configurat,configuration,41,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10438:272,Deployability,configurat,configuration,272,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10438:41,Modifiability,config,configuration,41,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10438:133,Modifiability,config,config,133,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10438:189,Modifiability,config,config,189,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10438:214,Modifiability,config,config,214,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10438:272,Modifiability,config,configuration,272,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10438:315,Modifiability,plugin,plugins,315,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint‚Ä¶), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10438
https://github.com/hail-is/hail/pull/10447:1448,Deployability,update,update,1448,"In this PR:. - I add `BlockMatrix.from_ndarray`. The implementation isn't great, it just basically just evals the ndarray and adds NDArray support to `ValueToBlockMatrix`. A better version wouldn't cross the Python / Java boundary at all, but I want something that works on all backends, so for now this will have to suffice. Any solution will at least need to communicate the shape of the ndarray back to python, since it's tracked in the block matrix type. ; - With this new method, I can now get many tests in `test_linalg.py` to run on the local / service backends. Most BM lowering was apparently untested before, so some bug fixes were necessary, including:; - Support requiredness analysis on BlockMatrix, even though the answer is always required. ; - Use `CompileAndEvaluate` rather than `Interpret` to evaluate the child node in `ValueToBlockMatrix`. ; - Casting between Int32 and Int64 in various places in lowering. Almost always the culprit was a bad interaction between ndarray shapes (which are Int64) and `StreamRange` argument (which is an Int32). This is sort of a pervasive ndarray problem that will need to be systematically addressed at some point. I don't anticipate anyone making a BlockMatrix with blocks big enough to blow 32 bits though. ; - Lots of fixes to the `BlockMatrixBroadcast` rule for getting diagonal of a BlockMatrix, as it was clearly never run. It had `MakeStream(StreamIR)` which was not allowed, it didn't update the context appropriately, and it used the wrong axis to determine if something was a row vector.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10447
https://github.com/hail-is/hail/pull/10447:504,Testability,test,tests,504,"In this PR:. - I add `BlockMatrix.from_ndarray`. The implementation isn't great, it just basically just evals the ndarray and adds NDArray support to `ValueToBlockMatrix`. A better version wouldn't cross the Python / Java boundary at all, but I want something that works on all backends, so for now this will have to suffice. Any solution will at least need to communicate the shape of the ndarray back to python, since it's tracked in the block matrix type. ; - With this new method, I can now get many tests in `test_linalg.py` to run on the local / service backends. Most BM lowering was apparently untested before, so some bug fixes were necessary, including:; - Support requiredness analysis on BlockMatrix, even though the answer is always required. ; - Use `CompileAndEvaluate` rather than `Interpret` to evaluate the child node in `ValueToBlockMatrix`. ; - Casting between Int32 and Int64 in various places in lowering. Almost always the culprit was a bad interaction between ndarray shapes (which are Int64) and `StreamRange` argument (which is an Int32). This is sort of a pervasive ndarray problem that will need to be systematically addressed at some point. I don't anticipate anyone making a BlockMatrix with blocks big enough to blow 32 bits though. ; - Lots of fixes to the `BlockMatrixBroadcast` rule for getting diagonal of a BlockMatrix, as it was clearly never run. It had `MakeStream(StreamIR)` which was not allowed, it didn't update the context appropriately, and it used the wrong axis to determine if something was a row vector.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10447
https://github.com/hail-is/hail/pull/10447:1366,Usability,clear,clearly,1366,"In this PR:. - I add `BlockMatrix.from_ndarray`. The implementation isn't great, it just basically just evals the ndarray and adds NDArray support to `ValueToBlockMatrix`. A better version wouldn't cross the Python / Java boundary at all, but I want something that works on all backends, so for now this will have to suffice. Any solution will at least need to communicate the shape of the ndarray back to python, since it's tracked in the block matrix type. ; - With this new method, I can now get many tests in `test_linalg.py` to run on the local / service backends. Most BM lowering was apparently untested before, so some bug fixes were necessary, including:; - Support requiredness analysis on BlockMatrix, even though the answer is always required. ; - Use `CompileAndEvaluate` rather than `Interpret` to evaluate the child node in `ValueToBlockMatrix`. ; - Casting between Int32 and Int64 in various places in lowering. Almost always the culprit was a bad interaction between ndarray shapes (which are Int64) and `StreamRange` argument (which is an Int32). This is sort of a pervasive ndarray problem that will need to be systematically addressed at some point. I don't anticipate anyone making a BlockMatrix with blocks big enough to blow 32 bits though. ; - Lots of fixes to the `BlockMatrixBroadcast` rule for getting diagonal of a BlockMatrix, as it was clearly never run. It had `MakeStream(StreamIR)` which was not allowed, it didn't update the context appropriately, and it used the wrong axis to determine if something was a row vector.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10447
https://github.com/hail-is/hail/pull/10448:24,Performance,latency,latency,24,Service tests have high latency to the JVM. Let's not separately call `hl.eval` so many times when we don't need to.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10448
https://github.com/hail-is/hail/pull/10448:8,Testability,test,tests,8,Service tests have high latency to the JVM. Let's not separately call `hl.eval` so many times when we don't need to.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10448
https://github.com/hail-is/hail/pull/10451:395,Testability,log,logic,395,"CHANGELOG: Fixed a memory leak triggered by `hl.literal(...).contains(...). This bug is present elsewhere in the code generator, but the set; contains function is probably the worst place for it to happen.; This leads to a full copy of the set where the binary search; is executed. The core problem was a bug in PArrayBackedContainer not casting; its codes properly, leading to the no-op coerce logic in PCanonicalArray; being bypassed in favor of the generic copy-the-world implementation of; `store`. The test I have added catches the memory leak, but now fails at; compile time at the assertion to `PCanonicalArray.store` instead.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10451
https://github.com/hail-is/hail/pull/10451:507,Testability,test,test,507,"CHANGELOG: Fixed a memory leak triggered by `hl.literal(...).contains(...). This bug is present elsewhere in the code generator, but the set; contains function is probably the worst place for it to happen.; This leads to a full copy of the set where the binary search; is executed. The core problem was a bug in PArrayBackedContainer not casting; its codes properly, leading to the no-op coerce logic in PCanonicalArray; being bypassed in favor of the generic copy-the-world implementation of; `store`. The test I have added catches the memory leak, but now fails at; compile time at the assertion to `PCanonicalArray.store` instead.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10451
https://github.com/hail-is/hail/pull/10451:588,Testability,assert,assertion,588,"CHANGELOG: Fixed a memory leak triggered by `hl.literal(...).contains(...). This bug is present elsewhere in the code generator, but the set; contains function is probably the worst place for it to happen.; This leads to a full copy of the set where the binary search; is executed. The core problem was a bug in PArrayBackedContainer not casting; its codes properly, leading to the no-op coerce logic in PCanonicalArray; being bypassed in favor of the generic copy-the-world implementation of; `store`. The test I have added catches the memory leak, but now fails at; compile time at the assertion to `PCanonicalArray.store` instead.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10451
https://github.com/hail-is/hail/pull/10453:246,Testability,benchmark,benchmark,246,"BLAS's DGEMM has an option for transposing input matrix, which is effectively just transposing. I've added support for this. It should save time and memory to not have to copy into column major format. . Didn't end up making my linear regression benchmark much faster, maybe 3%. It's using a bit less memory too.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10453
https://github.com/hail-is/hail/pull/10454:108,Testability,test,testing,108,"Opening this PR and then closing it immediately. For reference when we decide to come back to this code. My testing script is here:. ```python3; #! /usr/bin/python. import hail as hl. #vcf = 'gs://my-bucket/vep/test_variant.vcf'; vcf = 'gs://my-bucket/vep/inputs/loftee_variant_grch37.vcf'; #vcf = 'gs://my-bucket/vep/inputs/loftee_variant_grch38.vcf'. hl.init(). #mt = hl.import_vcf(vcf, reference_genome='GRCh38'); mt = hl.import_vcf(vcf, reference_genome='GRCh37'); mt = hl.vep(mt, requester_pays_project='my-project', tolerate_parse_error=True); #mt.write('gs://my-bucket/vep/test-output-grch38.mt', overwrite=True); mt.write('gs://my-bucket/vep/test-output-grch37.mt', overwrite=True); ht = mt.rows(); print(ht.vep.collect()); print(mt.globals); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10454
https://github.com/hail-is/hail/pull/10454:580,Testability,test,test-output-,580,"Opening this PR and then closing it immediately. For reference when we decide to come back to this code. My testing script is here:. ```python3; #! /usr/bin/python. import hail as hl. #vcf = 'gs://my-bucket/vep/test_variant.vcf'; vcf = 'gs://my-bucket/vep/inputs/loftee_variant_grch37.vcf'; #vcf = 'gs://my-bucket/vep/inputs/loftee_variant_grch38.vcf'. hl.init(). #mt = hl.import_vcf(vcf, reference_genome='GRCh38'); mt = hl.import_vcf(vcf, reference_genome='GRCh37'); mt = hl.vep(mt, requester_pays_project='my-project', tolerate_parse_error=True); #mt.write('gs://my-bucket/vep/test-output-grch38.mt', overwrite=True); mt.write('gs://my-bucket/vep/test-output-grch37.mt', overwrite=True); ht = mt.rows(); print(ht.vep.collect()); print(mt.globals); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10454
https://github.com/hail-is/hail/pull/10454:650,Testability,test,test-output-,650,"Opening this PR and then closing it immediately. For reference when we decide to come back to this code. My testing script is here:. ```python3; #! /usr/bin/python. import hail as hl. #vcf = 'gs://my-bucket/vep/test_variant.vcf'; vcf = 'gs://my-bucket/vep/inputs/loftee_variant_grch37.vcf'; #vcf = 'gs://my-bucket/vep/inputs/loftee_variant_grch38.vcf'. hl.init(). #mt = hl.import_vcf(vcf, reference_genome='GRCh38'); mt = hl.import_vcf(vcf, reference_genome='GRCh37'); mt = hl.vep(mt, requester_pays_project='my-project', tolerate_parse_error=True); #mt.write('gs://my-bucket/vep/test-output-grch38.mt', overwrite=True); mt.write('gs://my-bucket/vep/test-output-grch37.mt', overwrite=True); ht = mt.rows(); print(ht.vep.collect()); print(mt.globals); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10454
https://github.com/hail-is/hail/pull/10459:36,Testability,log,log,36,Make it easier to get to the change log from Hail's PyPI project page.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10459
https://github.com/hail-is/hail/pull/10464:104,Deployability,Release,Release,104,"Bumps [flask-cors](https://github.com/corydolphin/flask-cors) from 3.0.8 to 3.0.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/releases"">flask-cors's releases</a>.</em></p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:200,Deployability,release,releases,200,"Bumps [flask-cors](https://github.com/corydolphin/flask-cors) from 3.0.8 to 3.0.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/releases"">flask-cors's releases</a>.</em></p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:223,Deployability,release,releases,223,"Bumps [flask-cors](https://github.com/corydolphin/flask-cors) from 3.0.8 to 3.0.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/releases"">flask-cors's releases</a>.</em></p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:265,Deployability,Release,Release,265,"Bumps [flask-cors](https://github.com/corydolphin/flask-cors) from 3.0.8 to 3.0.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/releases"">flask-cors's releases</a>.</em></p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:1527,Deployability,Update,Update,1527,"CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentati",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:1798,Deployability,Release,Release,1798,"<details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:2653,Deployability,Update,Update,2653,"-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3267,Deployability,update,updates,3267,"e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4419,Deployability,upgrade,upgrade,4419,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4595,Deployability,upgrade,upgrade,4595,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4765,Deployability,upgrade,upgrade,4765,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:1603,Integrability,depend,dependabot,1603,"ly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:1854,Integrability,depend,dependabot,1854,"ask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:2114,Integrability,depend,dependabot,2114,"ource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-ma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:2984,Integrability,Depend,Dependabot,2984,"1b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3024,Integrability,depend,dependabot-badges,3024,"1b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3083,Integrability,depend,dependency-name,3083,"thub-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3247,Integrability,depend,dependabot-security-updates,3247,"e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3304,Integrability,Depend,Dependabot,3304," guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3450,Integrability,depend,dependabot,3450,"aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3479,Integrability,depend,dependabot-automerge-start,3479,"indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3517,Integrability,depend,dependabot-automerge-end,3517,"indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3569,Integrability,Depend,Dependabot,3569,"it/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3636,Integrability,Depend,Dependabot,3636,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3686,Integrability,depend,dependabot,3686,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3730,Integrability,depend,dependabot,3730,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3827,Integrability,depend,dependabot,3827,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3896,Integrability,depend,dependabot,3896,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3987,Integrability,depend,dependabot,3987,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4080,Integrability,depend,dependabot,4080,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4140,Integrability,depend,dependabot,4140,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4186,Integrability,Depend,Dependabot,4186,"/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4272,Integrability,depend,dependabot,4272,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4338,Integrability,Depend,Dependabot,4338,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4448,Integrability,depend,dependabot,4448,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4514,Integrability,Depend,Dependabot,4514,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4624,Integrability,depend,dependabot,4624,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4647,Integrability,depend,dependency,4647,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4687,Integrability,Depend,Dependabot,4687,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4725,Integrability,depend,dependency,4725,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4794,Integrability,depend,dependabot,4794,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:4913,Integrability,depend,dependabot,4913,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:5038,Integrability,depend,dependabot,5038,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:5163,Integrability,depend,dependabot,5163,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:289,Security,Secur,Security,289,"Bumps [flask-cors](https://github.com/corydolphin/flask-cors) from 3.0.8 to 3.0.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/releases"">flask-cors's releases</a>.</em></p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:966,Security,Secur,Security,966,"flask-cors](https://github.com/corydolphin/flask-cors) from 3.0.8 to 3.0.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/releases"">flask-cors's releases</a>.</em></p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3216,Security,secur,security-vulnerabilities,3216,"e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:3258,Security,secur,security-updates,3258,"e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:5309,Security,secur,security,5309,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:5350,Security,Secur,Security,5350,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:685,Usability,simpl,simply,685,"Bumps [flask-cors](https://github.com/corydolphin/flask-cors) from 3.0.8 to 3.0.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/releases"">flask-cors's releases</a>.</em></p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:1282,Usability,simpl,simply,1282,"</p>; <blockquote>; <h2>Release 3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks <a href=""https://github.com/praetorian-colby-morgan""><code>@‚Äãpraetorian-colby-morgan</code></a>). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/corydolphin/flask-cors/blob/master/CHANGELOG.md"">flask-cors's changelog</a>.</em></p>; <blockquote>; <h2>3.0.9</h2>; <h3>Security</h3>; <ul>; <li>Escape path before evaluating resource rules (thanks to Colby Morgan). Prior to this, flask-cors incorrectly; evaluated CORS resource matching before path expansion. E.g. &quot;/api/../foo.txt&quot; would incorrectly match resources for; &quot;/api/*&quot; whereas the path actually expands simply to &quot;/foo.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10464:2319,Usability,simpl,simple,2319,"o.txt&quot;</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/91babb941e07a1f45636bdcb75675f13ce1503a2""><code>91babb9</code></a> Update Api docs for credentialed requests (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/522d98936f3995480fe3132b55415d74298d6790""><code>522d989</code></a> Release version 3.0.9 (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/67c4b2cc98ae87cf1fa7df4f97fd81b40c79b895""><code>67c4b2c</code></a> Fix request path normalization (<a href=""https://github-redirect.dependabot.com/corydolphin/flask-cors/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/5c6e05e996f10be1df1f2ad178560e54a2f82f1b""><code>5c6e05e</code></a> docs: Fix simple typo, garaunteed -&gt; guaranteed</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/566aef21accd0a15cf127a41edbe14a40c80728c""><code>566aef2</code></a> Fixed over-indentation</li>; <li><a href=""https://github.com/corydolphin/flask-cors/commit/8a4e6e7057924d124a39ec08f446345bc19e4c5b""><code>8a4e6e7</code></a> Update changelog to give proper kudos to <a href=""https://github.com/juanmaneo""><code>@‚Äãjuanmaneo</code></a> and <a href=""https://github.com/jdevera""><code>@‚Äãjdevera</code></a></li>; <li>See full diff in <a href=""https://github.com/corydolphin/flask-cors/compare/3.0.8...3.0.9"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flask-cors&package-manager=pip&previous-version=3.0.8&new-version=3.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10464
https://github.com/hail-is/hail/pull/10467:18,Deployability,configurat,configuration,18,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10467
https://github.com/hail-is/hail/pull/10467:408,Deployability,configurat,configuration,408,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10467
https://github.com/hail-is/hail/pull/10467:18,Modifiability,config,configuration,18,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10467
https://github.com/hail-is/hail/pull/10467:408,Modifiability,config,configuration,408,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10467
https://github.com/hail-is/hail/pull/10467:390,Safety,avoid,avoids,390,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10467
https://github.com/hail-is/hail/pull/10469:625,Performance,perform,performance,625,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10469
https://github.com/hail-is/hail/pull/10469:38,Testability,test,test,38,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10469
https://github.com/hail-is/hail/pull/10469:59,Testability,log,logistic,59,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10469
https://github.com/hail-is/hail/pull/10469:183,Testability,log,logistic,183,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10469
https://github.com/hail-is/hail/pull/10469:268,Testability,test,test,268,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10469
https://github.com/hail-is/hail/pull/10469:428,Testability,log,logistic,428,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10469
https://github.com/hail-is/hail/pull/10469:597,Testability,benchmark,benchmark,597,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10469
https://github.com/hail-is/hail/pull/10473:332,Security,access,accession,332,"Adds dbSNP build 154 Hail Tables to datasets API and annotation DB. . The dataset named `dbSNP` includes all fields, and the one named `dbSNP_rsid` only contains locus, alleles, and rsIDs. The process used to generate the tables is outlined in the notebook. I needed to use the GRCh37/38 assembly reports to map contigs from RefSeq accession numbers to chromosomes, and could then import the VCFs. . To make parsing/formatting strings possible after importing the VCFs as MatrixTables, the following `INFO` fields in the VCF headers were changed from `Number=.` to `Number=1`: FREQ, CLNHGVS, CLNVI, CLNORIGIN, CLNSIG, CLNDISB, CLNDN, CLNREVSTAT, CLNACC. . Biallelic and multiallelic variants were present. The multiallelic variants were first filtered out to be split, and then were unioned back with the biallelic variants to create the final tables.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10473
https://github.com/hail-is/hail/pull/10474:43,Availability,down,download,43,Adds [CADD](https://cadd.gs.washington.edu/download) v1.6 Hail Tables to datasets API/annotation DB.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10474
https://github.com/hail-is/hail/pull/10476:208,Testability,test,test,208,"This PR lowers `MatrixBlockMatrixWriter`, which writes an entry field of a `MatrixTable` out into a `BlockMatrix`. Its main use is to support `BlockMatrix.from_entry_expr`. Accordingly, the `from_entry_expr` test on the `LocalBackend` is now passing. . Alright, removed all shuffles from this, should be good for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10476
https://github.com/hail-is/hail/pull/10481:122,Deployability,update,updated,122,"Before ndarrays, the two choices for children of `ValueToBlockMatrix` were array or float. This simplify rule hadn't been updated to reflect that ndarrays were now an option, so it was falling through to the float case when it saw an ndarray. . By fixing this, I've gotten `test_paired_elementwise_ops` to work on lowered backends.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10481
https://github.com/hail-is/hail/pull/10481:96,Usability,simpl,simplify,96,"Before ndarrays, the two choices for children of `ValueToBlockMatrix` were array or float. This simplify rule hadn't been updated to reflect that ndarrays were now an option, so it was falling through to the float case when it saw an ndarray. . By fixing this, I've gotten `test_paired_elementwise_ops` to work on lowered backends.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10481
https://github.com/hail-is/hail/pull/10482:5,Deployability,pipeline,pipelines,5,Most pipelines will take the old path.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10482
https://github.com/hail-is/hail/pull/10484:430,Modifiability,variab,variables,430,"Batch Client:; - Added new parameter max_idle_time (seconds); - Removed BatchBuilder and fused it with Batch; - Added 2 new operations:; - Commit ; - Close; - Kept submit which is the same as close for backwards compatibility reasons; - commit, close, and submit now return the batch so these methods can be chained together; - create_batch stayed the same. Tests:; - Added 3 new tests for new functionality; - Renamed a bunch of variables in the tests and cleaned up the variables. Database:; - Added time_last_updated for determining how long a batch has been idle; - Added max_idle_time; - Added a closed field and changed the possible states for a batch to created, running, and complete (removed open); - Changed close_batch to be commit_staged_jobs. Changes were made to make sure this worked even if there were 0 jobs to actually commit. ; - Changed cancel_batch to always set cancelled = 1 and closed = 1 regardless of whether the batch is actually running. The time_completed is only set if no jobs are currently running. Otherwise, the time_completed will be set in MJC. **It also commits any jobs that are pending before cancelling the batch.** I'm not sure if we want this behavior or not. Driver:; - Runs a loop every 60 seconds to close batches with max_idle_time greater than that specified. UI:; - The UI changed the batches table to be time_created instead of time_closed as the Submitted/Created column. The duration is the time from time_complete - time_created for newer batches instead of time_complete - time_closed.; - Added a close button; - An open batch (even one just in the created state) can be cancelled or closed. **This might be confusing**. Other:; - Cancel is idempotent; - Getting the batch state and time_completed correct was tricky.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10484
https://github.com/hail-is/hail/pull/10484:472,Modifiability,variab,variables,472,"Batch Client:; - Added new parameter max_idle_time (seconds); - Removed BatchBuilder and fused it with Batch; - Added 2 new operations:; - Commit ; - Close; - Kept submit which is the same as close for backwards compatibility reasons; - commit, close, and submit now return the batch so these methods can be chained together; - create_batch stayed the same. Tests:; - Added 3 new tests for new functionality; - Renamed a bunch of variables in the tests and cleaned up the variables. Database:; - Added time_last_updated for determining how long a batch has been idle; - Added max_idle_time; - Added a closed field and changed the possible states for a batch to created, running, and complete (removed open); - Changed close_batch to be commit_staged_jobs. Changes were made to make sure this worked even if there were 0 jobs to actually commit. ; - Changed cancel_batch to always set cancelled = 1 and closed = 1 regardless of whether the batch is actually running. The time_completed is only set if no jobs are currently running. Otherwise, the time_completed will be set in MJC. **It also commits any jobs that are pending before cancelling the batch.** I'm not sure if we want this behavior or not. Driver:; - Runs a loop every 60 seconds to close batches with max_idle_time greater than that specified. UI:; - The UI changed the batches table to be time_created instead of time_closed as the Submitted/Created column. The duration is the time from time_complete - time_created for newer batches instead of time_complete - time_closed.; - Added a close button; - An open batch (even one just in the created state) can be cancelled or closed. **This might be confusing**. Other:; - Cancel is idempotent; - Getting the batch state and time_completed correct was tricky.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10484
https://github.com/hail-is/hail/pull/10484:358,Testability,Test,Tests,358,"Batch Client:; - Added new parameter max_idle_time (seconds); - Removed BatchBuilder and fused it with Batch; - Added 2 new operations:; - Commit ; - Close; - Kept submit which is the same as close for backwards compatibility reasons; - commit, close, and submit now return the batch so these methods can be chained together; - create_batch stayed the same. Tests:; - Added 3 new tests for new functionality; - Renamed a bunch of variables in the tests and cleaned up the variables. Database:; - Added time_last_updated for determining how long a batch has been idle; - Added max_idle_time; - Added a closed field and changed the possible states for a batch to created, running, and complete (removed open); - Changed close_batch to be commit_staged_jobs. Changes were made to make sure this worked even if there were 0 jobs to actually commit. ; - Changed cancel_batch to always set cancelled = 1 and closed = 1 regardless of whether the batch is actually running. The time_completed is only set if no jobs are currently running. Otherwise, the time_completed will be set in MJC. **It also commits any jobs that are pending before cancelling the batch.** I'm not sure if we want this behavior or not. Driver:; - Runs a loop every 60 seconds to close batches with max_idle_time greater than that specified. UI:; - The UI changed the batches table to be time_created instead of time_closed as the Submitted/Created column. The duration is the time from time_complete - time_created for newer batches instead of time_complete - time_closed.; - Added a close button; - An open batch (even one just in the created state) can be cancelled or closed. **This might be confusing**. Other:; - Cancel is idempotent; - Getting the batch state and time_completed correct was tricky.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10484
https://github.com/hail-is/hail/pull/10484:380,Testability,test,tests,380,"Batch Client:; - Added new parameter max_idle_time (seconds); - Removed BatchBuilder and fused it with Batch; - Added 2 new operations:; - Commit ; - Close; - Kept submit which is the same as close for backwards compatibility reasons; - commit, close, and submit now return the batch so these methods can be chained together; - create_batch stayed the same. Tests:; - Added 3 new tests for new functionality; - Renamed a bunch of variables in the tests and cleaned up the variables. Database:; - Added time_last_updated for determining how long a batch has been idle; - Added max_idle_time; - Added a closed field and changed the possible states for a batch to created, running, and complete (removed open); - Changed close_batch to be commit_staged_jobs. Changes were made to make sure this worked even if there were 0 jobs to actually commit. ; - Changed cancel_batch to always set cancelled = 1 and closed = 1 regardless of whether the batch is actually running. The time_completed is only set if no jobs are currently running. Otherwise, the time_completed will be set in MJC. **It also commits any jobs that are pending before cancelling the batch.** I'm not sure if we want this behavior or not. Driver:; - Runs a loop every 60 seconds to close batches with max_idle_time greater than that specified. UI:; - The UI changed the batches table to be time_created instead of time_closed as the Submitted/Created column. The duration is the time from time_complete - time_created for newer batches instead of time_complete - time_closed.; - Added a close button; - An open batch (even one just in the created state) can be cancelled or closed. **This might be confusing**. Other:; - Cancel is idempotent; - Getting the batch state and time_completed correct was tricky.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10484
https://github.com/hail-is/hail/pull/10484:447,Testability,test,tests,447,"Batch Client:; - Added new parameter max_idle_time (seconds); - Removed BatchBuilder and fused it with Batch; - Added 2 new operations:; - Commit ; - Close; - Kept submit which is the same as close for backwards compatibility reasons; - commit, close, and submit now return the batch so these methods can be chained together; - create_batch stayed the same. Tests:; - Added 3 new tests for new functionality; - Renamed a bunch of variables in the tests and cleaned up the variables. Database:; - Added time_last_updated for determining how long a batch has been idle; - Added max_idle_time; - Added a closed field and changed the possible states for a batch to created, running, and complete (removed open); - Changed close_batch to be commit_staged_jobs. Changes were made to make sure this worked even if there were 0 jobs to actually commit. ; - Changed cancel_batch to always set cancelled = 1 and closed = 1 regardless of whether the batch is actually running. The time_completed is only set if no jobs are currently running. Otherwise, the time_completed will be set in MJC. **It also commits any jobs that are pending before cancelling the batch.** I'm not sure if we want this behavior or not. Driver:; - Runs a loop every 60 seconds to close batches with max_idle_time greater than that specified. UI:; - The UI changed the batches table to be time_created instead of time_closed as the Submitted/Created column. The duration is the time from time_complete - time_created for newer batches instead of time_complete - time_closed.; - Added a close button; - An open batch (even one just in the created state) can be cancelled or closed. **This might be confusing**. Other:; - Cancel is idempotent; - Getting the batch state and time_completed correct was tricky.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10484
https://github.com/hail-is/hail/pull/10489:68,Testability,test,tested,68,"We had a lowering defined for `BlockMatrixSlice`, but it was barely tested and not correct. In this PR I rewrote it to be correct and enabled the python tests in `test_linalg` that check slicing behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10489
https://github.com/hail-is/hail/pull/10489:153,Testability,test,tests,153,"We had a lowering defined for `BlockMatrixSlice`, but it was barely tested and not correct. In this PR I rewrote it to be correct and enabled the python tests in `test_linalg` that check slicing behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10489
https://github.com/hail-is/hail/pull/10492:16,Performance,perform,performance,16,"I want to track performance of logistic regression. Currently, on my laptop these two benchmarks clock in at 11 seconds for Breeze and 62 seconds for ndarrays. Now that I have a metric I'll try and optimize.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10492
https://github.com/hail-is/hail/pull/10492:198,Performance,optimiz,optimize,198,"I want to track performance of logistic regression. Currently, on my laptop these two benchmarks clock in at 11 seconds for Breeze and 62 seconds for ndarrays. Now that I have a metric I'll try and optimize.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10492
https://github.com/hail-is/hail/pull/10492:31,Testability,log,logistic,31,"I want to track performance of logistic regression. Currently, on my laptop these two benchmarks clock in at 11 seconds for Breeze and 62 seconds for ndarrays. Now that I have a metric I'll try and optimize.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10492
https://github.com/hail-is/hail/pull/10492:86,Testability,benchmark,benchmarks,86,"I want to track performance of logistic regression. Currently, on my laptop these two benchmarks clock in at 11 seconds for Breeze and 62 seconds for ndarrays. Now that I have a metric I'll try and optimize.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10492
https://github.com/hail-is/hail/pull/10493:135,Deployability,update,update,135,"rsync -v prints every file that's being copied, which is way too much terminal noise. I feel like it didn't used to do this? Maybe mac update changed it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10493
https://github.com/hail-is/hail/pull/10495:2,Testability,test,tested,2,I tested this in my namespace. We can customize the parameters -- this implementation is aggressive at making sure we're at maximum efficiency.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10495
https://github.com/hail-is/hail/pull/10498:819,Security,access,access,819,"Summary of changes:; - Add S3AsyncFS which is implemented in terms of the AWS Python client library, boto3. boto3 is sync (there is an in-progress async version but I decided not to use it to start). The operations are nearly identical to GCS, except S3 supports explicit API requests for multi-part uploads (unlike GCS, where we implement it in terms of compose).; - The only tricky bit is `create`, which needs an async stream writer, but a synchronous stream reader that is passed to boto3.; - I split up test_aiogoogle.py. The GCS specific tests stay there, and AsyncFS tests move to test_fs.py.; - Add S3 to the AsyncFS and copy tests. I created an S3 bucket (hail-test-dy5rg) and test user credentials (added to K8s as test-aws-key). I'm still trying to figure out how to give the rest of the services team admin access to the AWS project, I might have to go through BITS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10498
https://github.com/hail-is/hail/pull/10498:544,Testability,test,tests,544,"Summary of changes:; - Add S3AsyncFS which is implemented in terms of the AWS Python client library, boto3. boto3 is sync (there is an in-progress async version but I decided not to use it to start). The operations are nearly identical to GCS, except S3 supports explicit API requests for multi-part uploads (unlike GCS, where we implement it in terms of compose).; - The only tricky bit is `create`, which needs an async stream writer, but a synchronous stream reader that is passed to boto3.; - I split up test_aiogoogle.py. The GCS specific tests stay there, and AsyncFS tests move to test_fs.py.; - Add S3 to the AsyncFS and copy tests. I created an S3 bucket (hail-test-dy5rg) and test user credentials (added to K8s as test-aws-key). I'm still trying to figure out how to give the rest of the services team admin access to the AWS project, I might have to go through BITS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10498
https://github.com/hail-is/hail/pull/10498:574,Testability,test,tests,574,"Summary of changes:; - Add S3AsyncFS which is implemented in terms of the AWS Python client library, boto3. boto3 is sync (there is an in-progress async version but I decided not to use it to start). The operations are nearly identical to GCS, except S3 supports explicit API requests for multi-part uploads (unlike GCS, where we implement it in terms of compose).; - The only tricky bit is `create`, which needs an async stream writer, but a synchronous stream reader that is passed to boto3.; - I split up test_aiogoogle.py. The GCS specific tests stay there, and AsyncFS tests move to test_fs.py.; - Add S3 to the AsyncFS and copy tests. I created an S3 bucket (hail-test-dy5rg) and test user credentials (added to K8s as test-aws-key). I'm still trying to figure out how to give the rest of the services team admin access to the AWS project, I might have to go through BITS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10498
https://github.com/hail-is/hail/pull/10498:634,Testability,test,tests,634,"Summary of changes:; - Add S3AsyncFS which is implemented in terms of the AWS Python client library, boto3. boto3 is sync (there is an in-progress async version but I decided not to use it to start). The operations are nearly identical to GCS, except S3 supports explicit API requests for multi-part uploads (unlike GCS, where we implement it in terms of compose).; - The only tricky bit is `create`, which needs an async stream writer, but a synchronous stream reader that is passed to boto3.; - I split up test_aiogoogle.py. The GCS specific tests stay there, and AsyncFS tests move to test_fs.py.; - Add S3 to the AsyncFS and copy tests. I created an S3 bucket (hail-test-dy5rg) and test user credentials (added to K8s as test-aws-key). I'm still trying to figure out how to give the rest of the services team admin access to the AWS project, I might have to go through BITS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10498
https://github.com/hail-is/hail/pull/10498:670,Testability,test,test-,670,"Summary of changes:; - Add S3AsyncFS which is implemented in terms of the AWS Python client library, boto3. boto3 is sync (there is an in-progress async version but I decided not to use it to start). The operations are nearly identical to GCS, except S3 supports explicit API requests for multi-part uploads (unlike GCS, where we implement it in terms of compose).; - The only tricky bit is `create`, which needs an async stream writer, but a synchronous stream reader that is passed to boto3.; - I split up test_aiogoogle.py. The GCS specific tests stay there, and AsyncFS tests move to test_fs.py.; - Add S3 to the AsyncFS and copy tests. I created an S3 bucket (hail-test-dy5rg) and test user credentials (added to K8s as test-aws-key). I'm still trying to figure out how to give the rest of the services team admin access to the AWS project, I might have to go through BITS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10498
https://github.com/hail-is/hail/pull/10498:686,Testability,test,test,686,"Summary of changes:; - Add S3AsyncFS which is implemented in terms of the AWS Python client library, boto3. boto3 is sync (there is an in-progress async version but I decided not to use it to start). The operations are nearly identical to GCS, except S3 supports explicit API requests for multi-part uploads (unlike GCS, where we implement it in terms of compose).; - The only tricky bit is `create`, which needs an async stream writer, but a synchronous stream reader that is passed to boto3.; - I split up test_aiogoogle.py. The GCS specific tests stay there, and AsyncFS tests move to test_fs.py.; - Add S3 to the AsyncFS and copy tests. I created an S3 bucket (hail-test-dy5rg) and test user credentials (added to K8s as test-aws-key). I'm still trying to figure out how to give the rest of the services team admin access to the AWS project, I might have to go through BITS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10498
https://github.com/hail-is/hail/pull/10498:725,Testability,test,test-aws-key,725,"Summary of changes:; - Add S3AsyncFS which is implemented in terms of the AWS Python client library, boto3. boto3 is sync (there is an in-progress async version but I decided not to use it to start). The operations are nearly identical to GCS, except S3 supports explicit API requests for multi-part uploads (unlike GCS, where we implement it in terms of compose).; - The only tricky bit is `create`, which needs an async stream writer, but a synchronous stream reader that is passed to boto3.; - I split up test_aiogoogle.py. The GCS specific tests stay there, and AsyncFS tests move to test_fs.py.; - Add S3 to the AsyncFS and copy tests. I created an S3 bucket (hail-test-dy5rg) and test user credentials (added to K8s as test-aws-key). I'm still trying to figure out how to give the rest of the services team admin access to the AWS project, I might have to go through BITS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10498
https://github.com/hail-is/hail/pull/10501:14,Testability,assert,assert,14,"We had an old assert that only applies to Spark, moved it into the execute method.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10501
https://github.com/hail-is/hail/pull/10502:316,Deployability,update,updated,316,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:492,Deployability,install,install,492,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:1657,Deployability,deploy,deployment,1657,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:86,Modifiability,layers,layers,86,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:1273,Modifiability,layers,layers,1273,"ache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:1394,Modifiability,layers,layers,1394,"rom Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:1763,Modifiability,layers,layers,1763,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:62,Performance,cache,cache,62,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:279,Performance,cache,cache,279,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:307,Performance,cache,cache,307,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:897,Performance,cache,cache,897,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:1692,Performance,cache,cached-as-possible,1692,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:1013,Testability,test,test,1013,"ace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:2370,Testability,Log,Logs,2370,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10502:768,Usability,Simpl,Simplify,768,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10502
https://github.com/hail-is/hail/pull/10503:37,Availability,error,error,37,"My first approach was to populate an error.html template and return that. I could get the title to be ""Error"", but none of the content was showing up. I couldn't figure out why, so I switched to raising HTTPErrorFound with the traceback message. This works fine, but it won't work if we need the decorator above the authentication decorators. This decorator has to be the furthest down the call stack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10503
https://github.com/hail-is/hail/pull/10503:103,Availability,Error,Error,103,"My first approach was to populate an error.html template and return that. I could get the title to be ""Error"", but none of the content was showing up. I couldn't figure out why, so I switched to raising HTTPErrorFound with the traceback message. This works fine, but it won't work if we need the decorator above the authentication decorators. This decorator has to be the furthest down the call stack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10503
https://github.com/hail-is/hail/pull/10503:381,Availability,down,down,381,"My first approach was to populate an error.html template and return that. I could get the title to be ""Error"", but none of the content was showing up. I couldn't figure out why, so I switched to raising HTTPErrorFound with the traceback message. This works fine, but it won't work if we need the decorator above the authentication decorators. This decorator has to be the furthest down the call stack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10503
https://github.com/hail-is/hail/pull/10503:237,Integrability,message,message,237,"My first approach was to populate an error.html template and return that. I could get the title to be ""Error"", but none of the content was showing up. I couldn't figure out why, so I switched to raising HTTPErrorFound with the traceback message. This works fine, but it won't work if we need the decorator above the authentication decorators. This decorator has to be the furthest down the call stack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10503
https://github.com/hail-is/hail/pull/10503:316,Security,authenticat,authentication,316,"My first approach was to populate an error.html template and return that. I could get the title to be ""Error"", but none of the content was showing up. I couldn't figure out why, so I switched to raising HTTPErrorFound with the traceback message. This works fine, but it won't work if we need the decorator above the authentication decorators. This decorator has to be the furthest down the call stack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10503
https://github.com/hail-is/hail/pull/10504:633,Availability,down,down,633,"Copied over from the Zulip thread:. Dan and I still have work to figure out the authentication strategy for browser-based REST requests, but as a workaround I've added a tiny aiohttp proxy that uses the python client library to fulfill the requests, which could enable local frontend work while we figure out the right way to do authentication and streaming data through websockets. Implementing the polling and separating it logically from the view components was actually a nice little case study in how to do this in React/Svelte, but is far from an honest or thorough comparison. If you want to run it for yourself, you can pull down the branch in that PR and then do the following (which I'll write dev docs for if this is something that we actually want to check in):. Install node if you do not have it; Run npm install in the $HAIL, $HAIL/js_common, and $HAIL/batch2/react-batch (or svelte-batch) directories; In one terminal in $HAIL/batch2, run python proxy.py; In another terminal in one of the react-batch or svelte-batch directories, run npm run dev; Go to localhost:3000 in your browser if it didn't pop up automatically",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10504
https://github.com/hail-is/hail/pull/10504:775,Deployability,Install,Install,775,"Copied over from the Zulip thread:. Dan and I still have work to figure out the authentication strategy for browser-based REST requests, but as a workaround I've added a tiny aiohttp proxy that uses the python client library to fulfill the requests, which could enable local frontend work while we figure out the right way to do authentication and streaming data through websockets. Implementing the polling and separating it logically from the view components was actually a nice little case study in how to do this in React/Svelte, but is far from an honest or thorough comparison. If you want to run it for yourself, you can pull down the branch in that PR and then do the following (which I'll write dev docs for if this is something that we actually want to check in):. Install node if you do not have it; Run npm install in the $HAIL, $HAIL/js_common, and $HAIL/batch2/react-batch (or svelte-batch) directories; In one terminal in $HAIL/batch2, run python proxy.py; In another terminal in one of the react-batch or svelte-batch directories, run npm run dev; Go to localhost:3000 in your browser if it didn't pop up automatically",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10504
https://github.com/hail-is/hail/pull/10504:819,Deployability,install,install,819,"Copied over from the Zulip thread:. Dan and I still have work to figure out the authentication strategy for browser-based REST requests, but as a workaround I've added a tiny aiohttp proxy that uses the python client library to fulfill the requests, which could enable local frontend work while we figure out the right way to do authentication and streaming data through websockets. Implementing the polling and separating it logically from the view components was actually a nice little case study in how to do this in React/Svelte, but is far from an honest or thorough comparison. If you want to run it for yourself, you can pull down the branch in that PR and then do the following (which I'll write dev docs for if this is something that we actually want to check in):. Install node if you do not have it; Run npm install in the $HAIL, $HAIL/js_common, and $HAIL/batch2/react-batch (or svelte-batch) directories; In one terminal in $HAIL/batch2, run python proxy.py; In another terminal in one of the react-batch or svelte-batch directories, run npm run dev; Go to localhost:3000 in your browser if it didn't pop up automatically",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10504
https://github.com/hail-is/hail/pull/10504:80,Security,authenticat,authentication,80,"Copied over from the Zulip thread:. Dan and I still have work to figure out the authentication strategy for browser-based REST requests, but as a workaround I've added a tiny aiohttp proxy that uses the python client library to fulfill the requests, which could enable local frontend work while we figure out the right way to do authentication and streaming data through websockets. Implementing the polling and separating it logically from the view components was actually a nice little case study in how to do this in React/Svelte, but is far from an honest or thorough comparison. If you want to run it for yourself, you can pull down the branch in that PR and then do the following (which I'll write dev docs for if this is something that we actually want to check in):. Install node if you do not have it; Run npm install in the $HAIL, $HAIL/js_common, and $HAIL/batch2/react-batch (or svelte-batch) directories; In one terminal in $HAIL/batch2, run python proxy.py; In another terminal in one of the react-batch or svelte-batch directories, run npm run dev; Go to localhost:3000 in your browser if it didn't pop up automatically",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10504
https://github.com/hail-is/hail/pull/10504:329,Security,authenticat,authentication,329,"Copied over from the Zulip thread:. Dan and I still have work to figure out the authentication strategy for browser-based REST requests, but as a workaround I've added a tiny aiohttp proxy that uses the python client library to fulfill the requests, which could enable local frontend work while we figure out the right way to do authentication and streaming data through websockets. Implementing the polling and separating it logically from the view components was actually a nice little case study in how to do this in React/Svelte, but is far from an honest or thorough comparison. If you want to run it for yourself, you can pull down the branch in that PR and then do the following (which I'll write dev docs for if this is something that we actually want to check in):. Install node if you do not have it; Run npm install in the $HAIL, $HAIL/js_common, and $HAIL/batch2/react-batch (or svelte-batch) directories; In one terminal in $HAIL/batch2, run python proxy.py; In another terminal in one of the react-batch or svelte-batch directories, run npm run dev; Go to localhost:3000 in your browser if it didn't pop up automatically",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10504
https://github.com/hail-is/hail/pull/10504:426,Testability,log,logically,426,"Copied over from the Zulip thread:. Dan and I still have work to figure out the authentication strategy for browser-based REST requests, but as a workaround I've added a tiny aiohttp proxy that uses the python client library to fulfill the requests, which could enable local frontend work while we figure out the right way to do authentication and streaming data through websockets. Implementing the polling and separating it logically from the view components was actually a nice little case study in how to do this in React/Svelte, but is far from an honest or thorough comparison. If you want to run it for yourself, you can pull down the branch in that PR and then do the following (which I'll write dev docs for if this is something that we actually want to check in):. Install node if you do not have it; Run npm install in the $HAIL, $HAIL/js_common, and $HAIL/batch2/react-batch (or svelte-batch) directories; In one terminal in $HAIL/batch2, run python proxy.py; In another terminal in one of the react-batch or svelte-batch directories, run npm run dev; Go to localhost:3000 in your browser if it didn't pop up automatically",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10504
https://github.com/hail-is/hail/pull/10505:12,Deployability,integrat,integrates,12,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10505
https://github.com/hail-is/hail/pull/10505:12,Integrability,integrat,integrates,12,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10505
https://github.com/hail-is/hail/pull/10505:172,Integrability,wrap,wrappers,172,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10505
https://github.com/hail-is/hail/pull/10505:310,Integrability,depend,dependencies,310,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10505
https://github.com/hail-is/hail/pull/10505:363,Integrability,depend,dependencies,363,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10505
https://github.com/hail-is/hail/pull/10505:98,Security,expose,exposed,98,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10505
https://github.com/hail-is/hail/pull/10505:655,Usability,learn,learning,655,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10505
https://github.com/hail-is/hail/pull/10508:21,Availability,error,error,21,CHANGELOG: Fixed the error 'Argument list too long: '/bin/sh'' when using the LocalBackend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10508
https://github.com/hail-is/hail/pull/10509:44,Modifiability,extend,extend,44,BoxedArrayBuilder's type parameter needs to extend AnyRef to avoid; runtime matches on the type for all operations on the array.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10509
https://github.com/hail-is/hail/pull/10509:61,Safety,avoid,avoid,61,BoxedArrayBuilder's type parameter needs to extend AnyRef to avoid; runtime matches on the type for all operations on the array.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10509
https://github.com/hail-is/hail/pull/10510:175,Testability,Benchmark,Benchmarks,175,This PR introduces `NDArrayProducer`s as an alternative to the current `NDArrayEmitter`. The redesign is described here: https://dev.hail.is/t/ndarray-emitter-redesign/217/2. Benchmarks against main: https://gist.github.com/johnc1231/58072d13dc7266a2b4ecff44a4f16994,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10510
https://github.com/hail-is/hail/pull/10511:101,Energy Efficiency,schedul,schedule,101,This PR fixes the problem where there were 3 free cores on an instance not in us-central. ci can not schedule jobs in other zones so ci could not progress.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10511
https://github.com/hail-is/hail/pull/10515:99,Availability,avail,available,99,"If a compatible annotation dataset can't be found in `index_compatible_version`, show the user the available versions and reference genome builds of the requested annotation dataset in the raised `ValueError`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10515
https://github.com/hail-is/hail/pull/10518:13,Safety,avoid,avoid,13,This lets us avoid crashes/ooms from rendering 90K intervals thousands of times. Stacked on #10517,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10518
https://github.com/hail-is/hail/issues/10524:350,Deployability,install,install,350,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I followed [these instructions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:375,Deployability,install,installing,375,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I followed [these instructions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:407,Deployability,install,installing,407,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I followed [these instructions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:487,Deployability,install,install,487,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I followed [these instructions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:504,Deployability,install,install,504,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I followed [these instructions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:672,Deployability,install,install,672,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I followed [these instructions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:1187,Integrability,wrap,wrapper,1187,"uctions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in __init__(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations); 141 conf.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:1238,Integrability,wrap,wrapper,1238,"uctions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in __init__(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations); 141 conf.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:1443,Integrability,wrap,wrapper,1443,"uctions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in __init__(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations); 141 conf.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:1560,Testability,log,log,1560,"ast); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in __init__(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations); 141 conf.set('spark.executor.extraClassPath', './hail-all-spark.jar'); 142 if sc is None:; --> 143 pyspark.SparkContext._ensure_initialized(conf=conf); 144 elif not quiet:; 145 sys.stderr.write(. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 329 with SparkContext._lock:; 330 if not SparkContext._gateway:; --> 331 SparkContext._gateway = gateway or launch_gateway(conf); 332 SparkCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:1942,Testability,log,log,1942,"ast); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in __init__(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations); 141 conf.set('spark.executor.extraClassPath', './hail-all-spark.jar'); 142 if sc is None:; --> 143 pyspark.SparkContext._ensure_initialized(conf=conf); 144 elif not quiet:; 145 sys.stderr.write(. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 329 with SparkContext._lock:; 330 if not SparkContext._gateway:; --> 331 SparkContext._gateway = gateway or launch_gateway(conf); 332 SparkCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/issues/10524:2180,Testability,log,log,2180," *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in __init__(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations); 141 conf.set('spark.executor.extraClassPath', './hail-all-spark.jar'); 142 if sc is None:; --> 143 pyspark.SparkContext._ensure_initialized(conf=conf); 144 elif not quiet:; 145 sys.stderr.write(. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 329 with SparkContext._lock:; 330 if not SparkContext._gateway:; --> 331 SparkContext._gateway = gateway or launch_gateway(conf); 332 SparkContext._jvm = SparkContext._gateway.jvm; 333. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/pyspark/java_gateway.py in launch_gateway(conf, popen_kwargs); 106; 107 if not os.path.isfile(conn_info_file):; --> 108 raise Exception(""Java gateway process exited before sending its port number""); 109; 110 with open(conn_info_file, ""rb"") as info:. Exception: Java gateway process exited before sending its port number; ```; I'm not famil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10524
https://github.com/hail-is/hail/pull/10526:215,Deployability,update,updates,215,"I've reformatted the urls in the `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1` buckets with a new naming scheme to make things more consistent and clean up the buckets. . This updates the `datasets.json` file with these new urls. Currently there are two copies of each dataset in each bucket, one at the old url and one at the new url. I will remove the datasets at the old urls in a few months to avoid disrupting users not running the latest release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10526
https://github.com/hail-is/hail/pull/10526:483,Deployability,release,release,483,"I've reformatted the urls in the `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1` buckets with a new naming scheme to make things more consistent and clean up the buckets. . This updates the `datasets.json` file with these new urls. Currently there are two copies of each dataset in each bucket, one at the old url and one at the new url. I will remove the datasets at the old urls in a few months to avoid disrupting users not running the latest release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10526
https://github.com/hail-is/hail/pull/10526:437,Safety,avoid,avoid,437,"I've reformatted the urls in the `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1` buckets with a new naming scheme to make things more consistent and clean up the buckets. . This updates the `datasets.json` file with these new urls. Currently there are two copies of each dataset in each bucket, one at the old url and one at the new url. I will remove the datasets at the old urls in a few months to avoid disrupting users not running the latest release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10526
https://github.com/hail-is/hail/pull/10527:55,Availability,avail,available,55,"To allow users on GCP to access datasets that are only available on AWS (e.g. pan-ukb LD block matrices and tables). . If trying to read dataset from `s3` path throws `FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""s3""`, then will read with the `s3a` prefix instead.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10527
https://github.com/hail-is/hail/pull/10527:25,Security,access,access,25,"To allow users on GCP to access datasets that are only available on AWS (e.g. pan-ukb LD block matrices and tables). . If trying to read dataset from `s3` path throws `FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""s3""`, then will read with the `s3a` prefix instead.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10527
https://github.com/hail-is/hail/issues/10530:380,Availability,error,error,380,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. When trying to use the load_dataset() function on any dataset, I receive the following error: . ```; mt = hl.experimental.load_dataset(name='1000_Genomes_chrMT',; version='phase_3',; reference_genome='GRCh37',; region='us',; cloud='gcp'); ```. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-7-5fec92db8e26> in <module>; ----> 1 mt = hl.experimental.load_dataset(name='1000_Genomes_chrMT',; 2 version='phase_3',; 3 reference_genome='GRCh37',; 4 region='us',; 5 cloud='gcp'). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/experimental/datasets.py in load_dataset(name, version, reference_genome, region, cloud); 109 return hl.read_table(path); 110 elif path.endswith('.mt'):; --> 111 return hl.read_matrix_table(path); 112 elif path.endswith('.bm'):; 113 return hl.linalg.BlockMatrix.read(path). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/methods/impex.py in read_matrix_table(path, _intervals, _filter_intervals, _drop_cols, _drop_rows, _n_partitions); 2009 :class:`.Ma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:2956,Availability,Error,Error,2956,"011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.Hado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:3107,Availability,Error,Error,3107,"011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.Hado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:5132,Availability,Error,Error,5132,"tedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.67-a673309b0445; Error summary: UnsupportedFileSystemException: No FileSystem for scheme ""gs""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:1544,Integrability,wrap,wrapper,1544,"-input-7-5fec92db8e26> in <module>; ----> 1 mt = hl.experimental.load_dataset(name='1000_Genomes_chrMT',; 2 version='phase_3',; 3 reference_genome='GRCh37',; 4 region='us',; 5 cloud='gcp'). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/experimental/datasets.py in load_dataset(name, version, reference_genome, region, cloud); 109 return hl.read_table(path); 110 elif path.endswith('.mt'):; --> 111 return hl.read_matrix_table(path); 112 elif path.endswith('.bm'):; 113 return hl.linalg.BlockMatrix.read(path). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/methods/impex.py in read_matrix_table(path, _intervals, _filter_intervals, _drop_cols, _drop_rows, _n_partitions); 2009 :class:`.MatrixTable`; 2010 """"""; -> 2011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = sel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:1595,Integrability,wrap,wrapper,1595,"-input-7-5fec92db8e26> in <module>; ----> 1 mt = hl.experimental.load_dataset(name='1000_Genomes_chrMT',; 2 version='phase_3',; 3 reference_genome='GRCh37',; 4 region='us',; 5 cloud='gcp'). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/experimental/datasets.py in load_dataset(name, version, reference_genome, region, cloud); 109 return hl.read_table(path); 110 elif path.endswith('.mt'):; --> 111 return hl.read_matrix_table(path); 112 elif path.endswith('.bm'):; 113 return hl.linalg.BlockMatrix.read(path). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/methods/impex.py in read_matrix_table(path, _intervals, _filter_intervals, _drop_cols, _drop_rows, _n_partitions); 2009 :class:`.MatrixTable`; 2010 """"""; -> 2011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = sel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:1801,Integrability,wrap,wrapper,1801,"-input-7-5fec92db8e26> in <module>; ----> 1 mt = hl.experimental.load_dataset(name='1000_Genomes_chrMT',; 2 version='phase_3',; 3 reference_genome='GRCh37',; 4 region='us',; 5 cloud='gcp'). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/experimental/datasets.py in load_dataset(name, version, reference_genome, region, cloud); 109 return hl.read_table(path); 110 elif path.endswith('.mt'):; --> 111 return hl.read_matrix_table(path); 112 elif path.endswith('.bm'):; 113 return hl.linalg.BlockMatrix.read(path). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/methods/impex.py in read_matrix_table(path, _intervals, _filter_intervals, _drop_cols, _drop_rows, _n_partitions); 2009 :class:`.MatrixTable`; 2010 """"""; -> 2011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = sel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:2362,Performance,load,loads,2362,"nc, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/methods/impex.py in read_matrix_table(path, _intervals, _filter_intervals, _drop_cols, _drop_rows, _n_partitions); 2009 :class:`.MatrixTable`; 2010 """"""; -> 2011 for rg_config in Env.backend().load_references_from_dataset(path):; 2012 hl.ReferenceGenome._from_config(rg_config); 2013 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/spark_backend.py in load_references_from_dataset(self, path); 321 ; 322 def load_references_from_dataset(self, path):; --> 323 return json.loads(Env.hail().variant.ReferenceGenome.fromHailDataset(self.fs._jfs, path)); 324 ; 325 def from_fasta_file(self, name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par):. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:3630,Performance,Cache,Cache,3630,"__call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:3707,Performance,Cache,Cache,3707,"command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.Method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/issues/10530:3561,Security,access,access,3561,"nv/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args); 1302 ; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306 . ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: UnsupportedFileSystemException: No FileSystem for scheme ""gs"". Java stack trace:; org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""gs""; 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.isDir(FS.scala:175); 	at is.hail.io.fs.FS.isDir$(FS.scala:173); 	at is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70); 	at is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30); 	at is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68); 	at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596); 	at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccess",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10530
https://github.com/hail-is/hail/pull/10534:69,Availability,down,download,69,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10534
https://github.com/hail-is/hail/pull/10534:144,Availability,down,download,144,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10534
https://github.com/hail-is/hail/pull/10534:283,Availability,error,error,283,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10534
https://github.com/hail-is/hail/pull/10534:987,Availability,down,downloads,987,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10534
https://github.com/hail-is/hail/pull/10534:925,Performance,concurren,concurrent,925,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10534
https://github.com/hail-is/hail/pull/10534:17,Testability,test,tested,17,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10534
https://github.com/hail-is/hail/pull/10535:195,Availability,avail,available,195,"Adds a MatrixTable for all variant-gene cis-eQTL associations tested in each tissue (including non-significant associations) for GTEx v8. MatrixTable has columns keyed by tissue, and contain all available tissues from GTEx V8. The `GTEx_MatrixTables` notebook documents how the MatrixTable were generated. . The eQTL MatrixTable is ~220 GiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10535
https://github.com/hail-is/hail/pull/10535:62,Testability,test,tested,62,"Adds a MatrixTable for all variant-gene cis-eQTL associations tested in each tissue (including non-significant associations) for GTEx v8. MatrixTable has columns keyed by tissue, and contain all available tissues from GTEx V8. The `GTEx_MatrixTables` notebook documents how the MatrixTable were generated. . The eQTL MatrixTable is ~220 GiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10535
https://github.com/hail-is/hail/pull/10539:100,Availability,error,error-checking,100,"While only used in a few places, this helps us generate; much better code in the case where we emit error-checking; IRs as below:. ```; If; <error_condition>; Die err_msg; <value we want>; ```. The code generator for the `If` node uses `SType.canonical` to; choose its result type, and casts both consequent and alternate; values to that type. We want the stype of the result here to be; the type of the alternate `<value we want>`, which we can achieve; by adding unreachable types/codes for the `SType.canonical` logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10539
https://github.com/hail-is/hail/pull/10539:515,Testability,log,logic,515,"While only used in a few places, this helps us generate; much better code in the case where we emit error-checking; IRs as below:. ```; If; <error_condition>; Die err_msg; <value we want>; ```. The code generator for the `If` node uses `SType.canonical` to; choose its result type, and casts both consequent and alternate; values to that type. We want the stype of the result here to be; the type of the alternate `<value we want>`, which we can achieve; by adding unreachable types/codes for the `SType.canonical` logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10539
https://github.com/hail-is/hail/pull/10542:82,Modifiability,variab,variable-size-list-layout,82,This implements [Apache Arrow](https://arrow.apache.org/docs/format/Columnar.html#variable-size-list-layout) style nested arrays. A more detailed write up will appear here as this gets closer to being ready.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10542
https://github.com/hail-is/hail/pull/10544:96,Deployability,Release,Release,96,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.4 to 1.26.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:185,Deployability,release,releases,185,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.4 to 1.26.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:205,Deployability,release,releases,205,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.4 to 1.26.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:523,Deployability,Update,Updated,523,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.4 to 1.26.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:1141,Deployability,Update,Updated,1141,"ithub.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Pytho",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:1487,Deployability,Release,Release,1487,"></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09dd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:1812,Deployability,Update,Update,1812,"ub.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3120,Deployability,update,updates,3120,"9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4272,Deployability,upgrade,upgrade,4272,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4448,Deployability,upgrade,upgrade,4448,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4618,Deployability,upgrade,upgrade,4618,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:2838,Integrability,Depend,Dependabot,2838,"s://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:2878,Integrability,depend,dependabot-badges,2878,"s://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:2937,Integrability,depend,dependency-name,2937,"bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3100,Integrability,depend,dependabot-security-updates,3100,"9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3157,Integrability,Depend,Dependabot,3157,"=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3303,Integrability,depend,dependabot,3303,"1""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3332,Integrability,depend,dependabot-automerge-start,3332,"href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3370,Integrability,depend,dependabot-automerge-end,3370,"href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3422,Integrability,Depend,Dependabot,3422,"bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3489,Integrability,Depend,Dependabot,3489,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3539,Integrability,depend,dependabot,3539,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3583,Integrability,depend,dependabot,3583,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3680,Integrability,depend,dependabot,3680,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3749,Integrability,depend,dependabot,3749,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3840,Integrability,depend,dependabot,3840,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3933,Integrability,depend,dependabot,3933,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3993,Integrability,depend,dependabot,3993,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4039,Integrability,Depend,Dependabot,4039,"1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4125,Integrability,depend,dependabot,4125,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4191,Integrability,Depend,Dependabot,4191,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4301,Integrability,depend,dependabot,4301,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4367,Integrability,Depend,Dependabot,4367,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4477,Integrability,depend,dependabot,4477,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4500,Integrability,depend,dependency,4500,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4540,Integrability,Depend,Dependabot,4540,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4578,Integrability,depend,dependency,4578,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4647,Integrability,depend,dependabot,4647,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4766,Integrability,depend,dependabot,4766,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:4891,Integrability,depend,dependabot,4891,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:5016,Integrability,depend,dependabot,5016,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:595,Performance,perform,performance,595,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.4 to 1.26.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:1213,Performance,perform,performance,1213,"uote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:1638,Performance,perform,performance,1638,"plitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3069,Security,secur,security-vulnerabilities,3069,"9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:3111,Security,secur,security-updates,3111,"9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:5162,Security,secur,security,5162,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:5203,Security,Secur,Security,5203,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10544:2678,Testability,test,test,2678,">; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Python 3.10 ssl module</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d725a9b56bb8baf87c9e6eee0e9edf010034b63b""><code>d725a9b</code></a> Add Python 3.10 to GitHub Actions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/339ad34c677c98fd9ad008de1d8bbeb9dbf34381""><code>339ad34</code></a> Use pytest==6.2.4 on Python 3.10+</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f271c9c3149e20d7feffb6429b135bbb6c09ddf4""><code>f271c9c</code></a> Apply latest Black formatting</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1884878aac87ef0494b282e940c32c24ee917d52""><code>1884878</code></a> [1.26] Properly proxy EOF on the SSLTransport test suite</li>; <li>See full diff in <a href=""https://github.com/urllib3/urllib3/compare/1.26.4...1.26.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.4&new-version=1.26.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10544
https://github.com/hail-is/hail/pull/10551:99,Availability,echo,echo,99,Adding support to nd arrays to include less than max dimension indexing and the use of ellipses to echo numpy behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10551
https://github.com/hail-is/hail/pull/10554:203,Testability,benchmark,benchmark,203,"Previously, we always made the stream into an array, then copied the array into the ndarray. Now we just insert the stream directly into the ndarray. This saves memory (~7% on the linear regression rows benchmark), and had no noticeable effect on speed in my experiments.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10554
https://github.com/hail-is/hail/pull/10555:281,Availability,error,error,281,"Currently, the only way to check if Hail can read URLs with a given scheme (`gs`, `s3`, etc) is to attempt to read a URL with that scheme. However, the same exception type is thrown whether the scheme is not supported or the file doesn't exist or something else went wrong and the error message is the only way to determine what went wrong. This adds a `hl.hadoop_scheme_supported` function, which returns a boolean indicating whether or not a URL scheme is supported. Discussed on Zulip: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Get.20supported.20URL.20schemes. @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10555
https://github.com/hail-is/hail/pull/10555:287,Integrability,message,message,287,"Currently, the only way to check if Hail can read URLs with a given scheme (`gs`, `s3`, etc) is to attempt to read a URL with that scheme. However, the same exception type is thrown whether the scheme is not supported or the file doesn't exist or something else went wrong and the error message is the only way to determine what went wrong. This adds a `hl.hadoop_scheme_supported` function, which returns a boolean indicating whether or not a URL scheme is supported. Discussed on Zulip: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Get.20supported.20URL.20schemes. @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10555
https://github.com/hail-is/hail/pull/10560:123,Testability,log,log,123,Missed this when updating MatrixRead. Main goal; here is not to dump out JSON partition intervals; a bunch of times in the log files.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10560
https://github.com/hail-is/hail/pull/10561:317,Deployability,release,release,317,"The assumption of the bounded gather operations is that threads of; control hold the bounding semaphore. That means, when the bounded; gather operations should be called from a thread of control which; holds the semaphore and when those operations need to block internally; (by calling wait or gather, say) it should release the semaphore. I; added a ""WithoutSemaphore"" semaphore release manager to implement this; pattern and use it in the bounded gather operations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10561
https://github.com/hail-is/hail/pull/10561:380,Deployability,release,release,380,"The assumption of the bounded gather operations is that threads of; control hold the bounding semaphore. That means, when the bounded; gather operations should be called from a thread of control which; holds the semaphore and when those operations need to block internally; (by calling wait or gather, say) it should release the semaphore. I; added a ""WithoutSemaphore"" semaphore release manager to implement this; pattern and use it in the bounded gather operations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10561
https://github.com/hail-is/hail/pull/10563:40,Integrability,message,messages,40,"This supresses ""coroutines not awaited"" messages in the case that a; bounded gather is cancelled before all subtasks have had a chance to; start.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10563
https://github.com/hail-is/hail/pull/10566:20,Testability,benchmark,benchmarks,20,Helps a little with benchmarks in whole stage codegen (which I'm working on in a different branch),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10566
https://github.com/hail-is/hail/pull/10567:171,Security,access,access,171,"See [discussion](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/CI.20Deploy.20Failure/near/241944633). spring.io no; longer supports [direct, unauthenticated access to its repository](https://spring.io/blog/2020/10/29/notice-of-permissions-changes-to-repo-spring-io-fall-and-winter-2020).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10567
https://github.com/hail-is/hail/pull/10574:16,Testability,log,logs,16,This means that logs from submitted gcloud jobs won't go into; the void by default.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10574
https://github.com/hail-is/hail/pull/10575:87,Integrability,interface,interface,87,"It's only used in tests, and is not something we wish to maintain as part of the ptype interface.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10575
https://github.com/hail-is/hail/pull/10575:18,Testability,test,tests,18,"It's only used in tests, and is not something we wish to maintain as part of the ptype interface.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10575
https://github.com/hail-is/hail/pull/10583:549,Modifiability,variab,variable,549,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10583
https://github.com/hail-is/hail/pull/10583:653,Modifiability,variab,variables,653,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10583
https://github.com/hail-is/hail/pull/10583:719,Modifiability,variab,variable,719,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10583
https://github.com/hail-is/hail/pull/10583:839,Modifiability,variab,variable,839,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10583
https://github.com/hail-is/hail/pull/10583:1410,Performance,perform,performance,1410,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10583
https://github.com/hail-is/hail/pull/10583:1140,Safety,avoid,avoids,1140,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10583
https://github.com/hail-is/hail/pull/10583:1437,Safety,avoid,avoids,1437,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10583
https://github.com/hail-is/hail/pull/10584:97,Testability,benchmark,benchmark,97,Use pure Python to do some IO and CPU work. This will help; identify fraught comparisons between benchmark runs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10584
https://github.com/hail-is/hail/issues/10590:1369,Availability,echo,echo,1369,"ion, So i can't use root permission); openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); Hail version: 0.2.68. **<My workflow>**; 1. Run start-master.sh and start-slaves.sh in spark sbin directory.; 2. (bash) pyspark. I got message below. ![image](https://user-images.githubusercontent.com/78582088/121848873-9f001980-cd25-11eb-9e9d-8854f204c8c3.png); ![image](https://user-images.githubusercontent.com/78582088/121848900-a58e9100-cd25-11eb-9655-58100401e0d3.png); ![image](https://user-images.githubusercontent.com/78582088/121848915-a9baae80-cd25-11eb-90ff-a4f12df5b322.png). How can i set up hail on spark?; Do i need to change java version?. Thank you for your services. My <bashrc>, <conf/spark-defaults.conf> and <./spark-env.sh> are below. **<.bashrc>**; ```; #SPARK; export SPARK_HOME=/home/edu1/tools/spark-2.2.0-bin-hadoop2.6; export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/python:$PATH; export PYTHONPATH=$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):$PYTHONPATH. # Hail; export HAIL_HOME=/home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail; export PATH=$PATH:$HAIL_HOME/bin; export PYTHONPATH=$PYTHONPATH:$HAIL_HOME/python; export SPARK_CLASSPATH=$HAIL_HOME/backend/hail-all-spark.jar. # JAVA (I just can modify .bashrc, so This would not apply to java path.); export JAVA_HOME=/home/edu1/tools/jdk-1.8.0_231; export PATH=$PATH:$JAVA_HOME/bin; export CLASSPATH=$JAVA_HOME/lib/tools.jar. # Hadoop; export HADOOP_INSTALL=/home/edu1/tools/hadoop-2.6.0; export PAHT=$PATH:$HADOOP_INSTALL/bin; export LD_LIBRARY_PATH=$HADOOP_INSTALL/lib/native; ```; **</spark/conf/spark-defaults.conf>**; ```; spark.master spark://training.server:7077. spark.serializer org.apache.spark.serializer.KryoSerializer; spark.kryo.registrator is.hail.kryo.HailKryoRegistrator; spark.speculation True. spark.driver.memory 37414m; spark.executor.memory 37414m; spark.execu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10590
https://github.com/hail-is/hail/issues/10590:26,Deployability,install,installing,26,"Hi, I'm studying Hail and installing Hail on spark. I have plan that run GWAS about 1000 genomes. So, I install and set up hail on spark. **<my environment>**; Linux: Centos 7.8; Python: 3.7.3 (anaconda); Apache spark: spark-2.2.0-bin-hadoop2.6; Hadoop: hadoop-2.6.0; Java -version (info. I'm using linux server by korea Institution, So i can't use root permission); openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); Hail version: 0.2.68. **<My workflow>**; 1. Run start-master.sh and start-slaves.sh in spark sbin directory.; 2. (bash) pyspark. I got message below. ![image](https://user-images.githubusercontent.com/78582088/121848873-9f001980-cd25-11eb-9e9d-8854f204c8c3.png); ![image](https://user-images.githubusercontent.com/78582088/121848900-a58e9100-cd25-11eb-9655-58100401e0d3.png); ![image](https://user-images.githubusercontent.com/78582088/121848915-a9baae80-cd25-11eb-90ff-a4f12df5b322.png). How can i set up hail on spark?; Do i need to change java version?. Thank you for your services. My <bashrc>, <conf/spark-defaults.conf> and <./spark-env.sh> are below. **<.bashrc>**; ```; #SPARK; export SPARK_HOME=/home/edu1/tools/spark-2.2.0-bin-hadoop2.6; export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/python:$PATH; export PYTHONPATH=$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):$PYTHONPATH. # Hail; export HAIL_HOME=/home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail; export PATH=$PATH:$HAIL_HOME/bin; export PYTHONPATH=$PYTHONPATH:$HAIL_HOME/python; export SPARK_CLASSPATH=$HAIL_HOME/backend/hail-all-spark.jar. # JAVA (I just can modify .bashrc, so This would not apply to java path.); export JAVA_HOME=/home/edu1/tools/jdk-1.8.0_231; export PATH=$PATH:$JAVA_HOME/bin; export CLASSPATH=$JAVA_HOME/lib/tools.jar. # Hadoop; export HADOOP_INSTALL=/home/edu1/tools/hadoop-2.6.0; export PAHT=$PATH:$HADOOP_INSTALL/bin; export LD_LIBRARY_PATH=$H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10590
https://github.com/hail-is/hail/issues/10590:104,Deployability,install,install,104,"Hi, I'm studying Hail and installing Hail on spark. I have plan that run GWAS about 1000 genomes. So, I install and set up hail on spark. **<my environment>**; Linux: Centos 7.8; Python: 3.7.3 (anaconda); Apache spark: spark-2.2.0-bin-hadoop2.6; Hadoop: hadoop-2.6.0; Java -version (info. I'm using linux server by korea Institution, So i can't use root permission); openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); Hail version: 0.2.68. **<My workflow>**; 1. Run start-master.sh and start-slaves.sh in spark sbin directory.; 2. (bash) pyspark. I got message below. ![image](https://user-images.githubusercontent.com/78582088/121848873-9f001980-cd25-11eb-9e9d-8854f204c8c3.png); ![image](https://user-images.githubusercontent.com/78582088/121848900-a58e9100-cd25-11eb-9655-58100401e0d3.png); ![image](https://user-images.githubusercontent.com/78582088/121848915-a9baae80-cd25-11eb-90ff-a4f12df5b322.png). How can i set up hail on spark?; Do i need to change java version?. Thank you for your services. My <bashrc>, <conf/spark-defaults.conf> and <./spark-env.sh> are below. **<.bashrc>**; ```; #SPARK; export SPARK_HOME=/home/edu1/tools/spark-2.2.0-bin-hadoop2.6; export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/python:$PATH; export PYTHONPATH=$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):$PYTHONPATH. # Hail; export HAIL_HOME=/home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail; export PATH=$PATH:$HAIL_HOME/bin; export PYTHONPATH=$PYTHONPATH:$HAIL_HOME/python; export SPARK_CLASSPATH=$HAIL_HOME/backend/hail-all-spark.jar. # JAVA (I just can modify .bashrc, so This would not apply to java path.); export JAVA_HOME=/home/edu1/tools/jdk-1.8.0_231; export PATH=$PATH:$JAVA_HOME/bin; export CLASSPATH=$JAVA_HOME/lib/tools.jar. # Hadoop; export HADOOP_INSTALL=/home/edu1/tools/hadoop-2.6.0; export PAHT=$PATH:$HADOOP_INSTALL/bin; export LD_LIBRARY_PATH=$H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10590
https://github.com/hail-is/hail/issues/10590:639,Integrability,message,message,639,"Hi, I'm studying Hail and installing Hail on spark. I have plan that run GWAS about 1000 genomes. So, I install and set up hail on spark. **<my environment>**; Linux: Centos 7.8; Python: 3.7.3 (anaconda); Apache spark: spark-2.2.0-bin-hadoop2.6; Hadoop: hadoop-2.6.0; Java -version (info. I'm using linux server by korea Institution, So i can't use root permission); openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); Hail version: 0.2.68. **<My workflow>**; 1. Run start-master.sh and start-slaves.sh in spark sbin directory.; 2. (bash) pyspark. I got message below. ![image](https://user-images.githubusercontent.com/78582088/121848873-9f001980-cd25-11eb-9e9d-8854f204c8c3.png); ![image](https://user-images.githubusercontent.com/78582088/121848900-a58e9100-cd25-11eb-9655-58100401e0d3.png); ![image](https://user-images.githubusercontent.com/78582088/121848915-a9baae80-cd25-11eb-90ff-a4f12df5b322.png). How can i set up hail on spark?; Do i need to change java version?. Thank you for your services. My <bashrc>, <conf/spark-defaults.conf> and <./spark-env.sh> are below. **<.bashrc>**; ```; #SPARK; export SPARK_HOME=/home/edu1/tools/spark-2.2.0-bin-hadoop2.6; export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/python:$PATH; export PYTHONPATH=$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):$PYTHONPATH. # Hail; export HAIL_HOME=/home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail; export PATH=$PATH:$HAIL_HOME/bin; export PYTHONPATH=$PYTHONPATH:$HAIL_HOME/python; export SPARK_CLASSPATH=$HAIL_HOME/backend/hail-all-spark.jar. # JAVA (I just can modify .bashrc, so This would not apply to java path.); export JAVA_HOME=/home/edu1/tools/jdk-1.8.0_231; export PATH=$PATH:$JAVA_HOME/bin; export CLASSPATH=$JAVA_HOME/lib/tools.jar. # Hadoop; export HADOOP_INSTALL=/home/edu1/tools/hadoop-2.6.0; export PAHT=$PATH:$HADOOP_INSTALL/bin; export LD_LIBRARY_PATH=$H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10590
https://github.com/hail-is/hail/issues/10590:2760,Testability,log,logDirectory,2760,"sion?. Thank you for your services. My <bashrc>, <conf/spark-defaults.conf> and <./spark-env.sh> are below. **<.bashrc>**; ```; #SPARK; export SPARK_HOME=/home/edu1/tools/spark-2.2.0-bin-hadoop2.6; export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/python:$PATH; export PYTHONPATH=$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):$PYTHONPATH. # Hail; export HAIL_HOME=/home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail; export PATH=$PATH:$HAIL_HOME/bin; export PYTHONPATH=$PYTHONPATH:$HAIL_HOME/python; export SPARK_CLASSPATH=$HAIL_HOME/backend/hail-all-spark.jar. # JAVA (I just can modify .bashrc, so This would not apply to java path.); export JAVA_HOME=/home/edu1/tools/jdk-1.8.0_231; export PATH=$PATH:$JAVA_HOME/bin; export CLASSPATH=$JAVA_HOME/lib/tools.jar. # Hadoop; export HADOOP_INSTALL=/home/edu1/tools/hadoop-2.6.0; export PAHT=$PATH:$HADOOP_INSTALL/bin; export LD_LIBRARY_PATH=$HADOOP_INSTALL/lib/native; ```; **</spark/conf/spark-defaults.conf>**; ```; spark.master spark://training.server:7077. spark.serializer org.apache.spark.serializer.KryoSerializer; spark.kryo.registrator is.hail.kryo.HailKryoRegistrator; spark.speculation True. spark.driver.memory 37414m; spark.executor.memory 37414m; spark.executor.instances 1. spark.driver.extraClassPath /home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail/backend/hail-all-spark.jar; spark.executor.extraClassPath /home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.jars /home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail/backend/hail-all-spark.jar. spark.eventLog.enabled true; spark.history.fs.logDirectory file:/tmp/spark-events; spark.enevtLog.dir file:/tmp/spark-events. spark.ui.reverseProxy true; spark.ui.reverseProxyUrl spark://training.server/spark; spark.executor.extraJavaOptions -Dlog4j.debug=true; ```; **</spark/conf/spark-env.sh>**; ```; export SPARK_WORKER_INSTANCES=1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10590
https://github.com/hail-is/hail/pull/10591:45,Availability,error,error,45,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10591
https://github.com/hail-is/hail/pull/10591:960,Availability,error,error,960,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10591
https://github.com/hail-is/hail/pull/10591:1634,Availability,error,error,1634,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10591
https://github.com/hail-is/hail/pull/10591:1750,Availability,down,downloading,1750,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10591
https://github.com/hail-is/hail/pull/10591:51,Integrability,message,message,51,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10591
https://github.com/hail-is/hail/pull/10591:966,Integrability,message,message,966,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10591
https://github.com/hail-is/hail/pull/10591:722,Performance,concurren,concurrent,722,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10591
https://github.com/hail-is/hail/pull/10594:52,Availability,down,down,52,"Konrad ran into a problem where we started shutting down the worker because it was idle, but in between checking if there were any jobs still running and shutting down the site, a create job request came in. MJS was sent to the driver, but MJC was never sent because the worker shut down. The driver thought the job failed to schedule because the deactivate request was sent in before create job could return. The end result was Konrad's job still ran, but the database was left with an attempt that has a start time but no end time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10594
https://github.com/hail-is/hail/pull/10594:163,Availability,down,down,163,"Konrad ran into a problem where we started shutting down the worker because it was idle, but in between checking if there were any jobs still running and shutting down the site, a create job request came in. MJS was sent to the driver, but MJC was never sent because the worker shut down. The driver thought the job failed to schedule because the deactivate request was sent in before create job could return. The end result was Konrad's job still ran, but the database was left with an attempt that has a start time but no end time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10594
https://github.com/hail-is/hail/pull/10594:283,Availability,down,down,283,"Konrad ran into a problem where we started shutting down the worker because it was idle, but in between checking if there were any jobs still running and shutting down the site, a create job request came in. MJS was sent to the driver, but MJC was never sent because the worker shut down. The driver thought the job failed to schedule because the deactivate request was sent in before create job could return. The end result was Konrad's job still ran, but the database was left with an attempt that has a start time but no end time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10594
https://github.com/hail-is/hail/pull/10594:326,Energy Efficiency,schedul,schedule,326,"Konrad ran into a problem where we started shutting down the worker because it was idle, but in between checking if there were any jobs still running and shutting down the site, a create job request came in. MJS was sent to the driver, but MJC was never sent because the worker shut down. The driver thought the job failed to schedule because the deactivate request was sent in before create job could return. The end result was Konrad's job still ran, but the database was left with an attempt that has a start time but no end time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10594
https://github.com/hail-is/hail/pull/10596:21,Deployability,update,update,21,Realized I forgot to update the GTEx `.rst` files along with the name changes in PR #10526.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10596
https://github.com/hail-is/hail/pull/10598:22,Deployability,update,updates,22,There's some nice QoL updates and new chart types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10598
https://github.com/hail-is/hail/pull/10600:65,Integrability,wrap,wrap,65,"We currently have a `@monitor_endpoint` decorator that we use to wrap aiohttp endpoints and produce prometheus metrics, but the same result can be achieved with fewer lines of code by using essentially the same wrapper as an aiohttp middleware. . Separately, I fixed the wrapper to correctly catch and report on endpoints that raise exceptions, as well as keeping track of the number of concurrent connections.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10600
https://github.com/hail-is/hail/pull/10600:211,Integrability,wrap,wrapper,211,"We currently have a `@monitor_endpoint` decorator that we use to wrap aiohttp endpoints and produce prometheus metrics, but the same result can be achieved with fewer lines of code by using essentially the same wrapper as an aiohttp middleware. . Separately, I fixed the wrapper to correctly catch and report on endpoints that raise exceptions, as well as keeping track of the number of concurrent connections.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10600
https://github.com/hail-is/hail/pull/10600:271,Integrability,wrap,wrapper,271,"We currently have a `@monitor_endpoint` decorator that we use to wrap aiohttp endpoints and produce prometheus metrics, but the same result can be achieved with fewer lines of code by using essentially the same wrapper as an aiohttp middleware. . Separately, I fixed the wrapper to correctly catch and report on endpoints that raise exceptions, as well as keeping track of the number of concurrent connections.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10600
https://github.com/hail-is/hail/pull/10600:387,Performance,concurren,concurrent,387,"We currently have a `@monitor_endpoint` decorator that we use to wrap aiohttp endpoints and produce prometheus metrics, but the same result can be achieved with fewer lines of code by using essentially the same wrapper as an aiohttp middleware. . Separately, I fixed the wrapper to correctly catch and report on endpoints that raise exceptions, as well as keeping track of the number of concurrent connections.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10600
https://github.com/hail-is/hail/pull/10606:173,Integrability,wrap,wrapping,173,"A user reviewing the code pointed out that the branches of this function were backwards. Is there any easy way to construct a test that would catch this? Is `sigmoid` worth wrapping up into like `hl.math.sigmoid` or something? It seems like the name `sigmoid` refers to many things, maybe it should be `expit` like it apparently is in R and scipy?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10606
https://github.com/hail-is/hail/pull/10606:126,Testability,test,test,126,"A user reviewing the code pointed out that the branches of this function were backwards. Is there any easy way to construct a test that would catch this? Is `sigmoid` worth wrapping up into like `hl.math.sigmoid` or something? It seems like the name `sigmoid` refers to many things, maybe it should be `expit` like it apparently is in R and scipy?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10606
https://github.com/hail-is/hail/pull/10607:304,Performance,load,load,304,"Adds unphased versions to `1000_Genomes_HighCov_autosomes` and `1000_Genomes_HighCov_chrX` datasets. Unphased versions contained multiallelic variants, so these were split with `split_multi_hts`. The `chrY` dataset had not had multiallelic variants split before, so this fixes that as well. The dataset `load` scripts were replaced with a notebook that contains more detail about the process (for both phased and unphased versions).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10607
https://github.com/hail-is/hail/pull/10608:149,Modifiability,inherit,inherited,149,"Now get:; ```; E AttributeError: ArrayStructExpression instance has no field, method, or property 'select'; E Did you mean:; E ArrayStructExpression inherited method: 'collect'; ```. instead of. ```; AttributeError: ArrayStructExpression instance has no field, method, or property 'select'; Did you mean:; ArrayStructExpression method: 'select'; ArrayStructExpression inherited method: 'collect'. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10608
https://github.com/hail-is/hail/pull/10608:368,Modifiability,inherit,inherited,368,"Now get:; ```; E AttributeError: ArrayStructExpression instance has no field, method, or property 'select'; E Did you mean:; E ArrayStructExpression inherited method: 'collect'; ```. instead of. ```; AttributeError: ArrayStructExpression instance has no field, method, or property 'select'; Did you mean:; ArrayStructExpression method: 'select'; ArrayStructExpression inherited method: 'collect'. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10608
https://github.com/hail-is/hail/pull/10613:18,Availability,error,error,18,"```; body='{\\n \""error\"": {\\n \""code\"": 403,\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""errors\"": [\\n {\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""domain\"": \""usageLimits\"",\\n \""reason\"": \""rateLimitExceeded\""\\n }\\n ],\\n \""status\"": \""PERMISSION_DENIED\""\\n }\\n}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10613
https://github.com/hail-is/hail/pull/10613:255,Availability,error,errors,255,"```; body='{\\n \""error\"": {\\n \""code\"": 403,\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""errors\"": [\\n {\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""domain\"": \""usageLimits\"",\\n \""reason\"": \""rateLimitExceeded\""\\n }\\n ],\\n \""status\"": \""PERMISSION_DENIED\""\\n }\\n}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10613
https://github.com/hail-is/hail/pull/10613:52,Integrability,message,message,52,"```; body='{\\n \""error\"": {\\n \""code\"": 403,\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""errors\"": [\\n {\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""domain\"": \""usageLimits\"",\\n \""reason\"": \""rateLimitExceeded\""\\n }\\n ],\\n \""status\"": \""PERMISSION_DENIED\""\\n }\\n}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10613
https://github.com/hail-is/hail/pull/10613:277,Integrability,message,message,277,"```; body='{\\n \""error\"": {\\n \""code\"": 403,\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""errors\"": [\\n {\\n \""message\"": \""Quota exceeded for quota group \\'default\\' and limit \\'Queries per user per 100 seconds\\' of service \\'compute.googleapis.com\\' for consumer \\'project_number:859893752941\\'.\"",\\n \""domain\"": \""usageLimits\"",\\n \""reason\"": \""rateLimitExceeded\""\\n }\\n ],\\n \""status\"": \""PERMISSION_DENIED\""\\n }\\n}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10613
https://github.com/hail-is/hail/pull/10614:59,Testability,test,tested,59,"I pulled this out of my local whitening branch. It's being tested there, but I'm not sure the best way to write unit tests. Do we have any similar tests for STypes?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10614
https://github.com/hail-is/hail/pull/10614:117,Testability,test,tests,117,"I pulled this out of my local whitening branch. It's being tested there, but I'm not sure the best way to write unit tests. Do we have any similar tests for STypes?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10614
https://github.com/hail-is/hail/pull/10614:147,Testability,test,tests,147,"I pulled this out of my local whitening branch. It's being tested there, but I'm not sure the best way to write unit tests. Do we have any similar tests for STypes?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10614
https://github.com/hail-is/hail/pull/10618:12,Performance,Cache,Cache,12,Added Chunk Cache to facilitate faster chunk interactions through less use of malloc and free.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10618
https://github.com/hail-is/hail/pull/10621:13,Integrability,rout,routes,13,"Non-existent routes have a `None` resource, so this was causing `500`s on requests that should have `404`'d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10621
https://github.com/hail-is/hail/issues/10623:546,Modifiability,config,config,546,"Using vep99 and Hail 0.2.57 (and 0.2.70) leads to the absence of vep transcript_consequences annotation (mt.vep.transcript_consequences). We verified that if we run vep annotation outside hail the annotation is present while hl.vep gives absent ones with NA in transcript_consequences field. Here I am attaching the vcf file (I changed extension to '.txt' to allow for the file upload) with two variants, where the chr4:113358472 one gives NA in the transcript_consequences field. The gene is in coding region and have many transcripts. Also vep config json is attached (also needed to change extension to '.txt').; [vep99-loftee-grch38-aws.txt](https://github.com/hail-is/hail/files/6733985/vep99-loftee-grch38-aws.txt). [batch109_subset.txt](https://github.com/hail-is/hail/files/6733958/batch109_subset.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10623
https://github.com/hail-is/hail/pull/10624:42,Availability,error,error,42,"A user was getting an index out-of-bounds error on `cdf.values[idx]`. I can't reproduce it, but this should guarantee the index is in bounds, and is a simplification besides.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10624
https://github.com/hail-is/hail/pull/10624:151,Usability,simpl,simplification,151,"A user was getting an index out-of-bounds error on `cdf.values[idx]`. I can't reproduce it, but this should guarantee the index is in bounds, and is a simplification besides.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10624
https://github.com/hail-is/hail/pull/10627:80,Deployability,configurat,configuration,80,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:27,Energy Efficiency,Monitor,Monitoring,27,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:186,Energy Efficiency,monitor,monitoring,186,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:808,Integrability,rout,route,808,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:80,Modifiability,config,configuration,80,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:731,Modifiability,config,config,731,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:282,Security,expose,exposed,282,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:472,Security,expose,exposed,472,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:521,Security,authenticat,authentication,521,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10627:800,Usability,simpl,simpler,800,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10627
https://github.com/hail-is/hail/pull/10633:57,Security,access,accessing,57,A use case is to add the `cloud-platform` scope to allow accessing GCP secrets from within a dataproc script.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10633
https://github.com/hail-is/hail/pull/10634:32,Availability,down,down,32,"Whenever the batch driver shuts down we have some errors due to unclosed aiohttp `ClientSession`s. Deploying the driver in asyncio debug mode revealed that these sessions were in the `ComputeClient` and `LoggingClient`, which we don't call `close` on (and we don't use them as context managers). After this change I was able to delete my driver pod without any unclosed client session errors (though plenty of cancelled errors, which is a separate issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10634
https://github.com/hail-is/hail/pull/10634:50,Availability,error,errors,50,"Whenever the batch driver shuts down we have some errors due to unclosed aiohttp `ClientSession`s. Deploying the driver in asyncio debug mode revealed that these sessions were in the `ComputeClient` and `LoggingClient`, which we don't call `close` on (and we don't use them as context managers). After this change I was able to delete my driver pod without any unclosed client session errors (though plenty of cancelled errors, which is a separate issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10634
https://github.com/hail-is/hail/pull/10634:385,Availability,error,errors,385,"Whenever the batch driver shuts down we have some errors due to unclosed aiohttp `ClientSession`s. Deploying the driver in asyncio debug mode revealed that these sessions were in the `ComputeClient` and `LoggingClient`, which we don't call `close` on (and we don't use them as context managers). After this change I was able to delete my driver pod without any unclosed client session errors (though plenty of cancelled errors, which is a separate issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10634
https://github.com/hail-is/hail/pull/10634:420,Availability,error,errors,420,"Whenever the batch driver shuts down we have some errors due to unclosed aiohttp `ClientSession`s. Deploying the driver in asyncio debug mode revealed that these sessions were in the `ComputeClient` and `LoggingClient`, which we don't call `close` on (and we don't use them as context managers). After this change I was able to delete my driver pod without any unclosed client session errors (though plenty of cancelled errors, which is a separate issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10634
https://github.com/hail-is/hail/pull/10634:99,Deployability,Deploy,Deploying,99,"Whenever the batch driver shuts down we have some errors due to unclosed aiohttp `ClientSession`s. Deploying the driver in asyncio debug mode revealed that these sessions were in the `ComputeClient` and `LoggingClient`, which we don't call `close` on (and we don't use them as context managers). After this change I was able to delete my driver pod without any unclosed client session errors (though plenty of cancelled errors, which is a separate issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10634
https://github.com/hail-is/hail/pull/10634:204,Testability,Log,LoggingClient,204,"Whenever the batch driver shuts down we have some errors due to unclosed aiohttp `ClientSession`s. Deploying the driver in asyncio debug mode revealed that these sessions were in the `ComputeClient` and `LoggingClient`, which we don't call `close` on (and we don't use them as context managers). After this change I was able to delete my driver pod without any unclosed client session errors (though plenty of cancelled errors, which is a separate issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10634
https://github.com/hail-is/hail/pull/10635:166,Availability,echo,echo,166,Added nd array map2 to correspond to existing scala nd array map2 ir. Also used map2 to facilitate writing user facing features hl.nd.maximum and hl.nd.minimum which echo np.maximum and np.minimum.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10635
https://github.com/hail-is/hail/issues/10644:104,Availability,error,error,104,"I am trying to convert the matrix table output from `vcf_combiner `to vcf format. ; I get the following error on running `export_vcf `: ; ```; Hail version: 0.2.64-1ef70187dc78; Error summary: HailException: Invalid type for format field 'gvcf_info'. Found 'struct{BaseQRankSum: float64, ExcessHet: float64, InbreedingCoeff: float64, MLEAC: array<int32>, MLEAF: array<float64>, MQRankSum: float64, RAW_MQandDP: array<int32>, ReadPosRankSum: float64}'.; ```; Description of the matrix table : ; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'LA': array<int32>; 'LGT': call; 'LAD': array<int32>; 'LPGT': call; 'LPL': array<int32>; 'RGQ': int32; 'END': int32; 'gvcf_info': struct {; BaseQRankSum: float64, ; ExcessHet: float64, ; InbreedingCoeff: float64, ; MLEAC: array<int32>, ; MLEAF: array<float64>, ; MQRankSum: float64, ; RAW_MQandDP: array<int32>, ; ReadPosRankSum: float64; }; 'DP': int32; 'GP': array<float64>; 'GQ': int32; 'MIN_DP': int32; 'PG': array<float64>; 'PID': str; 'PS': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; ```; The gVCF was produced by GATK4.2 dragen-mode.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10644
https://github.com/hail-is/hail/issues/10644:178,Availability,Error,Error,178,"I am trying to convert the matrix table output from `vcf_combiner `to vcf format. ; I get the following error on running `export_vcf `: ; ```; Hail version: 0.2.64-1ef70187dc78; Error summary: HailException: Invalid type for format field 'gvcf_info'. Found 'struct{BaseQRankSum: float64, ExcessHet: float64, InbreedingCoeff: float64, MLEAC: array<int32>, MLEAF: array<float64>, MQRankSum: float64, RAW_MQandDP: array<int32>, ReadPosRankSum: float64}'.; ```; Description of the matrix table : ; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'LA': array<int32>; 'LGT': call; 'LAD': array<int32>; 'LPGT': call; 'LPL': array<int32>; 'RGQ': int32; 'END': int32; 'gvcf_info': struct {; BaseQRankSum: float64, ; ExcessHet: float64, ; InbreedingCoeff: float64, ; MLEAC: array<int32>, ; MLEAF: array<float64>, ; MQRankSum: float64, ; RAW_MQandDP: array<int32>, ; ReadPosRankSum: float64; }; 'DP': int32; 'GP': array<float64>; 'GQ': int32; 'MIN_DP': int32; 'PG': array<float64>; 'PID': str; 'PS': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; ```; The gVCF was produced by GATK4.2 dragen-mode.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10644
https://github.com/hail-is/hail/pull/10645:60,Availability,error,error,60,Pending confirmation from Ben W. that this is indeed a user error...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10645
https://github.com/hail-is/hail/pull/10647:22,Usability,usab,usable,22,"This makes Hail Batch usable from a VM that's run under a service account, without requiring a JSON key. #assign services",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10647
https://github.com/hail-is/hail/pull/10648:72,Deployability,deploy,deploy,72,Another attempt at #10376 with some fixes of bugs seen during the first deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10648
https://github.com/hail-is/hail/pull/10649:12,Deployability,release,release,12,I'd like to release to get weighted linear regression support for linear regression rows released.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10649
https://github.com/hail-is/hail/pull/10649:89,Deployability,release,released,89,I'd like to release to get weighted linear regression support for linear regression rows released.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10649
https://github.com/hail-is/hail/pull/10652:20,Safety,timeout,timeout,20,I think our default timeout of 5 seconds is why the disk operations are slow. The `wait` endpoint returns within 2 minutes. https://cloud.google.com/compute/docs/api/how-tos/api-requests-responses#handling_api_responses,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10652
https://github.com/hail-is/hail/pull/10653:30,Testability,test,tests,30,We were getting warnings when tests ran that we were internally using deprecated methods still.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10653
https://github.com/hail-is/hail/pull/10654:50,Deployability,update,updated-desc,50,"If you sort the GitHub PRs by `is:pr is:open sort:updated-desc`, you'll see that some PRs which have merge conflicts are continuously tested by CI. Really old PRs that are stuck in this state reach a GitHub quota for status updates to a PR for a given SHA. It seems to me that we shouldn't retest any PR that is `source_sha_failed`, and I think this should resolve that particular issue, but am not sure if this is necessarily the best place to insert this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10654
https://github.com/hail-is/hail/pull/10654:121,Deployability,continuous,continuously,121,"If you sort the GitHub PRs by `is:pr is:open sort:updated-desc`, you'll see that some PRs which have merge conflicts are continuously tested by CI. Really old PRs that are stuck in this state reach a GitHub quota for status updates to a PR for a given SHA. It seems to me that we shouldn't retest any PR that is `source_sha_failed`, and I think this should resolve that particular issue, but am not sure if this is necessarily the best place to insert this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10654
https://github.com/hail-is/hail/pull/10654:224,Deployability,update,updates,224,"If you sort the GitHub PRs by `is:pr is:open sort:updated-desc`, you'll see that some PRs which have merge conflicts are continuously tested by CI. Really old PRs that are stuck in this state reach a GitHub quota for status updates to a PR for a given SHA. It seems to me that we shouldn't retest any PR that is `source_sha_failed`, and I think this should resolve that particular issue, but am not sure if this is necessarily the best place to insert this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10654
https://github.com/hail-is/hail/pull/10654:134,Testability,test,tested,134,"If you sort the GitHub PRs by `is:pr is:open sort:updated-desc`, you'll see that some PRs which have merge conflicts are continuously tested by CI. Really old PRs that are stuck in this state reach a GitHub quota for status updates to a PR for a given SHA. It seems to me that we shouldn't retest any PR that is `source_sha_failed`, and I think this should resolve that particular issue, but am not sure if this is necessarily the best place to insert this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10654
https://github.com/hail-is/hail/pull/10654:457,Testability,log,logic,457,"If you sort the GitHub PRs by `is:pr is:open sort:updated-desc`, you'll see that some PRs which have merge conflicts are continuously tested by CI. Really old PRs that are stuck in this state reach a GitHub quota for status updates to a PR for a given SHA. It seems to me that we shouldn't retest any PR that is `source_sha_failed`, and I think this should resolve that particular issue, but am not sure if this is necessarily the best place to insert this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10654
https://github.com/hail-is/hail/pull/10655:34,Availability,error,errors,34,Providing users with more helpful errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10655
https://github.com/hail-is/hail/pull/10656:23,Deployability,deploy,deploy,23,I tested this with dev deploy using both my broad developer account and my personal account to make sure regular users only saw their own billing information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10656
https://github.com/hail-is/hail/pull/10656:2,Testability,test,tested,2,I tested this with dev deploy using both my broad developer account and my personal account to make sure regular users only saw their own billing information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10656
https://github.com/hail-is/hail/pull/10662:30,Testability,log,logic,30,CHANGELOG: Fixed partitioning logic in `hl.import_plink`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10662
https://github.com/hail-is/hail/pull/10663:338,Usability,intuit,intuitive,338,"It's possible to get a list of (key, value) tuples from DictExpression by casting it to an array. ```python; hl.eval(hl.array(hl.literal({""foo"": 1, ""bar"": 2}))); # [('bar', 2), ('foo', 1)]; ```. This adds an `items` method to DictExpression that returns the same thing. This aligns with the `items` method of Python dicts and may be more intuitive/discoverable than casting to an array. #assign compiler",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10663
https://github.com/hail-is/hail/pull/10664:80,Availability,down,downloaded,80,"I made any command that is larger than 10KB is written to a file instead and is downloaded as an input file to be run (same as what we do for Python jobs). I also added a new field `user_code` that represents the user's code that they want to run versus the command we have Docker run. I formatted the `user_code` for Python jobs to show the actual function definition and the arguments used to call the function. It's probably not perfect (i.e. JobResourceFile), but better than nothing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10664
https://github.com/hail-is/hail/pull/10666:31,Energy Efficiency,allocate,allocate,31,"With StackStruct, now we don't allocate in the `MakeTuple` here.; Should deforest better, and be faster in general.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10666
https://github.com/hail-is/hail/pull/10675:6,Deployability,deploy,deployed,6,"I dev deployed all *_image steps on a single worker running `main` and saw many fail with corrupted filesystems. I imagine this is because multiple jobs were extracting the same filesystem into the same place. The previous change to using a r/w lock for pulling and deleting images is correct, but we must lock on the image id when *extracting* the actual filesystem. With this change everything passed in my dev. The `BATCH_WORKER_IMAGE_ID` fix from before didn't actually work because of not properly escaping the `{` in the f-string. I also moved the `docker rmi` step to be first in the image cleanup process because I imagine if docker refuses to remove an image we shouldn't remove it from our own cache either.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10675
https://github.com/hail-is/hail/pull/10675:704,Performance,cache,cache,704,"I dev deployed all *_image steps on a single worker running `main` and saw many fail with corrupted filesystems. I imagine this is because multiple jobs were extracting the same filesystem into the same place. The previous change to using a r/w lock for pulling and deleting images is correct, but we must lock on the image id when *extracting* the actual filesystem. With this change everything passed in my dev. The `BATCH_WORKER_IMAGE_ID` fix from before didn't actually work because of not properly escaping the `{` in the f-string. I also moved the `docker rmi` step to be first in the image cleanup process because I imagine if docker refuses to remove an image we shouldn't remove it from our own cache either.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10675
https://github.com/hail-is/hail/pull/10679:183,Safety,avoid,avoids,183,"Random hex strings, while valid, are a little dangerous to use as hostnames because some tools (Spark) could attempt to interpret an entirely numeric name as an IP address. This just avoids that danger entirely.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10679
https://github.com/hail-is/hail/pull/10680:9,Security,hash,hashcodes,9,Draft of hashcodes for SValues,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10680
https://github.com/hail-is/hail/pull/10681:56,Performance,cache,cache,56,Docker requires the `-f` tag to purge an image from its cache if multiple tags reference the same image ID. Checked to make sure that this couldn't accidentally disturb the worker container and added an assert because we should never even try to remove the worker image.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10681
https://github.com/hail-is/hail/pull/10681:203,Testability,assert,assert,203,Docker requires the `-f` tag to purge an image from its cache if multiple tags reference the same image ID. Checked to make sure that this couldn't accidentally disturb the worker container and added an assert because we should never even try to remove the worker image.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10681
https://github.com/hail-is/hail/issues/10682:78,Availability,error,error,78,"When trying to lift over the gnomAD 3.1 hail table to GRCh37, I encounter the error mentioned above. This code works:. ```python; import hail as hl. hail_table = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'; chain_file = 'gs://hail-common/references/grch38_to_grch37.over.chain.gz'; ht = hl.read_table(hail_table).head(10_000). GRCh37 = hl.get_reference('GRCh37'); GRCh38 = hl.get_reference('GRCh38'); GRCh38.add_liftover(chain_file, GRCh37). hl.eval(hl.liftover(hl.locus('chr1', 1034245, 'GRCh38'), 'GRCh37')); # Locus(contig=1, position=969625, reference_genome=GRCh37); ```. However, when trying to lift over the entire table it fails:; ```; ht = ht.annotate(; locus_GRCh37 = hl.liftover(ht.locus, 'GRCh37'); ); ht.show(); ```. I got the same error when trying to lift over an older gnomAD version (2.1) from GRCh37 to GRCh38, which used to work according to my best knowledge. Also, this way of lifting over a hail table is following the recommended process on the documentation [here](https://hail.is/docs/0.2/guides/genetics.html?highlight=prs#liftover-variants-from-one-coordinate-system-to-another). I'm quite confident there must be something I'm doing wrong, but now I'm stuck, any help would be highly welcome. Thanks!. The code is running on a Google Cloud Dataproc cluster, Python 3.8, hail version: `'0.2.71-f3a54b530979'`. Error stack:; ```python; --------------------------------------------------------------------------- / 1]; FatalError Traceback (most recent call last); /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj); 392 if cls is not object \; 393 and callable(cls.__dict__.get('__repr__')):; --> 394 return _repr_pprint(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:788,Availability,error,error,788,"When trying to lift over the gnomAD 3.1 hail table to GRCh37, I encounter the error mentioned above. This code works:. ```python; import hail as hl. hail_table = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'; chain_file = 'gs://hail-common/references/grch38_to_grch37.over.chain.gz'; ht = hl.read_table(hail_table).head(10_000). GRCh37 = hl.get_reference('GRCh37'); GRCh38 = hl.get_reference('GRCh38'); GRCh38.add_liftover(chain_file, GRCh37). hl.eval(hl.liftover(hl.locus('chr1', 1034245, 'GRCh38'), 'GRCh37')); # Locus(contig=1, position=969625, reference_genome=GRCh37); ```. However, when trying to lift over the entire table it fails:; ```; ht = ht.annotate(; locus_GRCh37 = hl.liftover(ht.locus, 'GRCh37'); ); ht.show(); ```. I got the same error when trying to lift over an older gnomAD version (2.1) from GRCh37 to GRCh38, which used to work according to my best knowledge. Also, this way of lifting over a hail table is following the recommended process on the documentation [here](https://hail.is/docs/0.2/guides/genetics.html?highlight=prs#liftover-variants-from-one-coordinate-system-to-another). I'm quite confident there must be something I'm doing wrong, but now I'm stuck, any help would be highly welcome. Thanks!. The code is running on a Google Cloud Dataproc cluster, Python 3.8, hail version: `'0.2.71-f3a54b530979'`. Error stack:; ```python; --------------------------------------------------------------------------- / 1]; FatalError Traceback (most recent call last); /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj); 392 if cls is not object \; 393 and callable(cls.__dict__.get('__repr__')):; --> 394 return _repr_pprint(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:1380,Availability,Error,Error,1380,"e, GRCh37). hl.eval(hl.liftover(hl.locus('chr1', 1034245, 'GRCh38'), 'GRCh37')); # Locus(contig=1, position=969625, reference_genome=GRCh37); ```. However, when trying to lift over the entire table it fails:; ```; ht = ht.annotate(; locus_GRCh37 = hl.liftover(ht.locus, 'GRCh37'); ); ht.show(); ```. I got the same error when trying to lift over an older gnomAD version (2.1) from GRCh37 to GRCh38, which used to work according to my best knowledge. Also, this way of lifting over a hail table is following the recommended process on the documentation [here](https://hail.is/docs/0.2/guides/genetics.html?highlight=prs#liftover-variants-from-one-coordinate-system-to-another). I'm quite confident there must be something I'm doing wrong, but now I'm stuck, any help would be highly welcome. Thanks!. The code is running on a Google Cloud Dataproc cluster, Python 3.8, hail version: `'0.2.71-f3a54b530979'`. Error stack:; ```python; --------------------------------------------------------------------------- / 1]; FatalError Traceback (most recent call last); /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj); 392 if cls is not object \; 393 and callable(cls.__dict__.get('__repr__')):; --> 394 return _repr_pprint(obj, self, cycle); 395; 396 return _default_pprint(obj, self, cycle). /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle); 698 """"""A pprint that just redirects to the normal repr function.""""""; 699 # Find newlines and replace them with p.break_(); --> 700 output = repr(obj); 701 lines = output.splitlines(); 702 with p.group():. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in __repr__(self); 1295; 1296",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:5585,Availability,Error,Error,5585,"1 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:5736,Availability,Error,Error,5736,"1 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:5986,Availability,failure,failure,5986,"t(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:6044,Availability,failure,failure,6044,"3 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:15660,Availability,Error,Error,15660,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:192,Deployability,release,release,192,"When trying to lift over the gnomAD 3.1 hail table to GRCh37, I encounter the error mentioned above. This code works:. ```python; import hail as hl. hail_table = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'; chain_file = 'gs://hail-common/references/grch38_to_grch37.over.chain.gz'; ht = hl.read_table(hail_table).head(10_000). GRCh37 = hl.get_reference('GRCh37'); GRCh38 = hl.get_reference('GRCh38'); GRCh38.add_liftover(chain_file, GRCh37). hl.eval(hl.liftover(hl.locus('chr1', 1034245, 'GRCh38'), 'GRCh37')); # Locus(contig=1, position=969625, reference_genome=GRCh37); ```. However, when trying to lift over the entire table it fails:; ```; ht = ht.annotate(; locus_GRCh37 = hl.liftover(ht.locus, 'GRCh37'); ); ht.show(); ```. I got the same error when trying to lift over an older gnomAD version (2.1) from GRCh37 to GRCh38, which used to work according to my best knowledge. Also, this way of lifting over a hail table is following the recommended process on the documentation [here](https://hail.is/docs/0.2/guides/genetics.html?highlight=prs#liftover-variants-from-one-coordinate-system-to-another). I'm quite confident there must be something I'm doing wrong, but now I'm stuck, any help would be highly welcome. Thanks!. The code is running on a Google Cloud Dataproc cluster, Python 3.8, hail version: `'0.2.71-f3a54b530979'`. Error stack:; ```python; --------------------------------------------------------------------------- / 1]; FatalError Traceback (most recent call last); /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj); 392 if cls is not object \; 393 and callable(cls.__dict__.get('__repr__')):; --> 394 return _repr_pprint(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:6919,Energy Efficiency,adapt,adapted,6919,"ck trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:7272,Energy Efficiency,schedul,scheduler,7272,_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableAr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:7344,Energy Efficiency,schedul,scheduler,7344,r.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:7866,Energy Efficiency,schedul,scheduler,7866,scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGSchedul,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:7964,Energy Efficiency,schedul,scheduler,7964,ntextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8056,Energy Efficiency,schedul,scheduler,8056,nagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8101,Energy Efficiency,adapt,adapted,8101, is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8385,Energy Efficiency,schedul,scheduler,8385,ache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8466,Energy Efficiency,schedul,scheduler,8466,rg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(Cont,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8567,Energy Efficiency,schedul,scheduler,8567,tor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCou,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8621,Energy Efficiency,adapt,adapted,8621,java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCoun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8720,Energy Efficiency,schedul,scheduler,8720,eadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCounts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8810,Energy Efficiency,schedul,scheduler,8810,ava:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCounts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:1380); 	at is.hail.expr.ir.TableSubset.execute$(TableIR.scala:1377); 	at is.hail.expr.ir.T,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8908,Energy Efficiency,schedul,scheduler,8908,ges(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCounts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:1380); 	at is.hail.expr.ir.TableSubset.execute$(TableIR.scala:1377); 	at is.hail.expr.ir.TableHead.execute(TableIR.scala:1386); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1905),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:9004,Energy Efficiency,schedul,scheduler,9004,DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCounts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:1380); 	at is.hail.expr.ir.TableSubset.execute$(TableIR.scala:1377); 	at is.hail.expr.ir.TableHead.execute(TableIR.scala:1386); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1905); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:784); 	at is.hail.expr.ir.Interpret$.alread,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:9169,Energy Efficiency,schedul,scheduler,9169,rray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCounts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:1380); 	at is.hail.expr.ir.TableSubset.execute$(TableIR.scala:1377); 	at is.hail.expr.ir.TableHead.execute(TableIR.scala:1386); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1905); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:784); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonComp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:11883,Energy Efficiency,adapt,adapted,11883,.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:29); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:381); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:365); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:14719,Energy Efficiency,adapt,adapted,14719,:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:15072,Energy Efficiency,schedul,scheduler,15072,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:15144,Energy Efficiency,schedul,scheduler,15144,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:3586,Integrability,wrap,wrapper,3586," __repr__(self):. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in _ascii_str(self); 1318 return s; 1319; -> 1320 rows, has_more, dtype = self.data(); 1321 fields = list(dtype); 1322 trunc_fields = [trunc(f) for f in fields]. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in data(self); 1302 row_dtype = t.row.dtype; 1303 t = t.select(**{k: hl._showstr(v) for (k, v) in t.row.items()}); -> 1304 rows, has_more = t._take_n(self.n); 1305 self._data = (rows, has_more, row_dtype); 1306 return self._data. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in _take_n(self, n); 1449 has_more = False; 1450 else:; -> 1451 rows = self.take(n + 1); 1452 has_more = len(rows) > n; 1453 rows = rows[:n]. <decorator-gen-1116> in take(self, n, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in take(self, n, _localize); 2119 """"""; 2120; -> 2121 return self.head(n).collect(_localize); 2122; 2123 @typecheck_method(n=int). <decorator-gen-1110> in collect(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:3637,Integrability,wrap,wrapper,3637," __repr__(self):. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in _ascii_str(self); 1318 return s; 1319; -> 1320 rows, has_more, dtype = self.data(); 1321 fields = list(dtype); 1322 trunc_fields = [trunc(f) for f in fields]. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in data(self); 1302 row_dtype = t.row.dtype; 1303 t = t.select(**{k: hl._showstr(v) for (k, v) in t.row.items()}); -> 1304 rows, has_more = t._take_n(self.n); 1305 self._data = (rows, has_more, row_dtype); 1306 return self._data. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in _take_n(self, n); 1449 has_more = False; 1450 else:; -> 1451 rows = self.take(n + 1); 1452 has_more = len(rows) > n; 1453 rows = rows[:n]. <decorator-gen-1116> in take(self, n, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in take(self, n, _localize); 2119 """"""; 2120; -> 2121 return self.head(n).collect(_localize); 2122; 2123 @typecheck_method(n=int). <decorator-gen-1110> in collect(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:3842,Integrability,wrap,wrapper,3842," __repr__(self):. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in _ascii_str(self); 1318 return s; 1319; -> 1320 rows, has_more, dtype = self.data(); 1321 fields = list(dtype); 1322 trunc_fields = [trunc(f) for f in fields]. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in data(self); 1302 row_dtype = t.row.dtype; 1303 t = t.select(**{k: hl._showstr(v) for (k, v) in t.row.items()}); -> 1304 rows, has_more = t._take_n(self.n); 1305 self._data = (rows, has_more, row_dtype); 1306 return self._data. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in _take_n(self, n); 1449 has_more = False; 1450 else:; -> 1451 rows = self.take(n + 1); 1452 has_more = len(rows) > n; 1453 rows = rows[:n]. <decorator-gen-1116> in take(self, n, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in take(self, n, _localize); 2119 """"""; 2120; -> 2121 return self.head(n).collect(_localize); 2122; 2123 @typecheck_method(n=int). <decorator-gen-1110> in collect(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:4172,Integrability,wrap,wrapper,4172,"s/hail/table.py in _take_n(self, n); 1449 has_more = False; 1450 else:; -> 1451 rows = self.take(n + 1); 1452 has_more = len(rows) > n; 1453 rows = rows[:n]. <decorator-gen-1116> in take(self, n, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in take(self, n, _localize); 2119 """"""; 2120; -> 2121 return self.head(n).collect(_localize); 2122; 2123 @typecheck_method(n=int). <decorator-gen-1110> in collect(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:4223,Integrability,wrap,wrapper,4223,"s/hail/table.py in _take_n(self, n); 1449 has_more = False; 1450 else:; -> 1451 rows = self.take(n + 1); 1452 has_more = len(rows) > n; 1453 rows = rows[:n]. <decorator-gen-1116> in take(self, n, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in take(self, n, _localize); 2119 """"""; 2120; -> 2121 return self.head(n).collect(_localize); 2122; 2123 @typecheck_method(n=int). <decorator-gen-1110> in collect(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:4428,Integrability,wrap,wrapper,4428,"s/hail/table.py in _take_n(self, n); 1449 has_more = False; 1450 else:; -> 1451 rows = self.take(n + 1); 1452 has_more = len(rows) > n; 1453 rows = rows[:n]. <decorator-gen-1116> in take(self, n, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in take(self, n, _localize); 2119 """"""; 2120; -> 2121 return self.head(n).collect(_localize); 2122; 2123 @typecheck_method(n=int). <decorator-gen-1110> in collect(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:10591,Integrability,Wrap,WrappedArray,10591,ffset(PartitionCounts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:1380); 	at is.hail.expr.ir.TableSubset.execute$(TableIR.scala:1377); 	at is.hail.expr.ir.TableHead.execute(TableIR.scala:1386); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1905); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:784); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.$anonfun$apply$1(InterpretNonCompilable.scala:25); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:10612,Integrability,Wrap,WrappedArray,10612,unts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:1380); 	at is.hail.expr.ir.TableSubset.execute$(TableIR.scala:1377); 	at is.hail.expr.ir.TableHead.execute(TableIR.scala:1386); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1905); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:784); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.$anonfun$apply$1(InterpretNonCompilable.scala:25); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:12109,Integrability,Wrap,WrappedArray,12109, is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:29); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:381); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:365); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:275); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:362); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeJSON$1(SparkBackend.scala:406); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:12130,Integrability,Wrap,WrappedArray,12130,lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:29); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:381); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:365); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:275); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:362); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeJSON$1(SparkBackend.scala:406); 	at is.hail.utils.Execut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:6919,Modifiability,adapt,adapted,6919,"ck trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8101,Modifiability,adapt,adapted,8101, is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8621,Modifiability,adapt,adapted,8621,java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCoun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:10182,Modifiability,rewrite,rewrite,10182,(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCounts.scala:73); 	at is.hail.rvd.RVD.head(RVD.scala:526); 	at is.hail.expr.ir.TableSubset.execute(TableIR.scala:1380); 	at is.hail.expr.ir.TableSubset.execute$(TableIR.scala:1377); 	at is.hail.expr.ir.TableHead.execute(TableIR.scala:1386); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1905); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:784); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.$anonfun$apply$1(InterpretNonCompilable.scala:25); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:6,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:10888,Modifiability,rewrite,rewriteChildren,10888,1905); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:784); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.$anonfun$apply$1(InterpretNonCompilable.scala:25); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:10984,Modifiability,rewrite,rewrite,10984,t$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.$anonfun$apply$1(InterpretNonCompilable.scala:25); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:11883,Modifiability,adapt,adapted,11883,.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:29); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:381); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:365); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:627); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:14719,Modifiability,adapt,adapted,14719,:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:5069,Performance,load,loads,5069,"ct(self, _localize). /opt/conda/miniconda3/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/table.py in collect(self, _localize); 1918 e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); 1919 if _localize:; -> 1920 return Env.backend().execute(e._ir); 1921 else:; 1922 return e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 96 raise HailUserError(message_and_trace) from None; 97; ---> 98 raise e. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 72 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stag",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:7626,Performance,concurren,concurrent,7626,richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:7711,Performance,concurren,concurrent,7711,Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:15426,Performance,concurren,concurrent,15426,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:15511,Performance,concurren,concurrent,15511,mand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.71-f3a54b530979; Error summary: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:5965,Safety,abort,aborted,5965,"t(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 73 try:; ---> 74 result = json.loads(self._jhc.backend().executeJSON(jir)); 75 value = ir.typ._from_json(result['value']); 76 timings = result['timings']. /usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 1302; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1306. /opt/conda/miniconda3/lib/python3.8/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 28 raise FatalError('Error summary: %s' % (deepest,), error_id) from None; 29 else:; ---> 30 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 31 'Hail version: %s\n'; 32 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None. FatalError: NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:7996,Safety,abort,abortStage,7996,DD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8088,Safety,abort,abortStage,8088, is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:8408,Safety,abort,abortStage,8408,askRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:22,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/issues/10682:1057,Usability,guid,guides,1057,"encounter the error mentioned above. This code works:. ```python; import hail as hl. hail_table = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'; chain_file = 'gs://hail-common/references/grch38_to_grch37.over.chain.gz'; ht = hl.read_table(hail_table).head(10_000). GRCh37 = hl.get_reference('GRCh37'); GRCh38 = hl.get_reference('GRCh38'); GRCh38.add_liftover(chain_file, GRCh37). hl.eval(hl.liftover(hl.locus('chr1', 1034245, 'GRCh38'), 'GRCh37')); # Locus(contig=1, position=969625, reference_genome=GRCh37); ```. However, when trying to lift over the entire table it fails:; ```; ht = ht.annotate(; locus_GRCh37 = hl.liftover(ht.locus, 'GRCh37'); ); ht.show(); ```. I got the same error when trying to lift over an older gnomAD version (2.1) from GRCh37 to GRCh38, which used to work according to my best knowledge. Also, this way of lifting over a hail table is following the recommended process on the documentation [here](https://hail.is/docs/0.2/guides/genetics.html?highlight=prs#liftover-variants-from-one-coordinate-system-to-another). I'm quite confident there must be something I'm doing wrong, but now I'm stuck, any help would be highly welcome. Thanks!. The code is running on a Google Cloud Dataproc cluster, Python 3.8, hail version: `'0.2.71-f3a54b530979'`. Error stack:; ```python; --------------------------------------------------------------------------- / 1]; FatalError Traceback (most recent call last); /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). /opt/conda/miniconda3/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj); 392 if cls is not object \; 393 and callable(cls.__dict__.get('__repr__')):; --> 394 return _repr_pprint(obj, self, cycle); 395; 396 return _default_pprint(obj, self, c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10682
https://github.com/hail-is/hail/pull/10683:369,Performance,load,loading,369,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10683
https://github.com/hail-is/hail/pull/10683:95,Testability,log,log,95,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10683
https://github.com/hail-is/hail/pull/10683:182,Testability,log,log,182,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10683
https://github.com/hail-is/hail/pull/10683:263,Testability,log,logging,263,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10683
https://github.com/hail-is/hail/pull/10683:312,Testability,log,log,312,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10683
https://github.com/hail-is/hail/pull/10683:381,Testability,log,log,381,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10683
https://github.com/hail-is/hail/pull/10683:446,Testability,test,test,446,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10683
https://github.com/hail-is/hail/pull/10684:113,Deployability,release,release,113,The main change here is changing a 7 to an 8. I'm not sure how any lowered array anything ever worked. We should release after this goes in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10684
https://github.com/hail-is/hail/pull/10685:51,Performance,perform,performance,51,Also rename so that these don't pop up as spurious performance changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10685
https://github.com/hail-is/hail/pull/10686:25,Deployability,deploy,deploy,25,"Right now if a `make ... deploy` step fails to build one of its images it continues (because the building now happens in `docker-build.sh`), which can result in running a service that expects an image not in GCR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10686
https://github.com/hail-is/hail/pull/10687:448,Modifiability,variab,variable,448,"<img width=""1168"" alt=""Screen Shot 2021-05-24 at 4 54 34 PM"" src=""https://user-images.githubusercontent.com/63973811/129767519-85ab5cf1-2da6-41ee-b52f-44bdf63f7118.png"">. The goal of this progress bar is to have a visual concept that could show the progress of every job created. We added Plotly to show how long it takes for each step to complete when creating a batch job. We decided to use the container_status already created and created a new variable that would hold the new data that Plotly would produce. We use the job-status variable to obtain all of the creating pulling and posting information created. We inserted the data collected through a for-loop that test for each possible test case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10687
https://github.com/hail-is/hail/pull/10687:535,Modifiability,variab,variable,535,"<img width=""1168"" alt=""Screen Shot 2021-05-24 at 4 54 34 PM"" src=""https://user-images.githubusercontent.com/63973811/129767519-85ab5cf1-2da6-41ee-b52f-44bdf63f7118.png"">. The goal of this progress bar is to have a visual concept that could show the progress of every job created. We added Plotly to show how long it takes for each step to complete when creating a batch job. We decided to use the container_status already created and created a new variable that would hold the new data that Plotly would produce. We use the job-status variable to obtain all of the creating pulling and posting information created. We inserted the data collected through a for-loop that test for each possible test case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10687
https://github.com/hail-is/hail/pull/10687:670,Testability,test,test,670,"<img width=""1168"" alt=""Screen Shot 2021-05-24 at 4 54 34 PM"" src=""https://user-images.githubusercontent.com/63973811/129767519-85ab5cf1-2da6-41ee-b52f-44bdf63f7118.png"">. The goal of this progress bar is to have a visual concept that could show the progress of every job created. We added Plotly to show how long it takes for each step to complete when creating a batch job. We decided to use the container_status already created and created a new variable that would hold the new data that Plotly would produce. We use the job-status variable to obtain all of the creating pulling and posting information created. We inserted the data collected through a for-loop that test for each possible test case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10687
https://github.com/hail-is/hail/pull/10687:693,Testability,test,test,693,"<img width=""1168"" alt=""Screen Shot 2021-05-24 at 4 54 34 PM"" src=""https://user-images.githubusercontent.com/63973811/129767519-85ab5cf1-2da6-41ee-b52f-44bdf63f7118.png"">. The goal of this progress bar is to have a visual concept that could show the progress of every job created. We added Plotly to show how long it takes for each step to complete when creating a batch job. We decided to use the container_status already created and created a new variable that would hold the new data that Plotly would produce. We use the job-status variable to obtain all of the creating pulling and posting information created. We inserted the data collected through a for-loop that test for each possible test case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10687
https://github.com/hail-is/hail/pull/10687:188,Usability,progress bar,progress bar,188,"<img width=""1168"" alt=""Screen Shot 2021-05-24 at 4 54 34 PM"" src=""https://user-images.githubusercontent.com/63973811/129767519-85ab5cf1-2da6-41ee-b52f-44bdf63f7118.png"">. The goal of this progress bar is to have a visual concept that could show the progress of every job created. We added Plotly to show how long it takes for each step to complete when creating a batch job. We decided to use the container_status already created and created a new variable that would hold the new data that Plotly would produce. We use the job-status variable to obtain all of the creating pulling and posting information created. We inserted the data collected through a for-loop that test for each possible test case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10687
https://github.com/hail-is/hail/pull/10693:340,Deployability,install,installation,340,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10693
https://github.com/hail-is/hail/pull/10693:748,Deployability,configurat,configuration,748,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10693
https://github.com/hail-is/hail/pull/10693:748,Modifiability,config,configuration,748,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10693
https://github.com/hail-is/hail/pull/10693:546,Performance,cache,caches,546,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10693
https://github.com/hail-is/hail/pull/10693:1203,Security,authenticat,authenticate,1203,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10693
https://github.com/hail-is/hail/pull/10696:6,Availability,error,error,6,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10696
https://github.com/hail-is/hail/pull/10696:517,Availability,failure,failure,517,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10696
https://github.com/hail-is/hail/pull/10696:78,Integrability,interface,interface,78,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10696
https://github.com/hail-is/hail/pull/10696:52,Safety,Timeout,TimeoutErrors,52,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10696
https://github.com/hail-is/hail/pull/10696:91,Safety,timeout,timeout,91,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10696
https://github.com/hail-is/hail/pull/10696:142,Safety,timeout,timeout,142,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10696
https://github.com/hail-is/hail/pull/10697:42,Availability,error,error,42,I verified the test failed with the same [error as KC's](https://discuss.hail.is/t/repartition-on-read/2148/2) before my change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10697
https://github.com/hail-is/hail/pull/10697:15,Testability,test,test,15,I verified the test failed with the same [error as KC's](https://discuss.hail.is/t/repartition-on-read/2148/2) before my change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10697
https://github.com/hail-is/hail/pull/10698:155,Security,access,accessible,155,"Adds the split `VariantDataset` representation, where reference block data and variant data are contained in separate `MatrixTable` objects. Functions are accessible via the `hail.vds` submodule, e.g. `hl.vds.sample_qc(vds)`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10698
https://github.com/hail-is/hail/pull/10702:37,Availability,error,error,37,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10702:1297,Availability,error,error,1297,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10702:1266,Energy Efficiency,reduce,reduced,1266,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10702:43,Integrability,message,messages,43,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10702:247,Integrability,message,message,247,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10702:1318,Integrability,message,messages,1318,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10702:1013,Testability,assert,assert,1013,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10702:1278,Testability,log,logging,1278,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10702
https://github.com/hail-is/hail/pull/10704:40,Integrability,interface,interface,40,"boto is synchronous. The AsyncFS create interface returns a writable file-like object, which means the S3 request has to be run in a thread in the background. Originally, I was using the thread pool for that. However, if we fill the thread pool with create requests, then there are no threads for other operations, so client code can deadlock. Instead, run the create requests in a new thread.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10704
https://github.com/hail-is/hail/pull/10705:64,Availability,error,error,64,"See https://github.com/erdewit/nest_asyncio/issues/11. Original error was this:. ```; _________________ ServiceTests.test_single_task_resource_group _________________. self = <test.hailtop.batch.test_batch.ServiceTests testMethod=test_single_task_resource_group>. def test_single_task_resource_group(self):; b = self.batch(); j = b.new_job(); j.declare_resource_group(output={'foo': '{root}.foo'}); j.command(f'echo ""hello"" > {j.output.foo}'); > res = b.run(). ../test/hailtop/batch/test_batch.py:484: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; batch/batch.py:565: in run; run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:411,Availability,echo,echo,411,"See https://github.com/erdewit/nest_asyncio/issues/11. Original error was this:. ```; _________________ ServiceTests.test_single_task_resource_group _________________. self = <test.hailtop.batch.test_batch.ServiceTests testMethod=test_single_task_resource_group>. def test_single_task_resource_group(self):; b = self.batch(); j = b.new_job(); j.declare_resource_group(output={'foo': '{root}.foo'}); j.command(f'echo ""hello"" > {j.output.foo}'); > res = b.run(). ../test/hailtop/batch/test_batch.py:484: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; batch/batch.py:565: in run; run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:1605,Energy Efficiency,schedul,schedules,1605,"run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:1652,Energy Efficiency,schedul,schedules,1652,"run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3655,Energy Efficiency,schedul,scheduled,3655,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3701,Energy Efficiency,schedul,scheduled,3701,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2244,Performance,queue,queue,2244," run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2414,Safety,timeout,timeout,2414,"nixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; bre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2465,Safety,timeout,timeout,2465,"alse>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._sch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2523,Safety,timeout,timeout,2523," full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(hand",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2566,Safety,timeout,timeout,2566,"allbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2654,Safety,timeout,timeout,2654,"; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently schedu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2722,Safety,timeout,timeout,2722,"); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2851,Safety,timeout,timeout,2851,"MER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3013,Safety,timeout,timeout,3013,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3111,Safety,timeout,timeout,3111,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3122,Safety,timeout,timeout,3122,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3190,Safety,timeout,timeout,3190,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3839,Safety,safe,safe,3839,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:176,Testability,test,test,176,"See https://github.com/erdewit/nest_asyncio/issues/11. Original error was this:. ```; _________________ ServiceTests.test_single_task_resource_group _________________. self = <test.hailtop.batch.test_batch.ServiceTests testMethod=test_single_task_resource_group>. def test_single_task_resource_group(self):; b = self.batch(); j = b.new_job(); j.declare_resource_group(output={'foo': '{root}.foo'}); j.command(f'echo ""hello"" > {j.output.foo}'); > res = b.run(). ../test/hailtop/batch/test_batch.py:484: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; batch/batch.py:565: in run; run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:219,Testability,test,testMethod,219,"See https://github.com/erdewit/nest_asyncio/issues/11. Original error was this:. ```; _________________ ServiceTests.test_single_task_resource_group _________________. self = <test.hailtop.batch.test_batch.ServiceTests testMethod=test_single_task_resource_group>. def test_single_task_resource_group(self):; b = self.batch(); j = b.new_job(); j.declare_resource_group(output={'foo': '{root}.foo'}); j.command(f'echo ""hello"" > {j.output.foo}'); > res = b.run(). ../test/hailtop/batch/test_batch.py:484: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; batch/batch.py:565: in run; run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:464,Testability,test,test,464,"See https://github.com/erdewit/nest_asyncio/issues/11. Original error was this:. ```; _________________ ServiceTests.test_single_task_resource_group _________________. self = <test.hailtop.batch.test_batch.ServiceTests testMethod=test_single_task_resource_group>. def test_single_task_resource_group(self):; b = self.batch(); j = b.new_job(); j.declare_resource_group(output={'foo': '{root}.foo'}); j.command(f'echo ""hello"" > {j.output.foo}'); > res = b.run(). ../test/hailtop/batch/test_batch.py:484: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; batch/batch.py:565: in run; run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2778,Testability,log,logging,2778,"S and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2807,Testability,log,logging,2807,"lled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2869,Testability,log,logger,2869,"MER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2876,Testability,log,log,2876,"lls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._read",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2953,Testability,log,logger,2953," is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = sel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:2960,Testability,log,log,2960,"ndle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop fr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3064,Testability,log,logger,3064,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10705:3071,Testability,log,log,3071,"d:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callbacks run this time around --; # they will be run the next time (after another I/O poll).; # Use an idiom that is thread-safe without using locks.; ntodo = len(self._ready); for i in range(ntodo):; > handle = self._ready.popleft(); E IndexError: pop from an empty deque; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10705
https://github.com/hail-is/hail/pull/10710:129,Performance,concurren,concurrent,129,I changed this from 2=>1 in April of last year unintentionally while debugging; (it's easy to get interleaved prints/logs with 2 concurrent worker threads). https://github.com/hail-is/hail/pull/8535/files#diff-bf51d09b286fddaa730b426824ccb12dac8b9032e0c88bde81882f3cb1423df8R14,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10710
https://github.com/hail-is/hail/pull/10710:117,Testability,log,logs,117,I changed this from 2=>1 in April of last year unintentionally while debugging; (it's easy to get interleaved prints/logs with 2 concurrent worker threads). https://github.com/hail-is/hail/pull/8535/files#diff-bf51d09b286fddaa730b426824ccb12dac8b9032e0c88bde81882f3cb1423df8R14,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10710
https://github.com/hail-is/hail/pull/10713:32,Integrability,message,message,32,"This PR eliminates this warning message:. ```; This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. I tested this change in the SQL Fiddle and still got the correct answer. We could remove all non-varying fields from the group by, but decided to leave the other sortable fields as that is the SQL standard.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10713
https://github.com/hail-is/hail/pull/10713:133,Testability,test,tested,133,"This PR eliminates this warning message:. ```; This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. I tested this change in the SQL Fiddle and still got the correct answer. We could remove all non-varying fields from the group by, but decided to leave the other sortable fields as that is the SQL standard.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10713
https://github.com/hail-is/hail/pull/10714:1093,Integrability,wrap,wrap,1093,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10714
https://github.com/hail-is/hail/pull/10714:793,Performance,queue,queue,793,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10714
https://github.com/hail-is/hail/pull/10714:87,Testability,log,log,87,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10714
https://github.com/hail-is/hail/pull/10714:458,Testability,log,log,458,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10714
https://github.com/hail-is/hail/pull/10714:715,Testability,log,log,715,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10714
https://github.com/hail-is/hail/pull/10714:1142,Usability,simpl,simplified,1142,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10714
https://github.com/hail-is/hail/pull/10717:9,Testability,benchmark,benchmark,9,‚Ä¶ate the benchmark,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10717
https://github.com/hail-is/hail/pull/10719:68,Performance,perform,performant,68,"This will make `TableHead` ~and `TableTail`~ lowered implementation performant enough to actually use. EDIT: I was doing something wrong with `TableTail`, decided to just leave it for a follow up PR instead of having this sit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10719
https://github.com/hail-is/hail/pull/10720:32,Availability,error,error,32,"It still thought it had `s`, an error message to print as a python traceback, rather than an error id like it has now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10720
https://github.com/hail-is/hail/pull/10720:93,Availability,error,error,93,"It still thought it had `s`, an error message to print as a python traceback, rather than an error id like it has now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10720
https://github.com/hail-is/hail/pull/10720:38,Integrability,message,message,38,"It still thought it had `s`, an error message to print as a python traceback, rather than an error id like it has now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10720
https://github.com/hail-is/hail/issues/10722:182,Availability,error,error-summary-nosuchelementexception-key-not-found-,182,Topics:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/key.20not.20found.3A.20GRCh38/near/247317975; https://discuss.hail.is/t/potential-liftover-issue-error-summary-nosuchelementexception-key-not-found-grch37/2154. Stack trace:; ```; java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike.default(MapLike.scala:235); 	at scala.collection.MapLike.default$(MapLike.scala:234); 	at scala.collection.AbstractMap.default(Map.scala:65); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:69); 	at is.hail.variant.ReferenceGenome.getLiftover(ReferenceGenome.scala:412); 	at is.hail.variant.ReferenceGenome.liftoverLocus(ReferenceGenome.scala:423); 	at __C700Compiled.applyregion0_8(Emit.scala); 	at __C700Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1936); ```. See Lindo's comment in the Zulip thread to replicate (hopefully),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10722
https://github.com/hail-is/hail/issues/10722:522,Security,Hash,HashMap,522,Topics:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/key.20not.20found.3A.20GRCh38/near/247317975; https://discuss.hail.is/t/potential-liftover-issue-error-summary-nosuchelementexception-key-not-found-grch37/2154. Stack trace:; ```; java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike.default(MapLike.scala:235); 	at scala.collection.MapLike.default$(MapLike.scala:234); 	at scala.collection.AbstractMap.default(Map.scala:65); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:69); 	at is.hail.variant.ReferenceGenome.getLiftover(ReferenceGenome.scala:412); 	at is.hail.variant.ReferenceGenome.liftoverLocus(ReferenceGenome.scala:423); 	at __C700Compiled.applyregion0_8(Emit.scala); 	at __C700Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1936); ```. See Lindo's comment in the Zulip thread to replicate (hopefully),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10722
https://github.com/hail-is/hail/issues/10722:536,Security,Hash,HashMap,536,Topics:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/key.20not.20found.3A.20GRCh38/near/247317975; https://discuss.hail.is/t/potential-liftover-issue-error-summary-nosuchelementexception-key-not-found-grch37/2154. Stack trace:; ```; java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike.default(MapLike.scala:235); 	at scala.collection.MapLike.default$(MapLike.scala:234); 	at scala.collection.AbstractMap.default(Map.scala:65); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:69); 	at is.hail.variant.ReferenceGenome.getLiftover(ReferenceGenome.scala:412); 	at is.hail.variant.ReferenceGenome.liftoverLocus(ReferenceGenome.scala:423); 	at __C700Compiled.applyregion0_8(Emit.scala); 	at __C700Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1936); ```. See Lindo's comment in the Zulip thread to replicate (hopefully),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10722
https://github.com/hail-is/hail/pull/10723:36,Deployability,update,update,36,CI is updating a lot of PRs in its `update` loop that don't need to be updated. I've replicated and examined one such scenario in my own dev namespace but I feel like this visibility would be nice to have by default.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10723
https://github.com/hail-is/hail/pull/10723:71,Deployability,update,updated,71,CI is updating a lot of PRs in its `update` loop that don't need to be updated. I've replicated and examined one such scenario in my own dev namespace but I feel like this visibility would be nice to have by default.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10723
https://github.com/hail-is/hail/pull/10726:132,Integrability,message,message,132,"This adds a new IR that does a `consoleLogInfo` as a side effect. I use it once, in `_linear_regression_rows_nd`, to mimic the info message we were logging to users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10726
https://github.com/hail-is/hail/pull/10726:148,Testability,log,logging,148,"This adds a new IR that does a `consoleLogInfo` as a side effect. I use it once, in `_linear_regression_rows_nd`, to mimic the info message we were logging to users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10726
https://github.com/hail-is/hail/pull/10727:9,Testability,test,test,9,Also add test for vds.sampleqc.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10727
https://github.com/hail-is/hail/pull/10734:27,Performance,perform,performance,27,"This was unused, but was a performance nightmare -- the reconstruction of a single type triggered a full reallocation of the nested structure. ![image](https://user-images.githubusercontent.com/10562794/127706520-a3202b3f-b478-407d-b64f-496a4f08f68b.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10734
https://github.com/hail-is/hail/pull/10736:463,Deployability,deploy,deployment,463,"A little pieced together but these are my notes from getting batch set up in a new GCP project. Most is copied from @cseed's recent branch with a few additional fixes. One not-great thing is that gateway fails to start if it can't resolve its upstream hosts, so I had to effectively remove the server blocks for anything not batch/batch-driver/auth by editing `letsencrypt/subdomains.txt` and then redeploying the gateway. Weird thing is that I deleted the batch deployment and new gateway pods started up fine, so perhaps it's something to do with kube-dns? Planning to look into this as a follow-up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10736
https://github.com/hail-is/hail/pull/10738:118,Availability,error,errors,118,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10738:274,Availability,error,error,274,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10738:797,Availability,down,downstream,797,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10738:856,Deployability,deploy,deploy,856,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10738:266,Safety,timeout,timeout,266,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10738:886,Safety,timeout,timeout,886,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10738:942,Safety,timeout,timeouts,942,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10738:708,Testability,test,test,708,"A couple of fixes to batch pool executor to both get rid of orphaned running forever jobs and exception not retrieved errors:. - `asyncio.wait` does not retrieve results. I had to change waits to gathers with return_exceptions=True to get the behavior we want.; - A timeout error with `asyncio.wait_for` cancels the task automatically. Therefore, the previous code would never cancel the batch because the task was already ""cancelled"".; - I made `asyncio_cancel` idempotent and made sure we cancel the batch if the task has been cancelled to address the issue above. I added a check to see if the batch is running before cancelling. I'm ambivalent on whether this change is necessary.; - I added an explicit test now to make sure all batches are terminated. I think this is a good change, but the downstream consequences could be if this runs forever on a deploy (relies on an explicit timeout). Although, `test_hailtop_batch_*` has explicit timeouts. So I think we're good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10738
https://github.com/hail-is/hail/pull/10740:168,Energy Efficiency,allocate,allocate,168,"I don't know if this happens to you guys, but when I try to compile `RegionSuite.scala` I get Scala compiler warnings about how it's not cool to do `operations(3) == (""allocate"", 64)`, since Scala can't tell if we mean:. `operations(3).==(""allocate"", 64)`. or . `operations(3).==((""allocate"", 64))`. It was annoying me, so I added the extra parens so it would stop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10740
https://github.com/hail-is/hail/pull/10740:240,Energy Efficiency,allocate,allocate,240,"I don't know if this happens to you guys, but when I try to compile `RegionSuite.scala` I get Scala compiler warnings about how it's not cool to do `operations(3) == (""allocate"", 64)`, since Scala can't tell if we mean:. `operations(3).==(""allocate"", 64)`. or . `operations(3).==((""allocate"", 64))`. It was annoying me, so I added the extra parens so it would stop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10740
https://github.com/hail-is/hail/pull/10740:282,Energy Efficiency,allocate,allocate,282,"I don't know if this happens to you guys, but when I try to compile `RegionSuite.scala` I get Scala compiler warnings about how it's not cool to do `operations(3) == (""allocate"", 64)`, since Scala can't tell if we mean:. `operations(3).==(""allocate"", 64)`. or . `operations(3).==((""allocate"", 64))`. It was annoying me, so I added the extra parens so it would stop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10740
https://github.com/hail-is/hail/pull/10741:403,Availability,error,error,403,"Previously, if you did:. ```; x = hl.bool(True); if x:; ....; ```. You'd get a message like: ""Expressions do not have a static length"", because in Python, truthiness is resolved by first checking if `__bool__` is defined, then checking if `__len__` is nonzero. This PR gives a better message suggesting that the user has in some way tried to coerce an `Expression` into a bool, and only shows the other error if someone does something that specifically needs the length.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10741
https://github.com/hail-is/hail/pull/10741:79,Integrability,message,message,79,"Previously, if you did:. ```; x = hl.bool(True); if x:; ....; ```. You'd get a message like: ""Expressions do not have a static length"", because in Python, truthiness is resolved by first checking if `__bool__` is defined, then checking if `__len__` is nonzero. This PR gives a better message suggesting that the user has in some way tried to coerce an `Expression` into a bool, and only shows the other error if someone does something that specifically needs the length.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10741
https://github.com/hail-is/hail/pull/10741:284,Integrability,message,message,284,"Previously, if you did:. ```; x = hl.bool(True); if x:; ....; ```. You'd get a message like: ""Expressions do not have a static length"", because in Python, truthiness is resolved by first checking if `__bool__` is defined, then checking if `__len__` is nonzero. This PR gives a better message suggesting that the user has in some way tried to coerce an `Expression` into a bool, and only shows the other error if someone does something that specifically needs the length.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10741
https://github.com/hail-is/hail/pull/10746:32,Testability,benchmark,benchmark,32,Necessary for the big aggregate benchmark to pass (albeit more slowly) in benchmarks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10746
https://github.com/hail-is/hail/pull/10746:74,Testability,benchmark,benchmarks,74,Necessary for the big aggregate benchmark to pass (albeit more slowly) in benchmarks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10746
https://github.com/hail-is/hail/issues/10747:838,Availability,Error,Error,838,"Building from source - hail 0.2.74. `$ git clone https://github.com/hail-is/hail.git`; `$ cd hail`; `$ git checkout tags/0.2.74`; `$ cd hail`; `$ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12 SPARK_VERSION=3.1.2`; `...`; `curl -sSL https://storage.googleapis.com/hail-common/libsimdpp-2.1.tar.gz > libsimdpp-2.1.tar.gz; tar -xzf libsimdpp-2.1.tar.gz`. `c++ -o build/ibs.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include/darwin -MD -MF build/ibs.d -MT build/ibs.o -c ibs.cpp`. `make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.`; `make: *** [native-lib-prebuilt] Error 2`. I've installed lz4 and still get this error. . https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements no longer documents lz4 as required, but 0.2.74 is breaking on it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10747
https://github.com/hail-is/hail/issues/10747:886,Availability,error,error,886,"Building from source - hail 0.2.74. `$ git clone https://github.com/hail-is/hail.git`; `$ cd hail`; `$ git checkout tags/0.2.74`; `$ cd hail`; `$ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12 SPARK_VERSION=3.1.2`; `...`; `curl -sSL https://storage.googleapis.com/hail-common/libsimdpp-2.1.tar.gz > libsimdpp-2.1.tar.gz; tar -xzf libsimdpp-2.1.tar.gz`. `c++ -o build/ibs.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include/darwin -MD -MF build/ibs.d -MT build/ibs.o -c ibs.cpp`. `make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.`; `make: *** [native-lib-prebuilt] Error 2`. I've installed lz4 and still get this error. . https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements no longer documents lz4 as required, but 0.2.74 is breaking on it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10747
https://github.com/hail-is/hail/issues/10747:151,Deployability,install,install-on-cluster,151,"Building from source - hail 0.2.74. `$ git clone https://github.com/hail-is/hail.git`; `$ cd hail`; `$ git checkout tags/0.2.74`; `$ cd hail`; `$ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12 SPARK_VERSION=3.1.2`; `...`; `curl -sSL https://storage.googleapis.com/hail-common/libsimdpp-2.1.tar.gz > libsimdpp-2.1.tar.gz; tar -xzf libsimdpp-2.1.tar.gz`. `c++ -o build/ibs.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include/darwin -MD -MF build/ibs.d -MT build/ibs.o -c ibs.cpp`. `make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.`; `make: *** [native-lib-prebuilt] Error 2`. I've installed lz4 and still get this error. . https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements no longer documents lz4 as required, but 0.2.74 is breaking on it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10747
https://github.com/hail-is/hail/issues/10747:853,Deployability,install,installed,853,"Building from source - hail 0.2.74. `$ git clone https://github.com/hail-is/hail.git`; `$ cd hail`; `$ git checkout tags/0.2.74`; `$ cd hail`; `$ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12 SPARK_VERSION=3.1.2`; `...`; `curl -sSL https://storage.googleapis.com/hail-common/libsimdpp-2.1.tar.gz > libsimdpp-2.1.tar.gz; tar -xzf libsimdpp-2.1.tar.gz`. `c++ -o build/ibs.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/include/darwin -MD -MF build/ibs.d -MT build/ibs.o -c ibs.cpp`. `make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.`; `make: *** [native-lib-prebuilt] Error 2`. I've installed lz4 and still get this error. . https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements no longer documents lz4 as required, but 0.2.74 is breaking on it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10747
https://github.com/hail-is/hail/pull/10752:1784,Availability,down,down,1784,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10752
https://github.com/hail-is/hail/pull/10752:1040,Performance,load,loaded,1040,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10752
https://github.com/hail-is/hail/pull/10752:245,Testability,test,test-copy,245,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10752
https://github.com/hail-is/hail/pull/10752:856,Testability,benchmark,benchmarking,856,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10752
https://github.com/hail-is/hail/pull/10752:1622,Testability,benchmark,benchmarking,1622,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10752
https://github.com/hail-is/hail/pull/10752:475,Usability,clear,clear,475,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10752
https://github.com/hail-is/hail/pull/10753:8,Performance,perform,performance,8,Improve performance usage of StringTableReader by implementing StringTablePartitionReader instead of using rvbs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10753
https://github.com/hail-is/hail/pull/10754:8,Testability,test,tests,8,The new tests in scala and python both catch this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10754
https://github.com/hail-is/hail/pull/10755:15,Usability,simpl,simplify,15,Added rules to simplify ArraySlice,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10755
https://github.com/hail-is/hail/pull/10756:315,Availability,error,error,315,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10756
https://github.com/hail-is/hail/pull/10756:59,Deployability,install,install,59,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10756
https://github.com/hail-is/hail/pull/10756:149,Deployability,install,install,149,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10756
https://github.com/hail-is/hail/pull/10756:227,Deployability,install,install,227,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10756
https://github.com/hail-is/hail/pull/10756:337,Deployability,update,updated,337,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10756
https://github.com/hail-is/hail/pull/10756:321,Integrability,message,message,321,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10756
https://github.com/hail-is/hail/pull/10758:164,Testability,benchmark,benchmark,164,"The slight increase in complexity is worth it for the code improvement.; We now will clearly support early truncation when possible (and can; enable the inner join benchmark to go through whole stage codegen),; and will propagate requiredness correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10758
https://github.com/hail-is/hail/pull/10758:85,Usability,clear,clearly,85,"The slight increase in complexity is worth it for the code improvement.; We now will clearly support early truncation when possible (and can; enable the inner join benchmark to go through whole stage codegen),; and will propagate requiredness correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10758
https://github.com/hail-is/hail/pull/10759:15,Usability,simpl,simplify,15,Added rules to simplify ArraySlice IR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10759
https://github.com/hail-is/hail/pull/10761:15,Safety,timeout,timeouts,15,BPE tests with timeouts need to explicitly cancel their jobs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10761
https://github.com/hail-is/hail/pull/10761:4,Testability,test,tests,4,BPE tests with timeouts need to explicitly cancel their jobs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10761
https://github.com/hail-is/hail/pull/10766:391,Testability,test,tests,391,"I ran into issues when broadcasting a very large struct of ndarrays for a huge linear regression, where the the total size was more than `MAX_INT` bytes. To solve this, I've changed `SerializableRegionValue` to use `ArrayOfByteArrayOutputStream` and `ArrayOfByteInputStream`, which create nested arrays of bytes instead of just one array, removing any maximum length issues. . PRing now for tests, also running benchmarks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10766
https://github.com/hail-is/hail/pull/10766:411,Testability,benchmark,benchmarks,411,"I ran into issues when broadcasting a very large struct of ndarrays for a huge linear regression, where the the total size was more than `MAX_INT` bytes. To solve this, I've changed `SerializableRegionValue` to use `ArrayOfByteArrayOutputStream` and `ArrayOfByteInputStream`, which create nested arrays of bytes instead of just one array, removing any maximum length issues. . PRing now for tests, also running benchmarks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10766
https://github.com/hail-is/hail/pull/10767:385,Availability,error,error,385,"There's a small CI feature where you can randomly assign someone from services and/or compiler team to review a PR by including a directive in the github PR body, e.g. #assign compiler. There's a slight bug where if the PR body is left blank, GitHub will report it as `None` instead of what I assumed would be """", which breaks the lines like `if ASSIGN_SERVICES in self.body` with the error that `None` is not iterable. This just inserts a guard against that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10767
https://github.com/hail-is/hail/pull/10769:90,Integrability,interface,interface,90,"This adds a `constructUninitialized` constructor to `PCanonicalNDArray`. Now that we have interface methods that mutate an ndarray, there's no reason to force initialization of the data to happen inside the constructor. And there are plenty of uses, like out parameters for lapack methods, that just want an uninitialized ndarray. Also, it seemed silly to be forced to memoize the new uninitialized ndarray before filling its data, when the constructor already had everything in locals, so I changed the ndarray constructors to return `SNDArrayValue`s.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10769
https://github.com/hail-is/hail/pull/10770:90,Deployability,install,install,90,"Adds support for using mkl, which is needed for some of the added lapack methods. To use, install mkl, then set the environment variable `HAIL_MKL_PATH` to the directory containing the mkl dylib files. In particular, it should contain `libmkl_rt.dylib`. On my laptop, I installed mkl through conda with `conda install -c intel mkl`, then set `HAIL_MKL_PATH=~/opt/miniconda3/envs/hail/lib/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10770
https://github.com/hail-is/hail/pull/10770:270,Deployability,install,installed,270,"Adds support for using mkl, which is needed for some of the added lapack methods. To use, install mkl, then set the environment variable `HAIL_MKL_PATH` to the directory containing the mkl dylib files. In particular, it should contain `libmkl_rt.dylib`. On my laptop, I installed mkl through conda with `conda install -c intel mkl`, then set `HAIL_MKL_PATH=~/opt/miniconda3/envs/hail/lib/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10770
https://github.com/hail-is/hail/pull/10770:310,Deployability,install,install,310,"Adds support for using mkl, which is needed for some of the added lapack methods. To use, install mkl, then set the environment variable `HAIL_MKL_PATH` to the directory containing the mkl dylib files. In particular, it should contain `libmkl_rt.dylib`. On my laptop, I installed mkl through conda with `conda install -c intel mkl`, then set `HAIL_MKL_PATH=~/opt/miniconda3/envs/hail/lib/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10770
https://github.com/hail-is/hail/pull/10770:128,Modifiability,variab,variable,128,"Adds support for using mkl, which is needed for some of the added lapack methods. To use, install mkl, then set the environment variable `HAIL_MKL_PATH` to the directory containing the mkl dylib files. In particular, it should contain `libmkl_rt.dylib`. On my laptop, I installed mkl through conda with `conda install -c intel mkl`, then set `HAIL_MKL_PATH=~/opt/miniconda3/envs/hail/lib/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10770
https://github.com/hail-is/hail/pull/10772:189,Deployability,configurat,configuration,189,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10772
https://github.com/hail-is/hail/pull/10772:411,Deployability,configurat,configuration,411,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10772
https://github.com/hail-is/hail/pull/10772:189,Modifiability,config,configuration,189,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10772
https://github.com/hail-is/hail/pull/10772:334,Modifiability,config,config,334,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10772
https://github.com/hail-is/hail/pull/10772:411,Modifiability,config,configuration,411,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10772
https://github.com/hail-is/hail/pull/10772:164,Performance,load,load,164,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10772
https://github.com/hail-is/hail/pull/10772:37,Security,authoriz,authorization,37,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10772
https://github.com/hail-is/hail/pull/10776:116,Availability,error,errors,116,Not sure if this is unidiomatic but I always write little scripts that don't close the service backend and end with errors about unclosed sessions. This auto-closes a `ServiceBackend`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10776
https://github.com/hail-is/hail/pull/10778:642,Performance,throughput,throughput,642,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:106,Testability,test,test,106,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:142,Testability,test,test-bucket,142,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:154,Testability,test,test,154,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:224,Testability,test,test,224,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:248,Testability,test,test-bucket,248,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:260,Testability,test,test,260,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:719,Testability,test,test,719,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:749,Testability,test,test-bucket,749,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:761,Testability,test,test,761,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:799,Testability,test,test,799,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:829,Testability,test,test-bucket,829,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:841,Testability,test,test,841,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:879,Testability,test,test,879,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:918,Testability,test,test-bucket,918,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10778:930,Testability,test,test,930,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10778
https://github.com/hail-is/hail/pull/10781:44,Availability,error,error,44,"#10676 correctly points out a documentation error, which I am addressing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10781
https://github.com/hail-is/hail/pull/10783:249,Availability,redundant,redundant,249,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:116,Integrability,wrap,wrappers,116,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:1093,Integrability,interface,interface,1093,"preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:1057,Modifiability,variab,variable,1057,"acked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:249,Safety,redund,redundant,249,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:311,Safety,avoid,avoid,311,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:188,Testability,assert,assert,188,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:1333,Testability,assert,assert,1333,"preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:1723,Testability,assert,assertHasShape,1723,"shed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, and further calls to methods with shape preconditions generate no new runtime assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:1775,Testability,assert,assertHasShape,1775,"shed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, and further calls to methods with shape preconditions generate no new runtime assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:2202,Testability,assert,assertions,2202,"shed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, and further calls to methods with shape preconditions generate no new runtime assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:2292,Testability,assert,assertions,2292,"shed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, and further calls to methods with shape preconditions generate no new runtime assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:469,Usability,simpl,simple,469,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:670,Usability,simpl,simple,670,"~Stacked on #10770~; ~Stacked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerce",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:1575,Usability,simpl,simple,1575,"shed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, and further calls to methods with shape preconditions generate no new runtime assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10783:1624,Usability,simpl,simplifying,1624,"shed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k, n)); val C_ = C.coerceToShape(cb, IndexedSeq(m, n)). \\ this generates no new dynamic shape checks; SNDArray.gemm(cb, A, B, C); ```; then we can emit one set of runtime shape assertions, and further calls to methods with shape preconditions generate no new runtime assertions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10783
https://github.com/hail-is/hail/pull/10785:35,Modifiability,refactor,refactor,35,"This is an attempt to modularize / refactor our terraform code regarding google service accounts and kubernetes secrets. This doesn't add any new functionality. Currently, our use of terraform is one flat file `main.tf` where we declare every `resource` in GCP that should exist. Examples of such resources are `google_service_account`, `google_service_account_key` and `kubernetes_secret`. For each of the accounts we create for various services, we end up creating these three resources (and sometimes IAM roles) in the same way. To abstract this, we can create a custom `module`, which is just a collection of resources, a set of inputs called `variables`, and a set of outputs. A module can then be ""instantiated"" using a `module` block in `main.tf`, providing it the source path of the module and values for its declared variables. Tested by hand in my own GCP project.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10785
https://github.com/hail-is/hail/pull/10785:648,Modifiability,variab,variables,648,"This is an attempt to modularize / refactor our terraform code regarding google service accounts and kubernetes secrets. This doesn't add any new functionality. Currently, our use of terraform is one flat file `main.tf` where we declare every `resource` in GCP that should exist. Examples of such resources are `google_service_account`, `google_service_account_key` and `kubernetes_secret`. For each of the accounts we create for various services, we end up creating these three resources (and sometimes IAM roles) in the same way. To abstract this, we can create a custom `module`, which is just a collection of resources, a set of inputs called `variables`, and a set of outputs. A module can then be ""instantiated"" using a `module` block in `main.tf`, providing it the source path of the module and values for its declared variables. Tested by hand in my own GCP project.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10785
https://github.com/hail-is/hail/pull/10785:826,Modifiability,variab,variables,826,"This is an attempt to modularize / refactor our terraform code regarding google service accounts and kubernetes secrets. This doesn't add any new functionality. Currently, our use of terraform is one flat file `main.tf` where we declare every `resource` in GCP that should exist. Examples of such resources are `google_service_account`, `google_service_account_key` and `kubernetes_secret`. For each of the accounts we create for various services, we end up creating these three resources (and sometimes IAM roles) in the same way. To abstract this, we can create a custom `module`, which is just a collection of resources, a set of inputs called `variables`, and a set of outputs. A module can then be ""instantiated"" using a `module` block in `main.tf`, providing it the source path of the module and values for its declared variables. Tested by hand in my own GCP project.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10785
https://github.com/hail-is/hail/pull/10785:837,Testability,Test,Tested,837,"This is an attempt to modularize / refactor our terraform code regarding google service accounts and kubernetes secrets. This doesn't add any new functionality. Currently, our use of terraform is one flat file `main.tf` where we declare every `resource` in GCP that should exist. Examples of such resources are `google_service_account`, `google_service_account_key` and `kubernetes_secret`. For each of the accounts we create for various services, we end up creating these three resources (and sometimes IAM roles) in the same way. To abstract this, we can create a custom `module`, which is just a collection of resources, a set of inputs called `variables`, and a set of outputs. A module can then be ""instantiated"" using a `module` block in `main.tf`, providing it the source path of the module and values for its declared variables. Tested by hand in my own GCP project.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10785
https://github.com/hail-is/hail/pull/10795:283,Performance,perform,performance,283,"This PR adds the Azure AsyncFS. I added a new secret with the Azure credentials for a new service account, adopted the container `hail-test-****` as the test container, and made the container have a 1 day retention policy (we'll want to check and make sure this actually works). The performance seemed decent. We'll probably want to benchmark this formally at some point, but I think it's good enough for now to have something that works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10795
https://github.com/hail-is/hail/pull/10795:135,Testability,test,test,135,"This PR adds the Azure AsyncFS. I added a new secret with the Azure credentials for a new service account, adopted the container `hail-test-****` as the test container, and made the container have a 1 day retention policy (we'll want to check and make sure this actually works). The performance seemed decent. We'll probably want to benchmark this formally at some point, but I think it's good enough for now to have something that works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10795
https://github.com/hail-is/hail/pull/10795:153,Testability,test,test,153,"This PR adds the Azure AsyncFS. I added a new secret with the Azure credentials for a new service account, adopted the container `hail-test-****` as the test container, and made the container have a 1 day retention policy (we'll want to check and make sure this actually works). The performance seemed decent. We'll probably want to benchmark this formally at some point, but I think it's good enough for now to have something that works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10795
https://github.com/hail-is/hail/pull/10795:333,Testability,benchmark,benchmark,333,"This PR adds the Azure AsyncFS. I added a new secret with the Azure credentials for a new service account, adopted the container `hail-test-****` as the test container, and made the container have a 1 day retention policy (we'll want to check and make sure this actually works). The performance seemed decent. We'll probably want to benchmark this formally at some point, but I think it's good enough for now to have something that works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10795
https://github.com/hail-is/hail/pull/10796:67,Security,access,accessing,67,"Since #10648, public jobs use external DNS and are prohibited from accessing the metadata server to resolve names such as `batch.hail` or `internal.hail`. This broke the ability to submit batches from within a job, since the batch client, recognizing it is in GCE, would attempt to use the fore-mentioned domains. This opens that communication channel by adding an `/etc/hosts` entry for the appropriate batch domain that points to the internal gateway. Relatedly, we currently have this iptables rule https://github.com/hail-is/hail/blob/81f4b1fbedfac288c717ae65664c3cd82b25ac2f/batch/batch/worker/worker.py#L210. for every job's network namespace that `ACCEPT`s packets leaving the worker. This includes packets destined for the internet but can also include those going to internal gateway, or other GCE nodes in our internal network (so maybe `internet_interface` is a slight misnomer). This means that public jobs can currently send requests to any ip address on our internal network (they cannot use our internal DNS, however). I removed this line and instead added more explicit rules (note the order is important):; - Allow packets to the internal gateway; - Allow packets looping back to the same worker the job is on; - Allow packets from public jobs leaving the worker and NOT destined for our internal network; - Allow packets NOT from public jobs leaving the worker destined anywhere. Since the default policy is to drop packets, this should forbid public jobs from talking to **other** addresses on the network with the exception of internal gateway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10796
https://github.com/hail-is/hail/pull/10797:550,Integrability,interface,interface,550,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10797
https://github.com/hail-is/hail/pull/10797:340,Performance,load,loaded,340,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10797
https://github.com/hail-is/hail/pull/10797:637,Performance,load,loadToSValue,637,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10797
https://github.com/hail-is/hail/pull/10797:88,Safety,predict,predict,88,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10797
https://github.com/hail-is/hail/pull/10797:815,Security,access,accessed,815,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10797
https://github.com/hail-is/hail/pull/10797:720,Testability,assert,assertion,720,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10797
https://github.com/hail-is/hail/pull/10797:761,Testability,assert,asserts,761,"~Stacked on #10791~. A few high level changes got mixed up in this PR, since I couldn't predict where a thread would lead once I started pulling. If you would like, I can try to disentangle them. Here are the conceptual changes:; * add a CodeBuilder argument to `getEmitParam`, so that parameters which are pointers to region values can be loaded into `SValue`s with multiple locals; * make `EmitValue` a concrete class, consisting of an optional boolean value and an `SValue`; * add `valueTuple` to both `SValue` and `EmitValue`; * copy the `SCode` interface onto `SValue`, to make it easier to replace `SCode`s with `SValues` ; * add `loadToSValue` to `SingleCodeType`; * add `SStreamValue`. This loses the single use assertion, but that doesn't seem like it asserts much, since the stream producer can be freely accessed without memoizing the `SStreamCode`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10797
https://github.com/hail-is/hail/pull/10798:18,Availability,avail,available,18,Adds optimization available in lowering process if number of rows per partition from child TableIR is known,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10798
https://github.com/hail-is/hail/pull/10798:5,Performance,optimiz,optimization,5,Adds optimization available in lowering process if number of rows per partition from child TableIR is known,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10798
https://github.com/hail-is/hail/pull/10802:33,Modifiability,refactor,refactor,33,This is a trivial (non-semantic) refactor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10802
https://github.com/hail-is/hail/pull/10807:66,Modifiability,config,config,66,"I manually added a `hail_test_gcs_bucket` field to the k8s global config and use that value wherever we have our current test bucket hard coded. I also added the necessary terraform to make that in a new cluster, though I have not done a new terraform run in my project. Once this and a couple more refactoring PRs go in I'll be able to run ci tests in a separate cluster and validate that the terraform is working correctly. cc: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10807
https://github.com/hail-is/hail/pull/10807:299,Modifiability,refactor,refactoring,299,"I manually added a `hail_test_gcs_bucket` field to the k8s global config and use that value wherever we have our current test bucket hard coded. I also added the necessary terraform to make that in a new cluster, though I have not done a new terraform run in my project. Once this and a couple more refactoring PRs go in I'll be able to run ci tests in a separate cluster and validate that the terraform is working correctly. cc: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10807
https://github.com/hail-is/hail/pull/10807:376,Security,validat,validate,376,"I manually added a `hail_test_gcs_bucket` field to the k8s global config and use that value wherever we have our current test bucket hard coded. I also added the necessary terraform to make that in a new cluster, though I have not done a new terraform run in my project. Once this and a couple more refactoring PRs go in I'll be able to run ci tests in a separate cluster and validate that the terraform is working correctly. cc: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10807
https://github.com/hail-is/hail/pull/10807:121,Testability,test,test,121,"I manually added a `hail_test_gcs_bucket` field to the k8s global config and use that value wherever we have our current test bucket hard coded. I also added the necessary terraform to make that in a new cluster, though I have not done a new terraform run in my project. Once this and a couple more refactoring PRs go in I'll be able to run ci tests in a separate cluster and validate that the terraform is working correctly. cc: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10807
https://github.com/hail-is/hail/pull/10807:344,Testability,test,tests,344,"I manually added a `hail_test_gcs_bucket` field to the k8s global config and use that value wherever we have our current test bucket hard coded. I also added the necessary terraform to make that in a new cluster, though I have not done a new terraform run in my project. Once this and a couple more refactoring PRs go in I'll be able to run ci tests in a separate cluster and validate that the terraform is working correctly. cc: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10807
https://github.com/hail-is/hail/pull/10812:580,Integrability,interface,interface,580,"hadoop_open has a somewhat strange behavior, when the global fs is; HadoopFS, BGzip and Gzip files are handled transparently by file; extension, so python reads and writes uncompressed data. This is not the; case if the global fs is LocalFS or GoogleCloudStorageFS. We'd; eventually like to move away from this behavior for HadoopFS altogether,; but we cannot change the behavior of hadoop_open without breaking user; code. To that end, rewrite HadoopFS.open to ignore codecs, and add legacy_open; to preserve the old behavior. As a result of this, we also implement the seekable interface in python; for HadoopFS opened files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10812
https://github.com/hail-is/hail/pull/10812:437,Modifiability,rewrite,rewrite,437,"hadoop_open has a somewhat strange behavior, when the global fs is; HadoopFS, BGzip and Gzip files are handled transparently by file; extension, so python reads and writes uncompressed data. This is not the; case if the global fs is LocalFS or GoogleCloudStorageFS. We'd; eventually like to move away from this behavior for HadoopFS altogether,; but we cannot change the behavior of hadoop_open without breaking user; code. To that end, rewrite HadoopFS.open to ignore codecs, and add legacy_open; to preserve the old behavior. As a result of this, we also implement the seekable interface in python; for HadoopFS opened files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10812
https://github.com/hail-is/hail/pull/10815:76,Availability,down,downloading,76,"This is the initial terraform setup to spinning up resources in azure, from downloading the `az` cli to getting a kubernetes cluster, database and network running. This contains instructions on initializing terraform using a remote backend in an azure container so multiple people should be able to apply terraform changes and have the same view of the system. Next step is setting up the gateway and internal gateways so that we can communicate between k8s services and VMs in the batch worker subnet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10815
https://github.com/hail-is/hail/pull/10819:4,Availability,error,error,4,"The error encountered when importing multiple VCFs with different INFO fields was not a good one. This improves the situation somewhat, by making it print the types instead of the ptypes, by calling attention to the fact that the issue is likely info fields, and by adding a test of this error message. . The situation could still be improved by pushing error ids through `TableRead`, but I have not done so yet. I will save that for a future PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10819
https://github.com/hail-is/hail/pull/10819:288,Availability,error,error,288,"The error encountered when importing multiple VCFs with different INFO fields was not a good one. This improves the situation somewhat, by making it print the types instead of the ptypes, by calling attention to the fact that the issue is likely info fields, and by adding a test of this error message. . The situation could still be improved by pushing error ids through `TableRead`, but I have not done so yet. I will save that for a future PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10819
https://github.com/hail-is/hail/pull/10819:354,Availability,error,error,354,"The error encountered when importing multiple VCFs with different INFO fields was not a good one. This improves the situation somewhat, by making it print the types instead of the ptypes, by calling attention to the fact that the issue is likely info fields, and by adding a test of this error message. . The situation could still be improved by pushing error ids through `TableRead`, but I have not done so yet. I will save that for a future PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10819
https://github.com/hail-is/hail/pull/10819:294,Integrability,message,message,294,"The error encountered when importing multiple VCFs with different INFO fields was not a good one. This improves the situation somewhat, by making it print the types instead of the ptypes, by calling attention to the fact that the issue is likely info fields, and by adding a test of this error message. . The situation could still be improved by pushing error ids through `TableRead`, but I have not done so yet. I will save that for a future PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10819
https://github.com/hail-is/hail/pull/10819:275,Testability,test,test,275,"The error encountered when importing multiple VCFs with different INFO fields was not a good one. This improves the situation somewhat, by making it print the types instead of the ptypes, by calling attention to the fact that the issue is likely info fields, and by adding a test of this error message. . The situation could still be improved by pushing error ids through `TableRead`, but I have not done so yet. I will save that for a future PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10819
https://github.com/hail-is/hail/pull/10827:270,Availability,Error,ErrorHandling,270,"When I test this, I get; ```; FatalError: IllegalFormatConversionException: d != java.lang.String. Java stack trace:; is.hail.utils.HailException: Encountered invalid type for format string %d: format specifier d does not accept type java.lang.String; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:168); 	at __C12Compiled.__m15format(Emit.scala); ... java.util.IllegalFormatConversionException: d != java.lang.String; 	at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4302); 	at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2793); 	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2747); 	at java.util.Formatter.format(Formatter.java:2520); 	at java.util.Formatter.format(Formatter.java:2455); 	at java.lang.String.format(String.java:2940); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:165); 	at __C12Compiled.__m15format(Emit.scala); ... Hail version: 0.2.74-4d495f1c5e01; Error summary: IllegalFormatConversionException: d != java.lang.String; ```. Why is the summary not the `HailException` string?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10827
https://github.com/hail-is/hail/pull/10827:290,Availability,Error,ErrorHandling,290,"When I test this, I get; ```; FatalError: IllegalFormatConversionException: d != java.lang.String. Java stack trace:; is.hail.utils.HailException: Encountered invalid type for format string %d: format specifier d does not accept type java.lang.String; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:168); 	at __C12Compiled.__m15format(Emit.scala); ... java.util.IllegalFormatConversionException: d != java.lang.String; 	at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4302); 	at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2793); 	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2747); 	at java.util.Formatter.format(Formatter.java:2520); 	at java.util.Formatter.format(Formatter.java:2455); 	at java.lang.String.format(String.java:2940); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:165); 	at __C12Compiled.__m15format(Emit.scala); ... Hail version: 0.2.74-4d495f1c5e01; Error summary: IllegalFormatConversionException: d != java.lang.String; ```. Why is the summary not the `HailException` string?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10827
https://github.com/hail-is/hail/pull/10827:333,Availability,Error,ErrorHandling,333,"When I test this, I get; ```; FatalError: IllegalFormatConversionException: d != java.lang.String. Java stack trace:; is.hail.utils.HailException: Encountered invalid type for format string %d: format specifier d does not accept type java.lang.String; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:168); 	at __C12Compiled.__m15format(Emit.scala); ... java.util.IllegalFormatConversionException: d != java.lang.String; 	at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4302); 	at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2793); 	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2747); 	at java.util.Formatter.format(Formatter.java:2520); 	at java.util.Formatter.format(Formatter.java:2455); 	at java.lang.String.format(String.java:2940); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:165); 	at __C12Compiled.__m15format(Emit.scala); ... Hail version: 0.2.74-4d495f1c5e01; Error summary: IllegalFormatConversionException: d != java.lang.String; ```. Why is the summary not the `HailException` string?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10827
https://github.com/hail-is/hail/pull/10827:354,Availability,Error,ErrorHandling,354,"When I test this, I get; ```; FatalError: IllegalFormatConversionException: d != java.lang.String. Java stack trace:; is.hail.utils.HailException: Encountered invalid type for format string %d: format specifier d does not accept type java.lang.String; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:168); 	at __C12Compiled.__m15format(Emit.scala); ... java.util.IllegalFormatConversionException: d != java.lang.String; 	at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4302); 	at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2793); 	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2747); 	at java.util.Formatter.format(Formatter.java:2520); 	at java.util.Formatter.format(Formatter.java:2455); 	at java.lang.String.format(String.java:2940); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:165); 	at __C12Compiled.__m15format(Emit.scala); ... Hail version: 0.2.74-4d495f1c5e01; Error summary: IllegalFormatConversionException: d != java.lang.String; ```. Why is the summary not the `HailException` string?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10827
https://github.com/hail-is/hail/pull/10827:1156,Availability,Error,Error,1156,"When I test this, I get; ```; FatalError: IllegalFormatConversionException: d != java.lang.String. Java stack trace:; is.hail.utils.HailException: Encountered invalid type for format string %d: format specifier d does not accept type java.lang.String; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:168); 	at __C12Compiled.__m15format(Emit.scala); ... java.util.IllegalFormatConversionException: d != java.lang.String; 	at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4302); 	at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2793); 	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2747); 	at java.util.Formatter.format(Formatter.java:2520); 	at java.util.Formatter.format(Formatter.java:2455); 	at java.lang.String.format(String.java:2940); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:165); 	at __C12Compiled.__m15format(Emit.scala); ... Hail version: 0.2.74-4d495f1c5e01; Error summary: IllegalFormatConversionException: d != java.lang.String; ```. Why is the summary not the `HailException` string?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10827
https://github.com/hail-is/hail/pull/10827:7,Testability,test,test,7,"When I test this, I get; ```; FatalError: IllegalFormatConversionException: d != java.lang.String. Java stack trace:; is.hail.utils.HailException: Encountered invalid type for format string %d: format specifier d does not accept type java.lang.String; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:168); 	at __C12Compiled.__m15format(Emit.scala); ... java.util.IllegalFormatConversionException: d != java.lang.String; 	at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4302); 	at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2793); 	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2747); 	at java.util.Formatter.format(Formatter.java:2520); 	at java.util.Formatter.format(Formatter.java:2455); 	at java.lang.String.format(String.java:2940); 	at is.hail.expr.ir.functions.UtilFunctions$.format(UtilFunctions.scala:165); 	at __C12Compiled.__m15format(Emit.scala); ... Hail version: 0.2.74-4d495f1c5e01; Error summary: IllegalFormatConversionException: d != java.lang.String; ```. Why is the summary not the `HailException` string?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10827
https://github.com/hail-is/hail/pull/10830:6,Deployability,upgrade,upgrades,6,These upgrades are simple and do not represent major version upgrades.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10830
https://github.com/hail-is/hail/pull/10830:61,Deployability,upgrade,upgrades,61,These upgrades are simple and do not represent major version upgrades.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10830
https://github.com/hail-is/hail/pull/10830:19,Usability,simpl,simple,19,These upgrades are simple and do not represent major version upgrades.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10830
https://github.com/hail-is/hail/issues/10831:317,Availability,error,error,317,"I am running AWS EMR 6.3.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.1.1; * Scala 2.12.10. On that cluster, I try to build HAIL version 0.2.60-de1845e1c2f6 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.1.1; ```. I got the error: . ```; Task :compileScala; Pruning sources from previous analysis, due to incompatible CompileSetup. /opt/broad-hail/hail/src/main/scala/is/hail/backend/service/ServiceBackend.scala:37: method toString in class IOUtils is deprecated: see corresponding Javadoc for more information.; new GoogleStorageFS(IOUtils.toString(is)); one error found; ```. FYI the same version of hail (0.2.60) on EMR 6.2.0 (same stack but with Spark 3.0.1) works without issue using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10831
https://github.com/hail-is/hail/issues/10831:654,Availability,error,error,654,"I am running AWS EMR 6.3.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.1.1; * Scala 2.12.10. On that cluster, I try to build HAIL version 0.2.60-de1845e1c2f6 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.1.1; ```. I got the error: . ```; Task :compileScala; Pruning sources from previous analysis, due to incompatible CompileSetup. /opt/broad-hail/hail/src/main/scala/is/hail/backend/service/ServiceBackend.scala:37: method toString in class IOUtils is deprecated: see corresponding Javadoc for more information.; new GoogleStorageFS(IOUtils.toString(is)); one error found; ```. FYI the same version of hail (0.2.60) on EMR 6.2.0 (same stack but with Spark 3.0.1) works without issue using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10831
https://github.com/hail-is/hail/issues/10831:217,Deployability,install,install-on-cluster,217,"I am running AWS EMR 6.3.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.1.1; * Scala 2.12.10. On that cluster, I try to build HAIL version 0.2.60-de1845e1c2f6 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.1.1; ```. I got the error: . ```; Task :compileScala; Pruning sources from previous analysis, due to incompatible CompileSetup. /opt/broad-hail/hail/src/main/scala/is/hail/backend/service/ServiceBackend.scala:37: method toString in class IOUtils is deprecated: see corresponding Javadoc for more information.; new GoogleStorageFS(IOUtils.toString(is)); one error found; ```. FYI the same version of hail (0.2.60) on EMR 6.2.0 (same stack but with Spark 3.0.1) works without issue using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10831
https://github.com/hail-is/hail/issues/10831:811,Deployability,install,install-on-cluster,811,"I am running AWS EMR 6.3.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.1.1; * Scala 2.12.10. On that cluster, I try to build HAIL version 0.2.60-de1845e1c2f6 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.1.1; ```. I got the error: . ```; Task :compileScala; Pruning sources from previous analysis, due to incompatible CompileSetup. /opt/broad-hail/hail/src/main/scala/is/hail/backend/service/ServiceBackend.scala:37: method toString in class IOUtils is deprecated: see corresponding Javadoc for more information.; new GoogleStorageFS(IOUtils.toString(is)); one error found; ```. FYI the same version of hail (0.2.60) on EMR 6.2.0 (same stack but with Spark 3.0.1) works without issue using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10831
https://github.com/hail-is/hail/issues/10832:216,Deployability,install,install-on-cluster,216,"I am running AWS EMR 6.2.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.0.1; * Scala 2.12.10. On that cluster, I am building HAIL version 0.2.74-0c3a74d12093 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```. Compilation succeeds. I am able to load Hail into my Zeppelin notebook pyspark kernel; I am able to load and describe hail table stored in AWS S3. ```; %pyspark; # Import and launch Hail; import hail as hl; hl.init(sc); # Load files; ht_s = hl.read_table('s3://npm-hail/SG10K_Health_r5.3.1/n9770/SG10K_Health_r5.3.1.n9770.samples.ht'); # Desctibe; ht_s.describe(); ```. I am not able to show data. ```; %pyspark; ht_s.show(); ```. the `show()` call run infinitely into zeppelin and SparkUI note a started job, but no task launched. FYI hail version 0.2.60 on same EMR stack works without issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10832
https://github.com/hail-is/hail/issues/10832:341,Performance,load,load,341,"I am running AWS EMR 6.2.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.0.1; * Scala 2.12.10. On that cluster, I am building HAIL version 0.2.74-0c3a74d12093 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```. Compilation succeeds. I am able to load Hail into my Zeppelin notebook pyspark kernel; I am able to load and describe hail table stored in AWS S3. ```; %pyspark; # Import and launch Hail; import hail as hl; hl.init(sc); # Load files; ht_s = hl.read_table('s3://npm-hail/SG10K_Health_r5.3.1/n9770/SG10K_Health_r5.3.1.n9770.samples.ht'); # Desctibe; ht_s.describe(); ```. I am not able to show data. ```; %pyspark; ht_s.show(); ```. the `show()` call run infinitely into zeppelin and SparkUI note a started job, but no task launched. FYI hail version 0.2.60 on same EMR stack works without issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10832
https://github.com/hail-is/hail/issues/10832:406,Performance,load,load,406,"I am running AWS EMR 6.2.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.0.1; * Scala 2.12.10. On that cluster, I am building HAIL version 0.2.74-0c3a74d12093 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```. Compilation succeeds. I am able to load Hail into my Zeppelin notebook pyspark kernel; I am able to load and describe hail table stored in AWS S3. ```; %pyspark; # Import and launch Hail; import hail as hl; hl.init(sc); # Load files; ht_s = hl.read_table('s3://npm-hail/SG10K_Health_r5.3.1/n9770/SG10K_Health_r5.3.1.n9770.samples.ht'); # Desctibe; ht_s.describe(); ```. I am not able to show data. ```; %pyspark; ht_s.show(); ```. the `show()` call run infinitely into zeppelin and SparkUI note a started job, but no task launched. FYI hail version 0.2.60 on same EMR stack works without issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10832
https://github.com/hail-is/hail/issues/10832:528,Performance,Load,Load,528,"I am running AWS EMR 6.2.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.0.1; * Scala 2.12.10. On that cluster, I am building HAIL version 0.2.74-0c3a74d12093 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```. Compilation succeeds. I am able to load Hail into my Zeppelin notebook pyspark kernel; I am able to load and describe hail table stored in AWS S3. ```; %pyspark; # Import and launch Hail; import hail as hl; hl.init(sc); # Load files; ht_s = hl.read_table('s3://npm-hail/SG10K_Health_r5.3.1/n9770/SG10K_Health_r5.3.1.n9770.samples.ht'); # Desctibe; ht_s.describe(); ```. I am not able to show data. ```; %pyspark; ht_s.show(); ```. the `show()` call run infinitely into zeppelin and SparkUI note a started job, but no task launched. FYI hail version 0.2.60 on same EMR stack works without issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10832
https://github.com/hail-is/hail/pull/10834:348,Deployability,install,installs,348,"- Added a shared image gallery to terraform; - Added a managed identity `batch-worker` to terraform; - Gave `batch-worker` ""acrpull"" privileges for the resource group; - Added new config variables in config.mk that are specific to Azure; - Added commands to batch/Makefile to create a boot disk image; - Added an Azure-specific startup script that installs Docker and the CLI and then authenticates and pulls the base image. The disk image we create is specialized. This means it has credentials in there after publishing it. I think this is okay and I specifically used the batch-worker managed identity to login for this. I can try and double check this assumption if you think I'm not correct after reading these docs: https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_create. > Accept system or user assigned identities separated by spaces. Use '[system]' to refer system assigned identity, or a resource id to refer user assigned identity. Check out help for more examples. I had to give the batch-worker managed identity in the resource group we want permissions to be an identity for a VM in the build-batch-worker-image resource group.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10834
https://github.com/hail-is/hail/pull/10834:180,Modifiability,config,config,180,"- Added a shared image gallery to terraform; - Added a managed identity `batch-worker` to terraform; - Gave `batch-worker` ""acrpull"" privileges for the resource group; - Added new config variables in config.mk that are specific to Azure; - Added commands to batch/Makefile to create a boot disk image; - Added an Azure-specific startup script that installs Docker and the CLI and then authenticates and pulls the base image. The disk image we create is specialized. This means it has credentials in there after publishing it. I think this is okay and I specifically used the batch-worker managed identity to login for this. I can try and double check this assumption if you think I'm not correct after reading these docs: https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_create. > Accept system or user assigned identities separated by spaces. Use '[system]' to refer system assigned identity, or a resource id to refer user assigned identity. Check out help for more examples. I had to give the batch-worker managed identity in the resource group we want permissions to be an identity for a VM in the build-batch-worker-image resource group.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10834
https://github.com/hail-is/hail/pull/10834:187,Modifiability,variab,variables,187,"- Added a shared image gallery to terraform; - Added a managed identity `batch-worker` to terraform; - Gave `batch-worker` ""acrpull"" privileges for the resource group; - Added new config variables in config.mk that are specific to Azure; - Added commands to batch/Makefile to create a boot disk image; - Added an Azure-specific startup script that installs Docker and the CLI and then authenticates and pulls the base image. The disk image we create is specialized. This means it has credentials in there after publishing it. I think this is okay and I specifically used the batch-worker managed identity to login for this. I can try and double check this assumption if you think I'm not correct after reading these docs: https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_create. > Accept system or user assigned identities separated by spaces. Use '[system]' to refer system assigned identity, or a resource id to refer user assigned identity. Check out help for more examples. I had to give the batch-worker managed identity in the resource group we want permissions to be an identity for a VM in the build-batch-worker-image resource group.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10834
https://github.com/hail-is/hail/pull/10834:200,Modifiability,config,config,200,"- Added a shared image gallery to terraform; - Added a managed identity `batch-worker` to terraform; - Gave `batch-worker` ""acrpull"" privileges for the resource group; - Added new config variables in config.mk that are specific to Azure; - Added commands to batch/Makefile to create a boot disk image; - Added an Azure-specific startup script that installs Docker and the CLI and then authenticates and pulls the base image. The disk image we create is specialized. This means it has credentials in there after publishing it. I think this is okay and I specifically used the batch-worker managed identity to login for this. I can try and double check this assumption if you think I'm not correct after reading these docs: https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_create. > Accept system or user assigned identities separated by spaces. Use '[system]' to refer system assigned identity, or a resource id to refer user assigned identity. Check out help for more examples. I had to give the batch-worker managed identity in the resource group we want permissions to be an identity for a VM in the build-batch-worker-image resource group.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10834
https://github.com/hail-is/hail/pull/10834:385,Security,authenticat,authenticates,385,"- Added a shared image gallery to terraform; - Added a managed identity `batch-worker` to terraform; - Gave `batch-worker` ""acrpull"" privileges for the resource group; - Added new config variables in config.mk that are specific to Azure; - Added commands to batch/Makefile to create a boot disk image; - Added an Azure-specific startup script that installs Docker and the CLI and then authenticates and pulls the base image. The disk image we create is specialized. This means it has credentials in there after publishing it. I think this is okay and I specifically used the batch-worker managed identity to login for this. I can try and double check this assumption if you think I'm not correct after reading these docs: https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_create. > Accept system or user assigned identities separated by spaces. Use '[system]' to refer system assigned identity, or a resource id to refer user assigned identity. Check out help for more examples. I had to give the batch-worker managed identity in the resource group we want permissions to be an identity for a VM in the build-batch-worker-image resource group.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10834
https://github.com/hail-is/hail/pull/10834:608,Testability,log,login,608,"- Added a shared image gallery to terraform; - Added a managed identity `batch-worker` to terraform; - Gave `batch-worker` ""acrpull"" privileges for the resource group; - Added new config variables in config.mk that are specific to Azure; - Added commands to batch/Makefile to create a boot disk image; - Added an Azure-specific startup script that installs Docker and the CLI and then authenticates and pulls the base image. The disk image we create is specialized. This means it has credentials in there after publishing it. I think this is okay and I specifically used the batch-worker managed identity to login for this. I can try and double check this assumption if you think I'm not correct after reading these docs: https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_create. > Accept system or user assigned identities separated by spaces. Use '[system]' to refer system assigned identity, or a resource id to refer user assigned identity. Check out help for more examples. I had to give the batch-worker managed identity in the resource group we want permissions to be an identity for a VM in the build-batch-worker-image resource group.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10834
https://github.com/hail-is/hail/pull/10836:368,Performance,throughput,throughput,368,"Google Cloud Storage documentation and [best practices] for object names; recommends avoiding sequential names like 'part-0nnnn'. We already use; UUIDs for randomness to avoid two tasks writing to the exact same; object, but by using the UUID as a prefix rather than a suffix we; (to a degree) uniformly distribute part file names over a range,; (hopefully) improving throughput. [best practices]: https://cloud.google.com/storage/docs/best-practices#naming",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10836
https://github.com/hail-is/hail/pull/10836:85,Safety,avoid,avoiding,85,"Google Cloud Storage documentation and [best practices] for object names; recommends avoiding sequential names like 'part-0nnnn'. We already use; UUIDs for randomness to avoid two tasks writing to the exact same; object, but by using the UUID as a prefix rather than a suffix we; (to a degree) uniformly distribute part file names over a range,; (hopefully) improving throughput. [best practices]: https://cloud.google.com/storage/docs/best-practices#naming",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10836
https://github.com/hail-is/hail/pull/10836:170,Safety,avoid,avoid,170,"Google Cloud Storage documentation and [best practices] for object names; recommends avoiding sequential names like 'part-0nnnn'. We already use; UUIDs for randomness to avoid two tasks writing to the exact same; object, but by using the UUID as a prefix rather than a suffix we; (to a degree) uniformly distribute part file names over a range,; (hopefully) improving throughput. [best practices]: https://cloud.google.com/storage/docs/best-practices#naming",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10836
https://github.com/hail-is/hail/pull/10838:318,Modifiability,config,config,318,This adds the following terraform capabilities:; - A reserved public IP for the k8s gateway; - A container registry and pull access for the k8s cluster; - A [private link](https://azure.microsoft.com/en-us/services/private-link/) for the mysql database that makes it accessible on the k8s subnet. I haven't set up the config to use the database yet but ensured that the hostname for the database was resolvable from a pod on the cluster. I think this covers most of the azure specific resources that we need. Most of the rest of our terraform for gcp creating k8s secrets for the database config and various service accounts. I'd like to approach that in a single chunk to find out how best to abstract those into modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10838
https://github.com/hail-is/hail/pull/10838:589,Modifiability,config,config,589,This adds the following terraform capabilities:; - A reserved public IP for the k8s gateway; - A container registry and pull access for the k8s cluster; - A [private link](https://azure.microsoft.com/en-us/services/private-link/) for the mysql database that makes it accessible on the k8s subnet. I haven't set up the config to use the database yet but ensured that the hostname for the database was resolvable from a pod on the cluster. I think this covers most of the azure specific resources that we need. Most of the rest of our terraform for gcp creating k8s secrets for the database config and various service accounts. I'd like to approach that in a single chunk to find out how best to abstract those into modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10838
https://github.com/hail-is/hail/pull/10838:125,Security,access,access,125,This adds the following terraform capabilities:; - A reserved public IP for the k8s gateway; - A container registry and pull access for the k8s cluster; - A [private link](https://azure.microsoft.com/en-us/services/private-link/) for the mysql database that makes it accessible on the k8s subnet. I haven't set up the config to use the database yet but ensured that the hostname for the database was resolvable from a pod on the cluster. I think this covers most of the azure specific resources that we need. Most of the rest of our terraform for gcp creating k8s secrets for the database config and various service accounts. I'd like to approach that in a single chunk to find out how best to abstract those into modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10838
https://github.com/hail-is/hail/pull/10838:267,Security,access,accessible,267,This adds the following terraform capabilities:; - A reserved public IP for the k8s gateway; - A container registry and pull access for the k8s cluster; - A [private link](https://azure.microsoft.com/en-us/services/private-link/) for the mysql database that makes it accessible on the k8s subnet. I haven't set up the config to use the database yet but ensured that the hostname for the database was resolvable from a pod on the cluster. I think this covers most of the azure specific resources that we need. Most of the rest of our terraform for gcp creating k8s secrets for the database config and various service accounts. I'd like to approach that in a single chunk to find out how best to abstract those into modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10838
https://github.com/hail-is/hail/pull/10839:13,Modifiability,config,config,13,I tested the config comes through correctly in my namespace.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10839
https://github.com/hail-is/hail/pull/10839:2,Testability,test,tested,2,I tested the config comes through correctly in my namespace.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10839
https://github.com/hail-is/hail/pull/10841:157,Deployability,install,install-cannot-resolve-the-version-from-artifactory,157,"Also, removed deprecated 2020-resolver arguments. This was necessary to resolve `avro` dependencies as per: https://stackoverflow.com/questions/65233583/pip-install-cannot-resolve-the-version-from-artifactory",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10841
https://github.com/hail-is/hail/pull/10841:87,Integrability,depend,dependencies,87,"Also, removed deprecated 2020-resolver arguments. This was necessary to resolve `avro` dependencies as per: https://stackoverflow.com/questions/65233583/pip-install-cannot-resolve-the-version-from-artifactory",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10841
https://github.com/hail-is/hail/pull/10842:476,Deployability,deploy,deploying,476,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10842
https://github.com/hail-is/hail/pull/10842:22,Modifiability,Refactor,Refactoring,22,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10842
https://github.com/hail-is/hail/pull/10842:108,Safety,redund,redundancy,108,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10842
https://github.com/hail-is/hail/pull/10842:162,Testability,log,logs,162,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10842
https://github.com/hail-is/hail/pull/10842:178,Testability,test,test,178,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10842
https://github.com/hail-is/hail/pull/10842:55,Usability,simpl,simple,55,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10842
https://github.com/hail-is/hail/pull/10842:527,Usability,simpl,simple,527,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10842
https://github.com/hail-is/hail/issues/10844:467,Deployability,install,install,467,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Where's ""hail-python.zip"" in Hail 0.2 (latest version). we used to be able to use ```aws s3 cp distributions/hail-python.zip s3://${RESOURCES_BUCKET}/artifacts/``` on aws to install Hail but this failed when we switched from 0.1 to 0.2. Has anyone encountered similar issues before?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10844
https://github.com/hail-is/hail/pull/10845:203,Deployability,deploy,deploy,203,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:251,Deployability,configurat,configuration,251,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:632,Deployability,deploy,deploy,632,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:251,Modifiability,config,configuration,251,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:288,Testability,log,logic,288,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:433,Testability,log,logic,433,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:17,Usability,simpl,simple,17,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:399,Usability,simpl,simple,399,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10845:669,Usability,simpl,simplify,669,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10845
https://github.com/hail-is/hail/pull/10847:75,Availability,Error,Error,75,Resolves #10843 . Now you'll see:. ```; Hail version: 0.2.74-467a12fcbef9; Error summary: HailException: No file or directory found at gs://hail-datasets-us/annotations/THIS_PATH_DOES_NOT_EXIST; ```. instead of the error about it not being a directory.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10847
https://github.com/hail-is/hail/pull/10847:215,Availability,error,error,215,Resolves #10843 . Now you'll see:. ```; Hail version: 0.2.74-467a12fcbef9; Error summary: HailException: No file or directory found at gs://hail-datasets-us/annotations/THIS_PATH_DOES_NOT_EXIST; ```. instead of the error about it not being a directory.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10847
https://github.com/hail-is/hail/pull/10848:106,Modifiability,refactor,refactoring,106,"I didn't give very good names to all the introduced `memoize`s, but those will all go away as we push the refactoring through.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10848
https://github.com/hail-is/hail/pull/10849:41,Deployability,update,update,41,@jigold @daniel-goldstein Can one of you update batch changelog? It's been a while since we released.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10849
https://github.com/hail-is/hail/pull/10849:92,Deployability,release,released,92,@jigold @daniel-goldstein Can one of you update batch changelog? It's been a while since we released.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10849
https://github.com/hail-is/hail/pull/10850:5,Testability,test,test,5,"This test is failing because `jq` reading back to its own stream is resulting in wiping the file. This seemed the easiest workaround. I also fixed the semantics of the test. I'm not sure why I previously prepended `batch` to the domain, what I'm really testing here is that something meant to be `internal.hail.is` can't pretend to be pointing to `hail.is`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10850
https://github.com/hail-is/hail/pull/10850:168,Testability,test,test,168,"This test is failing because `jq` reading back to its own stream is resulting in wiping the file. This seemed the easiest workaround. I also fixed the semantics of the test. I'm not sure why I previously prepended `batch` to the domain, what I'm really testing here is that something meant to be `internal.hail.is` can't pretend to be pointing to `hail.is`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10850
https://github.com/hail-is/hail/pull/10850:253,Testability,test,testing,253,"This test is failing because `jq` reading back to its own stream is resulting in wiping the file. This seemed the easiest workaround. I also fixed the semantics of the test. I'm not sure why I previously prepended `batch` to the domain, what I'm really testing here is that something meant to be `internal.hail.is` can't pretend to be pointing to `hail.is`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10850
https://github.com/hail-is/hail/pull/10854:146,Deployability,deploy,deploy-config,146,"Another try at #10796. It's all the same excpet I've fixed the `test_cant_submit_to_default_with_other_ns_creds` test, which had been wiping the `deploy-config.json` after trying to read AND write to it. Now the test tries to change the `default_namespace` to `""default""`. This should succeed in the default namespace but is expected to fail in other namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10854
https://github.com/hail-is/hail/pull/10854:153,Modifiability,config,config,153,"Another try at #10796. It's all the same excpet I've fixed the `test_cant_submit_to_default_with_other_ns_creds` test, which had been wiping the `deploy-config.json` after trying to read AND write to it. Now the test tries to change the `default_namespace` to `""default""`. This should succeed in the default namespace but is expected to fail in other namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10854
https://github.com/hail-is/hail/pull/10854:113,Testability,test,test,113,"Another try at #10796. It's all the same excpet I've fixed the `test_cant_submit_to_default_with_other_ns_creds` test, which had been wiping the `deploy-config.json` after trying to read AND write to it. Now the test tries to change the `default_namespace` to `""default""`. This should succeed in the default namespace but is expected to fail in other namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10854
https://github.com/hail-is/hail/pull/10854:212,Testability,test,test,212,"Another try at #10796. It's all the same excpet I've fixed the `test_cant_submit_to_default_with_other_ns_creds` test, which had been wiping the `deploy-config.json` after trying to read AND write to it. Now the test tries to change the `default_namespace` to `""default""`. This should succeed in the default namespace but is expected to fail in other namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10854
https://github.com/hail-is/hail/pull/10855:12,Performance,race condition,race condition,12,There was a race condition where `crun run` could have been cancelled before the container was actually created. This change fixes this problem by checking whether the container exists before trying to delete it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10855
https://github.com/hail-is/hail/pull/10857:16,Modifiability,refactor,refactoring,16,to make further refactoring easier.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10857
https://github.com/hail-is/hail/issues/10858:53,Availability,error,error,53,"HailException: error while checking subtype:; super: struct{trait_type: str, phenocode: str, pheno_sex: str, coding: str, modifier: str, pheno_data: array<struct{}>, description: str, description_more: str, coding_description: str, category: str, n_cases_full_cohort_both_sexes: int64, n_cases_full_cohort_females: int64, n_cases_full_cohort_males: int64, col_array: tuple(1:struct{n_cases: int32, n_controls: int32, heritability: struct{estimates: struct{ldsc: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; sub: struct{trait_type: str, phenocode: str, pheno_sex: str, coding: str, modifier: str, pheno_data: array<struct{}>, description: str, description_more: str, coding_description: str, category: str, n_cases_full_cohort_both_s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10858
https://github.com/hail-is/hail/issues/10858:3483,Availability,Error,ErrorHandling,3483,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10858
https://github.com/hail-is/hail/issues/10858:3503,Availability,Error,ErrorHandling,3503,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10858
https://github.com/hail-is/hail/issues/10858:3545,Availability,Error,ErrorHandling,3545,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10858
https://github.com/hail-is/hail/issues/10858:3566,Availability,Error,ErrorHandling,3566,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10858
https://github.com/hail-is/hail/issues/10858:4164,Energy Efficiency,adapt,adapted,4164,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10858
https://github.com/hail-is/hail/issues/10858:4164,Modifiability,adapt,adapted,4164,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10858
https://github.com/hail-is/hail/pull/10860:732,Availability,error,errors,732,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:261,Energy Efficiency,monitor,monitors,261,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:799,Energy Efficiency,monitor,monitor,799,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:77,Integrability,interface,interface,77,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:241,Integrability,wrap,wraps,241,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:291,Integrability,interface,interface,291,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:694,Testability,test,tested,694,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:746,Testability,Log,Logs,746,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10860:779,Testability,test,testing,779,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10860
https://github.com/hail-is/hail/pull/10862:60,Energy Efficiency,monitor,monitoring,60,"See [the metrics explorer](https://console.cloud.google.com/monitoring/metrics-explorer?project=hail-vdc&pageState=%7B%22xyChart%22:%7B%22dataSets%22:%5B%7B%22timeSeriesFilter%22:%7B%22filter%22:%22metric.type%3D%5C%22kubernetes.io%2Fcontainer%2Fcpu%2Frequest_utilization%5C%22%20resource.type%3D%5C%22k8s_container%5C%22%20metadata.user_labels.%5C%22app%5C%22%3D%5C%22memory%5C%22%22,%22minAlignmentPeriod%22:%2260s%22,%22aggregations%22:%5B%7B%22perSeriesAligner%22:%22ALIGN_MEAN%22,%22crossSeriesReducer%22:%22REDUCE_SUM%22,%22groupByFields%22:%5B%22resource.label.%5C%22container_name%5C%22%22%5D%7D,%7B%22crossSeriesReducer%22:%22REDUCE_NONE%22,%22groupByFields%22:%5B%5D%7D%5D%7D,%22targetAxis%22:%22Y1%22,%22plotType%22:%22LINE%22%7D%5D,%22options%22:%7B%22mode%22:%22COLOR%22%7D,%22constantLines%22:%5B%5D,%22timeshiftDuration%22:%220s%22,%22y1Axis%22:%7B%22label%22:%22y1Axis%22,%22scale%22:%22LINEAR%22%7D%7D,%22isAutoRefresh%22:true,%22timeSelection%22:%7B%22timeRange%22:%226w%22%7D%7D). There is very little CPU use by redis and only bursty use of CPU use by python when I run query experiments. <img width=""1391"" alt=""Screen Shot 2021-09-10 at 7 34 24 PM"" src=""https://user-images.githubusercontent.com/106194/132927815-2cdbff0b-4697-42f3-b836-44e20d63e2dd.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10862
https://github.com/hail-is/hail/pull/10864:35,Performance,cache,cache,35,"Memory's local storage is a simple cache, so it is safe for k8s to; evict it from the node if the node is underutilized. https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10864
https://github.com/hail-is/hail/pull/10864:51,Safety,safe,safe,51,"Memory's local storage is a simple cache, so it is safe for k8s to; evict it from the node if the node is underutilized. https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10864
https://github.com/hail-is/hail/pull/10864:28,Usability,simpl,simple,28,"Memory's local storage is a simple cache, so it is safe for k8s to; evict it from the node if the node is underutilized. https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10864
https://github.com/hail-is/hail/pull/10865:76,Energy Efficiency,monitor,monitoring,76,"It never uses more than 3% of its request. https://console.cloud.google.com/monitoring/metrics-explorer?project=hail-vdc&pageState=%7B%22xyChart%22:%7B%22dataSets%22:%5B%7B%22timeSeriesFilter%22:%7B%22filter%22:%22metric.type%3D%5C%22kubernetes.io%2Fcontainer%2Fcpu%2Frequest_utilization%5C%22%20resource.type%3D%5C%22k8s_container%5C%22%20metadata.user_labels.%5C%22app%5C%22%3D%5C%22internal-gateway%5C%22%22,%22minAlignmentPeriod%22:%2260s%22,%22aggregations%22:%5B%7B%22perSeriesAligner%22:%22ALIGN_MEAN%22,%22crossSeriesReducer%22:%22REDUCE_SUM%22,%22groupByFields%22:%5B%22metadata.user_labels.%5C%22app%5C%22%22%5D%7D,%7B%22crossSeriesReducer%22:%22REDUCE_NONE%22,%22groupByFields%22:%5B%5D%7D%5D%7D,%22targetAxis%22:%22Y1%22,%22plotType%22:%22LINE%22%7D%5D,%22options%22:%7B%22mode%22:%22COLOR%22%7D,%22constantLines%22:%5B%5D,%22timeshiftDuration%22:%220s%22,%22y1Axis%22:%7B%22label%22:%22y1Axis%22,%22scale%22:%22LINEAR%22%7D%7D,%22isAutoRefresh%22:true,%22timeSelection%22:%7B%22timeRange%22:%22custom%22,%22start%22:%222021-07-02T15:10:00.000Z%22,%22end%22:%222021-09-13T15:10:41.820Z%22%7D%7D",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10865
https://github.com/hail-is/hail/pull/10866:60,Availability,down,down,60,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10866:751,Availability,error,errors,751,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10866:476,Deployability,deploy,deploy,476,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10866:629,Modifiability,config,config,629,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10866:907,Modifiability,config,config,907,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10866:180,Testability,test,tests,180,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10866:191,Testability,test,test,191,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10866:771,Testability,test,tests,771,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10866
https://github.com/hail-is/hail/pull/10868:158,Availability,error,error,158,"Currently there are multiple compatible versions of the `CADD` (1.4 and 1.6) and `gnomad_genome_sites` (2.1.1 and 3.1) annotation datasets, which leads to an error in `index_compatible_version`. This PR addresses that by annotating with the highest version number if there are multiple compatible versions of the same dataset. Will likely add fix in future to allow user to specify the version they would like to use, this was just a fairly quick and easy fix in the meantime.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10868
https://github.com/hail-is/hail/pull/10869:258,Deployability,deploy,deployment,258,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:325,Deployability,deploy,deployment,325,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:613,Deployability,deploy,deployments,613,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:946,Deployability,configurat,configuration,946,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:1016,Deployability,deploy,deployment,1016,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:629,Energy Efficiency,reduce,reduces,629,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:762,Energy Efficiency,reduce,reduces,762,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:774,Integrability,depend,dependencies,774,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:50,Modifiability,variab,variables,50,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:129,Modifiability,config,config,129,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:181,Modifiability,config,config,181,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:226,Modifiability,config,config,226,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:378,Modifiability,variab,variables,378,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:531,Modifiability,variab,variables,531,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:693,Modifiability,variab,variables,693,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:717,Modifiability,config,config,717,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:791,Modifiability,config,config,791,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:818,Modifiability,variab,variables,818,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:850,Modifiability,config,config,850,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10869:946,Modifiability,config,configuration,946,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10869
https://github.com/hail-is/hail/pull/10870:39,Modifiability,config,config,39,I manually added a field to the global-config for the requester pays bucket used in batch tests. Adding it here to build.py's view of global fields so that CI can template #10866 in the future and actually test it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10870
https://github.com/hail-is/hail/pull/10870:90,Testability,test,tests,90,I manually added a field to the global-config for the requester pays bucket used in batch tests. Adding it here to build.py's view of global fields so that CI can template #10866 in the future and actually test it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10870
https://github.com/hail-is/hail/pull/10870:206,Testability,test,test,206,I manually added a field to the global-config for the requester pays bucket used in batch tests. Adding it here to build.py's view of global fields so that CI can template #10866 in the future and actually test it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10870
https://github.com/hail-is/hail/pull/10873:68,Testability,test,tests,68,"Adds the `BlockMatrix` implementation of `pc_relate` and associated tests. Currently, this will only run on the local backend.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10873
https://github.com/hail-is/hail/pull/10874:86,Testability,test,tests,86,Adds the tall skinny (table of NDArrays) implementation of `pc_relate` and associated tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10874
https://github.com/hail-is/hail/pull/10877:55,Integrability,interface,interfaces,55,Stacked on #10876. Refactors all the function registry interfaces to take/return (S)Values.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10877
https://github.com/hail-is/hail/pull/10877:19,Modifiability,Refactor,Refactors,19,Stacked on #10876. Refactors all the function registry interfaces to take/return (S)Values.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10877
https://github.com/hail-is/hail/issues/10882:185,Integrability,interface,interface,185,"**Hail version: 0.2.74**; **Spark version: 3.1.1 (Hadoop 3.2)**. --. Hello, and thanks for putting your time and effort into the development of Hail. I'm trying to create a low-level R interface providing methods for interacting with Hail data structures. So far I've been able to import an example VCF file as a `TableValue`/`RDD` object, but - due to a type mismatch caused by its `locus` field - I'm not able to convert it any further (to a Spark data frame, for example). Reproducible example (but please replace paths):. ```{scala}; val backend = is.hail.backend.spark.SparkBackend(sc, ""Hail"", ""local"", ""local[*]"", false, 1, ""/tmp"", ""/tmp""); val hc = is.hail.HailContext(backend, ""/path/to/hail.log"", true, false, 50, false, 3); val hadoop_conf = sc.hadoopConfiguration; val shconf = new is.hail.utils.SerializableHadoopConfiguration(hadoop_conf); val fs = new is.hail.io.fs.HadoopFS(shconf); val pool = is.hail.annotations.RegionPool(false); val reg = is.hail.annotations.Region(0, pool); val timer = new is.hail.utils.ExecutionTimer(""timer""); val ec = new is.hail.expr.ir.ExecuteContext(""/tmp"", ""/tmp"", backend, fs, reg, timer, null); val text_input = is.hail.utils.TextInputFilterAndReplace(); val reader_params = is.hail.io.vcf.MatrixVCFReaderParameters(Seq(""/path/to/tdt_tiny.vcf""), Set(), ""Float64"", Option(""/path/to/tdt_tiny.vcf""), Some(1), Some(1), Some(1), Option(""GRCh37""), Map(), true, false, false, false, text_input, """"); val reader = is.hail.io.vcf.MatrixVCFReader.apply(ec, reader_params); val ttyp = reader.fullType; val tread = is.hail.expr.ir.TableRead(ttyp, false, reader); val tval = reader.apply(tread, ec); val rdd = tval.rdd; ```. Now, it should be possible to run `tval.toDF()`, but it throws:. > scala.MatchError: locus<GRCh37> (of class is.hail.types.virtual.TLocus); > at is.hail.expr.SparkAnnotationImpex$.exportType(AnnotationImpex.scala:42); > at is.hail.types.virtual.Type.schema(Type.scala:168). That's because `SparkAnnotationImpex.exportType()` doesn't support `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10882
https://github.com/hail-is/hail/issues/10882:700,Testability,log,log,700,"**Hail version: 0.2.74**; **Spark version: 3.1.1 (Hadoop 3.2)**. --. Hello, and thanks for putting your time and effort into the development of Hail. I'm trying to create a low-level R interface providing methods for interacting with Hail data structures. So far I've been able to import an example VCF file as a `TableValue`/`RDD` object, but - due to a type mismatch caused by its `locus` field - I'm not able to convert it any further (to a Spark data frame, for example). Reproducible example (but please replace paths):. ```{scala}; val backend = is.hail.backend.spark.SparkBackend(sc, ""Hail"", ""local"", ""local[*]"", false, 1, ""/tmp"", ""/tmp""); val hc = is.hail.HailContext(backend, ""/path/to/hail.log"", true, false, 50, false, 3); val hadoop_conf = sc.hadoopConfiguration; val shconf = new is.hail.utils.SerializableHadoopConfiguration(hadoop_conf); val fs = new is.hail.io.fs.HadoopFS(shconf); val pool = is.hail.annotations.RegionPool(false); val reg = is.hail.annotations.Region(0, pool); val timer = new is.hail.utils.ExecutionTimer(""timer""); val ec = new is.hail.expr.ir.ExecuteContext(""/tmp"", ""/tmp"", backend, fs, reg, timer, null); val text_input = is.hail.utils.TextInputFilterAndReplace(); val reader_params = is.hail.io.vcf.MatrixVCFReaderParameters(Seq(""/path/to/tdt_tiny.vcf""), Set(), ""Float64"", Option(""/path/to/tdt_tiny.vcf""), Some(1), Some(1), Some(1), Option(""GRCh37""), Map(), true, false, false, false, text_input, """"); val reader = is.hail.io.vcf.MatrixVCFReader.apply(ec, reader_params); val ttyp = reader.fullType; val tread = is.hail.expr.ir.TableRead(ttyp, false, reader); val tval = reader.apply(tread, ec); val rdd = tval.rdd; ```. Now, it should be possible to run `tval.toDF()`, but it throws:. > scala.MatchError: locus<GRCh37> (of class is.hail.types.virtual.TLocus); > at is.hail.expr.SparkAnnotationImpex$.exportType(AnnotationImpex.scala:42); > at is.hail.types.virtual.Type.schema(Type.scala:168). That's because `SparkAnnotationImpex.exportType()` doesn't support `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10882
https://github.com/hail-is/hail/pull/10884:44,Availability,error,errors,44,Reverts hail-is/hail#10693 while we inspect errors pulling large numbers of images at once.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10884
https://github.com/hail-is/hail/pull/10887:2,Testability,test,test,2,‚Ä¶ test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10887
https://github.com/hail-is/hail/pull/10891:165,Deployability,deploy,deployment,165,Removed the line to template `service-account-batch-pods.yaml` as it no longer exists and added the missing `default_ns.name` field to the jinja environment for the deployment template.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10891
https://github.com/hail-is/hail/pull/10892:122,Deployability,pipeline,pipeline,122,"Create a new class `VariantDatasetCombiner` that implements a simple state; machine for running a VariantDataset combiner pipeline from start to; finish. The `VariantDatasetCombiner` structure contains pretty much everything the; old `experimental.vcf_combiner.run_combiner` method took as arguments:; `temp_path`, `output_path`, list of gvcf paths, etc.. What is new is the; addition of a `save_path` and `vds_paths`. The `save_path` is used as; a filesystem or object storage location to save the state of the; combiner so that it may be resumed. The vds paths implement the long; awaited mixed hail and gvcf inputs, that was never completed for the VCF; combiner. End users should not call `VariantDatasetCombiner.__init__`; instead preferring the free function new_combiner. The; `VariantDatasetCombiner` API is fairly simple:. * @property finished; * save: serialize the current state to save_path; * step: does the next unit of work for the combiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10892
https://github.com/hail-is/hail/pull/10892:1383,Performance,load,load,1383,"ddition of a `save_path` and `vds_paths`. The `save_path` is used as; a filesystem or object storage location to save the state of the; combiner so that it may be resumed. The vds paths implement the long; awaited mixed hail and gvcf inputs, that was never completed for the VCF; combiner. End users should not call `VariantDatasetCombiner.__init__`; instead preferring the free function new_combiner. The; `VariantDatasetCombiner` API is fairly simple:. * @property finished; * save: serialize the current state to save_path; * step: does the next unit of work for the combiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added to the end of the list and then sorted. Since python's sort; function is stable, this will produce a stable output for a given set of; inputs and batch size. In order to preserve the gvcf sample ordering; from the paths, we work from the start of the gvcf list. Once all the gvcfs are combined, we combine the variant datasets in; reverse order (smallest number of samples to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10892
https://github.com/hail-is/hail/pull/10892:2702,Testability,log,log,2702,"ombiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added to the end of the list and then sorted. Since python's sort; function is stable, this will produce a stable output for a given set of; inputs and batch size. In order to preserve the gvcf sample ordering; from the paths, we work from the start of the gvcf list. Once all the gvcfs are combined, we combine the variant datasets in; reverse order (smallest number of samples to largest number of samples); inserting the new vds into the same sorted order until there are no; remaining input vdses, writing it to `output_path`. ## Not Implemented Yet. Other stepping and planning strategies may be preferable in the future,; for example we could work the vds arguments into a list of lists based; on the log base `branch_factor` of the number of samples to create; a tiered list that is processed in order rather than reinserting new; datasets into a sorted list. This may create opportunites for large; scale parallelism on a future service backend.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10892
https://github.com/hail-is/hail/pull/10892:62,Usability,simpl,simple,62,"Create a new class `VariantDatasetCombiner` that implements a simple state; machine for running a VariantDataset combiner pipeline from start to; finish. The `VariantDatasetCombiner` structure contains pretty much everything the; old `experimental.vcf_combiner.run_combiner` method took as arguments:; `temp_path`, `output_path`, list of gvcf paths, etc.. What is new is the; addition of a `save_path` and `vds_paths`. The `save_path` is used as; a filesystem or object storage location to save the state of the; combiner so that it may be resumed. The vds paths implement the long; awaited mixed hail and gvcf inputs, that was never completed for the VCF; combiner. End users should not call `VariantDatasetCombiner.__init__`; instead preferring the free function new_combiner. The; `VariantDatasetCombiner` API is fairly simple:. * @property finished; * save: serialize the current state to save_path; * step: does the next unit of work for the combiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10892
https://github.com/hail-is/hail/pull/10892:540,Usability,resume,resumed,540,"Create a new class `VariantDatasetCombiner` that implements a simple state; machine for running a VariantDataset combiner pipeline from start to; finish. The `VariantDatasetCombiner` structure contains pretty much everything the; old `experimental.vcf_combiner.run_combiner` method took as arguments:; `temp_path`, `output_path`, list of gvcf paths, etc.. What is new is the; addition of a `save_path` and `vds_paths`. The `save_path` is used as; a filesystem or object storage location to save the state of the; combiner so that it may be resumed. The vds paths implement the long; awaited mixed hail and gvcf inputs, that was never completed for the VCF; combiner. End users should not call `VariantDatasetCombiner.__init__`; instead preferring the free function new_combiner. The; `VariantDatasetCombiner` API is fairly simple:. * @property finished; * save: serialize the current state to save_path; * step: does the next unit of work for the combiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10892
https://github.com/hail-is/hail/pull/10892:823,Usability,simpl,simple,823,"Create a new class `VariantDatasetCombiner` that implements a simple state; machine for running a VariantDataset combiner pipeline from start to; finish. The `VariantDatasetCombiner` structure contains pretty much everything the; old `experimental.vcf_combiner.run_combiner` method took as arguments:; `temp_path`, `output_path`, list of gvcf paths, etc.. What is new is the; addition of a `save_path` and `vds_paths`. The `save_path` is used as; a filesystem or object storage location to save the state of the; combiner so that it may be resumed. The vds paths implement the long; awaited mixed hail and gvcf inputs, that was never completed for the VCF; combiner. End users should not call `VariantDatasetCombiner.__init__`; instead preferring the free function new_combiner. The; `VariantDatasetCombiner` API is fairly simple:. * @property finished; * save: serialize the current state to save_path; * step: does the next unit of work for the combiner (described in more detail below); * run: until `finished`, calls `save` then `step` followed by one last `save`. The public api is present in 2 free functions:. * new_combiner: public VariantDatasetCombiner constructor; * load_combiner. if `new_combiner` is not given a `save_path`, it will compute one based on; the SHA-256 of it's inputs. If the file at `save_path` is present, and; a valid `VariantDatasetCombiner`, it will load it and use it, unless the; `force` argument to `new_combiner` is True. This way, every combiner has; a `save_path`. ## The `step` algorithm. If there are any gvcf paths still uncombined, they are combined; `branch_factor` at a time, using `batch_size` parallel matrix table; writes to achive good parallelism. If there is only one merged vds, and; no vds arguments remaining to be combined it is written to; `output_path`. Otherwise the new vds paths are appended to the vds list; and the vds list is sorted by number of samples. The vds paths are always kept sorted by samples and new items are only; ever added ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10892
https://github.com/hail-is/hail/pull/10893:40,Integrability,depend,dependent,40,Refactor SBaseStruct.isFieldMissing and dependent methods to `CodeBuilder => Value` style.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10893
https://github.com/hail-is/hail/pull/10893:0,Modifiability,Refactor,Refactor,0,Refactor SBaseStruct.isFieldMissing and dependent methods to `CodeBuilder => Value` style.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10893
https://github.com/hail-is/hail/pull/10894:13,Modifiability,Refactor,Refactors,13,"This PR:. 1. Refactors `ResultOp` to just take an index and return whatever agg result is at that index. No more returning a suffix of the total aggregator tuple based on a starting index. This is necessary for me to effectively implement my Fold aggregator, which will be included in a subsequent PR. ; 2. Pushes `EmitType` through aggregators, uses them as the basis for analyzing the requiredness of aggregator results.; 3. Changes `_storeResult` on aggregators to instead just be `_result`, which directly returns an `IEmitCode`. No reason that `ResultOp` had to be so wound up in `PType`s, and for the most part this made the code simpler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10894
https://github.com/hail-is/hail/pull/10894:636,Usability,simpl,simpler,636,"This PR:. 1. Refactors `ResultOp` to just take an index and return whatever agg result is at that index. No more returning a suffix of the total aggregator tuple based on a starting index. This is necessary for me to effectively implement my Fold aggregator, which will be included in a subsequent PR. ; 2. Pushes `EmitType` through aggregators, uses them as the basis for analyzing the requiredness of aggregator results.; 3. Changes `_storeResult` on aggregators to instead just be `_result`, which directly returns an `IEmitCode`. No reason that `ResultOp` had to be so wound up in `PType`s, and for the most part this made the code simpler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10894
https://github.com/hail-is/hail/pull/10899:48,Modifiability,variab,variables,48,This removes unused google-specific environment variables from the auth modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10899
https://github.com/hail-is/hail/pull/10901:23,Performance,load,loadElement,23,Fixed reuse of code in loadElement in SJavaArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10901
https://github.com/hail-is/hail/pull/10902:24,Performance,load,loadElement,24,fixed reuse code bug in loadElement in SJavaArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10902
https://github.com/hail-is/hail/issues/10903:239,Availability,error,error,239,"https://discuss.hail.is/. These are the following commands I ran in a ipython shell:-; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). I am using Hail 0.2.77. ; I am getting an error ‚ÄúPy4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.‚Äù while running the second line. . Thanks!; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10903
https://github.com/hail-is/hail/issues/10903:264,Availability,error,error,264,"https://discuss.hail.is/. These are the following commands I ran in a ipython shell:-; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). I am using Hail 0.2.77. ; I am getting an error ‚ÄúPy4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.‚Äù while running the second line. . Thanks!; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10903
https://github.com/hail-is/hail/pull/10906:70,Integrability,wrap,wrapper,70,"~Stacked on #10905~. This PR refactors; * BinarySearch; * BLAS/LAPACK wrapper methods; * CodeBuilder.assign (the SSettable overload); * SSettable.store - Most uses now call with an SValue, but the SCode overload is still used in SCode subclasses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10906
https://github.com/hail-is/hail/pull/10906:29,Modifiability,refactor,refactors,29,"~Stacked on #10905~. This PR refactors; * BinarySearch; * BLAS/LAPACK wrapper methods; * CodeBuilder.assign (the SSettable overload); * SSettable.store - Most uses now call with an SValue, but the SCode overload is still used in SCode subclasses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10906
https://github.com/hail-is/hail/pull/10907:286,Integrability,interface,interface,286,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10907
https://github.com/hail-is/hail/pull/10907:29,Modifiability,refactor,refactors,29,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10907
https://github.com/hail-is/hail/pull/10907:525,Modifiability,refactor,refactorings,525,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10907
https://github.com/hail-is/hail/pull/10907:916,Performance,load,load,916,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10907
https://github.com/hail-is/hail/pull/10907:204,Security,access,access,204,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10907
https://github.com/hail-is/hail/pull/10907:874,Security,access,access,874,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10907
https://github.com/hail-is/hail/pull/10907:440,Usability,simpl,simplification,440,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10907
https://github.com/hail-is/hail/pull/10911:962,Deployability,deploy,deployment,962,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:235,Integrability,depend,dependencies,235,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:22,Modifiability,config,config,22,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:136,Modifiability,variab,variables,136,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:162,Modifiability,variab,variables,162,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:373,Modifiability,variab,variables,373,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:453,Modifiability,config,config,453,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:555,Modifiability,variab,variables,555,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:717,Modifiability,config,config,717,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:804,Modifiability,config,config,804,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10911:543,Security,expose,expose,543,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10911
https://github.com/hail-is/hail/pull/10919:1024,Availability,toler,toleration,1024,"he following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the res",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1141,Availability,toler,toleration,1141," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1235,Availability,redundant,redundant,1235," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1245,Availability,toler,tolerations,1245," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1294,Availability,toler,tolerations,1294," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:935,Deployability,configurat,configuration,935,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1061,Deployability,deploy,deployments,1061,"he following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the res",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1800,Energy Efficiency,adapt,adapt,1800," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:703,Modifiability,config,config,703,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:718,Modifiability,config,config,718,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:840,Modifiability,refactor,refactor,840,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:935,Modifiability,config,configuration,935,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1339,Modifiability,Refactor,Refactor,1339," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1800,Modifiability,adapt,adapt,1800," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:1235,Safety,redund,redundant,1235," creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured out the rest of the terraform/bootstrap process should follow pretty quickly. Stacked on #10911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:322,Security,certificate,certificate,322,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10919:375,Security,certificate,certificates,375,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10919
https://github.com/hail-is/hail/pull/10920:534,Integrability,interface,interface,534,"This PR tries to address the comments in #10860. However, there are a bunch of additional changes as well. I did everything necessary to make the entire batch code base cloud-agnostic. Before I fully test the entire system and someone proofreads this PR carefully, I would like your input on the design and get high level comments. It's a lot of work to test this branch!!!! Once everyone is happy with this structure, then I can start writing the Azure components. List of changes:; - Created a new CloudResourceManager that has the interface for interacting with VMs and managing cloud resources; - Renamed WorkerConfig to InstanceConfig; - Created a new Disk interface in batch/worker/disk.py; - Renamed the instance's zone field to location as Azure's zones mean something different.; - Moved all resource utility calculations to its own module; - resource_utils.py contains a set of functions that calls the appropriate implementation based on the cloud parameter. I moved all GCP specific implementations into the batch/gcp module. There is one cyclic dependency I couldn't break right now which is the InstanceCollectionManager -> InstanceCollection -> Pool/JPIM all need a CloudResourceManager while the CloudResourceManager needs to know the list of active instances and the global number of cores from InstanceCollectionManager to appropriately clean up resources and select a zone. I think of the InstanceCollectionManager hierarchy as being the manager of virtual instances and the CloudResourceManager manages the physical VM instances. I had to put the constructor functions for the InstanceConfig in utils.py because of cyclic imports even though the import was inside the static method constructor function in my original attempt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10920
https://github.com/hail-is/hail/pull/10920:662,Integrability,interface,interface,662,"This PR tries to address the comments in #10860. However, there are a bunch of additional changes as well. I did everything necessary to make the entire batch code base cloud-agnostic. Before I fully test the entire system and someone proofreads this PR carefully, I would like your input on the design and get high level comments. It's a lot of work to test this branch!!!! Once everyone is happy with this structure, then I can start writing the Azure components. List of changes:; - Created a new CloudResourceManager that has the interface for interacting with VMs and managing cloud resources; - Renamed WorkerConfig to InstanceConfig; - Created a new Disk interface in batch/worker/disk.py; - Renamed the instance's zone field to location as Azure's zones mean something different.; - Moved all resource utility calculations to its own module; - resource_utils.py contains a set of functions that calls the appropriate implementation based on the cloud parameter. I moved all GCP specific implementations into the batch/gcp module. There is one cyclic dependency I couldn't break right now which is the InstanceCollectionManager -> InstanceCollection -> Pool/JPIM all need a CloudResourceManager while the CloudResourceManager needs to know the list of active instances and the global number of cores from InstanceCollectionManager to appropriately clean up resources and select a zone. I think of the InstanceCollectionManager hierarchy as being the manager of virtual instances and the CloudResourceManager manages the physical VM instances. I had to put the constructor functions for the InstanceConfig in utils.py because of cyclic imports even though the import was inside the static method constructor function in my original attempt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10920
https://github.com/hail-is/hail/pull/10920:1058,Integrability,depend,dependency,1058,"This PR tries to address the comments in #10860. However, there are a bunch of additional changes as well. I did everything necessary to make the entire batch code base cloud-agnostic. Before I fully test the entire system and someone proofreads this PR carefully, I would like your input on the design and get high level comments. It's a lot of work to test this branch!!!! Once everyone is happy with this structure, then I can start writing the Azure components. List of changes:; - Created a new CloudResourceManager that has the interface for interacting with VMs and managing cloud resources; - Renamed WorkerConfig to InstanceConfig; - Created a new Disk interface in batch/worker/disk.py; - Renamed the instance's zone field to location as Azure's zones mean something different.; - Moved all resource utility calculations to its own module; - resource_utils.py contains a set of functions that calls the appropriate implementation based on the cloud parameter. I moved all GCP specific implementations into the batch/gcp module. There is one cyclic dependency I couldn't break right now which is the InstanceCollectionManager -> InstanceCollection -> Pool/JPIM all need a CloudResourceManager while the CloudResourceManager needs to know the list of active instances and the global number of cores from InstanceCollectionManager to appropriately clean up resources and select a zone. I think of the InstanceCollectionManager hierarchy as being the manager of virtual instances and the CloudResourceManager manages the physical VM instances. I had to put the constructor functions for the InstanceConfig in utils.py because of cyclic imports even though the import was inside the static method constructor function in my original attempt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10920
https://github.com/hail-is/hail/pull/10920:200,Testability,test,test,200,"This PR tries to address the comments in #10860. However, there are a bunch of additional changes as well. I did everything necessary to make the entire batch code base cloud-agnostic. Before I fully test the entire system and someone proofreads this PR carefully, I would like your input on the design and get high level comments. It's a lot of work to test this branch!!!! Once everyone is happy with this structure, then I can start writing the Azure components. List of changes:; - Created a new CloudResourceManager that has the interface for interacting with VMs and managing cloud resources; - Renamed WorkerConfig to InstanceConfig; - Created a new Disk interface in batch/worker/disk.py; - Renamed the instance's zone field to location as Azure's zones mean something different.; - Moved all resource utility calculations to its own module; - resource_utils.py contains a set of functions that calls the appropriate implementation based on the cloud parameter. I moved all GCP specific implementations into the batch/gcp module. There is one cyclic dependency I couldn't break right now which is the InstanceCollectionManager -> InstanceCollection -> Pool/JPIM all need a CloudResourceManager while the CloudResourceManager needs to know the list of active instances and the global number of cores from InstanceCollectionManager to appropriately clean up resources and select a zone. I think of the InstanceCollectionManager hierarchy as being the manager of virtual instances and the CloudResourceManager manages the physical VM instances. I had to put the constructor functions for the InstanceConfig in utils.py because of cyclic imports even though the import was inside the static method constructor function in my original attempt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10920
https://github.com/hail-is/hail/pull/10920:354,Testability,test,test,354,"This PR tries to address the comments in #10860. However, there are a bunch of additional changes as well. I did everything necessary to make the entire batch code base cloud-agnostic. Before I fully test the entire system and someone proofreads this PR carefully, I would like your input on the design and get high level comments. It's a lot of work to test this branch!!!! Once everyone is happy with this structure, then I can start writing the Azure components. List of changes:; - Created a new CloudResourceManager that has the interface for interacting with VMs and managing cloud resources; - Renamed WorkerConfig to InstanceConfig; - Created a new Disk interface in batch/worker/disk.py; - Renamed the instance's zone field to location as Azure's zones mean something different.; - Moved all resource utility calculations to its own module; - resource_utils.py contains a set of functions that calls the appropriate implementation based on the cloud parameter. I moved all GCP specific implementations into the batch/gcp module. There is one cyclic dependency I couldn't break right now which is the InstanceCollectionManager -> InstanceCollection -> Pool/JPIM all need a CloudResourceManager while the CloudResourceManager needs to know the list of active instances and the global number of cores from InstanceCollectionManager to appropriately clean up resources and select a zone. I think of the InstanceCollectionManager hierarchy as being the manager of virtual instances and the CloudResourceManager manages the physical VM instances. I had to put the constructor functions for the InstanceConfig in utils.py because of cyclic imports even though the import was inside the static method constructor function in my original attempt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10920
https://github.com/hail-is/hail/pull/10923:194,Availability,error,errors,194,"I ran into issues in the development of the query service wherein, if the; aiohttp `ClientSession` is not created in the same event pool as the one; from which the request is made, then unusual errors occur. This change makes it harder to accidentally create a BatchClient in the; wrong event loop because the factory method, `BatchClient.create` is; itself an async function. It is still possible to create the BatchClient; in one event loop and use it in another (do not do this!!), but that seems; to be an unlikely mistake. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10923
https://github.com/hail-is/hail/pull/10924:156,Availability,error,error,156,"- Botocore (AWS) produces its own ConnectionClosedError (:sad:).; - TIMEDOUT is yet another transient socket issue; - I have seen EAI_NONAME as a transient error, even though I would hope to receive EAI_AGAIN.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10924
https://github.com/hail-is/hail/pull/10928:6,Testability,test,tests,6,These tests only work if fewer than 200 batches were created between the batches; under test and the `list_batches` API call. This is obviously not necessarily; true in general and *certainly* not true when the Hail Query on Hail Batch is; spewing hundreds of batches per second into the system.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10928
https://github.com/hail-is/hail/pull/10928:88,Testability,test,test,88,These tests only work if fewer than 200 batches were created between the batches; under test and the `list_batches` API call. This is obviously not necessarily; true in general and *certainly* not true when the Hail Query on Hail Batch is; spewing hundreds of batches per second into the system.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10928
https://github.com/hail-is/hail/pull/10929:133,Testability,log,logged,133,"In aiohttp, a task is cancelled when the requesting user drops the connection. This; is not an exception condition and should not be logged as such.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10929
https://github.com/hail-is/hail/pull/10930:133,Testability,log,logged,133,"In aiohttp, a task is cancelled when the requesting user drops the connection. This; is not an exception condition and should not be logged as such.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10930
https://github.com/hail-is/hail/pull/10931:46,Testability,log,log,46,A CancelledError should not be an exceptional log. A user dropped the connection.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10931
https://github.com/hail-is/hail/pull/10932:439,Deployability,update,updated,439,"While developing Hail Query on Hail Batch, I discovered that, for very fast requests,; I spent significant percentages of the time constructing aiohttp client sessions. This; change makes all the `XXX_authenticated_YYY_only` methods use a single, shared client; session from the `app`. This change requires every server to have a `client_session` key in the `app` which; points at a `httpx.ClientSession`. While making this change, I also updated every; instance I saw of adhoc client session creation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10932
https://github.com/hail-is/hail/pull/10933:27,Energy Efficiency,reduce,reduce,27,Motivated by work I did to reduce unnecessary client session allocation. I doubt; this has a significant impact since these client sessions are not created as; rapidly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10933
https://github.com/hail-is/hail/pull/10935:79,Availability,error,errors,79,"While working on Hail Query on Hail Batch, I frequently encountered transient; errors in rmtree when cleaning up temporary cloud directories. This change; ensures rmtree is resilient to such failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10935
https://github.com/hail-is/hail/pull/10935:173,Availability,resilien,resilient,173,"While working on Hail Query on Hail Batch, I frequently encountered transient; errors in rmtree when cleaning up temporary cloud directories. This change; ensures rmtree is resilient to such failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10935
https://github.com/hail-is/hail/pull/10935:191,Availability,failure,failures,191,"While working on Hail Query on Hail Batch, I frequently encountered transient; errors in rmtree when cleaning up temporary cloud directories. This change; ensures rmtree is resilient to such failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10935
https://github.com/hail-is/hail/pull/10936:114,Availability,error,error,114,I had to change the return type and override delete for Azure as they just return a status code unless there's an error. The `resp.json()` didn't work for the Azure response type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10936
https://github.com/hail-is/hail/pull/10938:116,Performance,throughput,throughput,116,"We are not permitted more than 10,000 parts. Moreover a tiny copy part size; seems to have negative consequences on throughput.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10938
https://github.com/hail-is/hail/pull/10943:101,Performance,scalab,scalability,101,"These two tests take a very long time in the service and do not benefit from the; massive horizontal scalability of Hail Query on Hail Batch. These tests also; somewhat frequently cause OOMs in the service backend, so the point is perhaps; moot. Nonetheless, I think this is an overall positive change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10943
https://github.com/hail-is/hail/pull/10943:10,Testability,test,tests,10,"These two tests take a very long time in the service and do not benefit from the; massive horizontal scalability of Hail Query on Hail Batch. These tests also; somewhat frequently cause OOMs in the service backend, so the point is perhaps; moot. Nonetheless, I think this is an overall positive change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10943
https://github.com/hail-is/hail/pull/10943:148,Testability,test,tests,148,"These two tests take a very long time in the service and do not benefit from the; massive horizontal scalability of Hail Query on Hail Batch. These tests also; somewhat frequently cause OOMs in the service backend, so the point is perhaps; moot. Nonetheless, I think this is an overall positive change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10943
https://github.com/hail-is/hail/pull/10944:78,Testability,test,tests,78,The ServiceBackend is not yet fast enough to serially execute huge numbers of tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10944
https://github.com/hail-is/hail/pull/10946:25,Availability,error,error,25,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10946
https://github.com/hail-is/hail/pull/10946:74,Performance,concurren,concurrent,74,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10946
https://github.com/hail-is/hail/pull/10946:115,Performance,concurren,concurrent,115,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10946
https://github.com/hail-is/hail/pull/10946:99,Safety,Timeout,TimeoutError,99,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10946
https://github.com/hail-is/hail/pull/10946:134,Safety,Timeout,TimeoutError,134,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10946
https://github.com/hail-is/hail/pull/10947:58,Testability,test,tests,58,I received a 503 in one of my other PRs from one of these tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10947
https://github.com/hail-is/hail/pull/10948:16,Testability,test,test,16,I just had this test fail in another PR. This change gives me the information; necessary to debug the issue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10948
https://github.com/hail-is/hail/pull/10949:57,Availability,down,down,57,"Currently there is a race condition in which we can shut down the filestore before; the aiohttp app stops accepting connections. If that happens, jobs will get; partially scheduled on the worker, but there will be no working file store so the; jobs cannot complete successfully. The partially scheduled jobs in turn leave the; worker in a bad, non-idle state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10949
https://github.com/hail-is/hail/pull/10949:171,Energy Efficiency,schedul,scheduled,171,"Currently there is a race condition in which we can shut down the filestore before; the aiohttp app stops accepting connections. If that happens, jobs will get; partially scheduled on the worker, but there will be no working file store so the; jobs cannot complete successfully. The partially scheduled jobs in turn leave the; worker in a bad, non-idle state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10949
https://github.com/hail-is/hail/pull/10949:293,Energy Efficiency,schedul,scheduled,293,"Currently there is a race condition in which we can shut down the filestore before; the aiohttp app stops accepting connections. If that happens, jobs will get; partially scheduled on the worker, but there will be no working file store so the; jobs cannot complete successfully. The partially scheduled jobs in turn leave the; worker in a bad, non-idle state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10949
https://github.com/hail-is/hail/pull/10949:21,Performance,race condition,race condition,21,"Currently there is a race condition in which we can shut down the filestore before; the aiohttp app stops accepting connections. If that happens, jobs will get; partially scheduled on the worker, but there will be no working file store so the; jobs cannot complete successfully. The partially scheduled jobs in turn leave the; worker in a bad, non-idle state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10949
https://github.com/hail-is/hail/pull/10950:171,Integrability,depend,dependency-mismatch,171,I hope this will alleviate some version incompatibilties between SciPy 1.6 and; NumPy >1.15. See [here](https://discuss.hail.is/t/unable-to-import-hail-due-to-numpy-scipy-dependency-mismatch/2285/3).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10950
https://github.com/hail-is/hail/pull/10953:181,Integrability,message,message,181,"I got sick of having things fail without sufficient debug information. This ensures; that the batch status and every job status and log is presented, untruncated, in; the assertion message from pylint.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10953
https://github.com/hail-is/hail/pull/10953:132,Testability,log,log,132,"I got sick of having things fail without sufficient debug information. This ensures; that the batch status and every job status and log is presented, untruncated, in; the assertion message from pylint.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10953
https://github.com/hail-is/hail/pull/10953:171,Testability,assert,assertion,171,"I got sick of having things fail without sufficient debug information. This ensures; that the batch status and every job status and log is presented, untruncated, in; the assertion message from pylint.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10953
https://github.com/hail-is/hail/pull/10957:107,Deployability,deploy,deployment,107,"this is the very first part of an effort to make build.yaml steps that should ultimately work in any cloud deployment cloud-agnostic from the perspective of build.yaml. In this case, it removes the gcp project environment variable from the build.yaml step and instead has bootstrap_create_accounts.py read it from a `GCPConfig`. This `GCPConfig` is constructed from fields of the global config, but the actual application code doesn't care about which secret it's coming from.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10957
https://github.com/hail-is/hail/pull/10957:222,Modifiability,variab,variable,222,"this is the very first part of an effort to make build.yaml steps that should ultimately work in any cloud deployment cloud-agnostic from the perspective of build.yaml. In this case, it removes the gcp project environment variable from the build.yaml step and instead has bootstrap_create_accounts.py read it from a `GCPConfig`. This `GCPConfig` is constructed from fields of the global config, but the actual application code doesn't care about which secret it's coming from.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10957
https://github.com/hail-is/hail/pull/10957:387,Modifiability,config,config,387,"this is the very first part of an effort to make build.yaml steps that should ultimately work in any cloud deployment cloud-agnostic from the perspective of build.yaml. In this case, it removes the gcp project environment variable from the build.yaml step and instead has bootstrap_create_accounts.py read it from a `GCPConfig`. This `GCPConfig` is constructed from fields of the global config, but the actual application code doesn't care about which secret it's coming from.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10957
https://github.com/hail-is/hail/pull/10958:123,Security,authenticat,authentication,123,We have had a ton of recent issues with Google thinking we are anonymous; users. I want more information in the logs about authentication.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10958
https://github.com/hail-is/hail/pull/10958:112,Testability,log,logs,112,We have had a ton of recent issues with Google thinking we are anonymous; users. I want more information in the logs about authentication.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10958
https://github.com/hail-is/hail/pull/10960:34,Modifiability,config,config,34,"I added a new field to the global config that is gs:// + hail_test_gcs_bucket named test_blob_storage_uri and use that wherever it doesn't matter that the backend be google storage, which is essentially everywhere except for the FS/copy tests, where we specifically want a test gcs bucket.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10960
https://github.com/hail-is/hail/pull/10960:237,Testability,test,tests,237,"I added a new field to the global config that is gs:// + hail_test_gcs_bucket named test_blob_storage_uri and use that wherever it doesn't matter that the backend be google storage, which is essentially everywhere except for the FS/copy tests, where we specifically want a test gcs bucket.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10960
https://github.com/hail-is/hail/pull/10960:273,Testability,test,test,273,"I added a new field to the global config that is gs:// + hail_test_gcs_bucket named test_blob_storage_uri and use that wherever it doesn't matter that the backend be google storage, which is essentially everywhere except for the FS/copy tests, where we specifically want a test gcs bucket.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10960
https://github.com/hail-is/hail/pull/10962:39,Deployability,configurat,configuration,39,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10962
https://github.com/hail-is/hail/pull/10962:39,Modifiability,config,configuration,39,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10962
https://github.com/hail-is/hail/pull/10962:69,Modifiability,config,config,69,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10962
https://github.com/hail-is/hail/pull/10962:99,Modifiability,variab,variables,99,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10962
https://github.com/hail-is/hail/pull/10962:162,Modifiability,config,config,162,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10962
https://github.com/hail-is/hail/pull/10962:18,Performance,load,loading,18,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10962
https://github.com/hail-is/hail/pull/10964:196,Safety,detect,detect,196,"Notably:. * Add options for `gvcf_info_to_keep` and `gvcf_reference_entry_fields_to_keep` to control those parameters to `transform_gvcf`.; * Limit gvcf merge tasks to 150,000 partitions.; * Auto-detect `gvcf_reference_entry_fields_to_keep` if it is not provided. If any VDS argument is present, use the reference_data entry type, otherwise, compute it from the first gvcf.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10964
https://github.com/hail-is/hail/pull/10968:504,Availability,error,error,504,"Currently, constructing a `hl.Struct` is slower than I'd like, since it requires copying every field from the input `kwargs` into the `Struct` object's `__dict__`. This PR's main goal was to avoid the need to do that, by just using the input `kwargs` dict we were already saving as `_fields`. . When working on this, I also noticed that we allowed users to mutate fields of a struct, and we didn't do so in a consistent way (`s.a` vs `s[""a""]` could return different answers). I avoid that by throwing an error from now on if a user tries to modify one of the ""main"" fields of the struct (i.e. the ones that are in the `_fields` array).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10968
https://github.com/hail-is/hail/pull/10968:191,Safety,avoid,avoid,191,"Currently, constructing a `hl.Struct` is slower than I'd like, since it requires copying every field from the input `kwargs` into the `Struct` object's `__dict__`. This PR's main goal was to avoid the need to do that, by just using the input `kwargs` dict we were already saving as `_fields`. . When working on this, I also noticed that we allowed users to mutate fields of a struct, and we didn't do so in a consistent way (`s.a` vs `s[""a""]` could return different answers). I avoid that by throwing an error from now on if a user tries to modify one of the ""main"" fields of the struct (i.e. the ones that are in the `_fields` array).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10968
https://github.com/hail-is/hail/pull/10968:478,Safety,avoid,avoid,478,"Currently, constructing a `hl.Struct` is slower than I'd like, since it requires copying every field from the input `kwargs` into the `Struct` object's `__dict__`. This PR's main goal was to avoid the need to do that, by just using the input `kwargs` dict we were already saving as `_fields`. . When working on this, I also noticed that we allowed users to mutate fields of a struct, and we didn't do so in a consistent way (`s.a` vs `s[""a""]` could return different answers). I avoid that by throwing an error from now on if a user tries to modify one of the ""main"" fields of the struct (i.e. the ones that are in the `_fields` array).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10968
https://github.com/hail-is/hail/pull/10970:2302,Availability,down,down,2302,"core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle the case where we have two k8s clusters running in our subscription.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:940,Deployability,Deploy,DeployConfig,940,"Stacked on #10920 and #10965. # Summary of Changes. ## hailtop.aiocloud.aioazure; - Renamed AzureResourcesClient to AzureResourceManagementClient; - Added AzureResourcesClient that hits a different API than before; - Added `AzureBaseClient.get_next_link` and `AzureBaseClient.delete_and_wait`. ## Batch; - Added `batch.azure` which mirrors the functionality of `batch.gcp`; - Renamed `worker_local_ssd_data_disk` to `local_ssd_data_disk` in the PoolConfig; - Renamed `worker_pd_ssd_data_disk_size_gb` to `external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:2793,Deployability,deploy,deploy,2793,"core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle the case where we have two k8s clusters running in our subscription.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:643,Modifiability,variab,variables,643,"Stacked on #10920 and #10965. # Summary of Changes. ## hailtop.aiocloud.aioazure; - Renamed AzureResourcesClient to AzureResourceManagementClient; - Added AzureResourcesClient that hits a different API than before; - Added `AzureBaseClient.get_next_link` and `AzureBaseClient.delete_and_wait`. ## Batch; - Added `batch.azure` which mirrors the functionality of `batch.gcp`; - Renamed `worker_local_ssd_data_disk` to `local_ssd_data_disk` in the PoolConfig; - Renamed `worker_pd_ssd_data_disk_size_gb` to `external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:1360,Modifiability,config,config,1360,"external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:1483,Modifiability,variab,variables,1483,"external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:2100,Performance,response time,response times,2100,": 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:2078,Safety,redund,redundancy,2078,": 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:1226,Security,secur,security,1226,"Added `AzureBaseClient.get_next_link` and `AzureBaseClient.delete_and_wait`. ## Batch; - Added `batch.azure` which mirrors the functionality of `batch.gcp`; - Renamed `worker_local_ssd_data_disk` to `local_ssd_data_disk` in the PoolConfig; - Renamed `worker_pd_ssd_data_disk_size_gb` to `external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:2593,Security,secur,security,2593,"core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle the case where we have two k8s clusters running in our subscription.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:1717,Testability,log,logs,1717,"Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10970:2637,Testability,test,tests,2637,"core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle the case where we have two k8s clusters running in our subscription.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10970
https://github.com/hail-is/hail/pull/10971:91,Testability,benchmark,benchmark,91,"Timings: https://gist.github.com/johnc1231/253aed65af131c2f67af2576c350d6a7. Basically the benchmark designed for this to fix got 3x better, and most other things didn't change. That's about what I expected.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10971
https://github.com/hail-is/hail/pull/10972:5,Testability,benchmark,benchmark,5,This benchmark tests speed of collecting data and converting to Python. I am expecting #10971 to speed this up considerably.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10972
https://github.com/hail-is/hail/pull/10972:15,Testability,test,tests,15,This benchmark tests speed of collecting data and converting to Python. I am expecting #10971 to speed this up considerably.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10972
https://github.com/hail-is/hail/issues/10978:475,Availability,error,error,475,"Hi Hail developers,; I am a new hail user and was struggling to process my multi-sample vcf file with hail. I first tried to read in my file with the code below and create annotate another column:. **import hail as hl; rt = hl.import_vcf('chr1_biallelic.vcf.gz',force_bgz=True,reference_genome=""GRCh38"",drop_samples=True).rows(); rt = rt.annotate(variant=rt.CHROM + ':' + rt.POSITION + "":"" + rt.REF + "":"" + rt.ALT)''**. However, running the third line gives me the following error:; **AttributeError: Table instance has no field, method, or property 'CHROM'**. Even though the CHROM header is present in my vcf file, it's not being read. Is it because of the metadata headers? Sorry if this is a basic question, I don't have any other resources to rely on and couldn't find any solutions online.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10978
https://github.com/hail-is/hail/pull/10982:346,Integrability,interface,interface,346,[`orjson`](https://github.com/ijl/orjson#serialize) is a fast JSON serialize/deserialize library. I found that it; improved the performance of the copy tool substantially. I suspect we will see a low-level improvement across all; services. We can't use aiohttp's normal json library overrides because aiohttp stubbornly refuses to support a JSON interface that doesn't (unnecessarily) return strings (which are then decoded to utf-8 bytes anyway). cc: @daniel-goldstein,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10982
https://github.com/hail-is/hail/pull/10982:128,Performance,perform,performance,128,[`orjson`](https://github.com/ijl/orjson#serialize) is a fast JSON serialize/deserialize library. I found that it; improved the performance of the copy tool substantially. I suspect we will see a low-level improvement across all; services. We can't use aiohttp's normal json library overrides because aiohttp stubbornly refuses to support a JSON interface that doesn't (unnecessarily) return strings (which are then decoded to utf-8 bytes anyway). cc: @daniel-goldstein,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10982
https://github.com/hail-is/hail/pull/10982:309,Testability,stub,stubbornly,309,[`orjson`](https://github.com/ijl/orjson#serialize) is a fast JSON serialize/deserialize library. I found that it; improved the performance of the copy tool substantially. I suspect we will see a low-level improvement across all; services. We can't use aiohttp's normal json library overrides because aiohttp stubbornly refuses to support a JSON interface that doesn't (unnecessarily) return strings (which are then decoded to utf-8 bytes anyway). cc: @daniel-goldstein,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10982
https://github.com/hail-is/hail/pull/10984:203,Integrability,interface,interface,203,"This replaces the `gcr-push-service-account-key` secret with a more general `container-registry-push-credentials` secret with username/password. Long term, it would be best to use the higher-level batch interface and just grant CI's credentials access to the container registry, but this is the smallest step I could think of to make the buildImage step able to work in azure (except for `cleanup`, as that relies on `gcloud`, but I'll have to address that later).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10984
https://github.com/hail-is/hail/pull/10984:135,Security,password,password,135,"This replaces the `gcr-push-service-account-key` secret with a more general `container-registry-push-credentials` secret with username/password. Long term, it would be best to use the higher-level batch interface and just grant CI's credentials access to the container registry, but this is the smallest step I could think of to make the buildImage step able to work in azure (except for `cleanup`, as that relies on `gcloud`, but I'll have to address that later).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10984
https://github.com/hail-is/hail/pull/10984:245,Security,access,access,245,"This replaces the `gcr-push-service-account-key` secret with a more general `container-registry-push-credentials` secret with username/password. Long term, it would be best to use the higher-level batch interface and just grant CI's credentials access to the container registry, but this is the smallest step I could think of to make the buildImage step able to work in azure (except for `cleanup`, as that relies on `gcloud`, but I'll have to address that later).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10984
https://github.com/hail-is/hail/pull/10985:867,Deployability,update,update,867,"Under ideal conditions, I observed this code complete 12 jobs per second in a 1000 job; batch. However, it's usually closer to 6 jobs per second. There's still a deadlock in; `mark_job_complete` which hamstrings performance. I haven't thought carefully about why my metric; (jobs per second) is much lower than our usual metric (MJC per second). I will eventually squash; the `mark_job_complete` deadlock, but I haven't the time currently. The two main issues were the `batches` field `cancelled` and the `instances` field; `free_cores_mcpu`. We frequently locked the entire batch row just to prevent a batch from being; cancelled while other work was done. This effectively serialized all operation on jobs in the same; batch. We had the dual issue with free_cores_mcpu: we lock the entire instance record everywhere to; prevent the instance changing state while we update job information. This serialized updates to; free_cores_mcpu which serialized job operation for any job on the same instance. We also had some unnecessarily lock-heavy code inside `add_attempt`. This block of code is trying to; insert a new attempt record and update the associated instance's free cores. I also fixed some out of date values and bad syntax in `estimated-current.sql`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10985
https://github.com/hail-is/hail/pull/10985:907,Deployability,update,updates,907,"Under ideal conditions, I observed this code complete 12 jobs per second in a 1000 job; batch. However, it's usually closer to 6 jobs per second. There's still a deadlock in; `mark_job_complete` which hamstrings performance. I haven't thought carefully about why my metric; (jobs per second) is much lower than our usual metric (MJC per second). I will eventually squash; the `mark_job_complete` deadlock, but I haven't the time currently. The two main issues were the `batches` field `cancelled` and the `instances` field; `free_cores_mcpu`. We frequently locked the entire batch row just to prevent a batch from being; cancelled while other work was done. This effectively serialized all operation on jobs in the same; batch. We had the dual issue with free_cores_mcpu: we lock the entire instance record everywhere to; prevent the instance changing state while we update job information. This serialized updates to; free_cores_mcpu which serialized job operation for any job on the same instance. We also had some unnecessarily lock-heavy code inside `add_attempt`. This block of code is trying to; insert a new attempt record and update the associated instance's free cores. I also fixed some out of date values and bad syntax in `estimated-current.sql`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10985
https://github.com/hail-is/hail/pull/10985:1134,Deployability,update,update,1134,"Under ideal conditions, I observed this code complete 12 jobs per second in a 1000 job; batch. However, it's usually closer to 6 jobs per second. There's still a deadlock in; `mark_job_complete` which hamstrings performance. I haven't thought carefully about why my metric; (jobs per second) is much lower than our usual metric (MJC per second). I will eventually squash; the `mark_job_complete` deadlock, but I haven't the time currently. The two main issues were the `batches` field `cancelled` and the `instances` field; `free_cores_mcpu`. We frequently locked the entire batch row just to prevent a batch from being; cancelled while other work was done. This effectively serialized all operation on jobs in the same; batch. We had the dual issue with free_cores_mcpu: we lock the entire instance record everywhere to; prevent the instance changing state while we update job information. This serialized updates to; free_cores_mcpu which serialized job operation for any job on the same instance. We also had some unnecessarily lock-heavy code inside `add_attempt`. This block of code is trying to; insert a new attempt record and update the associated instance's free cores. I also fixed some out of date values and bad syntax in `estimated-current.sql`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10985
https://github.com/hail-is/hail/pull/10985:212,Performance,perform,performance,212,"Under ideal conditions, I observed this code complete 12 jobs per second in a 1000 job; batch. However, it's usually closer to 6 jobs per second. There's still a deadlock in; `mark_job_complete` which hamstrings performance. I haven't thought carefully about why my metric; (jobs per second) is much lower than our usual metric (MJC per second). I will eventually squash; the `mark_job_complete` deadlock, but I haven't the time currently. The two main issues were the `batches` field `cancelled` and the `instances` field; `free_cores_mcpu`. We frequently locked the entire batch row just to prevent a batch from being; cancelled while other work was done. This effectively serialized all operation on jobs in the same; batch. We had the dual issue with free_cores_mcpu: we lock the entire instance record everywhere to; prevent the instance changing state while we update job information. This serialized updates to; free_cores_mcpu which serialized job operation for any job on the same instance. We also had some unnecessarily lock-heavy code inside `add_attempt`. This block of code is trying to; insert a new attempt record and update the associated instance's free cores. I also fixed some out of date values and bad syntax in `estimated-current.sql`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10985
https://github.com/hail-is/hail/pull/10986:693,Modifiability,config,config,693,- Added the AzureGraphClient which creates applications and service principals; - Modified Auth to create service principals and delete them; - Added two fields to the auth database that optionally store the application ID and the credentials secret name; - Had to modify AzureCredentials a bit to account for a different scope (one of the Azure Credentials types cannot take multiple scopes for some reason); - There's an auth database migration here!; - I tried to figure out what API calls result in the same result in the portal. It's possible the exact calls are not quite right (ex: addPassword on the application versus the service principal). TODO:; - Figure out how to use the global config in auth/Makefile to template global.cloud; - Double check the service principal creation is correct (I know the appID and password end up working to create resources at least). cc: @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10986
https://github.com/hail-is/hail/pull/10986:822,Security,password,password,822,- Added the AzureGraphClient which creates applications and service principals; - Modified Auth to create service principals and delete them; - Added two fields to the auth database that optionally store the application ID and the credentials secret name; - Had to modify AzureCredentials a bit to account for a different scope (one of the Azure Credentials types cannot take multiple scopes for some reason); - There's an auth database migration here!; - I tried to figure out what API calls result in the same result in the portal. It's possible the exact calls are not quite right (ex: addPassword on the application versus the service principal). TODO:; - Figure out how to use the global config in auth/Makefile to template global.cloud; - Double check the service principal creation is correct (I know the appID and password end up working to create resources at least). cc: @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10986
https://github.com/hail-is/hail/pull/10989:89,Testability,test,test,89,"Adds functionality to allow `hardy_weinberg_test` to return p-value from one-sided exact test of excess heterozygosity by passing `one_sided=True`. By default, `one_sided=False` and the p-value returned is from the two-sided exact test of HWE.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10989
https://github.com/hail-is/hail/pull/10989:231,Testability,test,test,231,"Adds functionality to allow `hardy_weinberg_test` to return p-value from one-sided exact test of excess heterozygosity by passing `one_sided=True`. By default, `one_sided=False` and the p-value returned is from the two-sided exact test of HWE.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10989
https://github.com/hail-is/hail/pull/10993:5,Deployability,update,updates,5,This updates `hl.variant_qc` to compute the p-value from the one-sided HWE test for excess heterozygosity as the field `p_value_excess_het`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10993
https://github.com/hail-is/hail/pull/10993:75,Testability,test,test,75,This updates `hl.variant_qc` to compute the p-value from the one-sided HWE test for excess heterozygosity as the field `p_value_excess_het`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10993
https://github.com/hail-is/hail/pull/10994:42,Performance,load,loadBit,42,"I'm trying to stop having us call `Region.loadBit` everywhere in `EBaseStruct.decode`. First step of that is not calling `setFieldMissing` and `setFieldPresent` everywhere. PRing for tests right now on first round of doing this, there are still some calls that need to be removed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10994
https://github.com/hail-is/hail/pull/10994:183,Testability,test,tests,183,"I'm trying to stop having us call `Region.loadBit` everywhere in `EBaseStruct.decode`. First step of that is not calling `setFieldMissing` and `setFieldPresent` everywhere. PRing for tests right now on first round of doing this, there are still some calls that need to be removed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10994
https://github.com/hail-is/hail/pull/11002:28,Deployability,deploy,deploy-config,28,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11002
https://github.com/hail-is/hail/pull/11002:54,Deployability,deploy,deploy-config,54,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11002
https://github.com/hail-is/hail/pull/11002:155,Deployability,deploy,deploy,155,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11002
https://github.com/hail-is/hail/pull/11002:198,Deployability,deploy,deploy,198,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11002
https://github.com/hail-is/hail/pull/11002:260,Deployability,update,updated,260,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11002
https://github.com/hail-is/hail/pull/11002:35,Modifiability,config,config,35,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11002
https://github.com/hail-is/hail/pull/11002:61,Modifiability,config,config,61,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11002
https://github.com/hail-is/hail/pull/11004:165,Performance,optimiz,optimization,165,"I added this when I was hitting class too large exceptions in a previous PR, to see if it would help. I didn't end up using it there, but it still seems like a good optimization. @tpoterba I haven't thought about this very hard, would be interested in your thoughts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11004
https://github.com/hail-is/hail/pull/11009:279,Modifiability,config,config,279,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11009
https://github.com/hail-is/hail/pull/11009:991,Modifiability,config,config,991,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11009
https://github.com/hail-is/hail/pull/11009:1064,Modifiability,config,config,1064,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11009
https://github.com/hail-is/hail/pull/11009:721,Security,access,access,721,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11009
https://github.com/hail-is/hail/pull/11009:969,Security,validat,validate,969,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11009
https://github.com/hail-is/hail/pull/11009:738,Testability,test,test,738,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11009
https://github.com/hail-is/hail/pull/11009:881,Testability,test,tested,881,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11009
https://github.com/hail-is/hail/pull/11012:0,Deployability,Update,Updates,0,Updates the GCP terraform to catch up with additions made for Azure. Soon the clouds will share the same module for the global-config so this won't be something we need to worry about.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11012
https://github.com/hail-is/hail/pull/11012:127,Modifiability,config,config,127,Updates the GCP terraform to catch up with additions made for Azure. Soon the clouds will share the same module for the global-config so this won't be something we need to worry about.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11012
https://github.com/hail-is/hail/pull/11014:924,Deployability,deploy,deployment,924,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11014:937,Integrability,depend,dependency,937,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11014:79,Modifiability,config,config,79,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11014:505,Security,access,access,505,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11014:307,Testability,log,logs,307,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11014:318,Testability,test,test,318,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11014:566,Testability,test,test,566,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11014:368,Usability,intuit,intuitive,368,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11014
https://github.com/hail-is/hail/pull/11015:168,Deployability,update,updates,168,Hopefully the dev doc can do most of the explaining here. Not sure exactly how often we want to rotate. Putting this up here now to get feedback and will apply the key updates it to `hail-vdc` later this week after a trial on my own cluster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11015
https://github.com/hail-is/hail/pull/11015:136,Usability,feedback,feedback,136,Hopefully the dev doc can do most of the explaining here. Not sure exactly how often we want to rotate. Putting this up here now to get feedback and will apply the key updates it to `hail-vdc` later this week after a trial on my own cluster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11015
https://github.com/hail-is/hail/pull/11017:183,Deployability,install,installed,183,"I've left out batch and gear so as not to interfere with #10920, but I intend to do those once there are no huge batch PRs on deck. I think I'm the only one with the pre-commit hooks installed because currently I can't make any small changes to batch files without introducing a ton of format changes (without disabling the pre-commit hooks).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11017
https://github.com/hail-is/hail/pull/11020:44,Integrability,message,messages,44,Some additional asserts and quality of life messages that I found necessary for confidently rotating keys.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11020
https://github.com/hail-is/hail/pull/11020:16,Testability,assert,asserts,16,Some additional asserts and quality of life messages that I found necessary for confidently rotating keys.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11020
https://github.com/hail-is/hail/pull/11025:174,Modifiability,config,config,174,Another small effort toward getting `gs://` out of our code. I'll follow up with a PR that puts the prod CI bucket into terraform. The `test_storage_uri` field of the global config was introduced in #11014,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11025
https://github.com/hail-is/hail/pull/11026:25,Modifiability,config,config,25,This logic can happen at config time instead of runtime.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11026
https://github.com/hail-is/hail/pull/11026:5,Testability,log,logic,5,This logic can happen at config time instead of runtime.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11026
https://github.com/hail-is/hail/pull/11027:52,Deployability,update,updated,52,I didn't test this yet -- do you want me to try the updated docs or should we wait until we redeploy the infrastructure next? Documentation is [here](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/network_security_group).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11027
https://github.com/hail-is/hail/pull/11027:190,Security,hash,hashicorp,190,I didn't test this yet -- do you want me to try the updated docs or should we wait until we redeploy the infrastructure next? Documentation is [here](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/network_security_group).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11027
https://github.com/hail-is/hail/pull/11027:9,Testability,test,test,9,I didn't test this yet -- do you want me to try the updated docs or should we wait until we redeploy the infrastructure next? Documentation is [here](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/network_security_group).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11027
https://github.com/hail-is/hail/pull/11028:62,Safety,safe,safely,62,I find this useful for identifying which local branches I can safely delete.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11028
https://github.com/hail-is/hail/pull/11030:346,Security,access,access,346,"Since the batch workers will ultimately need to communicate with the database for CI jobs, it seems asymmetrical to put the private database connection in the k8s subnet. I've created a third small subnet just for the private database connection. Traffic across subnets is allowed by default so VMs in k8s and the batch workers should be able to access the database IP. This way we can put more restrictive rules in later (like batch workers should not be able to address VMs in the k8s subnet) without interfering with the database connection. I've applied this change to the azure instance and verified that auth and the admin pod work with the new IP.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11030
https://github.com/hail-is/hail/pull/11033:209,Modifiability,variab,variables,209,"#11005 Added support for the BuildImage job in build.py to work with `GOOGLE_APPLICATION_CREDENTIALS` or `AZURE_APPLICATION_CREDENTIALS`, but that doesn't mean that the backing secret behind those environment variables has to be different in each case. Ultimately, we would like the same k8s terraform to generate secrets for both clouds, so this changes both `gcr-push-service-account-key` and `acr-push-credentials` to just be `registry-push-credentials`. I have copied the `gcr-push-service-account-key` into a new secret with the appropriate name in `hail-vdc`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11033
https://github.com/hail-is/hail/pull/11035:205,Deployability,deploy,deploy,205,"Cleans up the azure instructions to be more complete. There are currently some small problems with the terraform SP and AD privileges, I'll circle back to that later. For now, just uses dev credentials to deploy terraform.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11035
https://github.com/hail-is/hail/pull/11036:47,Modifiability,config,config,47,"The terraform sets `gcp_project` in the global-config not `project`. A few weeks ago I did a manual copy & rename of `project -> gcp_project`, `zone -> gcp_zone` etc. This is the follow-through for that conversion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11036
https://github.com/hail-is/hail/pull/11037:124,Availability,down,download,124,"This adds the oauth2 client secret as a terraform resource instead of uploading it by hand to k8s (though you still have to download it manually from the console. It also makes use of the `bootstrap_utils.sh` script that I introduced for azure for cloud-agnostic steps, trimming down the GCP instructions by a lot.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11037
https://github.com/hail-is/hail/pull/11037:279,Availability,down,down,279,"This adds the oauth2 client secret as a terraform resource instead of uploading it by hand to k8s (though you still have to download it manually from the console. It also makes use of the `bootstrap_utils.sh` script that I introduced for azure for cloud-agnostic steps, trimming down the GCP instructions by a lot.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11037
https://github.com/hail-is/hail/pull/11038:8,Security,expose,exposes,8,"This PR exposes the `lower_tail` and `log_p` parameters in `hl.pnorm`, `hl.qnorm`, `hl.pchisqtail`, and `hl.qchisqtail`, per this [feature request](https://hail.zulipchat.com/#narrow/stream/127634-Feature-Requests/topic/log_p.20option.20for.20pnorm.2Fpchisqtail). . While I was at it I made a few other changes, including:; - Added `hl.dnorm` and `hl.dchisq` to allow for computation of normal and chi-squared probability densities.; - Exposed `mu` and `sigma` in `hl.dnorm`, `hl.pnorm`, and `hl.qnorm` so users can specify different mean/sd values if they wish. By default these functions will still use `mu=0` and `sigma=1` for standard normal densities/cumulative probabilities/quantiles if not otherwise specified by the user.; - Added `ncp` parameter to `hl.qchisqtail` to allow users to specify a non-centrality parameter when computing quantiles.; - Added tests for all of these functions to `test_expr.py`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11038
https://github.com/hail-is/hail/pull/11038:436,Security,Expose,Exposed,436,"This PR exposes the `lower_tail` and `log_p` parameters in `hl.pnorm`, `hl.qnorm`, `hl.pchisqtail`, and `hl.qchisqtail`, per this [feature request](https://hail.zulipchat.com/#narrow/stream/127634-Feature-Requests/topic/log_p.20option.20for.20pnorm.2Fpchisqtail). . While I was at it I made a few other changes, including:; - Added `hl.dnorm` and `hl.dchisq` to allow for computation of normal and chi-squared probability densities.; - Exposed `mu` and `sigma` in `hl.dnorm`, `hl.pnorm`, and `hl.qnorm` so users can specify different mean/sd values if they wish. By default these functions will still use `mu=0` and `sigma=1` for standard normal densities/cumulative probabilities/quantiles if not otherwise specified by the user.; - Added `ncp` parameter to `hl.qchisqtail` to allow users to specify a non-centrality parameter when computing quantiles.; - Added tests for all of these functions to `test_expr.py`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11038
https://github.com/hail-is/hail/pull/11038:863,Testability,test,tests,863,"This PR exposes the `lower_tail` and `log_p` parameters in `hl.pnorm`, `hl.qnorm`, `hl.pchisqtail`, and `hl.qchisqtail`, per this [feature request](https://hail.zulipchat.com/#narrow/stream/127634-Feature-Requests/topic/log_p.20option.20for.20pnorm.2Fpchisqtail). . While I was at it I made a few other changes, including:; - Added `hl.dnorm` and `hl.dchisq` to allow for computation of normal and chi-squared probability densities.; - Exposed `mu` and `sigma` in `hl.dnorm`, `hl.pnorm`, and `hl.qnorm` so users can specify different mean/sd values if they wish. By default these functions will still use `mu=0` and `sigma=1` for standard normal densities/cumulative probabilities/quantiles if not otherwise specified by the user.; - Added `ncp` parameter to `hl.qchisqtail` to allow users to specify a non-centrality parameter when computing quantiles.; - Added tests for all of these functions to `test_expr.py`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11038
https://github.com/hail-is/hail/pull/11040:508,Deployability,update,updated,508,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11040
https://github.com/hail-is/hail/pull/11040:11,Performance,cache,cache,11,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11040
https://github.com/hail-is/hail/pull/11040:535,Performance,cache,cache,535,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11040
https://github.com/hail-is/hail/pull/11040:347,Safety,avoid,avoids,347,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11040
https://github.com/hail-is/hail/pull/11040:92,Testability,test,tests,92,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11040
https://github.com/hail-is/hail/pull/11040:133,Testability,test,tests,133,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11040
https://github.com/hail-is/hail/pull/11044:295,Safety,avoid,avoiding,295,This change allows batchces which fit in one bunch to be created in one; request instead of three. I found this saved a few hundred milliseconds; for batches with 1 job. This path is already well tested because most; of our test batches fit in one job. To be clear: all the savings here is from avoiding two round-trips to front-end.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11044
https://github.com/hail-is/hail/pull/11044:196,Testability,test,tested,196,This change allows batchces which fit in one bunch to be created in one; request instead of three. I found this saved a few hundred milliseconds; for batches with 1 job. This path is already well tested because most; of our test batches fit in one job. To be clear: all the savings here is from avoiding two round-trips to front-end.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11044
https://github.com/hail-is/hail/pull/11044:224,Testability,test,test,224,This change allows batchces which fit in one bunch to be created in one; request instead of three. I found this saved a few hundred milliseconds; for batches with 1 job. This path is already well tested because most; of our test batches fit in one job. To be clear: all the savings here is from avoiding two round-trips to front-end.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11044
https://github.com/hail-is/hail/pull/11044:259,Usability,clear,clear,259,This change allows batchces which fit in one bunch to be created in one; request instead of three. I found this saved a few hundred milliseconds; for batches with 1 job. This path is already well tested because most; of our test batches fit in one job. To be clear: all the savings here is from avoiding two round-trips to front-end.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11044
https://github.com/hail-is/hail/pull/11045:3324,Availability,down,down,3324,"orization(A, W, p)`, and `R = U_1 S V'_1` is an SVD, then `W' f(A'A) W` is well-approximated by `V_1[:k, :] f(S^2) V_1[:k, :]'`, and this is exact if `f` is a degree-p polynomial. We can combine the above into an estimator for the p-th spectral moment, `ùúá_p = ‚àë_i ùúÜ_i^p`. We get an unbiased estimator by generating a random vector `v`, and using `E(v' (A'A)^p v) = tr((A'A)^p) = ùúá_p`. The estimator can be computed exactly using the Krylov factorization as above, i.e. `v' (A'A)^p v = V_1[0, :] S^{2p} V_1[0, :]'`. But this estimator has large variance, so we can just average over many independent estimators. We combine `k` random vectors into a random matrix `V_0`, compute `_krylov_factorization(A, V_0, p)`, and then `V'_0 (A'A)^p V_0 = V_1[:k, :] f(S^2) V_1[:k, :]'`, from which we can average the individual estimates. This spectral moments estimator is implemented in `KrylovFactorization.spectral_moments`. # SVD and moments; Finally, in practice we may need to average over too many estimators to get the desired accuracy, so we use one more trick to bring down the variance. This depends on the following property:; * If `Q` is an orthonormal matrix, then `tr(X) = tr(QQ'XQQ') + tr((I-QQ')X(I-QQ'))`, i.e. the trace decomposes into a sum of the traces on the projection onto the subspace spanned by `Q`, and the projection onto its complement. To see this, use the additivity and cyclicity of the trace (`tr(X+Y) = tr(X) + tr(Y)`, `tr(XY) = tr(YX)`). In particular, cyclicity implies `tr(QQ'XQQ') = tr(QQ'X) = tr(XQQ')`. Now we do two passes. In the first pass, we use some method (such as a Krylov factorization) to compute an orthonormal basis `V` for a subspace which approximates the dominant subspace. Even if this is not a very accurate approximation, the result is that `tr(QQ' X^p QQ')` is much larger than the complement `tr((I-QQ')X^p(I-QQ'))`. But the former we can compute exactly, and all the error comes from our estimate of the much smaller piece. This variance reduction tr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:4174,Availability,error,error,4174,"ting a random vector `v`, and using `E(v' (A'A)^p v) = tr((A'A)^p) = ùúá_p`. The estimator can be computed exactly using the Krylov factorization as above, i.e. `v' (A'A)^p v = V_1[0, :] S^{2p} V_1[0, :]'`. But this estimator has large variance, so we can just average over many independent estimators. We combine `k` random vectors into a random matrix `V_0`, compute `_krylov_factorization(A, V_0, p)`, and then `V'_0 (A'A)^p V_0 = V_1[:k, :] f(S^2) V_1[:k, :]'`, from which we can average the individual estimates. This spectral moments estimator is implemented in `KrylovFactorization.spectral_moments`. # SVD and moments; Finally, in practice we may need to average over too many estimators to get the desired accuracy, so we use one more trick to bring down the variance. This depends on the following property:; * If `Q` is an orthonormal matrix, then `tr(X) = tr(QQ'XQQ') + tr((I-QQ')X(I-QQ'))`, i.e. the trace decomposes into a sum of the traces on the projection onto the subspace spanned by `Q`, and the projection onto its complement. To see this, use the additivity and cyclicity of the trace (`tr(X+Y) = tr(X) + tr(Y)`, `tr(XY) = tr(YX)`). In particular, cyclicity implies `tr(QQ'XQQ') = tr(QQ'X) = tr(XQQ')`. Now we do two passes. In the first pass, we use some method (such as a Krylov factorization) to compute an orthonormal basis `V` for a subspace which approximates the dominant subspace. Even if this is not a very accurate approximation, the result is that `tr(QQ' X^p QQ')` is much larger than the complement `tr((I-QQ')X^p(I-QQ'))`. But the former we can compute exactly, and all the error comes from our estimate of the much smaller piece. This variance reduction trick is implemented in `_pca_and_moments`, which computes two Krylov factorizations. The first is used to compute the principal components, and the `V` factor is also used to decompose the moments estimator as above. The second factorization is used to estimate the remaining component of the moments estimator.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:817,Energy Efficiency,Reduce,Reduced,817,"Refactors `_blanczos_pca` into reusable and composable pieces, and uses those pieces to implement spectral moments estimators. # Krylov factorization; The core iteration of `_blanczos_pca` is factored out into `_krylov_factorization`. `_krylov_factorization(A, V0, p)` takes a matrix `A` (represented as a table of ndarrays) and a starting block `V0` (a local ndarray), and computes matrices (for now local) `U`, `R`, and `V`, such that:; * `U` and `V` are orthonormal matrices (i.e. `U'U = V'V = I`); * the columns of `V` are a basis for the block Krylov subspace `K_p(A'A, V_0)`, where `K_p(X, Y) = span(Y, XY, ... X^pY)`; * `UR = AV`, and hence `U` is a basis for the block Krylov subspace `A K_p(A'A, V_0) = K_p(AA', AV_0)`; * `V` is an extension of `V_0`, i.e. `V = hcat(V_0, ...)`; * `R` is upper triangular. # Reduced SVD; From a Krylov factorization, a reduced SVD can be easily computed: If `R = U_1 S V'_1` is a full SVD of `R` (which is small and easily computable), then `(U U_1[:, :k]) S[:k, :k] (V V_1[:, :k])'` is a reduced SVD of `A`. This is implemented in `KrylovFactorization.reduced_svd`. # Spectral moments; We can also easily compute estimates of spectral moments, i.e. moments of the set of all eigenvalues of `A'A`. The estimator exploits the following key facts:; * If `v` is a random vector of independent entries with mean 0, std. dev. 1 (equivalently `E(v) = 0`, `E(vv') = I`), then `E(v'Xv) = tr(X)`; * `tr(X)` equals the sum of the eigenvalues of `X`, `‚àë_i ùúÜ_i`. More generally, if `f` is any matrix function, `tr(f(X)) = ‚àë_i f(ùúÜ_i)`.; * If `w` is a unit-norm vector, and `UR = AV` is the factorization `_krylov_factorization(A, w, p)`, then `w' f(A'A) w` is well-approximated by `w' f(VV'A'AVV') w = w'V f(R'U'UR) V'w = w'V f(R'R) V'w`, and is exact if `f` is a degree `2p+1` polynomial. Moreover, since `w` is the first column of `V`, i.e. `w = Ve_1`, the above further simplifies `w'V f(R'R) V'w = e'_1 f(R'R) e_1`. Finally, if `R = U_1 S V'_1` is an SVD, this reduces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:861,Energy Efficiency,reduce,reduced,861,"Refactors `_blanczos_pca` into reusable and composable pieces, and uses those pieces to implement spectral moments estimators. # Krylov factorization; The core iteration of `_blanczos_pca` is factored out into `_krylov_factorization`. `_krylov_factorization(A, V0, p)` takes a matrix `A` (represented as a table of ndarrays) and a starting block `V0` (a local ndarray), and computes matrices (for now local) `U`, `R`, and `V`, such that:; * `U` and `V` are orthonormal matrices (i.e. `U'U = V'V = I`); * the columns of `V` are a basis for the block Krylov subspace `K_p(A'A, V_0)`, where `K_p(X, Y) = span(Y, XY, ... X^pY)`; * `UR = AV`, and hence `U` is a basis for the block Krylov subspace `A K_p(A'A, V_0) = K_p(AA', AV_0)`; * `V` is an extension of `V_0`, i.e. `V = hcat(V_0, ...)`; * `R` is upper triangular. # Reduced SVD; From a Krylov factorization, a reduced SVD can be easily computed: If `R = U_1 S V'_1` is a full SVD of `R` (which is small and easily computable), then `(U U_1[:, :k]) S[:k, :k] (V V_1[:, :k])'` is a reduced SVD of `A`. This is implemented in `KrylovFactorization.reduced_svd`. # Spectral moments; We can also easily compute estimates of spectral moments, i.e. moments of the set of all eigenvalues of `A'A`. The estimator exploits the following key facts:; * If `v` is a random vector of independent entries with mean 0, std. dev. 1 (equivalently `E(v) = 0`, `E(vv') = I`), then `E(v'Xv) = tr(X)`; * `tr(X)` equals the sum of the eigenvalues of `X`, `‚àë_i ùúÜ_i`. More generally, if `f` is any matrix function, `tr(f(X)) = ‚àë_i f(ùúÜ_i)`.; * If `w` is a unit-norm vector, and `UR = AV` is the factorization `_krylov_factorization(A, w, p)`, then `w' f(A'A) w` is well-approximated by `w' f(VV'A'AVV') w = w'V f(R'U'UR) V'w = w'V f(R'R) V'w`, and is exact if `f` is a degree `2p+1` polynomial. Moreover, since `w` is the first column of `V`, i.e. `w = Ve_1`, the above further simplifies `w'V f(R'R) V'w = e'_1 f(R'R) e_1`. Finally, if `R = U_1 S V'_1` is an SVD, this reduces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:1031,Energy Efficiency,reduce,reduced,1031,"Refactors `_blanczos_pca` into reusable and composable pieces, and uses those pieces to implement spectral moments estimators. # Krylov factorization; The core iteration of `_blanczos_pca` is factored out into `_krylov_factorization`. `_krylov_factorization(A, V0, p)` takes a matrix `A` (represented as a table of ndarrays) and a starting block `V0` (a local ndarray), and computes matrices (for now local) `U`, `R`, and `V`, such that:; * `U` and `V` are orthonormal matrices (i.e. `U'U = V'V = I`); * the columns of `V` are a basis for the block Krylov subspace `K_p(A'A, V_0)`, where `K_p(X, Y) = span(Y, XY, ... X^pY)`; * `UR = AV`, and hence `U` is a basis for the block Krylov subspace `A K_p(A'A, V_0) = K_p(AA', AV_0)`; * `V` is an extension of `V_0`, i.e. `V = hcat(V_0, ...)`; * `R` is upper triangular. # Reduced SVD; From a Krylov factorization, a reduced SVD can be easily computed: If `R = U_1 S V'_1` is a full SVD of `R` (which is small and easily computable), then `(U U_1[:, :k]) S[:k, :k] (V V_1[:, :k])'` is a reduced SVD of `A`. This is implemented in `KrylovFactorization.reduced_svd`. # Spectral moments; We can also easily compute estimates of spectral moments, i.e. moments of the set of all eigenvalues of `A'A`. The estimator exploits the following key facts:; * If `v` is a random vector of independent entries with mean 0, std. dev. 1 (equivalently `E(v) = 0`, `E(vv') = I`), then `E(v'Xv) = tr(X)`; * `tr(X)` equals the sum of the eigenvalues of `X`, `‚àë_i ùúÜ_i`. More generally, if `f` is any matrix function, `tr(f(X)) = ‚àë_i f(ùúÜ_i)`.; * If `w` is a unit-norm vector, and `UR = AV` is the factorization `_krylov_factorization(A, w, p)`, then `w' f(A'A) w` is well-approximated by `w' f(VV'A'AVV') w = w'V f(R'U'UR) V'w = w'V f(R'R) V'w`, and is exact if `f` is a degree `2p+1` polynomial. Moreover, since `w` is the first column of `V`, i.e. `w = Ve_1`, the above further simplifies `w'V f(R'R) V'w = e'_1 f(R'R) e_1`. Finally, if `R = U_1 S V'_1` is an SVD, this reduces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:1994,Energy Efficiency,reduce,reduces,1994,"s a reduced SVD of `A`. This is implemented in `KrylovFactorization.reduced_svd`. # Spectral moments; We can also easily compute estimates of spectral moments, i.e. moments of the set of all eigenvalues of `A'A`. The estimator exploits the following key facts:; * If `v` is a random vector of independent entries with mean 0, std. dev. 1 (equivalently `E(v) = 0`, `E(vv') = I`), then `E(v'Xv) = tr(X)`; * `tr(X)` equals the sum of the eigenvalues of `X`, `‚àë_i ùúÜ_i`. More generally, if `f` is any matrix function, `tr(f(X)) = ‚àë_i f(ùúÜ_i)`.; * If `w` is a unit-norm vector, and `UR = AV` is the factorization `_krylov_factorization(A, w, p)`, then `w' f(A'A) w` is well-approximated by `w' f(VV'A'AVV') w = w'V f(R'U'UR) V'w = w'V f(R'R) V'w`, and is exact if `f` is a degree `2p+1` polynomial. Moreover, since `w` is the first column of `V`, i.e. `w = Ve_1`, the above further simplifies `w'V f(R'R) V'w = e'_1 f(R'R) e_1`. Finally, if `R = U_1 S V'_1` is an SVD, this reduces to `e'_1 f(R'R) e_1 = e'_1 f(V_1 S^2 V'_1) e_1 = e'_1 V_1 f(S^2) V'_1 e_1 = V_1[0, :] f(S^2) V_1[0, :]'`.; * The previous bullet has a block generalization. If `W` is an orthonormal matrix with `k` columns, and `UR = AV` is the factorization `_krylov_factorization(A, W, p)`, and `R = U_1 S V'_1` is an SVD, then `W' f(A'A) W` is well-approximated by `V_1[:k, :] f(S^2) V_1[:k, :]'`, and this is exact if `f` is a degree-p polynomial. We can combine the above into an estimator for the p-th spectral moment, `ùúá_p = ‚àë_i ùúÜ_i^p`. We get an unbiased estimator by generating a random vector `v`, and using `E(v' (A'A)^p v) = tr((A'A)^p) = ùúá_p`. The estimator can be computed exactly using the Krylov factorization as above, i.e. `v' (A'A)^p v = V_1[0, :] S^{2p} V_1[0, :]'`. But this estimator has large variance, so we can just average over many independent estimators. We combine `k` random vectors into a random matrix `V_0`, compute `_krylov_factorization(A, V_0, p)`, and then `V'_0 (A'A)^p V_0 = V_1[:k, :] f(S^2) V_1[:k, :]'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:3348,Integrability,depend,depends,3348," `f` is a degree-p polynomial. We can combine the above into an estimator for the p-th spectral moment, `ùúá_p = ‚àë_i ùúÜ_i^p`. We get an unbiased estimator by generating a random vector `v`, and using `E(v' (A'A)^p v) = tr((A'A)^p) = ùúá_p`. The estimator can be computed exactly using the Krylov factorization as above, i.e. `v' (A'A)^p v = V_1[0, :] S^{2p} V_1[0, :]'`. But this estimator has large variance, so we can just average over many independent estimators. We combine `k` random vectors into a random matrix `V_0`, compute `_krylov_factorization(A, V_0, p)`, and then `V'_0 (A'A)^p V_0 = V_1[:k, :] f(S^2) V_1[:k, :]'`, from which we can average the individual estimates. This spectral moments estimator is implemented in `KrylovFactorization.spectral_moments`. # SVD and moments; Finally, in practice we may need to average over too many estimators to get the desired accuracy, so we use one more trick to bring down the variance. This depends on the following property:; * If `Q` is an orthonormal matrix, then `tr(X) = tr(QQ'XQQ') + tr((I-QQ')X(I-QQ'))`, i.e. the trace decomposes into a sum of the traces on the projection onto the subspace spanned by `Q`, and the projection onto its complement. To see this, use the additivity and cyclicity of the trace (`tr(X+Y) = tr(X) + tr(Y)`, `tr(XY) = tr(YX)`). In particular, cyclicity implies `tr(QQ'XQQ') = tr(QQ'X) = tr(XQQ')`. Now we do two passes. In the first pass, we use some method (such as a Krylov factorization) to compute an orthonormal basis `V` for a subspace which approximates the dominant subspace. Even if this is not a very accurate approximation, the result is that `tr(QQ' X^p QQ')` is much larger than the complement `tr((I-QQ')X^p(I-QQ'))`. But the former we can compute exactly, and all the error comes from our estimate of the much smaller piece. This variance reduction trick is implemented in `_pca_and_moments`, which computes two Krylov factorizations. The first is used to compute the principal components, and the `V`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:0,Modifiability,Refactor,Refactors,0,"Refactors `_blanczos_pca` into reusable and composable pieces, and uses those pieces to implement spectral moments estimators. # Krylov factorization; The core iteration of `_blanczos_pca` is factored out into `_krylov_factorization`. `_krylov_factorization(A, V0, p)` takes a matrix `A` (represented as a table of ndarrays) and a starting block `V0` (a local ndarray), and computes matrices (for now local) `U`, `R`, and `V`, such that:; * `U` and `V` are orthonormal matrices (i.e. `U'U = V'V = I`); * the columns of `V` are a basis for the block Krylov subspace `K_p(A'A, V_0)`, where `K_p(X, Y) = span(Y, XY, ... X^pY)`; * `UR = AV`, and hence `U` is a basis for the block Krylov subspace `A K_p(A'A, V_0) = K_p(AA', AV_0)`; * `V` is an extension of `V_0`, i.e. `V = hcat(V_0, ...)`; * `R` is upper triangular. # Reduced SVD; From a Krylov factorization, a reduced SVD can be easily computed: If `R = U_1 S V'_1` is a full SVD of `R` (which is small and easily computable), then `(U U_1[:, :k]) S[:k, :k] (V V_1[:, :k])'` is a reduced SVD of `A`. This is implemented in `KrylovFactorization.reduced_svd`. # Spectral moments; We can also easily compute estimates of spectral moments, i.e. moments of the set of all eigenvalues of `A'A`. The estimator exploits the following key facts:; * If `v` is a random vector of independent entries with mean 0, std. dev. 1 (equivalently `E(v) = 0`, `E(vv') = I`), then `E(v'Xv) = tr(X)`; * `tr(X)` equals the sum of the eigenvalues of `X`, `‚àë_i ùúÜ_i`. More generally, if `f` is any matrix function, `tr(f(X)) = ‚àë_i f(ùúÜ_i)`.; * If `w` is a unit-norm vector, and `UR = AV` is the factorization `_krylov_factorization(A, w, p)`, then `w' f(A'A) w` is well-approximated by `w' f(VV'A'AVV') w = w'V f(R'U'UR) V'w = w'V f(R'R) V'w`, and is exact if `f` is a degree `2p+1` polynomial. Moreover, since `w` is the first column of `V`, i.e. `w = Ve_1`, the above further simplifies `w'V f(R'R) V'w = e'_1 f(R'R) e_1`. Finally, if `R = U_1 S V'_1` is an SVD, this reduces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11045:1902,Usability,simpl,simplifies,1902,"S V'_1` is a full SVD of `R` (which is small and easily computable), then `(U U_1[:, :k]) S[:k, :k] (V V_1[:, :k])'` is a reduced SVD of `A`. This is implemented in `KrylovFactorization.reduced_svd`. # Spectral moments; We can also easily compute estimates of spectral moments, i.e. moments of the set of all eigenvalues of `A'A`. The estimator exploits the following key facts:; * If `v` is a random vector of independent entries with mean 0, std. dev. 1 (equivalently `E(v) = 0`, `E(vv') = I`), then `E(v'Xv) = tr(X)`; * `tr(X)` equals the sum of the eigenvalues of `X`, `‚àë_i ùúÜ_i`. More generally, if `f` is any matrix function, `tr(f(X)) = ‚àë_i f(ùúÜ_i)`.; * If `w` is a unit-norm vector, and `UR = AV` is the factorization `_krylov_factorization(A, w, p)`, then `w' f(A'A) w` is well-approximated by `w' f(VV'A'AVV') w = w'V f(R'U'UR) V'w = w'V f(R'R) V'w`, and is exact if `f` is a degree `2p+1` polynomial. Moreover, since `w` is the first column of `V`, i.e. `w = Ve_1`, the above further simplifies `w'V f(R'R) V'w = e'_1 f(R'R) e_1`. Finally, if `R = U_1 S V'_1` is an SVD, this reduces to `e'_1 f(R'R) e_1 = e'_1 f(V_1 S^2 V'_1) e_1 = e'_1 V_1 f(S^2) V'_1 e_1 = V_1[0, :] f(S^2) V_1[0, :]'`.; * The previous bullet has a block generalization. If `W` is an orthonormal matrix with `k` columns, and `UR = AV` is the factorization `_krylov_factorization(A, W, p)`, and `R = U_1 S V'_1` is an SVD, then `W' f(A'A) W` is well-approximated by `V_1[:k, :] f(S^2) V_1[:k, :]'`, and this is exact if `f` is a degree-p polynomial. We can combine the above into an estimator for the p-th spectral moment, `ùúá_p = ‚àë_i ùúÜ_i^p`. We get an unbiased estimator by generating a random vector `v`, and using `E(v' (A'A)^p v) = tr((A'A)^p) = ùúá_p`. The estimator can be computed exactly using the Krylov factorization as above, i.e. `v' (A'A)^p v = V_1[0, :] S^{2p} V_1[0, :]'`. But this estimator has large variance, so we can just average over many independent estimators. We combine `k` random vectors into a rando",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11045
https://github.com/hail-is/hail/pull/11046:136,Deployability,upgrade,upgrade,136,"Python 3.6 is end-of-life in just over a month, and since it's the default version on an `ubuntu:18.04` image, it seemed a good time to upgrade across the board to 20.04. I'm not sure if there's a better time to do this or if we should make an announcement to users that the default image is changing. Feel free to let it sit here until we want to make the switch. After this, the only occurrence of `ubuntu:18.04` in the codebase is in the third-party images that we mirror in our registry (so it will still be there, just not used by default). cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11046
https://github.com/hail-is/hail/pull/11050:62,Performance,Optimiz,Optimization,62,"PRing for test suite, but it's mostly working. . Todo:. - [x] Optimization for already sorted tables; - [x] ~~Handle sort by descending~~ (deferred to subsequent PR); - [x] Handle tables with no partitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11050
https://github.com/hail-is/hail/pull/11050:10,Testability,test,test,10,"PRing for test suite, but it's mostly working. . Todo:. - [x] Optimization for already sorted tables; - [x] ~~Handle sort by descending~~ (deferred to subsequent PR); - [x] Handle tables with no partitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11050
https://github.com/hail-is/hail/pull/11051:375,Security,access,access,375,"Currently we use the `10.0.0.0/15` range to assign local IPs to jobs on a worker. This is problematic if other resources in our infrastructure collide with this range. Since these are low address ranges it is quite likely that this could happen, and indeed has happened to me in a dev project where the database was assigned a `10.1.x.x` ip and CI jobs that require database access could not run. I don't have a particularly good reason to pick `10.150.0.0/15` as the new range other than I have not seen anything else use it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11051
https://github.com/hail-is/hail/pull/11053:952,Availability,down,down,952,"This introduces the necessary pieces of infrastructure to run CI in GCP and a couple of small changes such that it can run as a secondary CI. This is currently failing because one of the secrets I introduce here in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:539,Deployability,deploy,deployment,539,"This introduces the necessary pieces of infrastructure to run CI in GCP and a couple of small changes such that it can run as a secondary CI. This is currently failing because one of the secrets I introduce here in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:1186,Deployability,deploy,deployments,1186," in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `req",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:1972,Deployability,Deploy,Deploy,1972," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:2050,Deployability,deploy,deploy,2050," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:2073,Deployability,deploy,deployment,2073," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:2092,Deployability,deploy,deploy,2092," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:2252,Deployability,deploy,deploy,2252," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:505,Modifiability,config,config,505,"This introduces the necessary pieces of infrastructure to run CI in GCP and a couple of small changes such that it can run as a secondary CI. This is currently failing because one of the secrets I introduce here in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:1133,Modifiability,refactor,refactor,1133," in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `req",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:1666,Testability,test,tests,1666," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11053:2172,Testability,test,test,2172," permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a collection of build.yaml steps that CI should run on `deploy`. This allows a deployment to only deploy a subset of the infrastructure to default, though it will run the entire test suite. Based on the semantics, `requested_step_names`, the empty list will deploy everything.; - Custom github context: This is just so CIs running in different clouds/environments don't step on each other in the GitHub statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11053
https://github.com/hail-is/hail/pull/11054:266,Availability,error,error,266,"1. Lazily load the tokens on the end user machine because the first time someone logs in to the default namespace, they will have no tokens. 2. Teach `get_tokens_file` to default to the default end-user location. 3. Add a bunch of types which would have caught this error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11054
https://github.com/hail-is/hail/pull/11054:10,Performance,load,load,10,"1. Lazily load the tokens on the end user machine because the first time someone logs in to the default namespace, they will have no tokens. 2. Teach `get_tokens_file` to default to the default end-user location. 3. Add a bunch of types which would have caught this error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11054
https://github.com/hail-is/hail/pull/11054:81,Testability,log,logs,81,"1. Lazily load the tokens on the end user machine because the first time someone logs in to the default namespace, they will have no tokens. 2. Teach `get_tokens_file` to default to the default end-user location. 3. Add a bunch of types which would have caught this error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11054
https://github.com/hail-is/hail/pull/11055:72,Testability,test,test,72,"Bringing over the azure-specific changes from #10970, this reflects the test batch instance that is currently running in azure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11055
https://github.com/hail-is/hail/pull/11058:281,Security,password,password,281,"Stacked on #11057. I tested the query string and made sure the list commands worked. However, I did not test the actual delete with xargs. This was copied almost verbatim from the gcp delete instances step. I'm pretty sure by not adding the `-x` flag at the top of the script, the password won't be printed to the command line output in the logs, but I'm not 100% sure. Also, for this to work, the test-gsa-key needs to be able to delete VM and network resources. Do we have this in GCP as well? I guess we must in order for Batch to be able to remove its instances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11058
https://github.com/hail-is/hail/pull/11058:21,Testability,test,tested,21,"Stacked on #11057. I tested the query string and made sure the list commands worked. However, I did not test the actual delete with xargs. This was copied almost verbatim from the gcp delete instances step. I'm pretty sure by not adding the `-x` flag at the top of the script, the password won't be printed to the command line output in the logs, but I'm not 100% sure. Also, for this to work, the test-gsa-key needs to be able to delete VM and network resources. Do we have this in GCP as well? I guess we must in order for Batch to be able to remove its instances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11058
https://github.com/hail-is/hail/pull/11058:104,Testability,test,test,104,"Stacked on #11057. I tested the query string and made sure the list commands worked. However, I did not test the actual delete with xargs. This was copied almost verbatim from the gcp delete instances step. I'm pretty sure by not adding the `-x` flag at the top of the script, the password won't be printed to the command line output in the logs, but I'm not 100% sure. Also, for this to work, the test-gsa-key needs to be able to delete VM and network resources. Do we have this in GCP as well? I guess we must in order for Batch to be able to remove its instances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11058
https://github.com/hail-is/hail/pull/11058:341,Testability,log,logs,341,"Stacked on #11057. I tested the query string and made sure the list commands worked. However, I did not test the actual delete with xargs. This was copied almost verbatim from the gcp delete instances step. I'm pretty sure by not adding the `-x` flag at the top of the script, the password won't be printed to the command line output in the logs, but I'm not 100% sure. Also, for this to work, the test-gsa-key needs to be able to delete VM and network resources. Do we have this in GCP as well? I guess we must in order for Batch to be able to remove its instances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11058
https://github.com/hail-is/hail/pull/11058:398,Testability,test,test-gsa-key,398,"Stacked on #11057. I tested the query string and made sure the list commands worked. However, I did not test the actual delete with xargs. This was copied almost verbatim from the gcp delete instances step. I'm pretty sure by not adding the `-x` flag at the top of the script, the password won't be printed to the command line output in the logs, but I'm not 100% sure. Also, for this to work, the test-gsa-key needs to be able to delete VM and network resources. Do we have this in GCP as well? I guess we must in order for Batch to be able to remove its instances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11058
https://github.com/hail-is/hail/pull/11059:53,Integrability,rout,router,53,I was trying to figure out how to implement the lazy router and just couldn't stand the huge hailtop/aiotools/fs.py file anymore. This is just a reorganization. Should be no actual code changes. Feel free to can this if you think it's unnecessary. FYI @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11059
https://github.com/hail-is/hail/pull/11060:12,Security,expose,expose,12,"To do this, expose the necessary methods to get the start and end bounds; of an interval queries position within the index. Then just add; a counter to the iterators.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11060
https://github.com/hail-is/hail/pull/11063:64,Safety,avoid,avoid,64,I made some changes in the second commit that were necessary to avoid the circular imports. Closing #11059 in favor of this PR. FYI: @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11063
https://github.com/hail-is/hail/pull/11068:7,Deployability,Update,Updated,7,"#11016 Updated default hail version in Makefile, but build.gradle warns if you're not using 3.1.1. That's silly, everything should be 3.1.2 now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11068
https://github.com/hail-is/hail/pull/11069:34,Deployability,update,updated,34,@jigold Anything that needs to be updated for batch?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11069
https://github.com/hail-is/hail/pull/11071:435,Deployability,configurat,configuration,435,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11071
https://github.com/hail-is/hail/pull/11071:831,Deployability,deploy,deploying,831,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11071
https://github.com/hail-is/hail/pull/11071:435,Modifiability,config,configuration,435,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11071
https://github.com/hail-is/hail/pull/11071:914,Modifiability,config,config,914,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11071
https://github.com/hail-is/hail/pull/11071:80,Testability,test,tests,80,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11071
https://github.com/hail-is/hail/pull/11071:1151,Testability,test,tests,1151,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11071
https://github.com/hail-is/hail/pull/11072:287,Integrability,Rout,RouterAsyncFS,287,"1. Implement hailtop.aiotools.delete.; 2. Unify most definitions of rmtree and actually use parallelism.; 3. Remove unnecesary and confusing async annotation on OnlineBoundedGather2.call.; 4. Substantially increase the complexity of the rmtree test. I had to fix a circularity caused by RouterAsyncFS referencing the other clouds. The fix was easy, I don't re-expose it in hailtop.aiotools.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11072
https://github.com/hail-is/hail/pull/11072:360,Security,expose,expose,360,"1. Implement hailtop.aiotools.delete.; 2. Unify most definitions of rmtree and actually use parallelism.; 3. Remove unnecesary and confusing async annotation on OnlineBoundedGather2.call.; 4. Substantially increase the complexity of the rmtree test. I had to fix a circularity caused by RouterAsyncFS referencing the other clouds. The fix was easy, I don't re-expose it in hailtop.aiotools.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11072
https://github.com/hail-is/hail/pull/11072:244,Testability,test,test,244,"1. Implement hailtop.aiotools.delete.; 2. Unify most definitions of rmtree and actually use parallelism.; 3. Remove unnecesary and confusing async annotation on OnlineBoundedGather2.call.; 4. Substantially increase the complexity of the rmtree test. I had to fix a circularity caused by RouterAsyncFS referencing the other clouds. The fix was easy, I don't re-expose it in hailtop.aiotools.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11072
https://github.com/hail-is/hail/pull/11075:32,Integrability,depend,dependency,32,We put this in to remove auth's dependency on batch when batch couldn't run in azure and now this is no longer necessary.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11075
https://github.com/hail-is/hail/pull/11077:41,Usability,usab,usability,41,I think this little change adds a lot of usability to the method. You do not need to know how the names of the tables are constructed in the function.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11077
https://github.com/hail-is/hail/pull/11078:111,Availability,error,error,111,"Previously, accidentally passing an empty string to the list of missing values throws an inscrutable assertion error. This checks in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11078
https://github.com/hail-is/hail/pull/11078:101,Testability,assert,assertion,101,"Previously, accidentally passing an empty string to the list of missing values throws an inscrutable assertion error. This checks in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11078
https://github.com/hail-is/hail/pull/11080:3314,Availability,error,errors,3314,"ect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatical errors (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2642"">#2642</a>)</li>; <li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status bad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:3430,Availability,error,error,3430,"/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatical errors (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2642"">#2642</a>)</li>; <li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with J",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:4405,Availability,error,errors,4405,"<li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with JSON documentation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3936"">#3936</a>)</li>; <li>Fixing README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3825"">#3825</a>)</li>; <li>Adding axios-multi-api to the ecosystem file (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3817"">#3817</a>)</li>; <li>Adding SECURITY.md to properly disclose security vulnerabilities (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3981"">#3981</a>)</li>; </ul>; <p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>; <ul>; <li><a href=""https://github.com/SashaKoro"">Sasha Korotkov</a></li>; <li><a href=""https://github.com/timemachine3030"">Daniel Lopretto</a></li>; <li><a href=""https://github.com/MikeBishop"">Mike Bishop</a></li>; <li><a href=""https://github.com/DigitalBrainJS"">Dmitriy Mozgovoy</a></li>; <li><a href=""https://git",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:8839,Availability,error,errors,8839,"ect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatical errors (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2642"">#2642</a>)</li>; <li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status bad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:8955,Availability,error,error,8955,"/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatical errors (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2642"">#2642</a>)</li>; <li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with J",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:9930,Availability,error,errors,9930,"<li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2637"">#2637</a>)</li>; <li>Adding new axios documentation website link (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3681"">#3681</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3707"">#3707</a>)</li>; <li>Updating documentation around dispatching requests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3772"">#3772</a>)</li>; <li>Adding documentation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with JSON documentation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3936"">#3936</a>)</li>; <li>Fixing README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3825"">#3825</a>)</li>; <li>Adding axios-multi-api to the ecosystem file (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3817"">#3817</a>)</li>; <li>Adding SECURITY.md to properly disclose security vulnerabilities (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3981"">#3981</a>)</li>; </ul>; <p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>; <ul>; <li><a href=""https://github.com/axios/axios/blob/master/mailto:jasonsaayman@gmail.com"">Jay</a></li>; <li><a href=""https://github.com/SashaKoro"">Sasha Korotkov</a></li>; <li><a href=""https://github.com/timemachine3030"">Daniel Lopretto</a></li>; <li><a href=""https://github.com/MikeBishop"">Mike Bishop</a></li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:90,Deployability,Release,Release,90,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:175,Deployability,release,releases,175,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:193,Deployability,release,releases,193,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:5012,Deployability,release,release,5012,"tation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with JSON documentation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3936"">#3936</a>)</li>; <li>Fixing README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3825"">#3825</a>)</li>; <li>Adding axios-multi-api to the ecosystem file (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3817"">#3817</a>)</li>; <li>Adding SECURITY.md to properly disclose security vulnerabilities (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3981"">#3981</a>)</li>; </ul>; <p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>; <ul>; <li><a href=""https://github.com/SashaKoro"">Sasha Korotkov</a></li>; <li><a href=""https://github.com/timemachine3030"">Daniel Lopretto</a></li>; <li><a href=""https://github.com/MikeBishop"">Mike Bishop</a></li>; <li><a href=""https://github.com/DigitalBrainJS"">Dmitriy Mozgovoy</a></li>; <li><a href=""https://github.com/bimbiltu"">Mark</a></li>; <li><a href=""https://github.com/piiih"">Philipe Gouveia Paix√£o</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/blob/master/CHANGELOG.md"">axios's changelog</a>.</em></p>; <blockquote>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Add",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:10537,Deployability,release,release,10537,"tation for the type guard isAxiosError (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3767"">#3767</a>)</li>; <li>Adding explanation of cancel token (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3803"">#3803</a>)</li>; <li>Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3953"">#3953</a>)</li>; <li>Fixing errors with JSON documentation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3936"">#3936</a>)</li>; <li>Fixing README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3825"">#3825</a>)</li>; <li>Adding axios-multi-api to the ecosystem file (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3817"">#3817</a>)</li>; <li>Adding SECURITY.md to properly disclose security vulnerabilities (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3981"">#3981</a>)</li>; </ul>; <p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>; <ul>; <li><a href=""https://github.com/axios/axios/blob/master/mailto:jasonsaayman@gmail.com"">Jay</a></li>; <li><a href=""https://github.com/SashaKoro"">Sasha Korotkov</a></li>; <li><a href=""https://github.com/timemachine3030"">Daniel Lopretto</a></li>; <li><a href=""https://github.com/MikeBishop"">Mike Bishop</a></li>; <li><a href=""https://github.com/DigitalBrainJS"">Dmitriy Mozgovoy</a></li>; <li><a href=""https://github.com/bimbiltu"">Mark</a></li>; <li><a href=""https://github.com/piiih"">Philipe Gouveia Paix√£o</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/axios/axios/commit/c0c87610911e1edebc923d0e932fea28cdfddae3""><code>c0c8761</code></a> [Updating] changelog to include links to issues and contributors</li>; <li><a href=""https://github.com/axios/axios/commit/619bb465da374bc152f58280bb64c4aae8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:12182,Deployability,Update,Update,12182,">; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/axios/axios/commit/c0c87610911e1edebc923d0e932fea28cdfddae3""><code>c0c8761</code></a> [Updating] changelog to include links to issues and contributors</li>; <li><a href=""https://github.com/axios/axios/commit/619bb465da374bc152f58280bb64c4aae8b78d4c""><code>619bb46</code></a> [Releasing] v0.21.2</li>; <li><a href=""https://github.com/axios/axios/commit/82c94555917834770bd1389fc0b4cd9ba35ec3fe""><code>82c9455</code></a> Create SECURITY.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3981"">#3981</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/5b457116e31db0e88fede6c428e969e87f290929""><code>5b45711</code></a> Security fix for ReDoS (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3980"">#3980</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/5bc9ea24dda14e74def0b8ae9cdb3fa1a0c77773""><code>5bc9ea2</code></a> Update ECOSYSTEM.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3817"">#3817</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e72813a385c32e4c3eeaeb4fcc4437dd124bbbcf""><code>e72813a</code></a> Fixing README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3818"">#3818</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e10a0270e988a641ba0f01509c4c3ba657afe5a5""><code>e10a027</code></a> Fix README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3825"">#3825</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e091491127893a476b0223ab72f788c3b30fc082""><code>e091491</code></a> Update README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3936"">#3936</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/b42fbad57b093bb7214991161c5355bd46b864d0""><code>b42fbad</code></a> Removed un-needed bracket</li>; <li><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:12883,Deployability,Update,Update,12883,"https://github.com/axios/axios/commit/5b457116e31db0e88fede6c428e969e87f290929""><code>5b45711</code></a> Security fix for ReDoS (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3980"">#3980</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/5bc9ea24dda14e74def0b8ae9cdb3fa1a0c77773""><code>5bc9ea2</code></a> Update ECOSYSTEM.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3817"">#3817</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e72813a385c32e4c3eeaeb4fcc4437dd124bbbcf""><code>e72813a</code></a> Fixing README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3818"">#3818</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e10a0270e988a641ba0f01509c4c3ba657afe5a5""><code>e10a027</code></a> Fix README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3825"">#3825</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e091491127893a476b0223ab72f788c3b30fc082""><code>e091491</code></a> Update README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3936"">#3936</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/b42fbad57b093bb7214991161c5355bd46b864d0""><code>b42fbad</code></a> Removed un-needed bracket</li>; <li><a href=""https://github.com/axios/axios/commit/520c8dccdef92cccbe51ea7cd96ad464c6401914""><code>520c8dc</code></a> Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3953"">#3953</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/axios/axios/compare/v0.21.1...v0.21.2"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.npmjs.com/~jasonsaayman"">jasonsaayman</a>, a new releaser for axios since your current version.</p>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:13676,Deployability,release,releaser,13676," href=""https://github-redirect.dependabot.com/axios/axios/issues/3825"">#3825</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e091491127893a476b0223ab72f788c3b30fc082""><code>e091491</code></a> Update README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3936"">#3936</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/b42fbad57b093bb7214991161c5355bd46b864d0""><code>b42fbad</code></a> Removed un-needed bracket</li>; <li><a href=""https://github.com/axios/axios/commit/520c8dccdef92cccbe51ea7cd96ad464c6401914""><code>520c8dc</code></a> Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3953"">#3953</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/axios/axios/compare/v0.21.1...v0.21.2"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.npmjs.com/~jasonsaayman"">jasonsaayman</a>, a new releaser for axios since your current version.</p>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=axios&package-manager=npm_and_yarn&previous-version=0.21.1&new-version=0.21.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:14040,Deployability,update,updates,14040,"m/axios/axios/commit/b42fbad57b093bb7214991161c5355bd46b864d0""><code>b42fbad</code></a> Removed un-needed bracket</li>; <li><a href=""https://github.com/axios/axios/commit/520c8dccdef92cccbe51ea7cd96ad464c6401914""><code>520c8dc</code></a> Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3953"">#3953</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/axios/axios/compare/v0.21.1...v0.21.2"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.npmjs.com/~jasonsaayman"">jasonsaayman</a>, a new releaser for axios since your current version.</p>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=axios&package-manager=npm_and_yarn&previous-version=0.21.1&new-version=0.21.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:15192,Deployability,upgrade,upgrade,15192,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:15368,Deployability,upgrade,upgrade,15368,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:15538,Deployability,upgrade,upgrade,15538,"ly by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hail-is/hail/network/alerts). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:433,Integrability,depend,dependabot,433,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:609,Integrability,depend,dependabot,609,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:732,Integrability,depend,dependabot,732,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:866,Integrability,depend,dependabot,866,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:993,Integrability,depend,dependabot,993,"Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1075,Integrability,depend,dependabot,1075," <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>; <blockquote>; <h2>v0.21.2</h2>; <h3>0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1248,Integrability,depend,dependabot,1248,"0.21.2 (September 4, 2021)</h3>; <p>Fixes and Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix fai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1375,Integrability,depend,dependabot,1375,"e promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-red",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1517,Integrability,depend,dependabot,1517," and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset fiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1654,Integrability,depend,dependabot,1654,"li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1773,Integrability,depend,dependancies,1773,".com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1820,Integrability,depend,dependabot,1820,"header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:1948,Integrability,depend,dependabot,1948,"ments (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:2073,Integrability,depend,dependabot,2073,"endabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:2155,Integrability,depend,dependabot,2155,"nd extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:2305,Integrability,depend,dependabot,2305,"ing parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:2387,Integrability,depend,dependabot,2387,"ios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatical errors (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2642"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
https://github.com/hail-is/hail/pull/11080:2582,Integrability,depend,dependabot,2582,"rity fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3712"">#3712</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3717"">#3717</a>)</li>; <li>Updating content-type header for application/json to not contain charset field, according do RFC 8259 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2154"">#2154</a>)</li>; <li>Fixing tests by bumping karma-sauce-launcher version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3813"">#3813</a>)</li>; <li>Changing testing process from Travis CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3938"">#3938</a>)</li>; </ul>; <p>Documentation:</p>; <ul>; <li>Updating documentation around the use of <code>AUTH_TOKEN</code> with multiple domain endpoints (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3539"">#3539</a>)</li>; <li>Remove duplication of item in changelog (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3523"">#3523</a>)</li>; <li>Fixing gramatical errors (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2642"">#2642</a>)</li>; <li>Fixing spelling error (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3567"">#3567</a>)</li>; <li>Moving gitpod metion (<a href=""https://github-redirect.depe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11080
